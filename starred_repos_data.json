[
  {
    "full_name": "jonclayden/ore",
    "name": "ore",
    "description": "An R interface to the Onigmo regular expression library",
    "language": "C",
    "topics": [
      "r",
      "regular-expressions",
      "regex",
      "text-analysis"
    ],
    "readme": "\n\n[![CRAN version](https://www.r-pkg.org/badges/version/ore)](https://cran.r-project.org/package=ore) [![r-universe version](https://jonclayden.r-universe.dev/badges/ore)](https://jonclayden.r-universe.dev) [![CI](https://github.com/jonclayden/ore/actions/workflows/ci.yaml/badge.svg)](https://github.com/jonclayden/ore/actions/workflows/ci.yaml) [![codecov](https://codecov.io/gh/jonclayden/ore/branch/master/graph/badge.svg?token=F3jq1JeLV8)](https://app.codecov.io/gh/jonclayden/ore) [![Dependencies](https://tinyverse.netlify.app/badge/ore)](https://cran.r-project.org/package=ore)\n\n# Oniguruma Regular Expressions (for R)\n\n**NB: If you are looking for [Oracle R Enterprise](https://docs.oracle.com/cd/E57012_01/doc.141/e56973/intro.htm), please note you're in the wrong place!**\n\nWelcome to the `ore` package for R. This package provides an alternative to R's standard functions for manipulating strings with regular expressions, based on the Oniguruma regular expression library (rather than PCRE, as in `base`). Although the regex features of the two libraries are quite similar, the R interface provided by `ore` has some notable advantages:\n\n- Regular expressions are themselves first-class objects (of class `ore`), stored with attributes containing information such as the number of parenthesised groups present within them. This means that it is not necessary to compile a particular regex more than once.\n- Search results focus around the matched substrings (including parenthesised groups), rather than the locations of matches. This saves extra work with `substr` to extract the matches themselves.\n- Performance is [substantially better](https://github.com/jonclayden/regex-performance), especially when matching against long strings.\n- Text can be easily drawn from a URL or other \"connection\", and matches early in large files can be found efficiently with incremental search.\n- Substitutions can be functions, as well as literal or back-referenced strings, and different replacemen",
    "url": "https://github.com/jonclayden/ore",
    "last_updated": "2025-03-22T11:16:55+00:00"
  },
  {
    "full_name": "rcmalli/keras-vggface",
    "name": "keras-vggface",
    "description": "VGGFace implementation with Keras Framework",
    "language": "Python",
    "topics": [
      "keras",
      "vggface",
      "tensorflow"
    ],
    "readme": "# keras-vggface [![Build Status](https://travis-ci.org/rcmalli/keras-vggface.svg?branch=master)](https://travis-ci.org/rcmalli/keras-vggface) [![PyPI Status](https://badge.fury.io/py/keras-vggface.svg)](https://badge.fury.io/py/keras-vggface) [![PyPI Status](https://pepy.tech/badge/keras-vggface)](https://pepy.tech/project/keras-vggface)\n\nOxford VGGFace  Implementation using Keras Functional Framework v2+\n\n- Models are converted from original caffe networks.\n- It supports only Tensorflow backend.\n- You can also load only feature extraction layers with VGGFace(include_top=False) initiation.\n- When you use it for the first time , weights are downloaded and stored in ~/.keras/models/vggface folder.\n- If you don't know where to start check the [blog posts](https://github.com/rcmalli/keras-vggface#projects--blog-posts) that are using this library.\n\n~~~bash\n# Most Recent One (Suggested)\npip install git+https://github.com/rcmalli/keras-vggface.git\n# Release Version\npip install keras_vggface\n~~~\n\n\n### Library Versions\n\n- Keras v2.2.4\n- Tensorflow v1.14.0\n- **Warning: Theano backend is not supported/tested for now.**\n\n### Example Usage\n\n#### Available Models\n\n```python\n\nfrom keras_vggface.vggface import VGGFace\n\n# Based on VGG16 architecture -> old paper(2015)\nvggface = VGGFace(model='vgg16') # or VGGFace() as default\n\n# Based on RESNET50 architecture -> new paper(2017)\nvggface = VGGFace(model='resnet50')\n\n# Based on SENET50 architecture -> new paper(2017)\nvggface = VGGFace(model='senet50')\n\n```\n\n\n#### Feature Extraction\n \n- Convolution Features\n\n    ```python\n    from keras.engine import  Model\n    from keras.layers import Input\n    from keras_vggface.vggface import VGGFace\n\n    # Convolution Features\n    vgg_features = VGGFace(include_top=False, input_shape=(224, 224, 3), pooling='avg') # pooling: None, avg or max\n\n    # After this point you can use your model to predict.\n    # ...\n\n    ```\n\n\n- Specific Layer Features\n\n    ```python\n    from keras.engine import  Model\n    ",
    "url": "https://github.com/rcmalli/keras-vggface",
    "last_updated": "2025-08-31T01:09:20+00:00"
  },
  {
    "full_name": "aws-samples/amazon-textract-transformer-pipeline",
    "name": "amazon-textract-transformer-pipeline",
    "description": "Post-process Amazon Textract results with Hugging Face transformer models for document understanding",
    "language": "Python",
    "topics": [
      "amazon-textract",
      "huggingface-transformers",
      "document-analysis",
      "ocr"
    ],
    "readme": "# Trainable Document Extraction with Transformer Models on Amazon SageMaker\n\nTo automate document-based business processes, we usually need to extract specific, standard data points from diverse input documents: For example, vendor and line-item details from purchase orders; customer name and date-of-birth from identity documents; or specific clauses in contracts.\n\nIn human-readable documents, **both layout and text content are important** to extract meaning: So accuracy may be disappointing when using text-only methods (like regular expressions or entity recognition models), position-only methods (like template-based models), or manual combinations of the two.\n\nThis sample and accompanying [blog post](https://aws.amazon.com/blogs/machine-learning/bring-structure-to-diverse-documents-with-amazon-textract-and-transformer-based-models-on-amazon-sagemaker/) demonstrate **trainable, multi-modal, layout-aware document understanding** on AWS using [Amazon SageMaker](https://aws.amazon.com/sagemaker/) and open-source models from [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - optionally integrated with [Amazon Textract](https://docs.aws.amazon.com/textract/latest/dg/what-is.html).\n\n\n## Solution Overview\n\nThis sample sets up a **document processing pipeline** orchestrated by [AWS Step Functions](https://aws.amazon.com/step-functions/), as shown below:\n\n![](img/architecture-overview.png \"Architecture overview diagram\")\n\nDocuments uploaded to the input bucket automatically trigger the workflow, which:\n\n1. Extracts document data using Amazon Textract (or an alternative OCR engine).\n2. Enriches the Textract/OCR JSON with extra insights using an ML model deployed in SageMaker.\n3. Consolidates the business-level fields in a post-processing Lambda function.\n4. (If any expected fields were missing or low-confidence), forwards the results to a human reviewer.\n\nIn the provided example, input documents are specimen **credit card agreements** per **[this d",
    "url": "https://github.com/aws-samples/amazon-textract-transformer-pipeline",
    "last_updated": "2025-08-25T22:04:56+00:00"
  },
  {
    "full_name": "chaimain/asgardpy",
    "name": "asgardpy",
    "description": "Analysis Software for GAmma-Ray Data in Python",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "Agardpy: Analysis Software for GAmma-Ray Data in Python\n=======================================================\n\n[![Build Status](https://github.com/chaimain/asgardpy/actions/workflows/main.yml/badge.svg?branch=main)](https://github.com/chaimain/asgardpy/actions?query=branch%3Amain) [![codecov](https://codecov.io/gh/chaimain/asgardpy/branch/main/graph/badge.svg?token=0XEI9W8AKJ)](https://codecov.io/gh/chaimain/asgardpy) [![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/chaimain/asgardpy/badges/quality-score.png?b=main)](https://scrutinizer-ci.com/g/chaimain/asgardpy/?branch=main) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8106369.svg)](https://doi.org/10.5281/zenodo.8106369) ![PyPI](https://img.shields.io/pypi/v/asgardpy?label=pypi%20asgardpy) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7699/badge)](https://bestpractices.coreinfrastructure.org/projects/7699) [![gammapy](https://img.shields.io/badge/powered%20by-gammapy-orange.svg?style=flat)](https://www.gammapy.org/) [![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](https://www.astropy.org/)\n\n'User-friendly' configuration-centred pipeline built over [Gammapy](https://github.com/gammapy/gammapy) to allow for easy simultaneous analysis of various datasets of different formats.\nExample: 3D Fermi-LAT (with various source models in the Region of Interest stored in XML file) + 1D energy-dependent directional cuts MAGIC/LST [``PointSkyRegion`` geometry for ``ON`` region] + 1D global directional cut VERITAS [``CircleSkyRegion`` geometry for ``ON`` region].\n\nFollow the documentation at https://asgardpy.readthedocs.io/en/latest/ for the main functionality of this pipeline.\nFollow the [Gammapy v1.3](https://docs.gammapy.org/1.3/) documentation for understanding the core Gammapy objects.\n\nCheck this [documentation page](https://asgardpy.readthedocs.io/en/latest/need_for_asgardpy.html) for seeing the list of reasons to use Asgardpy over Gam",
    "url": "https://github.com/chaimain/asgardpy",
    "last_updated": "2025-03-31T16:34:22+00:00"
  },
  {
    "full_name": "meh/ruby-tesseract-ocr",
    "name": "ruby-tesseract-ocr",
    "description": "A Ruby wrapper library to the tesseract-ocr API.",
    "language": "Ruby",
    "topics": [
      "rubynlp",
      "tesseract-ocr",
      "ruby",
      "wrapper"
    ],
    "readme": "ruby-tesseract - Ruby bindings and wrapper\n==========================================\nThis wrapper binds the TessBaseAPI object through ffi-inline (which means it\nwill work on JRuby too) and then proceeds to wrap said API in a more ruby-esque\nEngine class.\n\nMaking it work\n--------------\nTo make this library work you need tesseract-ocr and leptonica libraries and\nheaders and a C++ compiler.\n\nThe gem is called `tesseract-ocr`.\n\nIf you're on a distribution that separates the libraries from headers, remember\nto install the *-dev* package.\n\nOn Debian you will need to install `libleptonica-dev` and `libtesseract-dev`.\n\nExamples\n--------\nFollowing are some examples that show the functionalities provided by\ntesseract-ocr.\n\n### Basic functionality of tesseract\n\n```ruby\nrequire 'tesseract'\n\ne = Tesseract::Engine.new {|e|\n  e.language  = :eng\n  e.blacklist = '|'\n}\n\ne.text_for('test/first.png').strip # => 'ABC'\n```\n\nYou can pass to `#text_for` either a path, an IO object, a string containing\nthe image or an object that responds to `#to_blob` (for example\nMagick::Image), keep in mind that the format has to be supported by leptonica.\n\n### Accessing advanced features\n\nWith advanced features you get access to blocks, paragraphs, lines, words and\nsymbols.\n\nReplace **level** in method names with either `block`, `paragraph`, `line`,\n`word` or `symbol`.\n\nThe following kind of accessors need a block to be passed and they pass to the\nblock each `Element` object. The Element object has various getters to access\ncertain features, I'll talk about them later.\n\nThe methods are:\n\n* `each_level`\n* `each_level_for`\n* `each_level_at`\n\nThe following accessors instead return an `Array` of `Element`s with cached\ngetters, the getters are cached beacause the values accessible in the `Element`\nare linked to the state of the internal API, and that state changes if you\naccess something else.\n\nThe methods are:\n\n*\t`levels`\n*\t`levels_for`\n*\t`levels_at`\n\nAgain, to `*_for` methods you can pass what you can pa",
    "url": "https://github.com/meh/ruby-tesseract-ocr",
    "last_updated": "2025-08-18T23:07:04+00:00"
  },
  {
    "full_name": "ContentMine/getpapers",
    "name": "getpapers",
    "description": "Get metadata, fulltexts or fulltext URLs of papers matching a search query",
    "language": "JavaScript",
    "topics": [],
    "readme": "# getpapers [![NPM version](https://img.shields.io/npm/v/getpapers.svg)][npm] [![license MIT](https://img.shields.io/github/license/contentmine/getpapers.svg)][license] [![Downloads](http://img.shields.io/npm/dm/getpapers.svg)][downloads]\n\n[npm]: https://www.npmjs.com/package/getpapers\n[license]: https://github.com/ContentMine/getpapers/blob/master/LICENSE\n[downloads]: https://nodei.co/npm/getpapers\nGet metadata, fulltexts or fulltext URLs of papers matching a search query using any of the following APIs:\n\n - EuropePMC\n - IEEE\n - ArXiv\n - Crossref (metadata, no fulltext)\n\ngetpapers can fetch article metadata, fulltexts (PDF or XML), and supplementary materials. It's designed for use in content mining, but you may find it useful for quickly acquiring large numbers of papers for reading, or for bibliometrics.\n\n## Installation\n\n### Installing nodeJS\n\nPlease follow [these cross-platform instructions](https://github.com/blahah/installing-node-tools)\n\n### Installing getpapers\n\n```bash\n$ npm install --global getpapers\n```\n\n## Usage\n\nUse `getpapers --help` to see the command-line help:\n\n```\n    -h, --help                output usage information\n    -V, --version             output the version number\n    -q, --query <query>       search query (required)\n    -o, --outdir <path>       output directory (required - will be created if not found)\n    --api <name>              API to search [eupmc, crossref, ieee, arxiv] (default: eupmc)\n    -x, --xml                 download fulltext XMLs if available\n    -p, --pdf                 download fulltext PDFs if available\n    -s, --supp                download supplementary files if available\n    -t, --minedterms          download text-mined terms if available\n    -l, --loglevel <level>    amount of information to log (silent, verbose, info*, data, warn, error, or debug)\n    -a, --all                 search all papers, not just open access\n    -n, --noexecute           report how many results match the query, but don't actually download",
    "url": "https://github.com/ContentMine/getpapers",
    "last_updated": "2025-06-15T22:50:02+00:00"
  },
  {
    "full_name": "microsoft/MLOps",
    "name": "MLOps",
    "description": "MLOps examples",
    "language": "Jupyter Notebook",
    "topics": [
      "mlops",
      "azureml"
    ],
    "readme": "---\npage_type: sample\nlanguages:\n- python\nproducts:\n- azure\n- azure-machine-learning-service\n- azure-devops\ndescription: \"MLOps end to end examples & solutions. A collection of examples showing different end to end scenarios operationalizing ML workflows with Azure Machine Learning, integrated with GitHub and other Azure services such as Data Factory and DevOps.\"\n---\n\n# Updated MLOps Guidance on Azure (2023)\nTo learn the more about the latest guidance from Microsoft about MLOps review the following links.\n\n- [Azure MLOps (v2) Solution Accelerator](https://github.com/Azure/mlops-v2)\n- [Set up MLOps with Azure DevOps](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-mlops-azureml?view=azureml-api-2&tabs=azure-shell)\n- [Set up MLOps with Github](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-mlops-github-azure-ml?view=azureml-api-2&tabs=azure-shell)\n- [Offical Microsoft Documentatation on MLOps](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment?view=azureml-api-2)\n- [Microsoft Learn cournse for intro to MLOps](https://learn.microsoft.com/en-us/training/paths/introduction-machine-learn-operations/)\n- [Microsoft Learn course for E2E MLOps](https://learn.microsoft.com/en-us/training/paths/build-first-machine-operations-workflow/)\n\n\n\n------------------------------\n\n# MLOps on Azure\n- [![Build Status](https://dev.azure.com/aidemos/MLOps/_apis/build/status/microsoft.MLOps?branchName=master)](https://dev.azure.com/aidemos/MLOps/_build/latest?definitionId=96?branchName=master)\n- [Example MLOps Release Pipeline](https://dev.azure.com/customai/DevopsForAI-AML/_release?view=all&_a=releases&definitionId=16)\n- [Official Python Azure MLOps repo](https://github.com/Microsoft/MLOpsPython)\n- [MLOps Architecture Deep Dive video](https://www.youtube.com/watch?v=nst3UAGpiBA)\n\n## What is MLOps?\nMLOps empowers data scientists and app developers to help bring ML models to production. \nMLOps enables you",
    "url": "https://github.com/microsoft/MLOps",
    "last_updated": "2025-08-26T07:29:29+00:00"
  },
  {
    "full_name": "go-ski/pbdSMITHY",
    "name": "pbdSMITHY",
    "description": "Build scripts and formulas for installing R and pbd on clusters with http://anthonydigirolamo.github.io/smithy/",
    "language": "Ruby",
    "topics": [],
    "readme": "pbdSMITHY\n=========\n\nBuild formula for installing R and pbdR packages on clusters with smithy:\nhttp://anthonydigirolamo.github.io/smithy/\n\nSmithy appears to have ceased development.\n",
    "url": "https://github.com/go-ski/pbdSMITHY",
    "last_updated": "2023-01-28T21:24:15+00:00"
  },
  {
    "full_name": "amirziai/flatten",
    "name": "flatten",
    "description": "Flatten JSON in Python",
    "language": "Python",
    "topics": [
      "python",
      "flattened-objects",
      "pandas",
      "unflatten",
      "flattens",
      "json"
    ],
    "readme": "[![Build Status](https://travis-ci.org/amirziai/flatten.svg?branch=master)](https://travis-ci.org/amirziai/flatten) [![PyPI version](https://badge.fury.io/py/flatten-json.svg)](https://badge.fury.io/py/flatten-json) [![Codacy Badge](https://api.codacy.com/project/badge/Coverage/7ae779ec4e99462f907c5afecfd5de48)](https://www.codacy.com/app/amirziai/flatten?utm_source=github.com&utm_medium=referral&utm_content=amirziai/flatten&utm_campaign=Badge_Coverage) \n\n# flatten_json\nFlattens JSON objects in Python. ```flatten_json``` flattens the hierarchy in your object which can be useful if you want to force your objects into a table.\n\n## Installation\n```bash\npip install flatten_json\n```\n\n## flatten\n\n### Usage\nLet's say you have the following object:\n```python\ndic = {\n    \"a\": 1,\n    \"b\": 2,\n    \"c\": [{\"d\": [2, 3, 4], \"e\": [{\"f\": 1, \"g\": 2}]}]\n}\n```\nwhich you want to flatten. Just apply ```flatten```:\n```python\nfrom flatten_json import flatten\nflatten(dic)\n```\n\nResults:\n```python\n{'a': 1,\n 'b': 2,\n 'c_0_d_0': 2,\n 'c_0_d_1': 3,\n 'c_0_d_2': 4,\n 'c_0_e_0_f': 1,\n 'c_0_e_0_g': 2}\n```\n\n### Usage with Pandas\nFor the following object:\n```python\ndic = [\n    {\"a\": 1, \"b\": 2, \"c\": {\"d\": 3, \"e\": 4}},\n    {\"a\": 0.5, \"c\": {\"d\": 3.2}},\n    {\"a\": 0.8, \"b\": 1.8},\n]\n```\nWe can apply `flatten` to each element in the array and then use pandas to capture the output as a dataframe:\n```python\ndic_flattened = [flatten(d) for d in dic]\n```\nwhich creates an array of flattened objects:\n```python\n[{'a': 1, 'b': 2, 'c_d': 3, 'c_e': 4},\n {'a': 0.5, 'c_d': 3.2},\n {'a': 0.8, 'b': 1.8}]\n```\nFinally you can use ```pd.DataFrame``` to capture the flattened array:\n```python\nimport pandas as pd\ndf = pd.DataFrame(dic_flattened)\n```\nThe final result as a Pandas dataframe:\n```\n\ta\tb\tc_d\tc_e\n0\t1\t2\t3\t4\n1\t0.5\tNaN\t3.2\tNaN\n2\t0.8\t1.8\tNaN\tNaN\n```\n\n### Custom separator\nBy default `_` is used to separate nested element. You can change this by passing the desired character:\n```python\nflatten({\"a\": [1]}, '|')\n```\nreturns:\n```py",
    "url": "https://github.com/amirziai/flatten",
    "last_updated": "2025-08-26T14:53:19+00:00"
  },
  {
    "full_name": "seankross/teaching-shiny",
    "name": "teaching-shiny",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "# Teaching Shiny with Knitr and Webshot\n\nThis repository is meant to illustrate a strategy for sharing R code for a Shiny\napplication while also documenting that code and showing a screenshot for the\nresulting app. For more information see\n[this blog post](http://seankross.com/2016/11/22/Teaching-Shiny-with-Knitr-and-Webshot.html).\n\n## Running this example\n\n```r\n# First make sure you have shiny, knitr, and webshot installed:\n\ninstall.packages(c(\"shiny\", \"knitr\", \"webshot\"))\n\n# Then knit teaching-shiny.Rmd:\n\nknitr::knit(\"teaching-shiny.Rmd\")\n\n# Take a look at teaching-shiny.html:\n\nbrowseURL(\"teaching-shiny.html\")\n\n# Now change something in app/ui.R and knit the document again:\n\nknitr::knit(\"teaching-shiny.Rmd\")\nbrowseURL(\"teaching-shiny.html\")\n\n# As you can see by changing just the source code for the app the resulting\n# html and screenshot of the app changed.\n```\n\nTo run the app itself:\n\n```r\nshiny::runGitHub(\"seankross/teaching-shiny\", subdir = \"app\"\n```\n\n## License\n\n[CC0](https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "url": "https://github.com/seankross/teaching-shiny",
    "last_updated": "2016-11-23T21:16:06+00:00"
  },
  {
    "full_name": "JonathanLink/PDFLayoutTextStripper",
    "name": "PDFLayoutTextStripper",
    "description": "Converts a pdf file into a text file while keeping the layout of the original pdf. Useful to extract the content from a table in a pdf file for instance. This is a subclass of PDFTextStripper class (from the Apache PDFBox library).",
    "language": "Java",
    "topics": [
      "layout",
      "text",
      "java",
      "pdf",
      "extract",
      "data-extraction",
      "pdfbox"
    ],
    "readme": "# PDFLayoutTextStripper\n\nConverts a PDF file into a text file while keeping the layout of the original PDF. Useful to extract the content from a table or a form in a PDF file. PDFLayoutTextStripper is a subclass of PDFTextStripper class (from the [Apache PDFBox](https://pdfbox.apache.org/) library).\n\n## Use cases\nData extraction from a table in a PDF file\n![example](sample.png)\n-\nData extraction from a form in a PDF file\n![example](sample2.png)\n\n## How to install\n\n### Maven\n```\n<dependency>\n  <groupId>io.github.jonathanlink</groupId>\n  <artifactId>PDFLayoutTextStripper</artifactId>\n  <version>2.2.3</version>\n</dependency>\n```\n\n### Manual\n1) Install **apache pdfbox** manually ([to get the v2.0.6 click here](https://mvnrepository.com/artifact/org.apache.pdfbox/pdfbox/2.0.6) ) and its two dependencies\ncommons-logging.jar and fontbox\n\n>**warning**: only pdfbox versions **from version 2.0.0 upwards** are compatible with this version of PDFLayoutTextStripper.java\n\n\n### How to use on Linux/Mac\n```\ncd PDFLayoutTextStripper\njavac -cp .:/pathto/pdfbox-2.0.6.jar:/pathto/commons-logging-1.2.jar:/pathto/PDFLayoutTextStripper/fontbox-2.0.6.jar *.java\njava -cp .:/pathto/pdfbox-2.0.6.jar:/pathto/commons-logging-1.2.jar:/pathto/PDFLayoutTextStripper/fontbox-2.0.6.jar test\n```\n\n### How to use on Windows\n\nThe same as for Linux (see above) but replace :  with ;\n\n## Sample code\n```\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport org.apache.pdfbox.io.RandomAccessFile;\nimport org.apache.pdfbox.pdfparser.PDFParser;\nimport org.apache.pdfbox.pdmodel.PDDocument;\nimport org.apache.pdfbox.text.PDFTextStripper;\n\npublic class test {\n\tpublic static void main(String[] args) {\n\t\tString string = null;\n        try {\n            PDFParser pdfParser = new PDFParser(new RandomAccessFile(new File(\"./samples/bus.pdf\"), \"r\"));\n            pdfParser.parse();\n            PDDocument pdDocument = new PDDocument(pdfParser.getDocument());\n            PDFTextStripper p",
    "url": "https://github.com/JonathanLink/PDFLayoutTextStripper",
    "last_updated": "2025-08-27T12:08:03+00:00"
  },
  {
    "full_name": "yemount/pose-animator",
    "name": "pose-animator",
    "description": "",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Pose Animator\n\nPose Animator takes a 2D vector illustration and animates its containing curves in real-time based on the recognition result from PoseNet and FaceMesh. It borrows the idea of skeleton-based animation from computer graphics and applies it to vector characters.\n\nThis is running in the browser in realtime using [TensorFlow.js](https://www.tensorflow.org/js). Check out more cool TF.js demos [here](https://www.tensorflow.org/js/demos).\n\n*This is not an officially supported Google product.*\n\n<img src=\"/resources/gifs/avatar-new-1.gif?raw=true\" alt=\"cameraDemo\" style=\"width: 250px;\"/>\n\n<img src=\"/resources/gifs/avatar-new-full-body.gif?raw=true\" alt=\"cameraDemo\" style=\"width: 250px;\"/>\n\nIn skeletal animation a character is represented in two parts:\n1. a surface used to draw the character, and \n1. a hierarchical set of interconnected bones used to animate the surface. \n\nIn Pose Animator, the surface is defined by the 2D vector paths in the input SVG files. For the bone structure, Pose Animator provides a predefined rig (bone hierarchy) representation, designed based on the keypoints from PoseNet and FaceMesh. This bone structure’s initial pose is specified in the input SVG file, along with the character illustration, while the real time bone positions are updated by the recognition result from ML models.\n\n<img src=\"https://firebasestorage.googleapis.com/v0/b/pose-animator-demo.appspot.com/o/ml-keypoints.png?alt=media\" style=\"width:250px;\"/>\n\n<img src=\"/resources/gifs/avatar-new-bezier-1.gif?raw=true\" alt=\"cameraDemo\" style=\"width: 250px;\"/>\n\n// TODO: Add blog post link.\nFor more details on its technical design please check out this blog post.\n\n### Demo 1: [Camera feed](https://pose-animator-demo.firebaseapp.com/camera.html)\n\nThe camera demo animates a 2D avatar in real-time from a webcam video stream.\n\n\n### Demo 2: [Static image](https://pose-animator-demo.firebaseapp.com/static_image.html)\n\nThe static image demo shows the avatar positioned from a single im",
    "url": "https://github.com/yemount/pose-animator",
    "last_updated": "2025-09-01T01:59:37+00:00"
  },
  {
    "full_name": "openinterpreter/open-interpreter",
    "name": "open-interpreter",
    "description": "A natural language interface for computers",
    "language": "Python",
    "topics": [
      "chatgpt",
      "gpt-4",
      "python",
      "interpreter",
      "javascript",
      "nodejs"
    ],
    "readme": "<h1 align=\"center\">● Open Interpreter</h1>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/Hvz9Axh84z\">\n        <img alt=\"Discord\" src=\"https://img.shields.io/discord/1146610656779440188?logo=discord&style=flat&logoColor=white\"/></a>\n    <a href=\"docs/README_JA.md\"><img src=\"https://img.shields.io/badge/ドキュメント-日本語-white.svg\" alt=\"JA doc\"/></a>\n    <a href=\"docs/README_ZH.md\"><img src=\"https://img.shields.io/badge/文档-中文版-white.svg\" alt=\"ZH doc\"/></a>\n    <a href=\"docs/README_ES.md\"> <img src=\"https://img.shields.io/badge/Español-white.svg\" alt=\"ES doc\"/></a>\n    <a href=\"docs/README_UK.md\"><img src=\"https://img.shields.io/badge/Українська-white.svg\" alt=\"UK doc\"/></a>\n    <a href=\"docs/README_IN.md\"><img src=\"https://img.shields.io/badge/Hindi-white.svg\" alt=\"IN doc\"/></a>\n    <a href=\"LICENSE\"><img src=\"https://img.shields.io/static/v1?label=license&message=AGPL&color=white&style=flat\" alt=\"License\"/></a>\n    <br>\n    <br><a href=\"https://0ggfznkwh4j.typeform.com/to/G21i9lJ2\">Get early access to the desktop app</a>‎ ‎ |‎ ‎ <a href=\"https://docs.openinterpreter.com/\">Documentation</a><br>\n</p>\n\n> [!NOTE]\n> **Open Interpreter 1.0** is almost here.\n>\n> Please help test the [development branch](https://github.com/OpenInterpreter/open-interpreter/tree/development) and share your experience in the [Discord](https://discord.gg/Hvz9Axh84z):\n> ```\n> pip install git+https://github.com/OpenInterpreter/open-interpreter.git@development\n> interpreter --help\n> ```\n\n<br>\n\n<img alt=\"local_explorer\" src=\"https://github.com/OpenInterpreter/open-interpreter/assets/63927363/d941c3b4-b5ad-4642-992c-40edf31e2e7a\">\n\n<br>\n</p>\n<br>\n\n```shell\npip install open-interpreter\n```\n\n> Not working? Read our [setup guide](https://docs.openinterpreter.com/getting-started/setup).\n\n```shell\ninterpreter\n```\n\n<br>\n\n**Open Interpreter** lets LLMs run code (Python, Javascript, Shell, and more) locally. You can chat with Open Interpreter through a ChatGPT-like interface in your terminal by running `$ int",
    "url": "https://github.com/openinterpreter/open-interpreter",
    "last_updated": "2025-09-02T09:15:24+00:00"
  },
  {
    "full_name": "avehtari/PSIS",
    "name": "PSIS",
    "description": "Pareto smoothed importance sampling (PSIS) and PSIS leave-one-out cross-validation for Python and Matlab/Octave",
    "language": "Python",
    "topics": [],
    "readme": "## Pareto smoothed importance sampling (PSIS) and PSIS leave-one-out cross-validation reference code\n\n### Introduction\n\nThese files implement Pareto smoothed importance sampling (PSIS) and\nPSIS leave-one-out cross-validation for Matlab/Octave and Python\n(Python port made by [Tuomas Sivula](https://github.com/tsivula)).\n\nThese code are not maintained and are here for historical\nreference. Instead of these, use well maintained implementations\navailable for R, Python, and Julia as listed below.\n\n### R\n\n- PSIS and PSIS-LOO are implemented in the [`loo` R package](https://github.com/stan-dev/loo), which is also available from [CRAN](https://cran.r-project.org/package=loo).\n- PSIS and all Pareto $\\hat{k}$ diagnostics are implemented in the [`posterior` R package](https://github.com/stan-dev/posterior), which is also available from [CRAN](https://cran.r-project.org/package=posterior).\n\n### Python\n\n- PSIS, PSIS-LOO, and Pareto $\\hat{k}$ diagnostics are implemented in the [`ArviZ.py` package](https://python.arviz.org/en/stable/).\n\n- In this repo \n    - 'psis.py'  - Includes the following functions in a Python (Numpy) module\n     - psislw  - Pareto smoothing of the log importance weights\n     - psisloo - Pareto smoothed importance sampling leave-one-out log predictive densities\n     - gpdfitnew - Estimate the paramaters for the Generalized Pareto Distribution\n     - gpinv - Inverse Generalised Pareto distribution function.\n     - sumlogs - Sum of vector where numbers are represented by their logarithms\n\n### Julia\n\n- PSIS, PSIS-LOO, and Pareto $\\hat{k}$ diagnostics are implemented in the [`ArviZ.jl` package](https://julia.arviz.org/ArviZ/stable/).\n\n### Matlab/Octave\n\n- In this repo \n    - 'psislw.m'  - Pareto smoothing of the log importance weights\n    - 'psisloo.m' - Pareto smoothed importance sampling leave-one-out log predictive densities\n    - 'gpdfitnew.m' - Estimate the paramaters for the Generalized Pareto Distribution\n    - 'sumlogs.m' - Sum of vector where numbers are",
    "url": "https://github.com/avehtari/PSIS",
    "last_updated": "2025-05-26T18:27:30+00:00"
  },
  {
    "full_name": "sherjilozair/char-rnn-tensorflow",
    "name": "char-rnn-tensorflow",
    "description": " Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow ",
    "language": "Python",
    "topics": [],
    "readme": "char-rnn-tensorflow\n===\n\n[![Join the chat at https://gitter.im/char-rnn-tensorflow/Lobby](https://badges.gitter.im/char-rnn-tensorflow/Lobby.svg)](https://gitter.im/char-rnn-tensorflow/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Coverage Status](https://coveralls.io/repos/github/sherjilozair/char-rnn-tensorflow/badge.svg)](https://coveralls.io/github/sherjilozair/char-rnn-tensorflow)\n[![Build Status](https://travis-ci.org/sherjilozair/char-rnn-tensorflow.svg?branch=master)](https://travis-ci.org/sherjilozair/char-rnn-tensorflow)\n\nMulti-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow.\n\nInspired from Andrej Karpathy's [char-rnn](https://github.com/karpathy/char-rnn).\n\n## Requirements\n- [Tensorflow 1.0](http://www.tensorflow.org)\n\n## Basic Usage\nTo train with default parameters on the tinyshakespeare corpus, run `python train.py`. To access all the parameters use `python train.py --help`.\n\nTo sample from a checkpointed model, `python sample.py`.\nSampling while the learning is still in progress (to check last checkpoint) works only in CPU or using another GPU.\nTo force CPU mode, use `export CUDA_VISIBLE_DEVICES=\"\"` and `unset CUDA_VISIBLE_DEVICES` afterward\n(resp. `set CUDA_VISIBLE_DEVICES=\"\"` and `set CUDA_VISIBLE_DEVICES=` on Windows).\n\nTo continue training after interruption or to run on more epochs, `python train.py --init_from=save`\n\n## Datasets\nYou can use any plain text file as input. For example you could download [The complete Sherlock Holmes](https://sherlock-holm.es/ascii/) as such:\n\n```bash\ncd data\nmkdir sherlock\ncd sherlock\nwget https://sherlock-holm.es/stories/plain-text/cnus.txt\nmv cnus.txt input.txt\n```\n\nThen start train from the top level directory using `python train.py --data_dir=./data/sherlock/`\n\nA quick tip to concatenate many small disparate `.txt` files into one large training file: `ls *.txt | xargs -L 1 cat >> input.txt`.\n\n## Tuning\n\nTuning your m",
    "url": "https://github.com/sherjilozair/char-rnn-tensorflow",
    "last_updated": "2025-08-09T15:37:38+00:00"
  },
  {
    "full_name": "dcomtois/summarytools",
    "name": "summarytools",
    "description": "R Package to Quickly and Neatly Summarize Data",
    "language": "R",
    "topics": [
      "html-report",
      "frequency-table",
      "pander",
      "descriptive-statistics",
      "r",
      "rstudio",
      "rstats",
      "pandoc",
      "rmarkdown",
      "markdown",
      "pandoc-markdown"
    ],
    "readme": "\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/summarytools)](https://CRAN.R-project.org/package=summarytools)\n[![](https://cranlogs.r-pkg.org/badges/summarytools)](http://cran.rstudio.com/web/packages/summarytools/index.html)\n[![](https://cranlogs.r-pkg.org/badges/grand-total/summarytools)](http://cran.rstudio.com/web/packages/summarytools/index.html)\n<span class=\"badge-paypal\"><a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=HMN3QJR7UMT7S&item_name=Help+scientists,+data+scientists+and+analysts+around+the+globe&currency_code=CAD&source=url\" title=\"Donate to this project using Paypal\"><img src=\"https://img.shields.io/badge/PayPal-Donate-blue\" alt=\"PayPal donate button\"/></a></span>\n\n<!-- badges: end -->\n\n⁠\n\n# Summarytools 1.1 is out!\n\n<a href=\"#latest\">Check out what’s new.</a>\n\nLove **summarytools**? Share the love, [buy me a\ncoffee!](https://buymeacoffee.com/dcomtois)\n\n<a href=\"https://buymeacoffee.com/dcomtois\"><img src=\"img/bmc_qr.png\" width=\"100\" height=\"100\"/></a>\n\n![](img/collage.png)\n\n**summarytools** is a an [*R*](https://www.r-project.org) package for\ndata cleaning, exploring, and simple reporting. The package was\ndeveloped with the following objectives in mind:\n\n- Provide a coherent set of easy-to-use descriptive functions akin to\n  those included in commercial statistical software suites such as SAS,\n  SPSS, and Stata\n- Offer flexibility in terms of output format & content  \n- Integrate well with commonly used software & tools for reporting (the\n  [RStudio](https://posit.co/products/open-source/rstudio/) IDE,\n  [Rmarkdown](https://rmarkdown.rstudio.com/),\n  [Quarto](https://quarto.org/docs/get-started/), and\n  [knitr](https://yihui.org/knitr/)) while also allowing for standalone,\n  simple report generation using any R interface\n\nOn a more personal level, I simply wish to share with the R community\nand the scientific community at large the functions I first developed\nfor myself, but realized would benef",
    "url": "https://github.com/dcomtois/summarytools",
    "last_updated": "2025-08-03T12:53:59+00:00"
  },
  {
    "full_name": "bond-anton/BDPotentiometer",
    "name": "BDPotentiometer",
    "description": "Module to operate Digital Potentiometer over SPI bus. Extends gpiozero SPIDevice class.",
    "language": "Python",
    "topics": [],
    "readme": "# BDPotentiometer\n\n![Build](https://github.com/bond-anton/BDPotentiometer/actions/workflows/python-package.yml/badge.svg)\n[![Documentation Status](https://readthedocs.org/projects/bdpotentiometer/badge/?version=latest)](https://bdpotentiometer.readthedocs.io/en/latest/?badge=latest)\n\nModule to operate Digital Potentiometer over SPI bus. Extends gpiozero SPIDev class.\n\n## Installation\n\nTo install BDPotentiometer use pip\n```shell\n$ pip install BDPotentiometer\n```\n\n## Usage\n\nJust import the correct potentiometer class, for example MCP4231, and start operating your pot.\n\n```python\nfrom BDPotentiometer.mcp4xxx import MCP4231\n\n\nmy_pot = MCP4231(r_ab=10e3, device=0)\nmy_pot.channels[0].value = 43\n```\n\nPlease see the [examples](examples) directory for more usage examples.\n\nDetailed documentation is available at https://bdpotentiometer.readthedocs.io\n\n## License\n\nBDPotentiometer is free open source software licensed under MIT License",
    "url": "https://github.com/bond-anton/BDPotentiometer",
    "last_updated": "2025-01-15T15:30:23+00:00"
  },
  {
    "full_name": "kashav/fsql",
    "name": "fsql",
    "description": "Search for files using a fun query language",
    "language": "Go",
    "topics": [
      "find",
      "golang"
    ],
    "readme": "# fsql [![Go](https://github.com/kashav/fsql/actions/workflows/go.yml/badge.svg)](https://github.com/kashav/fsql/actions/workflows/go.yml)\n\n>Search through your filesystem with SQL-esque queries.\n\n## Contents\n\n- [Demo](#demo)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Query Syntax](#query-syntax)\n- [Examples](#usage-examples)\n- [Contribute](#contribute)\n- [License](#license)\n\n## Demo\n\n[![fsql.gif](./media/fsql.gif)](https://asciinema.org/a/120534)\n\n## Installation\n\n#### Binaries\n\n[View latest release](https://github.com/kashav/fsql/releases/latest).\n\n#### Via Go\n\n```sh\n$ go get -u -v github.com/kashav/fsql/...\n$ which fsql\n$GOPATH/bin/fsql\n```\n\n#### Via Homebrew\n\n```sh\n$ brew install fsql\n$ which fsql\n/usr/local/bin/fsql\n```\n\n#### Build manually\n\n```sh\n$ git clone https://github.com/kashav/fsql.git $GOPATH/src/github.com/kashav/fsql\n$ cd $_ # $GOPATH/src/github.com/kashav/fsql\n$ make\n$ ./fsql\n```\n\n## Usage\n\nfsql expects a single query via stdin. You may also choose to use fsql in interactive mode.\n\nView the usage dialogue with the `-help` flag.\n\n```sh\n$ fsql -help\nusage: fsql [options] [query]\n  -v  print version and exit (shorthand)\n  -version\n      print version and exit\n```\n\n## Query syntax\n\nIn general, each query requires a `SELECT` clause (to specify which attributes will be shown), a `FROM` clause (to specify which directories to search), and a `WHERE` clause (to specify conditions to test against).\n\n```console\n>>> SELECT attribute, ... FROM source, ... WHERE condition;\n```\n\nYou may choose to omit the `SELECT` and `WHERE` clause.\n\nIf you're providing your query via stdin, quotes are **not** required, however you'll have to escape _reserved_ characters (e.g. `*`, `<`, `>`, etc).\n\n### Attribute\n\nCurrently supported attributes include `name`, `size`, `time`, `hash`, `mode`.\n\nUse `all` or `*` to choose all; if no attribute is provided, this is chosen by default.\n\n**Examples**:\n\nEach group features a set of equivalent clauses.\n\n```console\n>>> SELECT name,",
    "url": "https://github.com/kashav/fsql",
    "last_updated": "2025-08-27T12:12:33+00:00"
  },
  {
    "full_name": "sahitpj/India-WhatsAppFakeNews-Dataset",
    "name": "India-WhatsAppFakeNews-Dataset",
    "description": "WhatsApps related deaths News Articles along with other articles across India during that period",
    "language": "Python",
    "topics": [
      "dataset",
      "python",
      "web-scraping",
      "fake-news"
    ],
    "readme": "# India WhatsApp Fake News Dataset [![DOI](https://zenodo.org/badge/157810232.svg)](https://zenodo.org/badge/latestdoi/157810232)\n\nThe following repository consists of about !million+ News Articles scraped from the Times of India website, from late 2017 to June 2018. \n\nThe data was then checked for keyowrds which could point us to news articles which covered stories about WhatsApp fake news cases in India, which was a growing concern at that time. To be clear, this is **not a dataset for fake articles**\n\n\n### Details \n- The file `Data.csv` has the following files, with the date, place, and the keywords mentioned. \n- The `webscrapper` consists of a `scrapy` spider which can be used to extract files from the news site\n- The files `archivelist_finder.py` and `extract_csv_data.py` can be used for reference in the process\n\n\n### Labelling Data and Insights\n\nAfter textfiles were preprocessed, keywords were found in the following dataset, which were selected to find news articles which had a good probabilty of being articles **about** Fake News. These were then crosschecked to see if the stories did correspond to them. \n\nThe following data was used by the BBC in order to help generate useful insights about Fake News trends in India, what type of fake news cases were being spread and how they were being spread. \n\n\n The complete file containing all articles in the form of `.txt` files can be found at the following link. https://drive.google.com/file/d/19IbOlTO18BAXYRQoVkWfQ6paad4v9sfB/view?usp=sharing\n \n \n ### Interesting things which could be done with this dataset\n \n A few helper ideas in case you were wondering, how this dataset can be useful.\n \n 1. Understanding fake news trends (the initial idea of this project). Could probably be extended to identifying trends and interesting facts, while trying to understand contemporary articles to them\n \n 2. Understanding news headlines. If news headlines to these articles definitely portray the matter they have. \n 3. Is there a part",
    "url": "https://github.com/sahitpj/India-WhatsAppFakeNews-Dataset",
    "last_updated": "2024-05-21T12:35:18+00:00"
  },
  {
    "full_name": "rlaphoenix/ChapSnap",
    "name": "ChapSnap",
    "description": "Resync Chapters by snapping them to Scene Changes",
    "language": "Python",
    "topics": [
      "chapters",
      "python",
      "scene-detection",
      "video",
      "matroska"
    ],
    "readme": "# ChapSnap\r\n\r\nResync Chapters by snapping them to Scene Changes.\r\n\r\n## Usage\r\n\r\n```\r\nUsage: chapsnap [OPTIONS] VIDEO [CHAPTERS]\r\n\r\n  Snap Chapters to Scene Changes.\r\n\r\n  VIDEO       The video file to snap chapters to scene changes. All video formats are\r\n              supported. You may alternatively provide a directory path to process\r\n              video files in batch.\r\n  [CHAPTERS]  Optional chapters file if you want to use chapters from a file\r\n              rather than ones already muxed with the video.\r\n\r\nOptions:\r\n  -t, --threshold FLOAT  Threshold on Scene Change probability scores. The\r\n                         lower the value, the more unlikely the frame is to be\r\n                         a Scene Change. Range: 0.0 (Impossible) - 1.0\r\n                         (Definite).\r\n  -o, --offset FLOAT     Offset to apply to each Chapter. A negative offset\r\n                         may result in fewer Chapters.\r\n  --trim INTEGER         Remove n Chapters from the start of the Video. A\r\n                         negative value will remove n Chapters from the end of\r\n                         the Video. Timestamps will be offset respectively.\r\n  -nf, --no-forward      Do not try to resync Chapters forward in time.\r\n  -nb, --no-backward     Do not try to resync Chapters backward in time.\r\n  -k, --keyframes        Only sync to Scene Changes on Keyframes (I-frames).\r\n  -0, --zero             Force the first chapter to be at `00:00:00.000`, even\r\n                         after offsets and trims.\r\n  -c, --chain            Chain sync adjustments from one Chapter to the next.\r\n                         E.g., Chapter 1 had -2, so Chapter 2 will begin with\r\n                         an offset of -2. Chapter 2 with -2 has a change of\r\n                         -1, so Chapter 3 will begin with an offset of -3 and\r\n                         so on.\r\n  --overwrite            Apply new Chapters to the input video file in-place,\r\n                         without making a duplicate.\r\n  --h",
    "url": "https://github.com/rlaphoenix/ChapSnap",
    "last_updated": "2025-08-10T11:56:33+00:00"
  },
  {
    "full_name": "ckreibich/scholar.py",
    "name": "scholar.py",
    "description": "A parser for Google Scholar, written in Python",
    "language": "Python",
    "topics": [],
    "readme": "scholar.py\n==========\n\nscholar.py is a Python module that implements a querier and parser for Google Scholar's output. Its classes can be used independently, but it can also be invoked as a command-line tool.\n\nThe script used to live at http://icir.org/christian/scholar.html, and I've moved it here so I can more easily manage the various patches and suggestions I'm receiving for scholar.py. Thanks guys, for all your interest! If you'd like to get in touch, email me at christian@icir.org or ping me [on Twitter](http://twitter.com/ckreibich).\n\nCheers,<br>\nChristian\n\nFeatures\n--------\n\n* Extracts publication title, most relevant web link, PDF link, number of citations, number of online versions, link to Google Scholar's article cluster for the work, Google Scholar's cluster of all works referencing the publication, and excerpt of content.\n* Extracts total number of hits as reported by Scholar (new in version 2.5)\n* Supports the full range of advanced query options provided by Google Scholar, such as title-only search, publication date timeframes, and inclusion/exclusion of patents and citations.\n* Supports article cluster IDs, i.e., information relating to the variants of an article already identified by Google Scholar\n* Supports retrieval of citation details in standard external formats as provided by Google Scholar, including BibTeX and EndNote.\n* Command-line tool prints entries in CSV format, simple plain text, or in the citation export format.\n* Cookie support for higher query volume, including ability to persist cookies to disk across invocations.\n\nNote\n----\n\nI will always strive to add features that increase the power of this\nAPI, but I will never add features that intentionally try to work\naround the query limits imposed by Google Scholar. Please don't ask me\nto add such features.\n\nExamples\n--------\n\nTry scholar.py --help for all available options. Note, the command line arguments changed considerably in version 2.0! A few examples:\n\nRetrieve one article writte",
    "url": "https://github.com/ckreibich/scholar.py",
    "last_updated": "2025-08-30T20:46:30+00:00"
  },
  {
    "full_name": "ArthurG/TrashDash",
    "name": "TrashDash",
    "description": "Track your trash, and use that data to help you make buying decisions",
    "language": "JavaScript",
    "topics": [
      "flask",
      "google-maps-api",
      "facebook-messenger-bot",
      "food-waste",
      "gamification",
      "vuejs2"
    ],
    "readme": "# TrashDash\n\n###### I won the 2nd place hack at StartHacks 2017!!!! :D \n###### [Hackerearth URL](https://www.hackerearth.com/sprints/start-hacks-1/dashboard/TrashWatch/submission/)\n\n## Problem\n\nEvery year in North America, the average consumer throws out between 95-115 kg of waste in just food products alone. How many drinks can we buy at Phils if we each save that money instead? Too many. In addition, as students, we also purchase many products that we use for one year then throw out such as tables, microwaves, and laptops. \n\n## Vision\n\nThe problem is with purchasing food or products that we think we will eat or use, but end up throwing out. We assume that it won't happen next week/month/term/year. TrashDash uses your trash data to persuade you otherwise. TrashDash aims to minimize the amount of trash that you produce: both before AND after purchase. \nTrashDash seeks to bring Fitbit-level user tracking and gamification to the problem of waste.  \n\n\n## How it's done\n\nEvery time you dispose a piece of garbage, you take a picture of it and send it to the TrashDash Facebook Messenger Bot. The bot recognizes what the product is, and estimates the value of the product. \n\nFirstly, the bot aims to use this information change users purchase habits. At a grocery store, the bot tells you the food product categories with the most waste. Maybe you would buy less of those categories so you throw out less. Maybe you ignore those aisles completely. Looking to buy oranges but you threw out a bunch last week? TrashDash tells you to put them down. \n\nThe other reason for this problem comes us impulsively purchase products we'll never use. TrashDash aims to reduce the amount of trash by reusing whenever possible. For high value products (such as phones or laptops), the bot will help you to post it to Kijiji. Just specify the title, description, and price and wait for the responses to roll in. \n\nBoth these solutions are held together with gamification.  TrashDash also shows you the amoun",
    "url": "https://github.com/ArthurG/TrashDash",
    "last_updated": "2022-05-05T15:24:05+00:00"
  },
  {
    "full_name": "saritasa-nest/django-import-export-extensions",
    "name": "django-import-export-extensions",
    "description": "Extension for the popular `django-import-export` package",
    "language": "Python",
    "topics": [
      "django",
      "import-export",
      "python",
      "celery",
      "csv",
      "json",
      "xlsx",
      "django-import-export"
    ],
    "readme": "# Django-import-export-extensions\n\n[![PyPI - Python Versions](https://img.shields.io/pypi/pyversions/django-import-export-extensions)](https://pypi.org/project/django-import-export-extensions/)\n[![PyPI - Django Versions](https://img.shields.io/pypi/frameworkversions/django/django-import-export-extensions)](https://pypi.org/project/django-import-export-extensions/)\n![PyPI](https://img.shields.io/pypi/v/django-import-export-extensions)\n\n[![Build status on Github](https://github.com/saritasa-nest/django-import-export-extensions/actions/workflows/checks.yml/badge.svg)](https://github.com/saritasa-nest/django-import-export-extensions/actions/workflows/checks.yml)\n[![Test coverage](https://coveralls.io/repos/github/saritasa-nest/django-import-export-extensions/badge.svg?branch=main)](https://coveralls.io/github/saritasa-nest/django-import-export-extensions?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/django-import-export-extensions/badge/?version=latest)](https://django-import-export-extensions.readthedocs.io/en/latest/?badge=latest)\n\n ![PyPI Downloads](https://static.pepy.tech/badge/django-import-export-extensions/month)\n\n## Links\n\n- [Documentation](<https://django-import-export-extensions.readthedocs.io>)\n- [GitHub](<https://github.com/saritasa-nest/django-import-export-extensions>)\n- [PyPI](<https://pypi.org/project/django-import-export-extensions>)\n- [Contibuting](<https://django-import-export-extensions.readthedocs.io/en/stable/contributing.html>)\n- [History](https://django-import-export-extensions.readthedocs.io/en/stable/history.html)\n\n## Description\n\n`django-import-export-extensions` extends the functionality of\n[django-import-export](https://github.com/django-import-export/django-import-export/)\nadding the following features:\n\n- Import/export resources in the background via Celery\n- Manage import/export jobs via Django Admin\n- DRF integration that allows to work with import/export jobs via API\n- Support [drf-spectacular](https://github.c",
    "url": "https://github.com/saritasa-nest/django-import-export-extensions",
    "last_updated": "2025-09-01T01:14:51+00:00"
  },
  {
    "full_name": "fivethirtyeight/data",
    "name": "data",
    "description": "Data and code behind the articles and graphics at FiveThirtyEight",
    "language": "Jupyter Notebook",
    "topics": [
      "data"
    ],
    "readme": "![GitHub repo size](https://img.shields.io/github/repo-size/fivethirtyeight/data)\n\nSee the [index](https://github.com/fivethirtyeight/data/blob/master/index.csv) for a list of the data and code we've published and their accompanying stories.\n\nAs of June 13, 2023, sports predictions and forecasts are [no longer being updated](https://awfulannouncing.com/disney/fivethirtyeight-no-more-sports-forecasts.html). \n\nUnless otherwise noted, our data sets are available under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/), and the code is available under the [MIT License](https://opensource.org/licenses/MIT). If you find this information useful, please [let us know](mailto:contact@fivethirtyeight.com).\n",
    "url": "https://github.com/fivethirtyeight/data",
    "last_updated": "2025-09-02T08:48:35+00:00"
  },
  {
    "full_name": "ChrisMuir/refinr",
    "name": "refinr",
    "description": "Cluster and merge similar string values: an R implementation of Open Refine clustering algorithms",
    "language": "C++",
    "topics": [
      "openrefine",
      "fuzzy-matching",
      "ngram",
      "approximate-string-matching",
      "data-cleaning",
      "data-clustering",
      "clustering",
      "cran",
      "r",
      "rstats"
    ],
    "readme": "refinr\n======\n\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/ChrisMuir/refinr?branch=master&svg=true)](https://ci.appveyor.com/project/ChrisMuir/refinr)\n[![Coverage Status](https://img.shields.io/codecov/c/github/ChrisMuir/refinr/master.svg)](https://app.codecov.io/gh/ChrisMuir/refinr)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/refinr)](https://cran.r-project.org/package=refinr)\n\n\nrefinr is designed to cluster and merge similar values within a character vector. It features two functions that are implementations of clustering algorithms from the open source software [OpenRefine](https://openrefine.org/). The cluster methods used are key collision and ngram fingerprint (more info on these [here](https://openrefine.org/docs/technical-reference/clustering-in-depth)).\n\nIn addition, there are a few add-on features included, to make the clustering/merging functions more useful. These include approximate string matching to allow for merging despite minor mispellings, the option to pass a dictionary vector to dictate edit values, and the option to pass a vector of strings to ignore during the clustering process.\n\nPlease [report](https://github.com/ChrisMuir/refinr/issues) issues, comments, or feature requests.\n\nInstallation\n------------\n\nInstall from CRAN:\n\n``` r\ninstall.packages(\"refinr\")\n```\n\nOr install the dev version from this repo:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"ChrisMuir/refinr\")\n```\n\nExample Usage\n-------------\n```r\nlibrary(refinr)\n```\n\n```r\nx <- c(\"Acme Pizza, Inc.\", \"ACME PIZZA COMPANY\", \"acme pizza LLC\", \"Acme Pizza, Inc.\")\nkey_collision_merge(x)\n#> [1] \"Acme Pizza, Inc.\" \"Acme Pizza, Inc.\" \"Acme Pizza, Inc.\" \"Acme Pizza, Inc.\"\n```\n\nA dictionary character vector can be passed to `key_collision_merge`, which will dictate merge values when a cluster has a match within the dict vector.\n```r\nx <- c(\"Acme Pizza, Inc.\", \"ACME PIZZA COMPANY\", \"acme pizza LLC\", \"Acme Pizza, Inc.\")\nkey_collision",
    "url": "https://github.com/ChrisMuir/refinr",
    "last_updated": "2024-08-26T18:58:00+00:00"
  },
  {
    "full_name": "FDA/openfda",
    "name": "openfda",
    "description": "openFDA is an FDA project to provide open APIs, raw data downloads, documentation and examples, and a developer community for an important collection of FDA public datasets.",
    "language": "Python",
    "topics": [],
    "readme": "openFDA\n=======\n\nopenFDA is a research project to provide open APIs, raw data downloads, documentation and examples, and a developer community for an important collection of FDA public datasets.\n\n*Please note: Do not rely on openFDA to make decisions regarding medical care. Always speak to your health provider about the risks and benefits of FDA-regulated products. We may limit or otherwise restrict your access to the API in line with our [Terms of Service](https://open.fda.gov/terms/).*\n\n# Contents\n\nThis repository contains the code which powers all of the `api.fda.gov` end points:\n\n* Python pipelines written with [Luigi](https://github.com/spotify/luigi) for processing public FDA data sets (drugs, foods, medical devices, and other) into a JSON format that can be loaded into Elasticsearch.\n\n* [Elasticsearch](http://www.elasticsearch.org/) schemas for the available data sets.\n\n* A [Node.js](https://github.com/joyent/node) API Server written with [Express](http://expressjs.com/), [Elasticsearch.js](http://www.elasticsearch.org/guide/en/elasticsearch/client/javascript-api/current/) and [Elastic.js](http://www.fullscale.co/elasticjs/) that communicates with Elasticsearch and provides the `api.fda.gov` JSON interface (documented in detail at https://open.fda.gov).\n\n# Prerequisites\n\n* Elasticsearch 7\n* Python 3.6 or above\n* Node 14 or above\n\n# Packaging\n\nRun `bootstrap.sh` to download and set up a virtualenv for the `openfda` python package and to download and set up the `openfda-api` node package.\n\n# Running in Docker\n\nIf you intend to try running openFDA yourself, we have put together a `docker-compose.yml` configuration\n that can help you get started. `docker-compose up` will:\n1. Start an [Elasticsearch](http://www.elasticsearch.org/) container\n2. Start an API container, which will expose port `8000` for queries.\n3. Start a Python 3 container that will run the NSDE, CAERS, Substance Data, Device Clearance, Device PMA and Device Event pipelines and\ncreate corresponding",
    "url": "https://github.com/FDA/openfda",
    "last_updated": "2025-08-29T17:51:31+00:00"
  },
  {
    "full_name": "jonocarroll/ggeasy",
    "name": "ggeasy",
    "description": "ggplot2 shortcuts (transformations made easy)",
    "language": "HTML",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# ggeasy <img src='man/figures/logo.gif' align=\"right\" height=\"138\" />\n\n<!-- http://www.clker.com/clipart-2-puzzle-pieces-connected.html -->\n\n[![Covrpage\nSummary](https://img.shields.io/badge/covrpage-Last_Build_2025_06_15-yellowgreen.svg)](https://github.com/jonocarroll/ggeasy/blob/master/tests/README.md)\n[![AppVeyor build\nstatus](https://ci.appveyor.com/api/projects/status/github/jonocarroll/ggeasy?branch=master&svg=true)](https://ci.appveyor.com/project/jonocarroll/ggeasy)\n[![R-CMD-check](https://github.com/jonocarroll/ggeasy/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/jonocarroll/ggeasy/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/jonocarroll/ggeasy/branch/master/graph/badge.svg)](https://app.codecov.io/gh/jonocarroll/ggeasy?branch=master)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/ggeasy)](https://CRAN.R-project.org/package=ggeasy)\n\nYou know how to make `ggplot2` graphics, right? No worries. Piece of\ncake.\n\nNow, can you please rotate the `x` axis labels to vertical?\n\n![](https://raw.githubusercontent.com/jonocarroll/ggeasy/master/inst/media/xkcd.png)\n<!-- ![](https://raw.githubusercontent.com/jonocarroll/ggeasy/master/inst/media/winona.gif) -->\n<!-- ![](https://raw.githubusercontent.com/jonocarroll/ggeasy/master/inst/media/sherlock.gif) -->\n\n`ggeasy` is here to make that a little easier.\n\n## Installation\n\nYou can install the latest released version of `ggeasy` from CRAN with:\n\n``` r\ninstall.packages(\"ggeasy\")\n```\n\nor the bleeding-edge development version from GitHub with\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"jonocarroll/ggeasy\")\n```\n\n## Reference\n\nSee the [`pkgdown` site](https://jonocarroll.github.io/ggeasy/).\n\n[@amrrs](https://github.com/amrrs) a.k.a.\n[@1littlecoder](https://twitter.com/1littlecoder) has produced a video\nwalkthrough using `ggeasy` which covers some of the major features:\n\n[![Wa",
    "url": "https://github.com/jonocarroll/ggeasy",
    "last_updated": "2025-08-13T20:09:30+00:00"
  },
  {
    "full_name": "void-echo/echo_logger",
    "name": "echo_logger",
    "description": "A simple logger for python, with color and time.",
    "language": "Python",
    "topics": [
      "logger",
      "monitoring-automation",
      "pip",
      "python"
    ],
    "readme": "# Echo Logger\n\npip repo is [here](https://pypi.org/project/echo-logger/).\n\nThis is a simple echo logger for informative, warning and error messages.\n\nTime and colored printing are supported.\n\nThis repository is mostly for my own use, but feel free to use it if you want.\n\n## Usage\n\n```python\n\n# build (before you do this, don't forget to change the version id in setup.py)\npython setup.py sdist bdist_wheel\n\n# upload (before you do this, please rmv the outdated files of previous versions in dist/)\ntwine upload dist/*\n\n# install\npip install echo-logger\n\n# upgrade\npip install echo-logger --upgrade\n```\n\n\n\n## Demo\n\nSimple usage:\n\n```python\nprint_info(\"Hello World!\")\nprint_warn(\"Hello World!\")\nprint_err(\"Hello World!\")\n```\n\n<img src=\"README.assets/image-20230529154733777.png\" alt=\"image-20230529154733777\" style=\"zoom: 67%;\" />\n\nIf you want to print with time:\n\n```python\nprint_info(\"Hello World!\", with_time=True)\nprint_warn(\"Hello World!\", with_time=True)\nprint_err(\"Hello World!\", with_time=True)\n```\n\n<img src=\"README.assets/image-20230529155434474.png\" alt=\"image-20230529155434474\" style=\"zoom:67%;\" />\n\nIf you want nothing to be printed (Let's say, you finished unit testing and want to run the whole program without any logger output):\n\n```python\necho_logger.echo_logger_debug = False  # disable any logger functions from echo_logger\nprint_info(\"Hello World!\")\nprint_warn(\"Hello World!\")\nprint_err(\"Hello World!\")\n# No output at all.\n```\n\nJust so simple and straightforward. \n",
    "url": "https://github.com/void-echo/echo_logger",
    "last_updated": "2025-05-21T04:49:34+00:00"
  },
  {
    "full_name": "nanxstats/MEF",
    "name": "MEF",
    "description": "Datasets and source code for reproducing the paper 'Integrating multiple evidence sources to predict adverse drug reactions based on systems pharmacology model'.",
    "language": "R",
    "topics": [
      "systems-pharmacology",
      "dataset",
      "collaborative-filtering",
      "machine-learning",
      "data-fusion",
      "adverse-events",
      "recommender-system"
    ],
    "readme": "# Multiple Evidence Fusion <img src=\"logo.png\" align=\"right\" alt=\"logo\" height=\"180\" width=\"180\" />\n\nThis repository archives the datasets, R code, and Python code for the paper 'Integrating multiple evidence sources to predict adverse drug reactions based on systems pharmacology model' <[DOI:10.1002/psp4.12002](http://doi.org/10.1002/psp4.12002)> ([PDF](https://nanx.me/papers/MEF.pdf)).\n\n## Paper Citation\n\nIf you find this code useful in your research, please cite our paper:\n\nFormatted citation:\n\nCao, D‐S., N. Xiao, Y‐J. Li, W‐B. Zeng, Y‐Z. Liang, A‐P. Lu, Q‐S. Xu, and A. F. Chen. (2015). Integrating multiple evidence sources to predict adverse drug reactions based on a systems pharmacology model. _CPT: Pharmacometrics & Systems Pharmacology_, 4(9), 498-506.\n\nBibTeX entry:\n\n```\n@article{MEF2015,\n  title={Integrating multiple evidence sources to predict adverse drug reactions based on a systems pharmacology model},\n  author={Cao, D-S and Xiao, N and Li, Y-J and Zeng, W-B and Liang, Y-Z and Lu, A-P and Xu, Q-S and Chen, AF},\n  journal={CPT: Pharmacometrics \\& Systems Pharmacology},\n  volume={4},\n  number={9},\n  pages={498--506},\n  year={2015},\n  publisher={Wiley Online Library}\n}\n```\n",
    "url": "https://github.com/nanxstats/MEF",
    "last_updated": "2022-12-01T17:03:08+00:00"
  },
  {
    "full_name": "fivethirtyeight/uber-tlc-foil-response",
    "name": "uber-tlc-foil-response",
    "description": "Uber trip data from a freedom of information request to NYC's Taxi & Limousine Commission",
    "language": "",
    "topics": [],
    "readme": "### Uber TLC FOIL Response\n\nThis directory contains data on over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015. Trip-level data on 10 other for-hire vehicle (FHV) companies, as well as aggregated data for 329 FHV companies, is also included. All the files are as they were received on August 3, Sept. 15 and Sept. 22, 2015. \n\nFiveThirtyEight obtained the data from the [NYC Taxi & Limousine Commission (TLC)](http://www.nyc.gov/html/tlc/html/home/home.shtml) by submitting a Freedom of Information Law request on July 20, 2015. The TLC has sent us the data in batches as it continues to review trip data Uber and other HFV companies have submitted to it. The TLC's correspondence with FiveThirtyEight is included in the files `TLC_letter.pdf`, `TLC_letter2.pdf` and `TLC_letter3.pdf`. TLC records requests can be made [here](http://www.nyc.gov/html/tlc/html/passenger/records.shtml).\n\nThis data was used for four FiveThirtyEight stories: [Uber Is Serving New York’s Outer Boroughs More Than Taxis Are](http://fivethirtyeight.com/features/uber-is-serving-new-yorks-outer-boroughs-more-than-taxis-are/), [Public Transit Should Be Uber’s New Best Friend](http://fivethirtyeight.com/features/public-transit-should-be-ubers-new-best-friend/), [Uber Is Taking Millions Of Manhattan Rides Away From Taxis](http://fivethirtyeight.com/features/uber-is-taking-millions-of-manhattan-rides-away-from-taxis/), and [Is Uber Making NYC Rush-Hour Traffic Worse?](http://fivethirtyeight.com/features/is-uber-making-nyc-rush-hour-traffic-worse/).\n\nIn the folder `uber-trip-data`, there are six files of raw data on Uber pickups in New York City from April to September 2014. The files are separated by month and each has the following columns:\n\nHeader | Definition\n---|---------\n`Date/Time` | The date and time of the Uber pickup\n`Lat` | The latitude of the Uber pickup\n`Lon` | The longitude of the Uber pickup\n`Base` | The [TLC ba",
    "url": "https://github.com/fivethirtyeight/uber-tlc-foil-response",
    "last_updated": "2025-08-30T06:02:06+00:00"
  },
  {
    "full_name": "leeper/apsa-leeper.bst",
    "name": "apsa-leeper.bst",
    "description": "BibTeX style file for political science (adapted from apsa.bst)",
    "language": "",
    "topics": [
      "bibtex",
      "latex",
      "citations",
      "bst"
    ],
    "readme": "# apsa-leeper.bst\nBibTeX style file for political science (adapted from apsa.bst)\n",
    "url": "https://github.com/leeper/apsa-leeper.bst",
    "last_updated": "2025-01-29T20:45:57+00:00"
  },
  {
    "full_name": "rstudio/tensorflow",
    "name": "tensorflow",
    "description": "TensorFlow for R",
    "language": "R",
    "topics": [],
    "readme": "\n## TensorFlow for R\n[![R build status](https://github.com/rstudio/tensorflow/workflows/R-CMD-check/badge.svg)](https://github.com/rstudio/tensorflow/actions?workflow=R-CMD-check) [![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/tensorflow)](https://cran.r-project.org/package=tensorflow) \n\n[TensorFlow™](https://www.tensorflow.org) is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. \n\nThe [TensorFlow API](https://www.tensorflow.org/api_docs/python/tf/all_symbols) is composed of a set of Python modules that enable constructing and executing TensorFlow graphs. The tensorflow package provides access to the complete TensorFlow API from within R. \n\n## Installation\n\nTo get started, install the tensorflow R package from GitHub as follows:\n\n```r\ndevtools::install_github(\"rstudio/tensorflow\")\n```\n\nThen, use the `install_tensorflow()` function to install TensorFlow:\n\n```r\nlibrary(tensorflow)\ninstall_tensorflow()\n```\n\nYou can confirm that the installation succeeded with:\n\n```r\nhello <- tf$constant(\"Hello\")\nprint(hello)\n```\n\nThis will provide you with a default installation of TensorFlow suitable for getting started with the tensorflow R package. See the [article on installation](https://tensorflow.rstudio.com/install/) to learn about more advanced options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed.\n\n## Documentation\n\nSee the package website for additional details on using the TensorFlow API from R: <https://tensorflow.rstudio.com>\n\nSee the TensorFlow API reference for details on all of the modules, classes, and functions within the API: <https://www.te",
    "url": "https://github.com/rstudio/tensorflow",
    "last_updated": "2025-08-24T14:26:45+00:00"
  },
  {
    "full_name": "tidyverse/style",
    "name": "style",
    "description": "The tidyverse style guide for R code",
    "language": "SCSS",
    "topics": [
      "r",
      "style-guide",
      "book"
    ],
    "readme": "<!-- badges: start -->\n[![bookdown](https://github.com/tidyverse/style/actions/workflows/bookdown.yaml/badge.svg)](https://github.com/tidyverse/style/actions/workflows/bookdown.yaml)\n<!-- badges: end -->\n\nAn R Style Guide written in **bookdown** (https://github.com/rstudio/bookdown).\n",
    "url": "https://github.com/tidyverse/style",
    "last_updated": "2025-08-19T13:32:39+00:00"
  },
  {
    "full_name": "thomasp85/patchwork",
    "name": "patchwork",
    "description": "The Composer of ggplots",
    "language": "R",
    "topics": [
      "rstats",
      "ggplot2",
      "visualization",
      "ggplot-extension"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# patchwork <a href='https://patchwork.data-imaginist.com'><img src='man/figures/logo.png' align=\"right\" height=\"131.5\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/thomasp85/patchwork/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/thomasp85/patchwork/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Release_Badge](http://www.r-pkg.org/badges/version-ago/patchwork)](https://CRAN.R-project.org/package=patchwork)\n[![CRAN_Download_Badge](http://cranlogs.r-pkg.org/badges/patchwork)](https://CRAN.R-project.org/package=patchwork)\n[![Codecov test\ncoverage](https://codecov.io/gh/thomasp85/patchwork/branch/main/graph/badge.svg)](https://app.codecov.io/gh/thomasp85/patchwork?branch=main)\n<!-- badges: end -->\n\nThe goal of `patchwork` is to make it ridiculously simple to combine\nseparate ggplots into the same graphic. As such it tries to solve the\nsame problem as `gridExtra::grid.arrange()` and `cowplot::plot_grid` but\nusing an API that incites exploration and iteration, and scales to\narbitrarily complex layouts.\n\n## Installation\n\nYou can install patchwork from CRAN using\n`install.packages('patchwork')`. Alternatively you can grab the\ndevelopment version from github using devtools:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"thomasp85/patchwork\")\n```\n\n## Basic example\n\nThe usage of `patchwork` is simple: just add plots together!\n\n``` r\nlibrary(ggplot2)\nlibrary(patchwork)\n\np1 <- ggplot(mtcars) + geom_point(aes(mpg, disp))\np2 <- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))\n\np1 + p2\n```\n\n![](man/figures/README-example-1.png)<!-- -->\n\npatchwork provides rich support for arbitrarily complex layouts with\nfull alignment. As an example, check out this very readable code for\nnesting three plots on top of a third:\n\n``` r\np3 <- ggplot(mtcars) + geom_smooth(aes(disp, qsec))\np4 <- ggplot(mtcars) + geom_bar(aes(carb))\n\n(p1 | p2 | p3) /\n      p4\n```\n\n",
    "url": "https://github.com/thomasp85/patchwork",
    "last_updated": "2025-08-28T20:35:07+00:00"
  },
  {
    "full_name": "gaborcsardi/argufy",
    "name": "argufy",
    "description": "Declarative function argument checks",
    "language": "R",
    "topics": [],
    "readme": "<!-- -*- mode: markdown -*- -->\n\n\n\n# argufy\n\n> Declarative Argument Checks\n\n[![Project Status: WIP - Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](http://www.repostatus.org/badges/latest/wip.svg)](http://www.repostatus.org/#wip)\n[![Linux Build Status](https://travis-ci.org/gaborcsardi/argufy.svg?branch=master)](https://travis-ci.org/gaborcsardi/argufy)\n[![Windows Build status](https://ci.appveyor.com/api/projects/status/github/gaborcsardi/argufy?svg=true)](https://ci.appveyor.com/project/gaborcsardi/argufy)\n[![](http://www.r-pkg.org/badges/version/argufy)](http://www.r-pkg.org/pkg/argufy)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/argufy)](http://www.r-pkg.org/pkg/argufy)\n[![Coverage Status](https://img.shields.io/codecov/c/github/gaborcsardi/argufy/master.svg)](https://codecov.io/github/gaborcsardi/argufy?branch=master)\n\n\nDeclare your functions with argument checks, and `argufy` generates\nand inserts the checking code for you.\n\n---\n\n  - [Installation](#installation)\n  - [Usage](#usage)\n    - [Introduction](#introduction)\n    - [Assertions](#assertions)\n\t- [Internal functions](#internal-functions)\n    - [Coercions](#coercions)\n    - [More concise assertions with `.`](#more-concise-assertions-with-)\n    - [Assertions involving multiple arguments](#assertions-involving-multiple-arguments)\n\t- [Reuse assertions for multiple functions](#reuse-assertions-for-multiple-functions)\n\t- [Without Roxygen](#without-roxygen)\n  - [Frequently asked questions](#frequently-asked-questions)\n  - [License](#license)\n\n## Installation\n\n\n```r\ndevtools::install_github(\"gaborcsardi/argufy\")\n```\n\n## Usage\n\n### Introduction\n\nTo use `argufy` in your R package, you need to import and call the\n`argufy_me` function. Once you called `argufy_me`, you can add\nassertions and coercions to your Roxygen headers, and these will be picked\nup automatically at installation time. You also need to add `argufy`\nto the `Rd",
    "url": "https://github.com/gaborcsardi/argufy",
    "last_updated": "2024-08-27T13:46:37+00:00"
  },
  {
    "full_name": "wikimedia-research/Discovery-Research-UserSatisfaction",
    "name": "Discovery-Research-UserSatisfaction",
    "description": "Research around automatically measuring user satisfaction with MediaWiki search",
    "language": "R",
    "topics": [],
    "readme": "# User Satisfaction\n\nResearch around automatically measuring user satisfaction with MediaWiki search. For more information, see the [corresponding article on Meta](https://meta.wikimedia.org/wiki/Research:Measuring_User_Search_Satisfaction).\n\n## Primary Investigators\n\n* Oliver Keyes ([okeyes@wikimedia.org](mailto:okeyes@wikimedia.org))\n* Mikhail Popov ([mpopov@wikimedia.org](mailto:mpopov@wikimedia.org))\n",
    "url": "https://github.com/wikimedia-research/Discovery-Research-UserSatisfaction",
    "last_updated": "2017-10-23T23:16:13+00:00"
  },
  {
    "full_name": "ropensci/git2r",
    "name": "git2r",
    "description": "R bindings to the libgit2 library",
    "language": "R",
    "topics": [
      "libgit2",
      "libgit2-library",
      "git",
      "git-client",
      "r",
      "rstats",
      "r-package"
    ],
    "readme": "[![R-CMD-check](https://github.com/ropensci/git2r/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ropensci/git2r/actions/workflows/R-CMD-check.yaml)\n[![CRAN status](https://www.r-pkg.org/badges/version/git2r)](https://cran.r-project.org/package=git2r)\n[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/last-month/git2r)](https://cran.r-project.org/package=git2r)\n[![Coverage Status](https://coveralls.io/repos/github/ropensci/git2r/badge.svg?branch=master)](https://coveralls.io/github/ropensci/git2r?branch=master)\n\n# Introduction\n\nThe `git2r` package gives you programmatic access to Git repositories\nfrom R. Internally the package uses the libgit2 library which is a\npure C implementation of the Git core methods. For more information\nabout libgit2, check out libgit2's website\n[(http://libgit2.github.com)](http://libgit2.github.com).\n\nSuggestions, bugs, forks and pull requests are appreciated. Get in\ntouch.\n\n## Installation\n\nTo install the version available on CRAN:\n\n```coffee\ninstall.packages(\"git2r\")\n```\n\nTo install the development version of `git2r`, it's easiest to use the\ndevtools package:\n\n```coffee\n# install.packages(\"remotes\")\nlibrary(remotes)\ninstall_github(\"ropensci/git2r\")\n```\n\n## Usage\n\n### Repository\n\nThe central object in the `git2r` package is the S3 class\n`git_repository`. The following three methods can instantiate a\nrepository; `init`, `repository` and `clone`.\n\n#### Create a new repository\n\nCreate a new repository in a temporary directory using `init`\n\n```coffee\nlibrary(git2r)\n```\n\n```\n#> Loading required package: methods\n```\n\n```coffee\n\n## Create a temporary directory to hold the repository\npath <- tempfile(pattern=\"git2r-\")\ndir.create(path)\n\n## Initialize the repository\nrepo <- init(path)\n\n## Display a brief summary of the new repository\nrepo\n```\n\n```\n#> Local:    /tmp/Rtmp7CXPlx/git2r-1ae2305c0e8d/\n#> Head:     nothing commited (yet)\n```\n\n```coffee\n\n## Check if repository is bare\nis_bare(repo)\n```\n\n```\n#> [1] FALSE\n",
    "url": "https://github.com/ropensci/git2r",
    "last_updated": "2025-07-30T22:59:56+00:00"
  },
  {
    "full_name": "rpremraj/mailR",
    "name": "mailR",
    "description": "A utility to send emails from the R programming environment",
    "language": "R",
    "topics": [],
    "readme": "[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/infuser)](http://cran.r-project.org/web/packages/mailR)\n[![Downloads](http://cranlogs.r-pkg.org/badges/infuser)](http://cran.rstudio.com/package=mailR)\n\nOverview\n========\nmailR allows users to send emails from R.\n\nIt is developed as a wrapper around [Apache Commons Email](http://commons.apache.org/proper/commons-email/) and offers several features to send emails from R such as:\n- using authentication-based SMTP servers\n- sending emails to multiple recipients (including the use of Cc, Bcc, and ReplyTo recipients)\n- attaching multiple files from the file system or from URLs\n- sending HTML formatted emails with inline images\n\nWhat's new in version 0.6\n-------------------------\n**18th January 2016**\n\n*Enhancements*\n- Refinement to the stripped down text version of HTML emails for incompatible clients (thanks to @dtenenba)\n- Ability to add email headers by passing a named list `headers` (thanks to @dtenenba)\n\nInstallation instructions\n=========================\nYou can install the latest development version of mailR using devtools:\n\n```R\ninstall.packages(\"devtools\", dep = T)\nlibrary(devtools)\ninstall_github(\"rpremraj/mailR\")\n\nlibrary(mailR)\n```\n\nThe latest release of mailR is available on [CRAN](http://cran.r-project.org/web/packages/mailR/):\n\n```R\ninstall.packages(\"mailR\", dep = T)\n\nlibrary(mailR)\n```\n\nUsage\n=====\nTo send an email via a SMTP server that does not require authentication:\n\n```R\nsend.mail(from = \"sender@gmail.com\",\n          to = c(\"Recipient 1 <recipient1@gmail.com>\", \"recipient2@gmail.com\"),\n          cc = c(\"CC Recipient <cc.recipient@gmail.com>\"),\n          bcc = c(\"BCC Recipient <bcc.recipient@gmail.com>\"),\n          subject = \"Subject of the email\",\n          body = \"Body of the email\",\n          smtp = list(host.name = \"aspmx.l.google.com\", port = 25),\n          authenticate = FALSE,\n          send = TRUE)\n```\n\n*Note that aspmx.l.google.com works for gmail recipients only. Check your gmail sp",
    "url": "https://github.com/rpremraj/mailR",
    "last_updated": "2025-07-17T23:23:47+00:00"
  },
  {
    "full_name": "not-the-fish/TSR---Test-Stats-Replication",
    "name": "TSR---Test-Stats-Replication",
    "description": "Test Statistics Based Research Quality Metrics and Replication in Psychology",
    "language": "R",
    "topics": [],
    "readme": "# TSR---Test-Stats-Replication\nCode and data for my SPSP 2016 poster on predicting Many Labs replication outcomes from article level metrics. \n\n[Please read this important blog post for more information about the methods, data, and analyses](http://www.erikasalomon.com/2016/01/article-level-metrics-and-many-labs-replication-outcomes/)\n\n******DRAFT VERSION*******\n\nTo generate the figures, download the whole repo and run the code in processMLdata.R.\n",
    "url": "https://github.com/not-the-fish/TSR---Test-Stats-Replication",
    "last_updated": "2019-10-21T07:25:10+00:00"
  },
  {
    "full_name": "eddelbuettel/rcpputs",
    "name": "rcpputs",
    "description": "Rcpp bindings for algorithms for unevenly spaced time series ",
    "language": "C++",
    "topics": [],
    "readme": "## RcppUTS [![Build Status](https://travis-ci.org/eddelbuettel/rcpputs.svg)](https://travis-ci.org/eddelbuettel/rcpputs) [![License](https://eddelbuettel.github.io/badges/GPL2+.svg)](http://www.gnu.org/licenses/gpl-2.0.html) \n\nRcpp bindings for UTS (Unevenly Spaced Time Series)\n\n### What is UTS?\n\nUTS is a library by [Andreas Eckner](http://eckner.com/index.html) described and made\navailable via his [research page](http://eckner.com/research.html) which also contains\na [zip archive](http://eckner.com/papers/uts_algorithms.zip). \n\n### What is RcppUTS?\n\nThis package wraps (the older library version of) UTS for use by R via\n[Rcpp](http://dirk.eddelbuettel.com/code/rcpp.html).\n\nSome examples are included, based on the example in the UTS library. Below we show the\nexponential-moving average operator, available condition on the `last`, `next` (ie \ncurrent) observation, or a linear interpolation.\n\n![EMA](https://eddelbuettel.github.io/rcpputs/figures/emaPlot.png)\\\n\nAlso available is a standard moving average with a fixed 'before' and 'after' window length.\n\n![SMA](https://eddelbuettel.github.io/rcpputs/figures/smaPlot.png)\\\n\nBoth figures are also produced by the respective `example()` commands associate with\nthe corresponding help files.\n\n### What else?\n\nWell once I told Andreas I was putting together a quick package, he got motivated too and\nreleased [an updated GitHUb repo for uts](https://github.com/andreas50/uts) as well\n[one for utsOperators](https://github.com/andreas50/utsOperators). You probably want to\nwatch those spaces.  His [utsOperators](https://github.com/andreas50/utsOperators) is\ncloser to what we do here with a focus on SMA, EMA and rolling operators.\n\n### Status\n\nThe package builds and checks cleanly.  \n\nMore functionality could be added, of course. Contributions are welcome.\n\n### Author\n\nDirk Eddelbuettel for the package and interface, Andreas Eckner for UTS.\n\n### License\n\nGPL (>= 2)\n\n\n",
    "url": "https://github.com/eddelbuettel/rcpputs",
    "last_updated": "2025-03-22T11:06:11+00:00"
  },
  {
    "full_name": "vosonlab/SocialMediaLab",
    "name": "SocialMediaLab",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# SocialMediaLab [![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/SocialMediaLab)](https://CRAN.R-project.org/package=SocialMediaLab) ![Downloads](https://cranlogs.r-pkg.org/badges/SocialMediaLab) ![Downloads](https://cranlogs.r-pkg.org/badges/grand-total/SocialMediaLab) [![Rdoc](http://www.rdocumentation.org/badges/version/SocialMediaLab)](http://www.rdocumentation.org/packages/SocialMediaLab)\n\nPlease note that the `SocialMediaLab` package has been renamed `vosonSML`. The `SocialMediaLab` package is no longer in active development. Please see [vosonSML on CRAN](https://cran.r-project.org/web/packages/vosonSML/index.html), and [vosonSML on github](https://github.com/vosonlab/vosonSML).\n\n## What does this package do?\n\n`SocialMediaLab` is an R package that provides a suite of tools for collecting and constructing networks from social media data. It provides easy-to-use functions for collecting data across popular platforms (Instagram, Facebook, Twitter, and YouTube) and generating different types of networks for analysis.\n\nSocialMediaLab was created by [Timothy Graham](http://uq.academia.edu/TimGraham) (who is also the maintainer of the package) and [Robert Ackland](https://researchers.anu.edu.au/researchers/ackland-rj).\n\nContributors:\n\n* [Chung-hong Chan](https://github.com/chainsawriot)\n\nThe latest 'official' version of the package can also be found on [CRAN](https://cran.r-project.org/web/packages/SocialMediaLab/index.html).\n\n### Current known issues\n\nIf you are getting the error `Error in check_twitter_oauth( )`, please find a [solution here](https://github.com/geoffjentry/twitteR/issues/90).\n\nIf you are having trouble getting data from Facebook, it is probably due to a known issue with authentication for the `Rfacebook` package, which [has a solution](https://github.com/vosonlab/SocialMediaLab/issues/28). \n\nInstagram API access is severely limited if you do not have an authorised app, which is significantly harder to obtain nowadays.\n\n### Special thank",
    "url": "https://github.com/vosonlab/SocialMediaLab",
    "last_updated": "2024-04-28T14:58:21+00:00"
  },
  {
    "full_name": "juliasilge/tidytext",
    "name": "tidytext",
    "description": "Text mining using tidy tools :sparkles::page_facing_up::sparkles:",
    "language": "R",
    "topics": [
      "text-mining",
      "r",
      "tidyverse",
      "tidy-data",
      "natural-language-processing"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tidytext: Text mining using tidy tools <img src=\"man/figures/tidytext.png\" align=\"right\" />\n\n**Authors:** [Julia Silge](https://juliasilge.com/), [David\nRobinson](http://varianceexplained.org/)<br/> **License:**\n[MIT](https://opensource.org/licenses/MIT)\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/juliasilge/tidytext/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/juliasilge/tidytext/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/tidytext)](https://cran.r-project.org/package=tidytext)\n[![Codecov test\ncoverage](https://codecov.io/gh/juliasilge/tidytext/branch/main/graph/badge.svg)](https://app.codecov.io/gh/juliasilge/tidytext?branch=main)\n[![DOI](https://zenodo.org/badge/22224/juliasilge/tidytext.svg)](https://zenodo.org/badge/latestdoi/22224/juliasilge/tidytext)\n[![JOSS](https://joss.theoj.org/papers/10.21105/joss.00037/status.svg)](https://joss.theoj.org/papers/10.21105/joss.00037)\n[![Downloads](https://cranlogs.r-pkg.org/badges/tidytext)](https://CRAN.R-project.org/package=tidytext)\n[![Total\nDownloads](https://cranlogs.r-pkg.org/badges/grand-total/tidytext?color=orange)](https://CRAN.R-project.org/package=tidytext)\n<!-- badges: end -->\n\nUsing [tidy data principles](https://doi.org/10.18637/jss.v059.i10) can\nmake many text mining tasks easier, more effective, and consistent with\ntools already in wide use. Much of the infrastructure needed for text\nmining with tidy data frames already exists in packages like\n[dplyr](https://cran.r-project.org/package=dplyr),\n[broom](https://cran.r-project.org/package=broom),\n[tidyr](https://cran.r-project.org/package=tidyr), and\n[ggplot2](https://cran.r-project.org/package=ggplot2). In this package,\nwe provide functions and supporting data sets to allow conversion of\ntext to and from tidy formats, and to switch seamlessly between tidy\ntools and existing text mining packages. Ch",
    "url": "https://github.com/juliasilge/tidytext",
    "last_updated": "2025-08-28T20:31:39+00:00"
  },
  {
    "full_name": "kbenoit/ITAUR-Short",
    "name": "ITAUR-Short",
    "description": "A Brief Introduction to Text Analysis Using R",
    "language": "HTML",
    "topics": [],
    "readme": "## Quantitative Text Analysis Using R: The Short Version\n\n[Kenneth Benoit](kbenoit@lse.ac.uk), Department of Methodology, LSE  \n[Paul Nulty](p.nulty@lse.ac.uk), Department of Methodology, LSE  \n\nPreviously presented at:  \n\n*  [GESIS Computational Social Science Winter Symposium, Köln (Cologne), 1 December 2015](http://www.gesis.org/css-wintersymposium/program/workshops-tutorials/quantitative-text-analysis-using-r/)  \n*  Princeton University, 27 October 2016  \n\n\n**Version:** 27 October 2016\n\nThis repository contains the workshop materials for a short workshop format *Quantitative to Text Analysis Using R*.  This project is supported through European Research Council grant ERC-2011-StG 283794-QUANTESS.\n\n### Instructions for using this resource ###\n\nYou have three options for downloading the course material found on this page:  \n\n1.  You can download the materials by clicking on each link.  \n\n2.  You can \"clone\" repository, using the buttons found to the right side of your browser window as you view this repository.  This is the button labelled \"Clone in Desktop\".  If you do not have a git client installed on your system, you will need to [get one here](https://git-scm.com/download/gui) and also to make sure that [git is installed](https://git-scm.com/downloads).  This is preferred, since you can refresh your clone as new content gets pushed to the course repository.  (And new material will get actively pushed to the course repository at least once per day as this course takes place.)\n\n3.  Statically, you can choose the button on the right marked \"Download zip\" which will download the entire repository as a zip file.\n\nYou can also subscribe to the repository if you have [a GitHub account](https://github.com), which will send you updates each time new changes are pushed to the repository.\n\n### Objectives\n\nThis workshop covers how to perform common text analysis and natural language processing tasks using R.  When used properly, R is a fast and powerful tool for managing",
    "url": "https://github.com/kbenoit/ITAUR-Short",
    "last_updated": "2022-03-31T17:14:04+00:00"
  },
  {
    "full_name": "trinker/hclustext",
    "name": "hclustext",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "hclustext   [![Follow](https://img.shields.io/twitter/follow/tylerrinker.svg?style=social)](https://twitter.com/intent/follow?screen_name=tylerrinker)\n============\n\n\n**Devlopment Moved to clustext Package** [**-CLICK HERE-**](https://github.com/trinker/clustext)\n\n[![Project Status: Inactive - The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](http://www.repostatus.org/badges/latest/inactive.svg)](http://www.repostatus.org/#inactive)\n[![Build\nStatus](https://travis-ci.org/trinker/hclustext.svg?branch=master)](https://travis-ci.org/trinker/hclustext)\n[![Coverage\nStatus](https://coveralls.io/repos/trinker/hclustext/badge.svg?branch=master)](https://coveralls.io/r/trinker/hclustext?branch=master)\n<a href=\"https://img.shields.io/badge/Version-0.1.1-orange.svg\"><img src=\"https://img.shields.io/badge/Version-0.1.1-orange.svg\" alt=\"Version\"/></a>\n</p>\n<img src=\"inst/hclustext_logo/r_hclustext.png\" width=\"150\" alt=\"readability Logo\">\n\n**hclustext** is a collection of optimized tools for clustering text\ndata via hierarchical clustering. There are many great R [clustering\ntools](https://cran.r-project.org/web/views/Cluster.html) to locate\ntopics within documents. I have had success with hierarchical clustering\nfor topic extraction. This package wraps many of the great R tools for\nclustering and working with sparse matrices to aide in the workflow\nassociated with topic extraction.\n\nThe general idea is that we turn the documents into a matrix of words.\nAfter this we weight the terms by importance using\n[tf-idf](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html).\nThis helps the more salient words to rise to the top. We then apply\ncosine distance measures to compare the terms (or features) of each\ndocument. Cosine distance works well with sparse matrices to produce\ndistances metrics between the documents. The hierarchical clustering is\nfit to separate the documents i",
    "url": "https://github.com/trinker/hclustext",
    "last_updated": "2021-07-24T22:57:31+00:00"
  },
  {
    "full_name": "TysonStanley/furniture",
    "name": "furniture",
    "description": "The furniture R package contains table1 for publication-ready simple and stratified descriptive statistics, tableC for publication-ready correlation matrixes, and other tables #rstats",
    "language": "R",
    "topics": [
      "exploratory-data-analysis",
      "tidyverse",
      "descriptive-statistics",
      "health",
      "cran",
      "tidy",
      "table1",
      "tables",
      "table-one"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n<!-- badges: start -->\n\n[![CRAN Status\nBadge](https://www.r-pkg.org/badges/version/furniture)](https://cran.r-project.org/package=furniture)\n![Downloads](https://cranlogs.r-pkg.org/badges/grand-total/furniture)\n[![R-CMD-check](https://github.com/TysonStanley/furniture/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/TysonStanley/furniture/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n# furniture: `v1.10.0` <img src=\"man/figures/furniture_hex_v2_full.png\" align=\"right\" width=\"40%\" height=\"40%\" />\n\nThe furniture R package contains functions to help with data\ncleaning/tidying (e.g., `washer()`, `rowmeans()`, `rowsums()`),\nexploratory data analysis and reporting (e.g., `table1()`, `tableC()`,\n`tableF()`). It currently contains eight main functions:\n\n1.  `table1()` : gives a well-formatted table for academic publication\n    of descriptive statistics. Very useful for quick analyses as well.\n    Notably, `table1()` now works with `dplyr::group_by()`.\n2.  `tableC()` : gives a well-formatted table of correlations.\n3.  `tableF()` : provides a thorough frequency table for quick checks of\n    the levels of a variable.\n4.  `washer()` : changes several values in a variable (very useful for\n    changing place holder values to missing).\n5.  `long()` : is a wrapper of `stats::reshape()`, takes the data from\n    wide to long format (long is often the tidy version of the data),\n    works well with the tidyverse, and can handle unbalanced multilevel\n    data.\n6.  `wide()` : also a wrapper of `stats::reshape()`, takes the data from\n    long to wide, and like `long()`, works well with the tidyverse and\n    can handle unbalanced multilevel data.\n7.  `rowmeans()` and `rowmeans.n()` : tidyverse friendly versions of\n    `rowMeans()`, where the `rowmeans.n()` function allows `n` number of\n    missing\n8.  `rowsums()` and `rowsums.n()` : tidyverse friendly versions of\n    `rowSums()`, where the ",
    "url": "https://github.com/TysonStanley/furniture",
    "last_updated": "2025-01-26T21:51:29+00:00"
  },
  {
    "full_name": "alan-turing-institute/CleverCSV",
    "name": "CleverCSV",
    "description": "CleverCSV is a Python package for handling messy CSV files. It provides a drop-in replacement for the builtin CSV module with improved dialect detection, and comes with a handy command line application for working with CSV files.",
    "language": "Python",
    "topics": [
      "csv",
      "csv-reader",
      "csv-format",
      "csv-parsing",
      "csv-files",
      "csv-parser",
      "csv-import",
      "csv-converter",
      "csv-reading",
      "csv-export",
      "python",
      "python3",
      "python-library",
      "data-science",
      "datascience",
      "data-mining",
      "data-analysis",
      "machine-learning"
    ],
    "readme": "<p align=\"center\">\n        <img width=\"60%\" src=\"https://raw.githubusercontent.com/alan-turing-institute/CleverCSV/eea72549195e37bd4347d87fd82bc98be2f1383d/.logo.png\">\n        <br>\n        <a href=\"https://github.com/alan-turing-institute/CleverCSV/actions\">\n                <img src=\"https://github.com/alan-turing-institute/CleverCSV/workflows/build/badge.svg\" alt=\"Github Actions Build Status\">\n        </a>\n        <a href=\"https://pypi.org/project/clevercsv/\">\n                <img src=\"https://badge.fury.io/py/clevercsv.svg\" alt=\"PyPI version\">\n        </a>\n        <a href=\"https://clevercsv.readthedocs.io/en/latest/?badge=latest\">\n                <img src=\"https://readthedocs.org/projects/clevercsv/badge/?version=latest\" alt=\"Documentation Status\">\n        </a>\n        <a href=\"https://pepy.tech/project/clevercsv\">\n                <img src=\"https://pepy.tech/badge/clevercsv\" alt=\"Downloads\">\n        </a>\n        <a href=\"https://mybinder.org/v2/gh/alan-turing-institute/CleverCSVDemo/master?filepath=CSV_dialect_detection_with_CleverCSV.ipynb\">\n                <img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Binder\">\n        </a>\n        <a href=\"https://rdcu.be/bLVur\">\n                <img src=\"https://img.shields.io/badge/DOI-10.1007%2Fs10618--019--00646--y-blue\">\n        </a>\n</p>\n\n*CleverCSV provides a drop-in replacement for the Python* ``csv`` *package \nwith improved dialect detection for messy CSV files. It also provides a handy \ncommand line tool that can standardize a messy file or generate Python code to \nimport it.*\n\n**Useful links:**\n\n- [CleverCSV on Github](https://github.com/alan-turing-institute/CleverCSV)\n- [CleverCSV on PyPI](https://pypi.org/project/clevercsv/)\n- [Documentation on ReadTheDocs](https://clevercsv.readthedocs.io/en/latest/)\n- [Demo of CleverCSV on Binder (interactive!)](https://mybinder.org/v2/gh/alan-turing-institute/CleverCSVDemo/master?filepath=CSV_dialect_detection_with_CleverCSV.ipynb)\n- [Research Paper on CSV dialect detectio",
    "url": "https://github.com/alan-turing-institute/CleverCSV",
    "last_updated": "2025-09-02T08:23:31+00:00"
  },
  {
    "full_name": "sharkdp/hyperfine",
    "name": "hyperfine",
    "description": "A command-line benchmarking tool",
    "language": "Rust",
    "topics": [
      "command-line",
      "tool",
      "benchmark",
      "rust",
      "cli",
      "terminal"
    ],
    "readme": "# hyperfine\n[![CICD](https://github.com/sharkdp/hyperfine/actions/workflows/CICD.yml/badge.svg)](https://github.com/sharkdp/hyperfine/actions/workflows/CICD.yml)\n[![Version info](https://img.shields.io/crates/v/hyperfine.svg)](https://crates.io/crates/hyperfine)\n[中文](https://github.com/chinanf-boy/hyperfine-zh)\n\nA command-line benchmarking tool.\n\n**Demo**: Benchmarking [`fd`](https://github.com/sharkdp/fd) and\n[`find`](https://www.gnu.org/software/findutils/):\n\n![hyperfine](https://i.imgur.com/z19OYxE.gif)\n\n### Sponsors\n\nA special *thank you* goes to our biggest <a href=\"doc/sponsors.md\">sponsor</a>:<br>\n\n<a href=\"https://www.warp.dev/hyperfine\">\n  <img src=\"doc/sponsors/warp-logo.png\" width=\"200\" alt=\"Warp\">\n  <br>\n  <strong>Warp, the intelligent terminal</strong>\n  <br>\n  <sub>Available on MacOS, Linux, Windows</sub>\n</a>\n\n## Features\n\n* Statistical analysis across multiple runs.\n* Support for arbitrary shell commands.\n* Constant feedback about the benchmark progress and current estimates.\n* Warmup runs can be executed before the actual benchmark.\n* Cache-clearing commands can be set up before each timing run.\n* Statistical outlier detection to detect interference from other programs and caching effects.\n* Export results to various formats: CSV, JSON, Markdown, AsciiDoc.\n* Parameterized benchmarks (e.g. vary the number of threads).\n* Cross-platform\n\n## Usage\n\n### Basic benchmarks\n\nTo run a benchmark, you can simply call `hyperfine <command>...`. The argument(s) can be any\nshell command. For example:\n```sh\nhyperfine 'sleep 0.3'\n```\n\nHyperfine will automatically determine the number of runs to perform for each command. By default,\nit will perform *at least* 10 benchmarking runs and measure for at least 3 seconds. To change this,\nyou can use the `-r`/`--runs` option:\n```sh\nhyperfine --runs 5 'sleep 0.3'\n```\n\nIf you want to compare the runtimes of different programs, you can pass multiple commands:\n```sh\nhyperfine 'hexdump file' 'xxd file'\n```\n\n### Warmup runs and pre",
    "url": "https://github.com/sharkdp/hyperfine",
    "last_updated": "2025-09-02T08:33:16+00:00"
  },
  {
    "full_name": "hadley/r4ds",
    "name": "r4ds",
    "description": "R for data science: a book",
    "language": "R",
    "topics": [
      "book",
      "bookdown",
      "data-science",
      "r"
    ],
    "readme": "# R for Data Science\n\n<!-- badges: start -->\n\n[![Render and deploy Book to Netlify](https://github.com/hadley/r4ds/actions/workflows/build_book.yaml/badge.svg)](https://github.com/hadley/r4ds/actions/workflows/build_book.yaml)\n\n<!-- badges: end -->\n\nThis repository contains the source of [R for Data Science](http://r4ds.hadley.nz) book.\nThe book is built using [Quarto](https://quarto.org/).\n\n## Images\n\n### Omnigraffle drawings\n\n-   Font: 12pt Guardian Sans Condensed / Ubuntu mono\n\n-   Export as 300 dpi png.\n\n-   Website font is 18 px = 13.5 pt, so scale dpi to match font sizes: 270 = 300 \\* 12 / 13.5.\n    (I also verified this empirically by screenshotting.)\n\n    ``` r\n    #| echo: FALSE\n    #| out.width: NULL\n    knitr::include_graphics(\"diagrams/transform.png\", dpi = 270)\n    ```\n\n### Screenshots\n\n-   Make sure you're using a light theme.\n    For small interface elements (eg. toolbars), zoom in twice.\n\n-   Screenshot with Cmd + Shift + 4.\n\n-   Don't need to set dpi:\n\n    ``` r\n    #| echo: FALSE\n    #| out.width: NULL\n    knitr::include_graphics(\"screenshots/rstudio-wg.png\")\n    ```\n\n### O'Reilly\n\nTo generate book for O'Reilly, build the book then:\n\n```{r}\n# pak::pak(\"hadley/htmlbook\")\nhtmlbook::convert_book()\n\nhtml <- list.files(\"oreilly\", pattern = \"[.]html$\", full.names = TRUE)\nfile.copy(html, \"../r-for-data-science-2e/\", overwrite = TRUE)\n\npngs <- list.files(\"oreilly\", pattern = \"[.]png$\", full.names = TRUE, recursive = TRUE)\ndest <- gsub(\"oreilly\", \"../r-for-data-science-2e/\", pngs)\nfs::dir_create(unique(dirname(dest)))\nfile.copy(pngs, dest, overwrite = TRUE)\n```\n\nThen commit and push to atlas.\n\n## Code of Conduct\n\nPlease note that r4ds uses a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html).\nBy contributing to this book, you agree to abide by its terms.\n",
    "url": "https://github.com/hadley/r4ds",
    "last_updated": "2025-09-02T07:59:43+00:00"
  },
  {
    "full_name": "anoopkunchukuttan/indic_nlp_library",
    "name": "indic_nlp_library",
    "description": "Resources and tools for Indian language Natural Language Processing",
    "language": "Python",
    "topics": [
      "python",
      "indian-languages",
      "natural-language-processing"
    ],
    "readme": "# Indic NLP Library\n\nThe goal of the Indic NLP Library is to build Python based libraries for common text processing and Natural Language Processing in Indian languages. Indian languages share a lot of similarity in terms of script, phonology, language syntax, etc. and this library is an attempt to provide a general solution to very commonly required toolsets for Indian language text.\n\nThe library provides the following functionalities:\n\n- Text Normalization\n- Script Information\n- Word Tokenization and Detokenization\n- Sentence Splitting \n- Word Segmentation\n- Syllabification\n- Script Conversion\n- Romanization\n- Indicization\n\n**Note**: _Shatanuvadak_ translation and _BrahmiNet_ transliteration APIs are no longer supported. You can use newer [IndicTrans](https://github.com/AI4Bharat/indicTrans) translation and [IndicXlit](https://github.com/AI4Bharat/IndicXlit) transliteration models we developed at [AI4Bharat](https://ai4bharat.iitm.ac.in). In fact, you can find many state-of-the-art datasets and models on the AI4Bharat homepage.   \n\nThe data resources required by the Indic NLP Library are hosted in a different repository. These resources are required for some modules. You can download from the [Indic NLP Resources](https://github.com/anoopkunchukuttan/indic_nlp_resources) project. \n\n**If you are interested in Indian language NLP resources, you should check the [Indic NLP Catalog](https://github.com/indicnlpweb/indicnlp_catalog) for pointers.**\n\n## Pre-requisites\n\n- Python 3.x \n   - (For Python 2.x version check the tag `PYTHON_2.7_FINAL_JAN_2019`. Not actively supporting Python 2.x anymore, but will try to maintain as much compatibility as possible)\n- [Indic NLP Resources](https://github.com/anoopkunchukuttan/indic_nlp_resources)\n- [Urduhack](https://github.com/urduhack/urduhack): Needed only if Urdu normalization is required. It has other dependencies like Tensorflow.\n- Other dependencies are listed in setup.py\n\n\n## Configuration\n\n- Installation from pip:\n\n    `pi",
    "url": "https://github.com/anoopkunchukuttan/indic_nlp_library",
    "last_updated": "2025-08-28T09:32:07+00:00"
  },
  {
    "full_name": "google/sentencepiece",
    "name": "sentencepiece",
    "description": "Unsupervised text tokenizer for Neural Network-based text generation.",
    "language": "C++",
    "topics": [
      "neural-machine-translation",
      "natural-language-processing",
      "word-segmentation"
    ],
    "readme": "# SentencePiece\n\n[![Build C++](https://github.com/google/sentencepiece/actions/workflows/cmake.yml/badge.svg)](https://github.com/google/sentencepiece/actions/workflows/cmake.yml)\n[![Build Wheels](https://github.com/google/sentencepiece/actions/workflows/wheel.yml/badge.svg)](https://github.com/google/sentencepiece/actions/workflows/wheel.yml)\n[![GitHub Issues](https://img.shields.io/github/issues/google/sentencepiece.svg)](https://github.com/google/sentencepiece/issues)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/sentencepiece)\n[![PyPI version](https://badge.fury.io/py/sentencepiece.svg)](https://badge.fury.io/py/sentencepiece)\n[![PyPi downloads](https://img.shields.io/pypi/dm/sentencepiece?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/sentencepiece/)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![SLSA 3](https://slsa.dev/images/gh-badge-level3.svg)](https://slsa.dev)\n\nSentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size\nis predetermined prior to the neural model training. SentencePiece implements\n**subword units** (e.g., **byte-pair-encoding (BPE)** [[Sennrich et al.](https://www.aclweb.org/anthology/P16-1162)]) and\n**unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)])\nwith the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n\n**This is not an official Google product.**\n\n## Technical highlights\n\n- **Purely data driven**: SentencePiece trains tokenization and detokenization\n  models from sentences. Pre-tokenization ([Moses tokenizer](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/",
    "url": "https://github.com/google/sentencepiece",
    "last_updated": "2025-09-02T05:23:12+00:00"
  },
  {
    "full_name": "jimhester/lookup",
    "name": "lookup",
    "description": "Lookup R full function definitions, including compiled code, S3 and S4 methods.",
    "language": "R",
    "topics": [
      "r"
    ],
    "readme": "# lookup\n\n> Lookup R function definitions, including compiled code, S3 and S4 methods\n> from packages installed locally, or from [GitHub](http://rpkg.gepuro.net/),\n> [CRAN](https://cran.r-project.org) or\n> [Bioconductor](https://www.bioconductor.org).\n\n[![Travis-CI Build Status](https://travis-ci.org/jimhester/lookup.svg?branch=master)](https://travis-ci.org/jimhester/lookup)\n[![Coverage Status](https://img.shields.io/codecov/c/github/jimhester/lookup/master.svg)](https://codecov.io/github/jimhester/lookup?branch=master)\n\n<p align=\"center\">\n  <img src=\"http://i.imgur.com/hiMtsWD.jpg\" alt=\"luke\">\n</p>\n\n## Installation\n```r\n# install.packages(\"devtools\")\ndevtools::install_github(\"jimhester/lookup\")\n```\nSee [Setup](#setup) for additional setup instructions.\n\n## Example\n\n### Normal Functions (with compiled code)\n![Imgur](http://i.imgur.com/TjyfFFU.png)\n\n### S3 generics and methods\n![Imgur](http://i.imgur.com/u4XM6NX.png)\n\n### S4 generics and methods\n![Imgur](http://i.imgur.com/kMEVDnv.png)\n\n### In RStudio IDE\n![Imgur](http://i.imgur.com/8iH3FdB.png)\n\n## Usage\n\n```r\n# Lookup a function\nlookup::lookup(body)\n#> base::body [closure] \n#> function (fun = sys.function(sys.parent())) \n#> {\n#>     if (is.character(fun)) \n#>         fun <- get(fun, mode = \"function\", envir = parent.frame())\n#>     .Internal(body(fun))\n#> }\n#> <bytecode: 0x7fa65cada988>\n#> <environment: namespace:base>\n#> // c source: src/main/builtin.c#L255-L266\n#> SEXP attribute_hidden do_body(SEXP call, SEXP op, SEXP args, SEXP rho)\n#> {\n#>     checkArity(op, args);\n#>     if (TYPEOF(CAR(args)) == CLOSXP)\n#>  return duplicate(BODY_EXPR(CAR(args)));\n#>     else {\n#>  if(!(TYPEOF(CAR(args)) == BUILTINSXP ||\n#>       TYPEOF(CAR(args)) == SPECIALSXP))\n#>      warningcall(call, _(\"argument is not a function\"));\n#>  return R_NilValue;\n#>     }\n#> }\n\n# Can also open a browser at that function's location\nlookup_browse()\n```\n\n## Setup\n\nlookup makes heavy use of the [GitHub API](https://developer.github.com/v3/),\nwhich h",
    "url": "https://github.com/jimhester/lookup",
    "last_updated": "2025-08-16T07:13:13+00:00"
  },
  {
    "full_name": "hrbrmstr/stringore",
    "name": "stringore",
    "description": "Tidy Regular Expression Operations with Extensive Character Encoding Support",
    "language": "C",
    "topics": [],
    "readme": "\n# stringore\n\nTidy Regular Expression Operations with Extensive Character Encoding\nSupport\n\n## Description\n\nThe ‘Onigmo’ regular expression ‘C’ libary\n<https://github.com/k-takata/Onigmo> — which is an extension of the\n‘Onigmura’ <https://github.com/kkos/oniguruma> library — provides\nsupport for a wide array of regular expression types and works across a\nwide array of character encodings. Methods are provided to perform\nsearching, matching and extraction in a tidy (i.e. “data first”) manner.\n\n[`ore`](https://github.com/jonclayden/ore) is another Onigmo-based R\npackage for handling regular expressions (it was the first to wrap this\nlibrary). It abstracts much of the complexity of Onigmo whereas the goal\nof `stringore` is to expose all of the features and power of Onigmo\nwhile also providing a “tidy” abstraction layer.\n\n## NOTE\n\nThis is a WIP and the API will likely go through a few rough iterations,\nso do not base any “production” work on it. If you decide to live\ndangerously, you can use\n[releases](https://github.com/hrbrmstr/stringore/releases) to checkpoint\nyour projects.\n\nSince it is a WIP, *your* thoughts are most welcome. Please [file an\nissue](https://github.com/hrbrmstr/stringore/issues) with any questions,\nnotes, ideas, etc.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `onigomo_library_version`: Return the version of the Onigmo library\n    being used by the package\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/stringore\")\n```\n\n## Usage\n\n``` r\nlibrary(stringore)\n\n# current verison\npackageVersion(\"stringore\")\n```\n\n    ## [1] '0.1.0'\n\n``` r\nonigomo_library_version()\n```\n\n    ## [1] \"6.1.3\"\n",
    "url": "https://github.com/hrbrmstr/stringore",
    "last_updated": "2022-07-07T04:15:37+00:00"
  },
  {
    "full_name": "ropensci-archive/fulltext",
    "name": "fulltext",
    "description": ":warning: ARCHIVED :warning: Search across and get full text for OA & closed journals",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/ropensci-archive/fulltext",
    "last_updated": "2025-08-28T20:07:41+00:00"
  },
  {
    "full_name": "hrbrmstr/include-in-pkgs",
    "name": "include-in-pkgs",
    "description": "㊢ Dependency-reducing code/files for 📦 authors (also see `freebase`) 👉🏼",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "helpers"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# include-in-pkgs\n\nR code/files that are meant to be *copied into new packages* (i.e. this\nrepo is not a pkg just files with code) to both reduce dependencies in\ngeneral but also reduce dependencies on compiled code.\n\nUnless otherwise noted in a comment near a function, these are all\nimplemented in base R.\n\nThe compiled code equivalents are (generally) faster than these and most\nof them also have more robust features. Basically: use these components\nif they cover your use-cases *and* if the benefits of a thinner\ndependency stack outweigh the performance penalty costs.\n\nContributions can be made via PR or issues.\n\n# IF YOU WANT EASIER ACCESS TO THESE IN A PACKAGE\n\nTry [`freebase`ing](https://github.com/hrbrmstr/freebase).\n\n## Available things\n\n  - `is_empty()`\n  - `%l0%`\n  - `%||%`\n  - `%@%`\n  - `safely()`\n  - `quietly()`\n  - `possibly()`\n  - `keep()`\n  - `discard()`\n  - `compact()`\n  - `bind_rows()`\n  - `map()`\n  - `map_chr()`\n  - `map_dbl()`\n  - `map_df()`\n  - `map_int()`\n  - `map_lgl()`\n  - `map2()`\n  - `map2_chr()`\n  - `map2_dbl()`\n  - `map2_df()`\n  - `map2_int()`\n  - `map2_lgl()`\n\n## Code of Conduct\n\nPlease note that this project is released with a [Contributor Code of\nConduct](CODE_OF_CONDUCT.md). By participating in this project you agree\nto abide by its terms.\n",
    "url": "https://github.com/hrbrmstr/include-in-pkgs",
    "last_updated": "2025-03-22T11:06:33+00:00"
  },
  {
    "full_name": "chroma-core/chroma",
    "name": "chroma",
    "description": "Open-source search and retrieval database for AI applications.",
    "language": "Rust",
    "topics": [
      "embeddings",
      "document-retrieval",
      "llms",
      "database",
      "llm",
      "rag",
      "rust",
      "rust-lang",
      "vector-database",
      "ai"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://trychroma.com\"><img src=\"https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png\" alt=\"Chroma logo\"></a>\n</p>\n\n<p align=\"center\">\n    <b>Chroma - the open-source embedding database</b>. <br />\n    The fastest way to build Python or JavaScript LLM apps with memory!\n</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/MMeYNTmh3x\" target=\"_blank\">\n      <img src=\"https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600\" alt=\"Discord\">\n  </a> |\n  <a href=\"https://github.com/chroma-core/chroma/blob/master/LICENSE\" target=\"_blank\">\n      <img src=\"https://img.shields.io/badge/License-Apache_2.0-blue.svg\" alt=\"License\">\n  </a> |\n  <a href=\"https://docs.trychroma.com/\" target=\"_blank\">\n      Docs\n  </a> |\n  <a href=\"https://www.trychroma.com/\" target=\"_blank\">\n      Homepage\n  </a>\n</p>\n\n```bash\npip install chromadb # python client\n# for javascript, npm install chromadb!\n# for client-server mode, chroma run --path /chroma_db_path\n```\n\n## Chroma Cloud\n\nOur hosted service, Chroma Cloud, powers serverless vector and full-text search. It's extremely fast, cost-effective, scalable and painless. Create a DB and try it out in under 30 seconds with $5 of free credits.\n\n[Get started with Chroma Cloud](https://trychroma.com/signup)\n\n## API\n\nThe core API is only 4 functions (run our [💡 Google Colab](https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing)):\n\n```python\nimport chromadb\n# setup Chroma in-memory, for easy prototyping. Can add persistence easily!\nclient = chromadb.Client()\n\n# Create collection. get_collection, get_or_create_collection, delete_collection also available!\ncollection = client.create_collection(\"all-my-documents\")\n\n# Add docs to the collection. Can also update and delete. Row-based API coming soon!\ncollection.add(\n    documents=[\"This is document1\", \"This is document2\"], # we handle tokenization, embedding, and indexing automatically. Yo",
    "url": "https://github.com/chroma-core/chroma",
    "last_updated": "2025-09-02T09:17:09+00:00"
  },
  {
    "full_name": "yixuan/rARPACK",
    "name": "rARPACK",
    "description": "Solvers for Large Scale Eigenvalue and SVD Problems",
    "language": "R",
    "topics": [],
    "readme": "### NOTE\n\n> **rARPACK** has been superseded by\n[RSpectra](https://github.com/yixuan/RSpectra) to avoid the confusion on\npackage name.\n\n> **rARPACK** was originally an R wrapper of the\n[ARPACK library](http://www.caam.rice.edu/software/ARPACK/)\nto solve large scale eigenvalue/vector problems. From version 0.8-0, it changed the backend to the\n[Spectra library](http://yixuan.cos.name/spectra/), so theoretically it\nno longer depended on ARPACK since then. From version 0.11-0,\n**rARPACK** was simply a shell of the **RSpectra** package.\n\n> Old sources are kept in the [archive](https://github.com/yixuan/rARPACK/tree/archive/) branch.\n\n> New users of **rARPACK** are advised to switch to the **RSpectra** package.\n\n## Solvers for Large Scale Eigenvalue and SVD Problems\n\n### Introduction\n\n**rARPACK** is typically used to compute a few eigen\nvalues/vectors of an `n` by `n` matrix, e.g., the `k` largest eigen values, which\nis usually more efficient than `eigen()` if `k << n`.\n\nCurrently this package provides function `eigs()` for eigenvalue/eigenvector\nproblems, and `svds()` for truncated SVD. Different matrix types in R,\nincluding sparse matrices, are supported. Below is a list of implemented ones:\n\n- `matrix` (defined in base R)\n- `dgeMatrix` (defined in **Matrix** package, for general matrices)\n- `dsyMatrix` (defined in **Matrix** package, for symmetric matrices)\n- `dgCMatrix` (defined in **Matrix** package, for column oriented sparse matrices)\n- `dgRMatrix` (defined in **Matrix** package, for row oriented sparse matrices)\n- `function` (implicitly specify the matrix by providing a function that calculates matrix product `A %*% x`)\n\n### Example\n\nWe first generate some matrices:\n\n```r\nlibrary(Matrix)\nn = 20\nk = 5\n\nset.seed(111)\nA1 = matrix(rnorm(n^2), n)  ## class \"matrix\"\nA2 = Matrix(A1)             ## class \"dgeMatrix\"\n```\n\nGeneral matrices have complex eigenvalues:\n\n```r\neigs(A1, k)\neigs(A2, k, opts = list(retvec = FALSE))  ## eigenvalues only\n```\n\n**rARPACK** also works on ",
    "url": "https://github.com/yixuan/rARPACK",
    "last_updated": "2025-05-25T11:52:14+00:00"
  },
  {
    "full_name": "mattblackwell/cousteau",
    "name": "cousteau",
    "description": "A beamer theme",
    "language": "TeX",
    "topics": [],
    "readme": "## Cousteau\n\nCousteau is a minimalist [Beamer][] theme with a light nautical touch. \n\nBelow you can see some example slides and for more, see the full set of [demo slides][demo].\n\n![cousteau example slides](http://www.mattblackwell.org/images/cousteau-two.png)\n\n## Requirements\n\nIf you are using  [XeTeX][] to typeset you slides, Cousteau assumes that the [Fira Sans][fira] fonts (including Fira Sans Compressed and Fira Mono) are installed on your system. If you are using pdflatex, Cousteau will use the `FiraSans`, `inconsolata`, and `newtxsf` packages for typefaces, with slightly less nice typography. \n\n## Installation\n\nCopy the `.sty` files into you `texmf` tree at `$TEXMFHOME/tex/latex`. You can find where your tree is located (that is, where `$TEXMFHOME` is on your system), you can run `kpsewhich -var-value TEXMFHOME` in a terminal. \n\n## Usage \n\nFor basic usage, simply put `\\usetheme{cousteau}` in your preamble. \n\nThere are a few special commands provided by the theme. In particular, `\\alertb{}` and `\\alertc{}` provide alternative alert colors for additional text highlighting. See `demo/cousteau-demo.tex` for more details. \n\n## Inspiration/Attribution\n\nParts of the design were inspired by the [Metropolis][] theme, with some code taken from that project. The custom alert functions inspired by the [IQSS theme][iqss]. \n\n## License\n\nThe theme itself is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License][cc]. This means that if you change the theme and re-distribute it, you *must* retain the copyright notice header and license it under the same CC-BY-SA license. This does not affect the presentation that you create with the theme.\n\n\n[Beamer]: https://github.com/josephwright/beamer\n[fira]: https://github.com/bBoxType/FiraSans\n[cc]: https://creativecommons.org/licenses/by-sa/4.0/\n[XeTeX]: http://xetex.sourceforge.net/\n[Metropolis]: https://github.com/matze/mtheme\n[iqss]: https://github.com/IQSS/iqss-beamer-theme\n[demo]: http://www.mattblac",
    "url": "https://github.com/mattblackwell/cousteau",
    "last_updated": "2025-03-03T06:38:39+00:00"
  },
  {
    "full_name": "r-dbi/RSQLite",
    "name": "RSQLite",
    "description": "R interface for SQLite",
    "language": "R",
    "topics": [
      "r",
      "sqlite3",
      "database"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# RSQLite\n\n<!-- badges: start -->\n\n[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html) [![rcc](https://github.com/r-dbi/RSQLite/workflows/rcc/badge.svg)](https://github.com/r-dbi/RSQLite/actions) [![Coverage Status](https://codecov.io/gh/r-dbi/RSQLite/branch/main/graph/badge.svg)](https://app.codecov.io/github/r-dbi/RSQLite?branch=main) [![CRAN status](https://www.r-pkg.org/badges/version/RSQLite)](https://cran.r-project.org/package=RSQLite) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3234/badge)](https://bestpractices.coreinfrastructure.org/projects/3234)\n\n<!-- badges: end -->\n\nEmbeds the SQLite database engine in R, providing a DBI-compliant interface. [SQLite](https://www.sqlite.org/index.html) is a public-domain, single-user, very light-weight database engine that implements a decent subset of the SQL 92 standard, including the core table creation, updating, insertion, and selection operations, plus transaction management.\n\nYou can install the latest released version from CRAN with:\n\n<pre class='chroma'>\n<span class='nf'><a href='https://rdrr.io/r/utils/install.packages.html'>install.packages</a></span><span class='o'>(</span><span class='s'>\"RSQLite\"</span><span class='o'>)</span></pre>\n\nOr install the latest development version from GitHub with:\n\n<pre class='chroma'>\n<span class='c'># install.packages(\"devtools\")</span>\n<span class='nf'>devtools</span><span class='nf'>::</span><span class='nf'><a href='https://devtools.r-lib.org/reference/remote-reexports.html'>install_github</a></span><span class='o'>(</span><span class='s'>\"r-dbi/RSQLite\"</span><span class='o'>)</span></pre>\n\nDiscussions associated with DBI and related database packages take place on [R-SIG-DB](https://stat.ethz.ch/mailman/listinfo/r-sig-db). The website [Databases using R](https://db.rstudio.com/) describ",
    "url": "https://github.com/r-dbi/RSQLite",
    "last_updated": "2025-08-25T16:26:42+00:00"
  },
  {
    "full_name": "evancz/elm-architecture-tutorial",
    "name": "elm-architecture-tutorial",
    "description": "How to create modular Elm code that scales nicely with your app",
    "language": "Elm",
    "topics": [
      "elm",
      "examples"
    ],
    "readme": "# Elm\n\n[Elm](https://elm-lang.org/) is a programming language that compiles to JavaScript. It is known for its friendly error messages, helping you find issues quickly and refactor large projects with confidence. Elm is also [very fast](https://elm-lang.org/blog/blazing-fast-html-round-two) and [very small](https://elm-lang.org/blog/small-assets-without-the-headache) when compared with React, Angular, Ember, etc.\n\nThis repo focuses on **The Elm Architecture**, an architecture pattern you see in all Elm programs. It has influenced projects like Redux that borrow core concepts but add many JS-focused ideas.\n\n\n## The Elm Architecture\n\nThe Elm Architecture is a simple pattern for architecting webapps. The core idea is that your code is built around a `Model` of your application state, a way to `update` your model, and a way to `view` your model.\n\nTo learn more about this, read the [the official guide][guide] and check out [this section][arch] which is all about The Elm Architecture. This repo is a collection of all the examples in that section, so you can follow along and compile things on your computer as you read through.\n\n[guide]: https://guide.elm-lang.org/\n[arch]: https://guide.elm-lang.org/architecture/\n\n\n## Run The Examples\n\nAfter you [install](https://guide.elm-lang.org/install.html), run the following commands in your terminal to download this repo and start a server that compiles Elm for you:\n\n```bash\ngit clone https://github.com/evancz/elm-architecture-tutorial.git\ncd elm-architecture-tutorial\nelm reactor\n```\n\nNow go to [http://localhost:8000/](http://localhost:8000/) and start looking at the `examples/` directory. When you edit an Elm file, just refresh the corresponding page in your browser and it will recompile!\n",
    "url": "https://github.com/evancz/elm-architecture-tutorial",
    "last_updated": "2025-08-30T03:01:23+00:00"
  },
  {
    "full_name": "soodoku/uncertainty",
    "name": "uncertainty",
    "description": "Is an Uncertain Prospect Less Preferred Than Its Worst Possible Outcome? New Evidence on the Uncertainty Effect",
    "language": "TeX",
    "topics": [],
    "readme": "### Is an Uncertain Prospect Less Preferred Than Its Worst Possible Outcome? New Evidence on the Uncertainty Effect\n\nIn a seminal article in the Quarterly Journal of Economics, Gneezy, List, and Wu report the discovery of the uncertainty effect. In a series of experiments, the authors find that people are averse to picking the uncertain option even when the certain choice is worse than the worst possible outcome of the uncertain option. We successfully replicate the main finding with two larger, more representative surveys. But our data suggest three qualifiers: 1. adding a phrase that clarifies that people are guaranteed to get at least the worst option or asking people to estimate the expected value of the uncertain option before choosing reduces the effect, 2. rephrasing the uncertain option in terms of natural frequencies undoes the effect, and 3. redoing the experiment with monetary choices also undoes the effect.\n\n<img src=\"figs/lucid_exp.png\" alt=\"Lucid Exp. Results\" width=\"350\">\n<img src=\"figs/prolific_exp1.png\" alt=\"Prolific Exp. 1 Results\" width=\"350\">\n<img src=\"figs/prolific_exp2.png\" alt=\"Prolific Results (Bonus)\" width=\"350\">\n\n\n### Data\n\n* [Lucid](data/lucid/)\n* [Prolific](data/prolific)\n\n### Scripts\n\n* [Scripts](scripts/)\n\n### Manuscripts\n\n* [Manuscript](ms/)\n\n### Authors\n\nDoug Ahler and Gaurav Sood\n",
    "url": "https://github.com/soodoku/uncertainty",
    "last_updated": "2023-06-06T06:04:07+00:00"
  },
  {
    "full_name": "google-deepmind/mujoco",
    "name": "mujoco",
    "description": "Multi-Joint dynamics with Contact. A general purpose physics simulator.",
    "language": "C++",
    "topics": [
      "robotics",
      "physics",
      "mujoco"
    ],
    "readme": "<h1>\n  <a href=\"#\"><img alt=\"MuJoCo\" src=\"banner.png\" width=\"100%\"/></a>\n</h1>\n\n<p>\n  <a href=\"https://github.com/google-deepmind/mujoco/actions/workflows/build.yml?query=branch%3Amain\" alt=\"GitHub Actions\">\n    <img src=\"https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco/build.yml?branch=main\">\n  </a>\n  <a href=\"https://mujoco.readthedocs.io/\" alt=\"Documentation\">\n    <img src=\"https://readthedocs.org/projects/mujoco/badge/?version=latest\">\n  </a>\n  <a href=\"https://github.com/google-deepmind/mujoco/blob/main/LICENSE\" alt=\"License\">\n    <img src=\"https://img.shields.io/github/license/google-deepmind/mujoco\">\n  </a>\n</p>\n\n**MuJoCo** stands for **Mu**lti-**Jo**int dynamics with **Co**ntact. It is a\ngeneral purpose physics engine that aims to facilitate research and development\nin robotics, biomechanics, graphics and animation, machine learning, and other\nareas which demand fast and accurate simulation of articulated structures\ninteracting with their environment.\n\nThis repository is maintained by [Google DeepMind](https://www.deepmind.com/).\n\nMuJoCo has a C API and is intended for researchers and developers. The runtime\nsimulation module is tuned to maximize performance and operates on low-level\ndata structures that are preallocated by the built-in XML compiler. The library\nincludes interactive visualization with a native GUI, rendered in OpenGL. MuJoCo\nfurther exposes a large number of utility functions for computing\nphysics-related quantities.\n\nWe also provide [Python bindings] and a plug-in for the [Unity] game engine.\n\n## Documentation\n\nMuJoCo's documentation can be found at [mujoco.readthedocs.io]. Upcoming\nfeatures due for the next release can be found in the [changelog] in the\n\"latest\" branch.\n\n## Getting Started\n\nThere are two easy ways to get started with MuJoCo:\n\n1. **Run `simulate` on your machine.**\n[This video](https://www.youtube.com/watch?v=P83tKA1iz2Y) shows a screen capture\nof `simulate`, MuJoCo's native interactive viewer. F",
    "url": "https://github.com/google-deepmind/mujoco",
    "last_updated": "2025-09-02T09:21:09+00:00"
  },
  {
    "full_name": "VikParuchuri/apartment-finder",
    "name": "apartment-finder",
    "description": "A Slack bot that helps you find an apartment.",
    "language": "Python",
    "topics": [],
    "readme": "Apartment finder\n-------------------\n\nThis repo contains the code for a bot that will scrape Craigslist for real-time listings matching specific criteria, then alert you in Slack.  This will let you quickly see the best new listings, and contact the owners.  You can adjust the settings to change your price range, what neighborhoods you want to look in, and what transit stations and other points of interest you'd like to be close to.\n\nI successfully used this tool to find an apartment when I moved from Boston to SF.  It saved a good amount of time and money.  Read more about it [here](https://www.dataquest.io/blog/apartment-finding-slackbot/).\n\nIt's recommended to follow the Docker installation and usage instructions.\n\nSettings\n--------------------\n\nLook in `settings.py` for a full list of all the configuration options.  Here's a high level overview:\n\n* `MIN_PRICE` -- the minimum listing price you want to search for.\n* `MAX_PRICE` -- the minimum listing price you want to search for.\n* `CRAIGSLIST_SITE` -- the regional Craigslist site you want to search in.\n* `AREAS` -- a list of areas of the regional Craiglist site that you want to search in.\n* `BOXES` -- coordinate boxes of the neighborhoods you want to look in.\n* `NEIGHBORHOODS` -- if the listing doesn't have coordinates, a list of neighborhoods to match on.\n* `MAX_TRANSIT_DISTANCE` -- the farthest you want to be from a transit station.\n* `TRANSIT_STATIONS` -- the coordinates of transit stations.\n* `CRAIGSLIST_HOUSING_SECTION` -- the subsection of Craigslist housing that you want to look in.\n* `SLACK_CHANNEL` -- the Slack channel you want the bot to post in.\n\nExternal Setup\n--------------------\n\nBefore using this bot, you'll need a Slack team, a channel for the bot to post into, and a Slack API key:\n\n* Create a Slack team, which you can do [here](https://slack.com/create#email).  \n* Create a channel for the listings to be posted into.  [Here's](https://get.slack.help/hc/en-us/articles/201402297-Creating-a-channel) ",
    "url": "https://github.com/VikParuchuri/apartment-finder",
    "last_updated": "2025-08-29T22:06:20+00:00"
  },
  {
    "full_name": "coolbutuseless/rd2list",
    "name": "rd2list",
    "description": "Convert Rd documentation to a structured, human-readable list",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# rd2list\n\n[![AppVeyor build\nstatus](https://ci.appveyor.com/api/projects/status/github/coolbutuseless/rd2list?branch=master&svg=true)](https://ci.appveyor.com/project/coolbutuseless/rd2list)\n[![Travis build\nstatus](https://travis-ci.org/coolbutuseless/rd2list.svg?branch=master)](https://travis-ci.org/coolbutuseless/rd2list)\n[![Coverage\nstatus](https://codecov.io/gh/coolbutuseless/rd2list/branch/master/graph/badge.svg)](https://codecov.io/github/coolbutuseless/rd2list?branch=master)\n\n`rd2list` is a package for extracting R documentation into a structured,\nhuman-readable list. Use this package if you’ve ever wanted to get the\nhelp text for a function as a string.\n\n  - `get_doc()` - get a structured, human-readable list of the\n    documentation for a function.\n  - `get_rd_doc()` - fetch the Rd object for a function from an\n    installed package.\n  - `rd2list()` - parse an Rd object into a structured, human-readable\n    list.\n\n## Installation\n\nYou can install from github\nwith:\n\n``` r\nremotes::install_github(\"coolbutuseless/rd2list\")\n```\n\n## Example - Getting documentation as a list\n\n``` r\ndoc <- rd2list::get_doc(function_name = 'geom_path', package = 'ggplot2')\n\ndoc$title\n#> [1] \"Connect observations\"\n```\n\n``` r\ndoc$description\n```\n\n    #> `geom_path()` connects the observations in the order in which they\n    #> appear in the data. `geom_line()` connects them in order of the\n    #> variable on the x axis. `geom_step()` creates a stairstep plot,\n    #> highlighting exactly when changes occur. The `group` aesthetic\n    #> determines which cases are connected together.\n\n``` r\ndoc$arguments$linejoin\n#> [1] \"Line join style (round, mitre, bevel).\"\n```\n\n``` r\ndoc$alias\n#> [1] \"geom_path\" \"geom_line\" \"geom_step\"\n```\n\n## Example - Getting documentation as an Rd object\n\nThe raw Rd documentation object can be fetched using `get_rd_doc()`.\n\n``` r\nrd2list::get_rd_doc(function_name = 'diag', package_name = 'ba",
    "url": "https://github.com/coolbutuseless/rd2list",
    "last_updated": "2025-03-22T11:00:50+00:00"
  },
  {
    "full_name": "notnews/nyt-civil-rights",
    "name": "nyt-civil-rights",
    "description": "Example of how to use the NYT API from R",
    "language": "R",
    "topics": [
      "nyt-api",
      "civil-rights"
    ],
    "readme": "## Civil Rights Coverage in the NYT Over Time\n\nTrack the number of news stories about African American civil rights issues over the decades. We track the coverage using a simple keyword search. Article featuring any of the following keywords are taken to deal with black civil rights:\naffirmative action, racial, voting rights, busing, african american, black, negro, racial discrimination, civil rights, segregation\n\nYou can track coverage aggregated over an entire year on the [NYT Chronicle](http://chronicle.nytlabs.com/?keyword=civil%20rights.black.african%20american.negro.racial.voting%20rights.racial%20descrimination.segregation.affirmative%20action.affirmative%20action.busing).\n\nIf you would like to get your own data, start by getting a [NYT Article Search API key](http://developer.nytimes.com/apps/register). Then you can use the R package, rtimes, using the following [script](scripts/using_rtimes.R) (Output: [Quarterly Data](nyt_rtimes.csv)) or the R package jsonlite using the following [script](scripts/using_jsonlite.R). You can plot the data using the following [script](scripts/plot.R). (See the [graph (pdf)](figs/nyt_aa.pdf).)\n\n<img src=\"figs/nyt_aa.png\" width=\"700\">\n\n**Note** \n\nThe scripts track the number of articles containing the keyword search. But sometimes a more apt unit of measurement is the proportion of articles mentioning the search term. The point is especially important over great lengths of time as, over time, the number of articles published in the NYT has been increasing. To get the proportion of articles, you need to use the 'source facet.' For instance, if you look at the end of the output given by the following\n[API Request](http://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=c2fede7bd9aea57c898f538e5ec0a1ee:6:68700045&facet_field=source&facet_filter=true&begin_date=20150710&end_date=20150710), you will see the total number of articles from various sources (Reuters, AP, NYT, Internet Video Archive, CNBC etc.) for the specified d",
    "url": "https://github.com/notnews/nyt-civil-rights",
    "last_updated": "2021-07-16T15:37:06+00:00"
  },
  {
    "full_name": "sckott/extcite",
    "name": "extcite",
    "description": "pull dois out of pdfs > content negotation > bib file",
    "language": "Ruby",
    "topics": [
      "doi",
      "pdf",
      "bib",
      "extract",
      "citations"
    ],
    "readme": "extcite\n=======\n\n[![gem version](https://img.shields.io/gem/v/extcite.svg)](https://rubygems.org/gems/extcite)\n![Ruby](https://github.com/sckott/extcite/workflows/Ruby/badge.svg)\n[![codecov.io](http://codecov.io/github/sckott/extcite/coverage.svg?branch=master)](http://codecov.io/github/sckott/extcite?branch=master)\n\n__`extcite` gets DOIS and generates citations for papers__\n\n## Install\n\n### Release version\n\n```\ngem install extcite\n```\n\n### Development version\n\n```\ngit clone git@github.com:sckott/extcite.git\ncd extcite\nrake install\n```\n\n> if `rake install` fails, try `sudo rake install`. If that fails, open an issue with what `rake install --trace` gives you\n\n## Examples\n\n### Within Ruby\n\n```ruby\nrequire 'extcite'\n```\n\nA single paper\n\n```ruby\nrequire 'net/http'\nFile.write(\"foo.pdf\", Net::HTTP.get(URI.parse(\"https://scottchamberlain.info/pdfs/GuoEtal2015PlosOne.pdf\")))\nExtcite.extract(path: 'foo.pdf')\n```\n\nbib citation is written to a file given in `file` param\n\nMany papers at once\n\n```ruby\nDir.mkdir('bar')\nFile.write(\"bar/foo1.pdf\", Net::HTTP.get(URI.parse(\"https://scottchamberlain.info/pdfs/Chamberlain&Szocs2013F1000Research.pdf\")))\nFile.write(\"bar/foo2.pdf\", Net::HTTP.get(URI.parse(\"https://scottchamberlain.info/pdfs/GuoEtal2015PlosOne.pdf\")))\nExtcite.extract(path: 'bar')\n```\n\n### On the CLI\n\nAll pdfs in the current directory:\n\n```shell\nextcite extract .\n```\n\nSingle paper\n\n```shell\nextcite extract foo.pdf\n```\n\n[changelog]: https://github.com/sckott/extcite/blob/master/CHANGELOG.md\n",
    "url": "https://github.com/sckott/extcite",
    "last_updated": "2022-04-16T16:20:00+00:00"
  },
  {
    "full_name": "Chicago/RSocrata",
    "name": "RSocrata",
    "description": "Provides easier interaction with Socrata open data portals http://dev.socrata.com. Users can provide a 'Socrata' data set resource URL, or a 'Socrata' Open Data API (SoDA) web query, or a 'Socrata' \"human-friendly\" URL, returns an R data frame. Converts dates to 'POSIX' format. Manages throttling by 'Socrata'.",
    "language": "R",
    "topics": [
      "r",
      "socrata",
      "soda",
      "government",
      "chicago",
      "open-data"
    ],
    "readme": "RSocrata\n========\n\n[![Gitter](https://badges.gitter.im/repo.svg)](https://app.gitter.im/#/room/#Chicago_RSocrata:gitter.im)\n[![downloads](https://cranlogs.r-pkg.org/badges/RSocrata)](https://CRAN.R-project.org/package=RSocrata)\n[![cran version](https://www.r-pkg.org/badges/version/RSocrata)](https://CRAN.R-project.org/package=RSocrata)\n\n**Master** \n\nStable beta branch. Test about-to-be-released features in a stable pre-release build before it is submitted to CRAN.\n\n[![Linux build - Master](https://img.shields.io/travis/Chicago/RSocrata/master.svg?style=flat-square&label=Linux%20build)](https://app.travis-ci.com/Chicago/RSocrata)[![Windows build - Master](https://img.shields.io/appveyor/ci/tomschenkjr/RSocrata/master.svg?style=flat-square&label=Windows%20build)](https://ci.appveyor.com/project/tomschenkjr/rsocrata/branch/master)[![Coverage - Master](https://img.shields.io/coveralls/Chicago/RSocrata/master.svg?style=flat-square&label=Coverage)](https://coveralls.io/github/Chicago/RSocrata?branch=master)\n\n**Dev**\n\n\"Nightly\" alpha branch. Test the latest features and bug fixes -- enjoy at your own risk.\n\n[![Linux build - Dev](https://img.shields.io/travis/Chicago/RSocrata/dev.svg?style=flat-square&label=Linux%20build)](https://app.travis-ci.com/Chicago/RSocrata)[![Windows build - Dev](https://img.shields.io/appveyor/ci/tomschenkjr/RSocrata/dev.svg?style=flat-square&label=Windows%20build)](https://ci.appveyor.com/project/tomschenkjr/rsocrata/branch/dev)[![Coverage - Dev](https://img.shields.io/coveralls/Chicago/RSocrata/dev.svg?style=flat-square&label=Coverage)](https://coveralls.io/github/Chicago/RSocrata?branch=dev)\n\nA tool for downloading and uploading Socrata datasets\n-----------------------------------------------------\n\nProvided with a URL to a dataset resource published on a [Socrata](https://www.tylertech.com/products/data-insights) webserver,\nor a Socrata [SoDA (Socrata Open Data Application Program Interface) web API](https://dev.socrata.com) query,\nor a Socrat",
    "url": "https://github.com/Chicago/RSocrata",
    "last_updated": "2025-08-31T21:32:37+00:00"
  },
  {
    "full_name": "elceef/dnstwist",
    "name": "dnstwist",
    "description": "Domain name permutation engine for detecting homograph phishing attacks, typo squatting, and brand impersonation",
    "language": "Python",
    "topics": [
      "phishing",
      "typosquatting",
      "domains",
      "dns",
      "osint",
      "idn",
      "fuzzing",
      "threat-hunting",
      "homograph-attack",
      "scanner",
      "threat-intelligence",
      "homoglyph"
    ],
    "readme": "![dnstwist](/docs/dnstwist.png)\n===============================\n\nSee what sort of trouble users can get in trying to type your domain name.\nFind lookalike domains that adversaries can use to attack you. Can detect\ntyposquatters, phishing attacks, fraud, and brand impersonation. Useful as an\nadditional source of targeted threat intelligence.\n\n![Demo](/docs/demo.gif)\n\nDNS fuzzing is an automated workflow that aims to uncover potentially malicious\ndomains that target your organization. This tool generates a comprehensive list\nof permutations based on a provided domain name, and subsequently verifies\nwhether any of these permutations are in use.\nAdditionally, it can generate fuzzy hashes of web pages to detect ongoing\nphishing attacks or brand impersonation, and much more!\n\nIn a hurry? Try it in your web browser: [dnstwist.it](https://dnstwist.it)\n\n\nKey features\n------------\n\n- Variety of highly effective domain fuzzing algorithms\n- Unicode domain names (IDN)\n- Additional domain permutations from dictionary files\n- Efficient multithreaded task distribution\n- Live phishing webpage detection:\n  - HTML similarity with fuzzy hashes (ssdeep/tlsh)\n  - Screenshot visual similarity with perceptual hashes (pHash)\n- Rogue MX host detection (intercepting misdirected e-mails)\n- GeoIP location\n- Export to CSV and JSON\n\n\nInstallation\n------------\n\n**Python PIP**\n\n```\n$ pip install dnstwist[full]\n```\n\nAlternatively install the bare minimum and add other requirements manually\ndepending on your needs:\n\n```\n$ pip install dnstwist\n```\n\n**Git**\n\nIf you want to run the latest version of the code, you can install it from Git:\n\n```\n$ git clone https://github.com/elceef/dnstwist.git\n$ cd dnstwist\n$ pip install .\n```\n\n**Debian/Ubuntu/Kali Linux**\n\nInvoke the following command to install the tool with all extra packages:\n\n```\n$ sudo apt install dnstwist\n```\n\n**Fedora Linux**\n\n```\n$ sudo dnf install dnstwist\n```\n\n**Arch Linux User Repository (yay)**\n\n```\n$ yay -S dnstwist\n```\n\n**macOS**\n\nThis wil",
    "url": "https://github.com/elceef/dnstwist",
    "last_updated": "2025-09-02T00:01:54+00:00"
  },
  {
    "full_name": "IQSS/IQSS.emacs",
    "name": "IQSS.emacs",
    "description": "Yet Another .emacs.d",
    "language": "HTML",
    "topics": [],
    "readme": "---\ntitle: Emacs for the rest of us\n---\n\n\nThis is an [Emacs](https://www.gnu.org/software/emacs/) configuration.\nThere are many like it, but this one is mine. If you like it, make it\nyours! It provides lots of functionality while keeping things light\nand fast. It tries to tames Emacs, making it behave more like other\napplications you use.\n\nProject goals and philosophy\n========================================\n\nThe main goal of this project is to provide an Emacs configuration\nthat works more or less they way you would expect an editor or IDE to\nwork in the second decade of the twenty-first century, without losing\nthe things that make Emacs special. The included packages were\nselected with social scientists in mind (e.g., it includes support for\nR, Stata, Python, Markdown, and LaTeX).\n\nThe overarching philosophy is pragmatism; we're trying to make Emacs\nas useful as possible, and to reduce the time needed to start using\nEmacs productively.\n\nFeature highlights\n==========================\n\nHighlights of this Emacs configuration:\n-   Literate programming configuration for running R, python, or other\n    programming languages inside markdown or org-mode files.\n-   Consistent and familiar code evaluation using `C-ret` (that's `Control + Return`).\n-   Consistent indentation and folding using the `tab` key.\n-   Consistent completion using `<tab>`.\n-   Support for LaTeX and other markup languages.\n-   Powerful and simple search-based tools for finding commands, files\n    and buffers, inserting citations etc.\n-   More standard select/copy/paste keys and right-click behavior makes\n    it more familiar to those new to Emacs.\n-   Multiple cursors, as in Sublime and VScode\n-   Convenient window management.\n\nInstallation\n=================\n\nIf you previously had another version of Emacs installed it is a good\nidea to move your `~/.emacs.d` configuration folder to a backup\nlocation before installing this Emacs configuration. If you do not yet\nhave Emacs, installers available for [Mac ",
    "url": "https://github.com/IQSS/IQSS.emacs",
    "last_updated": "2024-10-19T17:41:24+00:00"
  },
  {
    "full_name": "mnielsen/neural-networks-and-deep-learning",
    "name": "neural-networks-and-deep-learning",
    "description": "Code samples for my book \"Neural Networks and Deep Learning\"",
    "language": "Python",
    "topics": [],
    "readme": "# Code samples for \"Neural Networks and Deep Learning\"\r\n\r\nThis repository contains code samples for my book on [\"Neural Networks\r\nand Deep Learning\"](http://neuralnetworksanddeeplearning.com).\r\n\r\nThe code is written for Python 2.6 or 2.7. There is a version for \r\nPython 3.8-3.10 [here](https://github.com/unexploredtest/neural-networks-and-deep-learning). \r\nI will not be updating the current repository for Python 3 compatibility.\r\n\r\nThe program `src/network3.py` uses version 0.6 or 0.7 of the Theano\r\nlibrary.  It needs modification for compatibility with later versions\r\nof the library.  I will not be making such modifications.\r\n\r\nAs the code is written to accompany the book, I don't intend to add\r\nnew features. However, bug reports are welcome, and you should feel\r\nfree to fork and modify the code.\r\n\r\n## License\r\n\r\nMIT License\r\n\r\nCopyright (c) 2012-2022 Michael Nielsen\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining\r\na copy of this software and associated documentation files (the\r\n\"Software\"), to deal in the Software without restriction, including\r\nwithout limitation the rights to use, copy, modify, merge, publish,\r\ndistribute, sublicense, and/or sell copies of the Software, and to\r\npermit persons to whom the Software is furnished to do so, subject to\r\nthe following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be\r\nincluded in all copies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\r\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\r\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\r\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\r\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\r\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\r\n",
    "url": "https://github.com/mnielsen/neural-networks-and-deep-learning",
    "last_updated": "2025-09-02T08:03:30+00:00"
  },
  {
    "full_name": "jennybc/access-r-source",
    "name": "access-r-source",
    "description": "How to get at R source. I am sick of Googling this. I am writing it down this time.",
    "language": "",
    "topics": [],
    "readme": "Accessing R Source\n================\n\n-   [TL;DR](#tldr)\n-   [References](#references)\n-   [Just print it](#just-print-it)\n-   [Function is an S3 generic](#function-is-an-s3-generic)\n-   [Compiled code](#compiled-code)\n\n*2017-07-31 update: Since I wrote this @jimhester has created the [lookup package](https://github.com/jimhester/lookup#readme) to automate this process. So if all you want is the result, just use that! If you want a bit more context, then keep reading. AFAIK this info is still fundamentally sound.*\n\nHow to get at R source. I am sick of Googling this. I am writing it down this time.\n\n### TL;DR\n\n``` r\nmethods(<S3_GENERIC>)\n<S3_GENERIC>.default\n<S3_GENERIC>.<CLASS>\ngetAnywhere(<S3_GENERIC>.<CLASS>)\ngetS3method(\"<S3_GENERIC>\", \"<CLASS>\")\n<NAMESPACE>:::<S3_GENERIC>.<CLASS>\n```\n\n### References\n\nThe definitive reference is this classic R News article:\n\n> Accessing the Sources\n>\n> Uwe Ligges\n>\n> <https://cran.r-project.org/doc/Rnews/Rnews_2006-4.pdf>\n>\n> Volume 6/4, October 2006. Go to page 43.\n\nAnother good reference is the help file for `method()`:\n\n<https://stat.ethz.ch/R-manual/R-patched/library/utils/html/methods.html>\n\n### Just print it\n\nIf you are lucky, just printing the function will work.\n\n``` r\nsetNames\n#> function (object = nm, nm) \n#> {\n#>     names(object) <- nm\n#>     object\n#> }\n#> <bytecode: 0x7fce93c98920>\n#> <environment: namespace:stats>\n```\n\nBut there are many ways this can fail.\n\n``` r\nvector             # .Internal\n#> function (mode = \"logical\", length = 0L) \n#> .Internal(vector(mode, length))\n#> <bytecode: 0x7fce94835c40>\n#> <environment: namespace:base>\nclass              # .Primitive\n#> function (x)  .Primitive(\"class\")\nsubset             # S3 generic\n#> function (x, ...) \n#> UseMethod(\"subset\")\n#> <bytecode: 0x7fce933c0178>\n#> <environment: namespace:base>\n```\n\nWhat then?\n\n### Function is an S3 generic\n\nThese are characterized by `UseMethod()` in the printed result:\n\n``` r\nsubset\n#> function (x, ...) \n#> UseMethod(\"subset\")\n#> <byte",
    "url": "https://github.com/jennybc/access-r-source",
    "last_updated": "2025-05-29T01:50:08+00:00"
  },
  {
    "full_name": "streamlit/streamlit",
    "name": "streamlit",
    "description": "Streamlit — A faster way to build and share data apps.",
    "language": "Python",
    "topics": [
      "python",
      "machine-learning",
      "data-science",
      "deep-learning",
      "data-visualization",
      "streamlit",
      "data-analysis",
      "developer-tools"
    ],
    "readme": "<br>\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png\" alt=\"Streamlit logo\" style=\"margin-top:50px\"></img>\n\n# Welcome to Streamlit 👋\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you’ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you're all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif\" alt=\"Streamlit Hello\" width=500 href=\"none\"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src=\"https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif\" width=300 alt=\"Little example\"></img>\n\n### Give me more!\n\nStreamlit come",
    "url": "https://github.com/streamlit/streamlit",
    "last_updated": "2025-09-02T08:48:42+00:00"
  },
  {
    "full_name": "graykode/nlp-tutorial",
    "name": "nlp-tutorial",
    "description": "Natural Language Processing Tutorial for Deep Learning Researchers",
    "language": "Jupyter Notebook",
    "topics": [
      "nlp",
      "natural-language-processing",
      "tutorial",
      "pytorch",
      "tensorflow",
      "transformer",
      "attention",
      "paper",
      "bert"
    ],
    "readme": "## nlp-tutorial\n\n<p align=\"center\"><img width=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png\" />  <img width=\"100\" src=\"https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png\" /></p>\n\n`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)\n\n- [08-14-2020] Old TensorFlow v1 code is archived in [the archive folder](archive). For beginner readability, only pytorch version 1.0 or higher is supported.\n\n\n## Curriculum - (Example Purpose)\n\n#### 1. Basic Embedding Model\n\n- 1-1. [NNLM(Neural Network Language Model)](1-1.NNLM) - **Predict Next Word**\n  - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n  - Colab - [NNLM.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM.ipynb)\n- 1-2. [Word2Vec(Skip-gram)](1-2.Word2Vec) - **Embedding Words and Show Graph**\n  - Paper - [Distributed Representations of Words and Phrases\n    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n  - Colab - [Word2Vec.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram(Softmax).ipynb)\n- 1-3. [FastText(Application Level)](1-3.FastText) - **Sentence Classification**\n  - Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)\n  - Colab - [FastText.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb)\n\n\n\n#### 2. CNN(Convolutional Neural Network)\n\n- 2-1. [TextCNN](2-1.TextCNN) - **Binary Sentiment Classific",
    "url": "https://github.com/graykode/nlp-tutorial",
    "last_updated": "2025-09-02T02:15:20+00:00"
  },
  {
    "full_name": "JeffreyBLewis/congressional-district-boundaries",
    "name": "congressional-district-boundaries",
    "description": "GIS boundaries in GeoJSON format for all US Congressional Districts, 1789 to 2012",
    "language": "",
    "topics": [],
    "readme": "congressional-district-boundaries\n=================================\nJeffrey B. Lewis, Brandon DeVine, and Lincoln Pritcher with Kenneth C. Martis\n\nThe webpage for this project can be found at\nhttp://cdmaps.polisci.ucla.edu.\n\nDescription\n-----------\n\nThese repositiories provides digital boundary definitions in GeoJson\nformat for every U.S. Congressional District in use between 1789 and\n2012. These were produced as part of NSF grant SBE-SES-0241647 between\n2009 and 2013.\n\nThe current release of these data is experimental. We have had done a\ngood deal of work to validate all of the shapes. However, it is quite\nlikely that some irregulaties remain. Please email jblewis@ucla.edu\nwith questions or suggestions for improvement. We hope to have a\nticketing system for bugs and a versioning system up soon. The\ndistrict definitions currently available should be considered a\npre-release version.\n\nMany districts were formed by aggregragating complete county shapes\nobtained from the National Historical Geographic Information System\n(NHGIS) project and the Newberry Library's Atlas of Historical County\nBoundaries. Where Congressional Districts boundaries did not coincide\nwith county boundaries districts shapes were constructed\ndistrict-by-district using a wide variety of legal and cartographic\nresources. Detailed descriptions of how particular districts were\nconstructed and the authorities upon which we relied are available (at\nthe moment) by request.  \n\n\nProject Team\n------------\n\nThe Principal Investigator on the project was Jeffrey\nB. Lewis. Brandon DeVine and Lincoln Pitcher researched district\ndefinitions and produced thousands of digital district boundaries. The\nproject relied heavily on Kenneth C. Martis' The Historical Atlas of\nUnited States Congressional Districts: 1789-1983. (New York: The Free\nPress, 1982). Martis also provided guidance, advice, and source\nmaterials used in the project.  \n\n\nHow to cite\n-----------\n\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pitcher, and Ke",
    "url": "https://github.com/JeffreyBLewis/congressional-district-boundaries",
    "last_updated": "2025-08-06T16:38:21+00:00"
  },
  {
    "full_name": "IBM/docAMR",
    "name": "docAMR",
    "description": "code for document level AMR representation and evaluation",
    "language": "Python",
    "topics": [],
    "readme": "# Setup\n\nmake an environment with python 3.7\n\nactivate environment\n```\npip install -r requirements.txt\n```\n\n# Create docAMR representation\n\nLink to NAACL 2022 paper DOCAMR: Multi-Sentence AMR Representation and Evaluation\n\nhttps://aclanthology.org/2022.naacl-main.256.pdf\n\n<img src=\"docAMR.jpg\" width=60% height=60%>\n\nTo create docAMR representation from gold AMR3.0 data and the coref annotation in xml format:\n```\npython doc_amr.py \n--amr3-path <path to AMR3 data> \n--coref-fof <file-with-list-of-xml-annotations-files> \n--out-amr <output file> \n--rep <representation>\n```\n```<path to AMR3 data>``` should point to uncompressed LDC data directory for LDC2020T02 with its original directory structure.\n\n```<file-with-list-of-xml-annotations-files>``` is one of the ```_coref.fof``` files included in this repository.\n\nDefault value for ```--rep``` is ```'docAMR'```. Other values can be: ```'no-merge'```,```'merge-names'```,```'merge-all'```. Use ```--help``` to read the descriptions of these representations.\n\n-------\n\nTo create docAMR representation from dcoument AMRs with no nodes merged\n```\npython doc_amr.py\n       --in-doc-amr-unmerged <path to document-level AMRs un no-merge format>\n       --rep <representation>\n       --out-amr <output file>\n```\n\n-------\n\nTo create docAMR representation from dcoument AMRs with pairwise edges between a representative node in the chain and the rest of the nodes in the chain:\n```\npython doc_amr.py\n       --in-doc-amr-pairwise <path to document-level AMR with pairwise coref edges>\n       --pairwise-coref-rel <relation label indicating coref edges>\n       --rep <representation>\n       --out-amr <output file>\n```\n\ndefault value for ```--pairwise-coref-rel``` is ```same-as```\n\n# Evaluate docAMR (docSmatch) \n\nUse docSmatch the same way as the standard Smatch. \n\n```\npython docSmatch/smatch.py -f <amr1> <amr2>\n```\n\nIt assumes that ```:snt``` relations connect sentences to the root. Moreover, it assumes that the numeric suffix of ```:snt``` is the s",
    "url": "https://github.com/IBM/docAMR",
    "last_updated": "2025-07-14T07:33:30+00:00"
  },
  {
    "full_name": "stan-dev/rstanarm",
    "name": "rstanarm",
    "description": "rstanarm R package for Bayesian applied regression modeling",
    "language": "R",
    "topics": [
      "stan",
      "multilevel-models",
      "r-package",
      "bayesian",
      "bayesian-inference",
      "bayesian-methods",
      "bayesian-statistics",
      "bayesian-data-analysis",
      "statistical-modeling",
      "r",
      "rstan",
      "rstanarm"
    ],
    "readme": "# rstanarm <img src=\"man/figures/stanlogo.png\" align=\"right\" width=\"120\" />\n\n<!-- badges: start -->\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/rstanarm?color=blue)](https://cran.r-project.org/package=rstanarm)\n[![Downloads](https://cranlogs.r-pkg.org/badges/rstanarm?color=blue)](https://cran.rstudio.com/package=rstanarm)\n[![R-CMD-check](https://github.com/stan-dev/rstanarm/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/rstanarm/actions)\n<!-- badges: end -->\n\n### Bayesian applied regression modeling (arm) via Stan\n\nThis is an R package that emulates other R model-fitting functions but uses\n[Stan](https://mc-stan.org) (via the **rstan** package) for the back-end\nestimation. The primary target audience is people who would be open to Bayesian\ninference if using Bayesian software were easier but would use frequentist\nsoftware otherwise. \n\nFitting models with **rstanarm** is also useful for experienced Bayesian\nsoftware users who want to take advantage the pre-compiled Stan programs that\nare written by Stan developers and carefully implemented to prioritize numerical\nstability and the avoidance of sampling problems.\n\nClick the arrows for more details:\n<details><summary>More detail</summary>\n\nThe **rstanarm** package is an appendage to the **rstan** package, the R\ninterface to [Stan](https://mc-stan.org/). **rstanarm** enables many of the most\ncommon applied regression models to be estimated using Markov Chain Monte Carlo,\nvariational approximations to the posterior distribution, or optimization. The\npackage allows these models to be specified using the customary R modeling\nsyntax (e.g., like that of `glm` with a `formula` and `data.frame`).\nAdditional arguments are provided for specifying prior distributions.\n\nThe set of models supported by **rstanarm** is large (and will continue to\ngrow), but also limited enough so that it is possible to integrate them\ntightly with the [`pp_check`](https://mc-stan.org/rstanarm/reference/pp_check.stanreg.",
    "url": "https://github.com/stan-dev/rstanarm",
    "last_updated": "2025-08-18T06:03:31+00:00"
  },
  {
    "full_name": "dirkschumacher/votingpower.r",
    "name": "votingpower.r",
    "description": "Measure voting power in R",
    "language": "R",
    "topics": [],
    "readme": "# Measure voting power in R\n[![Build Status](https://travis-ci.org/dirkschumacher/votingpower.r.png?branch=master)](https://travis-ci.org/dirkschumacher/votingpower.r)\n[![Coverage Status](https://coveralls.io/repos/dirkschumacher/votingpower.r/badge.svg?branch=master)](https://coveralls.io/r/dirkschumacher/votingpower.r?branch=master)\n\nHave you ever asked yourself if weighted voting is fair? This package implements (or will implement) some of the most popular voting power measurement methods. \n\nImagine you are a representative of the country Luxembourg in the European Economic Community of 1958. At that time Germany, France and Italy had 4 votes each, Belgium and the Netherlands 2 and Luxembourg 1 vote. The necessary quota was 12 out of the 17 votes. The problem was however that there existed no situation where the vote of Luxembourg counted, i.e. would change the outcome of a \"yes\" to \"no\". Luxembourg was never critical.\n\nThis topic is a bit esoteric but quite interesting. A number of algorithms exists both for measuring power and for the inverse problem: designing weights such that a specific voting power is achieved. The currently implemented methods have an exponential running time. However there exist more advanced methods when dealing with larger voting bodies.\n\nMore can be found [here](https://en.wikipedia.org/wiki/Banzhaf_power_index) or in the book of Felsenthal & Machover (1998). Below is a plot of the voting power of the EEC members in relation to the quota.\n\nCurrent version: 0.1.0\n\n# Installation\n\nTo install the current development version use devtools:\n\n```R \ndevtools::install_github(\"dirkschumacher/votingpower.r\")\n```\n\n## Quick start\nLet us recreate the example of the EEC from above.\n\n```R\nlibrary(votingpower)\neec <- create_weighted_voting_game(12, c(4, 4, 4, 2, 2, 1))\n\n# the Banzhaf score counts the coalitions where a voter is critical.\nscore <- compute_bz_score(eec)\nprint(score) \n# > [1] 10 10 10 6 6 0\n\n# The probability of a voter being critical is ",
    "url": "https://github.com/dirkschumacher/votingpower.r",
    "last_updated": "2023-07-25T13:56:49+00:00"
  },
  {
    "full_name": "Stability-AI/stability-sdk",
    "name": "stability-sdk",
    "description": "SDK for interacting with stability.ai APIs (e.g. stable diffusion inference)",
    "language": "Jupyter Notebook",
    "topics": [
      "stable-diffusion",
      "ai-art",
      "generative-art",
      "latent-diffusion",
      "multimodal"
    ],
    "readme": "# stability-sdk\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stability-ai/stability-sdk/blob/main/nbs/demo_colab.ipynb)\n\nClient implementations that interact with the Stability API. \n\n## Getting an API key\n\nFollow the [instructions](https://platform.stability.ai/docs/getting-started/authentication) on [Platform](https://platform.stability.ai) to obtain an API key.\n\n## PyPI Package Installation\n\nInstall the [PyPI](https://pypi.org/project/stability-sdk/) package via:\n\n`pip install stability-sdk`\n\n## Python Client\n\n`client.py` is both a command line client and an API class that wraps the gRPC based API. To try the client:\n\n- Use Python venv: `python3 -m venv pyenv`\n- Set up in venv dependencies: `pyenv/bin/pip3 install -e .`\n- `pyenv/bin/activate` to use the venv.\n- Set the `STABILITY_HOST` environment variable. This is by default set to the production endpoint `grpc.stability.ai:443`.\n- Set the `STABILITY_KEY` environment variable.\n\nThen to invoke:\n\n`python3 -m stability_sdk generate -W 1024 -H 1024 \"A stunning house.\"`\n\nIt will generate and put PNGs in your current directory.\n\nTo upscale:\n`python3 -m stability_sdk upscale -i \"/path/to/image.png\"`\n\n## Animation UI\n\nInstall with \n`pip install stability-sdk[anim_ui]`\n\nThen run with \n`python3 -m stability_sdk animate --gui`\n\n## SDK Usage\n\nBe sure to check out [Platform](https://platform.stability.ai) for comprehensive documentation on how to interact with our API.\n\n## Command line usage\n\n```\nusage: python -m stability_sdk generate [-h] [--height HEIGHT] [--width WIDTH] \n                [--start_schedule START_SCHEDULE] [--end_schedule END_SCHEDULE] \n                [--cfg_scale CFG_SCALE] [--sampler SAMPLER] [--steps STEPS] \n                [--style_preset STYLE_PRESET] [--seed SEED] [--prefix PREFIX] [--engine ENGINE]\n                [--num_samples NUM_SAMPLES] [--artifact_types ARTIFACT_TYPES]\n                [--no-store] [--show] [--init_image ",
    "url": "https://github.com/Stability-AI/stability-sdk",
    "last_updated": "2025-08-31T22:48:37+00:00"
  },
  {
    "full_name": "toddwschneider/nyc-taxi-data",
    "name": "nyc-taxi-data",
    "description": "Import public NYC taxi and for-hire vehicle (Uber, Lyft) trip data into a PostgreSQL or ClickHouse database",
    "language": "R",
    "topics": [
      "clickhouse",
      "nyc",
      "nyc-taxi-dataset",
      "postgresql"
    ],
    "readme": "# New York City Taxi and For-Hire Vehicle Data\n\nScripts to download, process, and analyze data from 3+ billion taxi and for-hire vehicle (Uber, Lyft, etc.) trips originating in New York City since 2009. There are separate sets of scripts for storing data in either a [PostgreSQL](https://www.postgresql.org/) or [ClickHouse](https://clickhouse.com/) database.\n\nMost of the [raw data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) comes from the NYC Taxi & Limousine Commission.\n\nThe repo was created originally in support of this post: [Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a Vengeance](https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/)\n\n## TLC 2022 Parquet Format Update\n\nThe TLC changed the raw data format from CSV to Apache Parquet in May 2022, including a full replacement of all historical files. This repo is now updated to handle the Parquet files in one of two ways:\n\n1. The \"old\" Postgres-based code still works, by adding an intermediate step that converts each Parquet file into a CSV before using the Postgres `COPY` command\n2. A [separate set of scripts](https://github.com/toddwschneider/nyc-taxi-data/tree/master/clickhouse) loads the Parquet files directly into a ClickHouse database\n\nAs part of the May 2022 update, the TLC added several new columns to the High Volume For-Hire Vehicle (Uber, Lyft) trip files, including information about passenger fares, driver pay, and time spent waiting for passengers. These new fields are available back to February 2019.\n\nThis repo no longer works with the old CSV files provided by the TLC. Those files are no longer available to download from the TLC's website, but if you happen to have them lying around and want to use this repo, you should look at [this older version of the code](https://github.com/toddwschneider/nyc-taxi-data/tree/2e805ab0f1bf362f890c6b6f227526c575f73b67) from before the Parquet file format change.\n\n## ClickHouse Instructions\n\nSee the",
    "url": "https://github.com/toddwschneider/nyc-taxi-data",
    "last_updated": "2025-08-30T12:45:51+00:00"
  },
  {
    "full_name": "gojiplus/tableqa",
    "name": "tableqa",
    "description": "Extract Useful Information From Tables for LLMs",
    "language": "Python",
    "topics": [],
    "readme": "# tableqa\n\n<tableqa> is a Python framework for **automatically extracting structured facts** (descriptive, comparative, temporal, and causal) from large tabular datasets. It converts raw columns and values into clear, human-readable statements and Q/A pairs, enabling rapid knowledge discovery, RAG corpus construction, and downstream training of language models.\n\n## 🎯 Objective\n\n* **Automate fact extraction** from rectangular (tabular) data without manual templating.\n* Support **descriptive** (univariate/bivariate), **comparative**, **temporal**, and **causal** analyses.\n* Output carefully worded facts suitable for reporting, dashboards, or fine‑tuning LLMs.\n\n## 🛠️ Problem Statement\n\nGiven a tabular dataset, current LLM‑driven approaches include:\n\n1. **Row embeddings** – embedding each row as a monolithic chunk (inefficient and loses structure).\n2. **NL→SQL** – generating queries from natural language (requires schema familiarity, starts from zero knowledge).\n3. **Existing RAG** – indexing raw rows/columns without domain insight.\n\n**Goal:** introduce a new pipeline that injects structured analyses and domain‑aware facts into a Retrieval‑Augmented Generation system, enabling direct, high‑quality querying of tabular data.\n\n## 🧩 Approach\n\nWe break the pipeline into three clear phases:\n\n1. **Metadata Understanding**\n\n   * Parse the codebook or schema to map variable codes to labels and capture type, missingness, and experimental notes.\n   * Use prior domain knowledge or an LLM to verify and enrich metadata interpretations.\n\n2. **Insight Extraction**\n\n   * **Descriptive**: run univariate (mean, median, mode), bivariate (correlation, group means), and temporal trend analyses; handle categorical, numeric, missing, and experimental variables appropriately.\n   * **Causal Proxies**: fit regression models with optional controls to surface associations in causal language.\n   * **Interpretation**: transform numbers into careful natural‑language statements with statistical context",
    "url": "https://github.com/gojiplus/tableqa",
    "last_updated": "2025-05-10T07:32:16+00:00"
  },
  {
    "full_name": "browserless/browserless",
    "name": "browserless",
    "description": "Deploy headless browsers in Docker. Run on our cloud or bring your own. Free for non-commercial uses.",
    "language": "TypeScript",
    "topics": [
      "docker",
      "chrome",
      "puppeteer",
      "browserless",
      "firefox",
      "nodejs",
      "playwright",
      "typescript",
      "webkit",
      "websocket"
    ],
    "readme": "<!-- markdownlint-disable commands-show-output first-line-h1 no-emphasis-as-heading no-inline-html -->\n\n<div align=\"center\">\n  <a href=\"https://browserless.io\" align=\"center\">\n    <center align=\"center\">\n      <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/logo-white.svg\" width=\"600\">\n        <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/logo.svg\" width=\"600\">\n        <img src=\"./assets/logo.svg\" alt=\"Browserless logo\" width=\"600\">\n      </picture>\n    </center>\n  </a>\n  <br>\n  <h3 align=\"center\"><center>Deploy headless browsers in Docker. Run on our cloud or bring your own.</center></h3>\n  <br>\n  <center>\n    <a href=\"https://hub.docker.com/r/browserless/chrome\">\n      <img src=\"https://img.shields.io/docker/pulls/browserless/chrome\" alt=\"Docker pulls\" />\n    </a>\n    <a href=\"https://github.com/browserless/browserless/tags\">\n      <img src=\"https://img.shields.io/github/package-json/v/browserless/chrome\" alt=\"Version\" />\n    </a>\n  </center>\n  <br>\n  <center>\n    <img src=\"https://github.com/browserless/chrome/actions/workflows/docker-chromium.yml/badge.svg\" alt=\"Chromium build\" />\n    <img src=\"https://github.com/browserless/chrome/actions/workflows/docker-firefox.yml/badge.svg\" alt=\"Firefox build\" />\n    <img src=\"https://github.com/browserless/chrome/actions/workflows/docker-webkit.yml/badge.svg\" alt=\"Webkit build\" />\n    <img src=\"https://github.com/browserless/chrome/actions/workflows/docker-edge.yml/badge.svg\" alt=\"Edge build\" />\n    <img src=\"https://github.com/browserless/chrome/actions/workflows/docker-multi.yml/badge.svg\" alt=\"Mulltibrowser build\" />\n  </center>\n  <br>\n  <hr>\n  <br>\n</div>\n\n> [!NOTE]  \n> Looking to bypass bot detectors and solve captchas? [We would recommend using BrowserQL as our stealthiest option](https://www.browserless.io/feature/browserql).\n\nBrowserless allows remote clients to connect and execute headless work, all inside of docker. It supports the standard, unforked Puppeteer and ",
    "url": "https://github.com/browserless/browserless",
    "last_updated": "2025-09-02T06:13:17+00:00"
  },
  {
    "full_name": "TigreGotico/jurebes",
    "name": "jurebes",
    "description": "intent engine",
    "language": "Python",
    "topics": [],
    "readme": "# jurebes\n\nJ.U.R.E.B.E.S: Joint Universal Rule-based Engine and Bagging Ensemble-based System\n\nThis acronym reflects a combined approach of using rule-based techniques along with a bagging ensemble-based approach for intent parsing in the JUREBES engine, written in Python with the use of NLTK and scikit-learn libraries.\n\n\n## Usage\n\n```python\nfrom jurebes import JurebesIntentContainer\n\n\nhello = [\"hello human\", \"hello there\", \"hey\", \"hello\", \"hi\"]\nname = [\"my name is {name}\", \"call me {name}\", \"I am {name}\",\n        \"the name is {name}\", \"{name} is my name\", \"{name} is my name\"]\njoke = [\"tell me a joke\", \"say a joke\", \"tell joke\"]\n\n\nengine = JurebesIntentContainer()\n\nengine.add_entity(\"name\", [\"jarbas\", \"bob\", \"João Casimiro Ferreira\"])\nengine.add_intent(\"hello\", hello)\nengine.add_intent(\"name\", name)\nengine.add_intent(\"joke\", joke)\n\nengine.train()\n\ntest_set = {\"name\": [\"I am groot\", \"my name is jarbas\",\n                     \"jarbas is the name\", \"they call me Ana Ferreira\"],\n            \"hello\": [\"hello beautiful\", \"hello bob\", \"hello world\"],\n            \"joke\": [\"say joke\", \"make me laugh\", \"do you know any joke\"]}\n\nfor intent, sents in test_set.items():\n    for sent in sents:\n        # optionally pass a threshold to return None instead of low confidence matches\n        print(sent, engine.calc_intent(sent, threshold=0.5))\n\n# I am groot IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'groot'})\n# my name is jarbas IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'jarbas'})\n# jarbas is the name IntentMatch(intent_name='name', confidence=0.9201351734080562, entities={'name': 'jarbas'})\n# call me Ana Ferreira IntentMatch(intent_name='name', confidence=1.0, entities={'name': 'ferreira'})\n# hello beautiful IntentMatch(intent_name='hello', confidence=0.8716522106345048, entities={})\n# hello bob IntentMatch(intent_name='hello', confidence=0.5400801051648911, entities={'name': 'bob'})\n# hello world IntentMatch(intent_name='hello', confidenc",
    "url": "https://github.com/TigreGotico/jurebes",
    "last_updated": "2025-04-17T19:49:55+00:00"
  },
  {
    "full_name": "clinicalml/embeddings",
    "name": "embeddings",
    "description": "Code for AMIA CRI 2016 paper \"Learning Low-Dimensional Representations of Medical Concepts\" ",
    "language": "Python",
    "topics": [],
    "readme": "# embeddings\nThis repository contains code accompanying publication of the paper: \n> Y. Choi, Y. Chiu, D. Sontag. [Learning Low-Dimensional Representations of Medical Concepts](http://cs.nyu.edu/~dsontag/papers/ChoiChiuSontag_AMIA_CRI16.pdf). To appear in Proceedings of the AMIA Summit on Clinical Research Informatics (CRI), 2016.\n\nIn the base directory there are three files containing the two best 300-dimensional embeddings learned in the paper, and the embeddings used in the previous work which we compared to:\n* `claims_codes_hs_300.txt.gz`: Embeddings of ICD-9 diagnosis and procedure codes, NDC medication codes, and LOINC laboratory codes, derived from a large claims dataset from 2005 to 2013 for roughly 4 million people.\n* `stanford_cuis_svd_300.txt.gz`: Embeddings of [UMLS](https://www.nlm.nih.gov/research/umls/) concept unique identifiers (CUIs), derived from 20 million clinical notes spanning 19 years of data from Stanford Hospital and Clinics, using a  [data set](http://datadryad.org/resource/doi:10.5061/dryad.jp917) released in a [paper](http://www.nature.com/articles/sdata201432) by Finlayson, LePendu & Shah.\n* `DeVine_etal_200.txt.gz`: Embeddings of UMLS CUIs learned by [De Vine et al. CIKM '14](http://dl.acm.org/citation.cfm?id=2661974), derived from 348,566 medical journal abstracts (courtesy of the authors).\n\nIn the `eval` directory there are three files of interest:\n* [`eval/Embedding_Evaluation.ipynb`](https://github.com/clinicalml/embeddings/blob/master/eval/Embedding_Evaluation.ipynb), an iPython notebook which reproduces the main results of the paper. If you come up with your own embeddings, you can use this benchmark to quantitatively compare them to our embeddings.\n* `eval/visualize_claims_embeddings.py` a Python program you can run which will allow you to look at nearest neighbors for the `claims_codes_hs_300.txt` embeddings (after decompressing the file using `gunzip`).\n* `eval/visualize_stanford_embeddings.py`, same as above but for the `stan",
    "url": "https://github.com/clinicalml/embeddings",
    "last_updated": "2025-06-13T16:32:43+00:00"
  },
  {
    "full_name": "oTree-org/oTree",
    "name": "oTree",
    "description": "Python framework for multiplayer decision games, behavioral experiments, and surveys",
    "language": "Python",
    "topics": [],
    "readme": "",
    "url": "https://github.com/oTree-org/oTree",
    "last_updated": "2025-08-27T07:28:50+00:00"
  },
  {
    "full_name": "tito/telenium",
    "name": "telenium",
    "description": "Automation for Kivy Application",
    "language": "Python",
    "topics": [
      "unittest",
      "selenium",
      "ide",
      "python",
      "kivy",
      "tests",
      "jsonrpc",
      "xpath"
    ],
    "readme": "# Telenium\n\nTelenium provide a framework to remote tests or automation Kivy-based application:\n\n- Selector support using XPATH-like syntax (`//BoxLayout[0]/Button[@text~=\"Close\"]`)\n- Create selector by touching the UI\n- Query or set attribute on any widgets\n- Execute remote code\n- `unittests` support\n- Integrate as a Kivy modules\n- Web IDE\n- Python 2 and 3 (since version 0.4, using json-rpc)\n\n![Telenium IDE](https://cloud.githubusercontent.com/assets/37904/22790912/f44b8166-eee7-11e6-9a78-120f78bde220.png)\n\n![Telenium IDE export](https://cloud.githubusercontent.com/assets/37904/22791059/70fb6988-eee8-11e6-91f4-0b87af33b5b6.png)\n\n# Installation\n\n```\npip install telenium\n```\n\n# Run the Telenium IDE\n\nIt will start a webserver on http://127.0.0.1:8080/ and automatically open a new\ntab in your favorite webbrowser. You'll be able to configure where your main.py\nis, and start writing tests directly:\n\n```\ntelenium\n```\n\nYou can also edit telenium-json:\n\n```\ntelenium tests/test-ui-myfeature.json\n```\n\n# Run the application with telenium module\n\nIf you don't use the IDE, in order to remote control your application,\nyou need the Telenium client installed within your application.\n\n## Method 1: Run your application with telenium client\n\nTelenium can execute your application and manually add telenium_client to it.\nJust do:\n\n```python\npython -m telenium.execute main.py\n```\n\n## Method 2: Import and install telenium client in your application\n\nTelenium client can be imported and installed at the start of your application:\n\nIf telenium is fully available:\n```python\nfrom telenium import install\ninstall()\n```\n\nIf you only included `telenium_client.py` module:\n```python\nfrom telenium_client import install\ninstall()\n```\n\n## Method 3: Add telenium_client as a Kivy modules into your application\n\nJust copy/paste `mods/telenium_client.py` in your application, then before\nrunning your application, initialize it:\n\n```python\nfrom os.path import dirname\nfrom kivy.modules import Modules\nfrom kivy.c",
    "url": "https://github.com/tito/telenium",
    "last_updated": "2024-11-13T18:37:50+00:00"
  },
  {
    "full_name": "eHarmony/face-parts-service",
    "name": "face-parts-service",
    "description": "A RESTful web service for detecting faces, pose, and fiducial markers from an image",
    "language": "C++",
    "topics": [],
    "readme": "### Status\n[![Build Status](https://travis-ci.org/eHarmony/face-parts-service.svg?branch=master)](https://travis-ci.org/eHarmony/face-parts-service)\n\n# Executive Summary\n\nThis is a RESTful API for segmenting human faces from an image.  The software is powered by [face parts](http://www.ics.uci.edu/~xzhu/face/) using [CImg](http://cimg.sourceforge.net/) for image loading.  The goal of this project is threefold:\n\n1.  Remove the dependency on Matlab\n2.  Make the code more usable by wrapping it in a RESTful API.  This makes use of a modified version of [QtWebApp](http://stefanfrings.de/qtwebapp/index-en.html)\n3.  Speed up the code by making use of [Threaded Building Blocks](https://www.threadingbuildingblocks.org/)\n\nHere is an example of the output\n\n![Submitting an image with Postman](/images/regular.jpg)\n\n# Mac Necessary Libraries (Tested on OSX 10.9.4)\n\n1.  Install XCode developer tools and homebrew\n2.  brew install libjpeg-turbo\n3.  brew install tbb\n4.  Install MacPorts from [here](https://www.macports.org/install.php)\n5.  sudo port install atlas +clang+nofortran (might take a while)\n6.  Download and install Qt 5.2.1 from [here](http://qt-project.org/downloads)\n\n# Red Hat Necessary Libraries (Tested on Centos 6.5)\n\n1.  sudo yum install atlas-devel.x86_64\n2.  sudo yum install libjpeg-devel\n3.  sudo yum install tbb-devel\n4.  sudo yum install qt5-qtbase-devel\n\n# Ubuntu Necessary Libraries (Tested on Ubuntu 14.04)\n\n1.  sudo apt-get install libatlas-base-dev\n2.  sudo apt-get install libjpeg-dev\n3.  sudo apt-get install libtbb-dev\n4.  sudo apt-get install qt5-default\n5.  sudo apt-get install maven\n\n(for running the code, don't install the devel versions, just use the regular versions)\n\n# Make Instructions\nWe are using maven as a build architecture.  I have tested this using [maven version 3.0.5](http://maven.apache.org/download.cgi).\n\n1.  Add qmake to your path\n2.  mvn install\n\n# Running the Server\n\nIn order to start the webserver run `target/classes/face-parts-service src",
    "url": "https://github.com/eHarmony/face-parts-service",
    "last_updated": "2023-03-15T08:10:06+00:00"
  },
  {
    "full_name": "facebookresearch/many-to-many-dijkstra",
    "name": "many-to-many-dijkstra",
    "description": "A predictive model developed to identify medium-voltage electrical distribution grid infrastructure using publicly available data sources.",
    "language": "Python",
    "topics": [],
    "readme": "# Pathfinder\n\nA many-to-many variant of the Dijkstra's shortest distance algorithm.\n\nOriginally developed to predict the locations of medium-voltage electrical distribution grid infrastructure using publicly available data sources.\n\nHow pathfinder works is described in the Model Tutorial in this repo.\nNotes on implementation and operation are embedded as comments within the code.\n\n## License\nPathfinder is MIT licensed, as found in the LICENSE file.\n",
    "url": "https://github.com/facebookresearch/many-to-many-dijkstra",
    "last_updated": "2025-09-01T00:33:45+00:00"
  },
  {
    "full_name": "GoogleCloudPlatform/python-docs-samples",
    "name": "python-docs-samples",
    "description": "Code samples used on cloud.google.com",
    "language": "Jupyter Notebook",
    "topics": [
      "python",
      "samples"
    ],
    "readme": "# Google Cloud Platform Python Samples\n\nPython samples for [Google Cloud Platform products][cloud].\n\n[![Build Status][py-2.7-shield]][py-2.7-link] [![Build Status][py-3.9-shield]][py-3.9-link] [![Build Status][py-3.10-shield]][py-3.10-link] [![Build Status][py-3.11-shield]][py-3.11-link] [![Build Status][py-3.12-shield]][py-3.12-link] [![Build Status][py-3.13-shield]][py-3.13-link]\n\n## Google Cloud Samples\n\nCheck out some of the samples found on this repository on the [Google Cloud Samples](https://cloud.google.com/docs/samples?l=python) page.\n\n## Setup\n\n1. Install [`pip` and `virtualenv`][cloud_python_setup] if you do not already have them.\n\n1. Clone this repository:\n\n    ```\n    git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git\n    ```\n\n1. Obtain authentication credentials.\n\n    Create local credentials by running the following command and following the\n    oauth2 flow (read more about the command [here][auth_command]):\n\n    ```\n    gcloud auth application-default login\n    ```\n\n    Read more about [Google Cloud Platform Authentication][gcp_auth].\n\n## How to run a sample\n\n1. Change directory to one of the sample folders, e.g. `logging/cloud-client`:\n\n    ```\n    cd logging/cloud-client/\n    ```\n\n1. Create a virtualenv. Samples are compatible with Python 3.6+.\n\n    ```\n    python3 -m venv env\n    source env/bin/activate\n    ```\n\n1. Install the dependencies needed to run the samples.\n\n    ```\n    pip install -r requirements.txt\n    ```\n\n1. Run the sample:\n\n    ```\n    python snippets.py\n    ```\n\n## Contributing\n\nContributions welcome! See the [Contributing Guide](CONTRIBUTING.md).\n\n[slack_badge]: https://img.shields.io/badge/slack-Google%20Cloud%20Platform-E01563.svg\t\n[slack_link]: https://googlecloud-community.slack.com/\n[cloud]: https://cloud.google.com/\n[cloud_python_setup]: https://cloud.google.com/python/setup\n[auth_command]: https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login\n[gcp_auth]: https://cloud.google",
    "url": "https://github.com/GoogleCloudPlatform/python-docs-samples",
    "last_updated": "2025-09-01T13:49:44+00:00"
  },
  {
    "full_name": "donnemartin/data-science-ipython-notebooks",
    "name": "data-science-ipython-notebooks",
    "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.",
    "language": "Python",
    "topics": [
      "python",
      "machine-learning",
      "deep-learning",
      "data-science",
      "big-data",
      "aws",
      "tensorflow",
      "theano",
      "caffe",
      "scikit-learn",
      "kaggle",
      "spark",
      "mapreduce",
      "hadoop",
      "matplotlib",
      "pandas",
      "numpy",
      "scipy",
      "keras"
    ],
    "readme": "<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif\">\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png\">\n  <br/>\n</p>\n\n# data-science-ipython-notebooks\n\n## Index\n\n* [deep-learning](#deep-learning)\n    * [tensorflow](#tensor-flow-tutorials)\n    * [theano](#theano-tutorials)\n    * [keras](#keras-tutorials)\n    * [caffe](#deep-learning-misc)\n* [scikit-learn](#scikit-learn)\n* [statistical-inference-scipy](#statistical-inference-scipy)\n* [pandas](#pandas)\n* [matplotlib](#matplotlib)\n* [numpy](#numpy)\n* [python-data](#python-data)\n* [kaggle-and-business-analyses](#kaggle-and-business-analyses)\n* [spark](#spark)\n* [mapreduce-python](#mapreduce-python)\n* [amazon web services](#aws)\n* [command lines](#commands)\n* [misc](#misc)\n* [notebook-installation](#notebook-installation)\n* [credits](#credits)\n* [contributing](#contributing)\n* [contact-info](#contact-info)\n* [license](#license)\n\n<br/>\n<p align=\"center\">\n  <img src=\"http://i.imgur.com/ZhKXrKZ.png\">\n</p>\n\n## deep-learning\n\nIPython Notebook(s) demonstrating deep learning functionality.\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://avatars0.githubusercontent.com/u/15658638?v=3&s=100\">\n</p>\n\n### tensor-flow-tutorials\n\nAdditional TensorFlow tutorials:\n\n* [pkmital/tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)\n* [nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n* [alrojo/tensorflow-tutorial](https://github.com/alrojo/tensorflow-tutorial)\n* [BinRoot/TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)\n* [tuanavu/tensorflow-basic-tutorials](https://github.com/tuanavu/tensorflow-basic-tutorials)\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------",
    "url": "https://github.com/donnemartin/data-science-ipython-notebooks",
    "last_updated": "2025-09-02T09:30:39+00:00"
  },
  {
    "full_name": "the-full-stack/website",
    "name": "website",
    "description": "Source for https://fullstackdeeplearning.com",
    "language": "HTML",
    "topics": [],
    "readme": "# The Full Stack Website\n\nThis website came online in January 2021.\n\nIt uses `mkdocs`, which you can set up with `make setup`.\n\nTo develop locally, run `make serve` and edit the files.\n\nTo deploy, push `main` branch to github and it will deploy via github action, or manually run `make deploy`.\n\n## Processing lecture notes\n\n### 2023\n\nSee the README file in [`lecture-notes-creator`](https://github.com/the-full-stack/website/tree/main/lecture-notes-creator) for instructions.\n\n### 2022\n\nDownload Google Doc as `input.docx`, then run:\n\n```\npandoc --extract-media=. input.docx -o output.md\ncat output.md | sed 's/^#/##/' | sed 's/^ *> //g' | sed s'/{.underline}//g' | sed 's/\\[\\[/[/g' | sed 's/\\]\\]/]/g' | sed 's/{width=.*}//g' | sed 's/{width=.*\"$//' | sed 's/^height=.*\"}//' > output.md\n```\n",
    "url": "https://github.com/the-full-stack/website",
    "last_updated": "2025-09-01T16:59:21+00:00"
  },
  {
    "full_name": "chaconnewu/free-data-science-books",
    "name": "free-data-science-books",
    "description": "Free resources for learning data science",
    "language": "",
    "topics": [],
    "readme": "List of Data Science/Big Data Resources\n======================\nThis list contains free learning resources for data science and big data related concepts, techniques, and applications. Inspired by [Free Programming Books](https://github.com/vhf/free-programming-books).\n\nEach entry provides the expected audience for the certain book (beginner, intermediate, or veteran). It may be subjective, but it provides some clue of how difficult the book is.\n\n\n### How To Contribute\n- Fork\n- Edit, and add your recommendations (for beginner, intermediate, or veteran) \n- Send a Pull Request\n\n\n### Index\n* [Data Science Introduction](#data-science-introduction)\n* [Data Processing](#big-data-processing)\n* [Data Analysis](#big-data-analysis)\n\t* [Fundamentals](#fundamentals)\n\t* [Network Analysis](#network-analysis)\n\t* [Statistics](#statistics)\n\t* [Data Mining](#data-mining)\n\t* [Machine Learning](#machine-learning)\n* [Data Science Application](#big-data-application)\n\t* [Data Visualization](#data-visualization)\n* [Uncategorized](#uncategorized)\n* [MOOCs about Data Science](#moocs)\t\n\n### Data Science Introduction\n* [Data Science: An Introduction](http://en.wikibooks.org/wiki/Data_Science:_An_Introduction) - Wikibook - `Beginner`\n* [Disruptive Possibilities: How Big Data Changes Everything](http://www.amazon.com/Disruptive-Possibilities-Data-Changes-Everything-ebook/dp/B00CLH387W) - Jeffrey Needham - `Beginner`\n* [Introduction to Data Science](http://jsresearch.net/) - Jeffery Stanton - `Beginner`\n* [Real-Time Big Data Analytics: Emerging Architecture](http://www.amazon.com/Real-Time-Big-Data-Analytics-Architecture-ebook/dp/B00DO33RSW) - Mike Barlow - `Beginner`\n* [The Evolution of Data Products](http://www.amazon.com/The-Evolution-Data-Products-ebook/dp/B005QEKQUY/ref=sr_1_63?s=digital-text&ie=UTF8&qid=1351898530&sr=1-63) - Mike Loukides - `Beginner`\n* [The Promise and Peril of Big Data](http://www.aspeninstitute.org/sites/default/files/content/docs/pubs/The_Promise_and_Peril_of_Big_Data.pd",
    "url": "https://github.com/chaconnewu/free-data-science-books",
    "last_updated": "2025-08-30T05:23:02+00:00"
  },
  {
    "full_name": "udacity/CarND-Traffic-Sign-Classifier-Project",
    "name": "CarND-Traffic-Sign-Classifier-Project",
    "description": "Classify Traffic Signs.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## Project: Build a Traffic Sign Recognition Program\n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\nOverview\n---\nIn this project, you will use what you've learned about deep neural networks and convolutional neural networks to classify traffic signs. You will train and validate a model so it can classify traffic sign images using the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset). After the model is trained, you will then try out your model on images of German traffic signs that you find on the web.\n\nWe have included an Ipython notebook that contains further instructions \nand starter code. Be sure to download the [Ipython notebook](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb). \n\nWe also want you to create a detailed writeup of the project. Check out the [writeup template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document.\n\nTo meet specifications, the project will require submitting three files: \n* the Ipython notebook with the code\n* the code exported as an html file\n* a writeup report either as a markdown or pdf file \n\nCreating a Great Writeup\n---\nA great writeup should include the [rubric points](https://review.udacity.com/#!/rubrics/481/view) as well as your description of how you addressed each point.  You should include a detailed description of the code used in each step (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples.  \n\nAll that said, please be concise!  We're not looking for you to write a book here, just a br",
    "url": "https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project",
    "last_updated": "2025-06-09T03:31:59+00:00"
  },
  {
    "full_name": "furkanbiten/GoodNews",
    "name": "GoodNews",
    "description": "Good News Everyone! - CVPR 2019 ",
    "language": "Python",
    "topics": [],
    "readme": "This is the code for a CVPR 2019 paper, called \n[GoodNews Everyone! Context driven entity aware captioning for news images](https://arxiv.org/abs/1904.01475). Enjoy!\n\nModel preview:\n\n![GoodNews Model!](https://github.com/furkanbiten/GoodNews/blob/master/model.jpg)\n\nHuge Thanks goes to [New York Times API](https://developer.nytimes.com/indexV2.html) for providing such a service for FREE!\n\nAnother Thanks to [@ruotianluo](https://github.com/ruotianluo) for providing the captioning code.\n\nDependencies/Requirements:\n```text\npytorch==1.0.0\nspacy==2.0.11\nh5py==2.7.0\nbs4==4.5.3\njoblib==0.12.2\nnltk==3.2.3\ntqdm==4.19.5\nurllib2==2.7\ngoose==1.0.25\nurlparse\nunidecode\n```\n\n# Introduction  \nWe took the first steps to move the captioning systems to interpretation (see the paper for more detail). \nTo this end, we have used [New York Times API](https://developer.nytimes.com/indexV2.html) \nto retrieve the articles, images and captions. \n\nThe structure of this repo is as follows:\n1. Getting the data \n2. Cleaning and formating the data\n3. How to train models\n\n# Get the data\nYou have 3 options to get the data. \n\n## Images only\nIf you want to download the images only and directly start working on the same dataset as ours, \nthen download the cleaned version of the dataset without images: \n[article+caption.json](https://cvcuab-my.sharepoint.com/:u:/g/personal/abiten_cvc_uab_cat/ERuh81o1cnJJoKne8hOe3MYBcFlTglof3vxCcfbyXa7z-w?e=jtQU0Z) \nand put it to data/ folder and \ndownload the [img_urls.json](https://cvcuab-my.sharepoint.com/:u:/g/personal/abiten_cvc_uab_cat/Ef0W_O-HU59Pn22b9Bni5oABLDtkBUFFMXiN5cl2vnQxFg?e=75fpWW)\nand put it in the `get_data/get_images_only/` folder.\n\nThen run  \n```bash\npython get_images.py --num_thread 16\n```\nThen, you will get the images. After that move to ``Clean and Format Data`` section.\n\nPS: I have recieved numerous emails regarding some of the images not present/broken in the img_urls.json. Which is why I decided to put the images on the drive to download in the n",
    "url": "https://github.com/furkanbiten/GoodNews",
    "last_updated": "2024-11-21T01:46:52+00:00"
  },
  {
    "full_name": "stan-dev/rstan",
    "name": "rstan",
    "description": "RStan, the R interface to Stan",
    "language": "R",
    "topics": [
      "stan",
      "mcmc",
      "bayesian-inference",
      "bayesian-data-analysis",
      "bayesian-statistics",
      "r",
      "r-package"
    ],
    "readme": "# RStan <img src=\"rstan/rstan/man/figures/stanlogo.png\" align=\"right\" width=\"120\" />\n\n<!-- badges: start -->\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/rstan?color=blue)](https://cran.r-project.org/package=rstan)\n[![Downloads](https://cranlogs.r-pkg.org/badges/rstan?color=blue)](https://cran.rstudio.com/package=rstan)\n[![Research software impact](http://depsy.org/api/package/cran/rstan/badge.svg)](https://depsy.org/package/r/rstan)\n[![R-CMD-check](https://github.com/stan-dev/rstan/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/rstan/actions)\n<!-- badges: end -->\n\n**RStan** is the R interface to [Stan](https://mc-stan.org).\n\n### Quick links\n\n* [mc-stan.org/rstan](https://mc-stan.org/rstan/) (online RStan documentation, vignettes)\n* [Stan documentation](https://mc-stan.org/users/documentation/) (language manual, case studies, and more)\n* [Ask a question](https://discourse.mc-stan.org) (Stan Forums on Discourse)\n* [Open an issue](https://github.com/stan-dev/rstan/issues) (GitHub issues for bug reports, feature requests)\n\n\n### Getting Started\n\nFor installation instructions and other tips on getting started with RStan see\n\n* [RStan Getting Started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)\n\nSeveral Stan users have also contributed translations of the _RStan Getting Started_ page:\n\n* [RStan Getting Started (French)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Français))\n* [RStan Getting Started (Japanese)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Japanese))\n* [RStan Getting Started (繁體中文)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87))\n* [RStan Getting Started (Portuguese)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Portugu%C3%AAs))\n\n### Source Repository\n\nRStan's source code repository is hosted here on GitHub. Stan's source repository is defined as a submodule. \nSee [how to work with stan submodule in rstan ",
    "url": "https://github.com/stan-dev/rstan",
    "last_updated": "2025-08-27T12:04:53+00:00"
  },
  {
    "full_name": "abresler/realtR",
    "name": "realtR",
    "description": "Real estate brokers are the....",
    "language": "R",
    "topics": [
      "broker",
      "r",
      "real-estate"
    ],
    "readme": "realtR\n================\n\n## Installation\n\n``` r\ndevtools::install_github(\"hadley/emo\")\ndevtools::install_github(\"abresler/realtR\")\n```\n\n### realtR\n\nThere are few service industries worse than residential real estate\nbrokerage industry.\n\nIndustry actors are some of the most overpaid, dishonest, people you\nwill encounter.\n\nThey hoard information, lie, are rarely held accountable for anything\nthey do. They are a well organized political cartel who ensure\nregardless of what happens, they get paid a sizable chunk of the overall\nproceeds from a transaction.\n\nThe consumer has a hard time educating themselves and can be at the\nmercy of these snake-oil salesmen.\n\nThe time has come to change for and `realtR` goes a **long** way towards\ndoing this.\n\nAnyone with a bit of R skills now has easy functional access to property\ninformation for every location in the United States.\n\nWith a few lines of R code you will have access to at-least as much, and\nin most cases, <strong>MORE</strong> information than brokers.\n\nYou can take a look at a bit of what the package does in\n<a href=\"https://asbcllc.com/r_packages/realtR/2018/introduction/index.html\" title=\"intro\" target=\"_blank\">this\nintroductory tutorial</a>.\n\n## Functions\n\n  - `geocode()` : Batch geocoder\n  - `listing_counts()` : Area summary listings\n  - `listings()` : Area detailed listings\n  - `vitality()` : Area market vitality\n  - `properties_near()` : Listings near a location\n  - `map_listings()` : Featured listings from a map\n  - `median_prices()` : Area median prices\n  - `parse_listing_urls()` : Detailed property listing information\n  - `mortgage_rates()` : Rolling 30 day mortgage rates by product\n\n## Dictionaries\n\n  - `dictionary_property_types()` : Search-able property types\n  - `dictionary_listing_features()` : Search-able property features\n\n## Fun Tools\n\n  - `summarise_broker_bullshit()` : Summarizes a long-winded broker\n    property description into `n` sentences using the\n    [PageRank](https://en.wikipedia.org/wiki/Page",
    "url": "https://github.com/abresler/realtR",
    "last_updated": "2025-04-16T02:08:36+00:00"
  },
  {
    "full_name": "hrbrmstr/bom",
    "name": "bom",
    "description": "Tools to Identify and Work with Byte Order Marks in R",
    "language": "C++",
    "topics": [
      "r",
      "rstats",
      "byte-order-mark",
      "bom"
    ],
    "readme": "\n`bom` : Tools to Identify and Work with Byte Order Marks\n\nByte order marks (BOM) appear at the beginning of a file or buffer and provide information about the encoding of the contents. R provides facilities to work with files and connections with BOMs but there are situatons where these facilities are not sufficient. Tools are provided to identify the presence and type of byte order marks in files and R objects as well as remove them.\n\nThe following functions are implemented:\n\n-   `file_bom_type`: Get BOM type (file)\n-   `file_has_bom`: Test if a file has a BOM\n-   `raw_bom_type`: Get BOM type (raw vector)\n-   `raw_has_bom`: Test if a raw vector has a BOM\n\n### TODO\n\n-   \\[ \\] Convert to S3 methods\n-   \\[ \\] BOM removal functions\n-   \\[ \\] Mechanism to return a `connection` sans BOM or identify there is a BOM\n\n### Installation\n\n``` r\ndevtools::install_git(\"https://gitlab.com/hrbrmstr/bom.git\")\n```\n\n``` r\noptions(width=120)\n```\n\nThere are some basic examples in the [Usage](#Usage) section, but this may be a better illustration. Say you have a CSV file:\n\n``` r\nfil <- system.file(\"examples\", \"stop_times.txt\", package=\"bom\")\n```\n\nAnd, say you want to read it in with a more modern CSV reader:\n\n``` r\nlibrary(readr)\n\ndf <- read_csv(fil)\n```\n\n    ## Parsed with column specification:\n    ## cols(\n    ##   `﻿trip_id` = col_integer(),\n    ##   arrival_time = col_time(format = \"\"),\n    ##   departure_time = col_time(format = \"\"),\n    ##   stop_id = col_integer(),\n    ##   stop_sequence = col_integer(),\n    ##   pickup_type = col_integer(),\n    ##   drop_off_type = col_integer()\n    ## )\n\nLet's look at that file:\n\n``` r\nprint(df, n=1)\n```\n\n    ## # A tibble: 64,827 × 7\n    ##   `﻿trip_id` arrival_time departure_time stop_id stop_sequence pickup_type drop_off_type\n    ##       <int>       <time>         <time>   <int>         <int>       <int>         <int>\n    ## 1     50000   29700 secs     29700 secs  120001            41           1             0\n    ## # ... with 6.483e+04 m",
    "url": "https://github.com/hrbrmstr/bom",
    "last_updated": "2025-03-22T11:20:43+00:00"
  },
  {
    "full_name": "CausalML/RemovingHiddenConfounding",
    "name": "RemovingHiddenConfounding",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# RemovingHiddenConfounding\n\nThis repository contains the code for replicate the experiments from the paper \n> [Nathan Kallus, Aahlad Manas Puli, Uri Shalit. Removing Hidden Confounding by Experimental Grounding. NIPS 2018.](http://papers.nips.cc/paper/8286-removing-hidden-confounding-by-experimental-grounding)\n\n## Simulation study\n\nThe file simulation.ipynb contains the code necessary for replicating the results in Section 5.1.\n\n## Study of STAR dataset\n\nThe file STAR-CATE.ipynb contains the code necessary for replicating the results in Section 5.2. In particular, this uses data taken from [Tennessee's Student/Teacher Achievement Ratio (STAR) project](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/10766).\n\n",
    "url": "https://github.com/CausalML/RemovingHiddenConfounding",
    "last_updated": "2025-06-26T22:01:26+00:00"
  },
  {
    "full_name": "sods/neurips2014",
    "name": "neurips2014",
    "description": "Tools for managing the Conference Management Toolkit from Microsoft.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "\n# Managing Microsoft's Conference Management Toolkit with Python for NIPS 2014\n\n### 28th October 2014 Neil D. Lawrence\n\n#### Updated May 2021 with pip installable cmtutils library\n\nAs well as pandas and the standard numpy/scipy stack, the library has a dependency on the `cmtutils` module.\n\n```\npip install git+https://github.com/lawrennd/cmtutils.git\n```\n\nIn 2014 [Corinna Cortes](http://research.google.com/pubs/author121.html) and I\nwere NIPS program Co-Chairs. Alan Saul was our Program Manager. As part of the\nprocess we wrote a lot of scripts for processing the data. The scripts I wrote\nused the `IPython notebook` (now [Project Jupyter](http://jupyter.org/)) and\n`pandas`. It was always my intention to summarise this work in case others find\nit useful. It is also quite a good document for summarising what is involved in\nprogram chairing a major conference like NIPS.\n\nThis notebook gives a guide to the scripts and utilities used for managing the\n[conference management toolkit](http://cmt.research.microsoft.com/cmt/) (CMT).\nIt is based around a library, `cmtutils`, which manages the submissions.  For\nreviewer management (which was the first thing written) the scripts are based\naround a local mirror of the CMT user data base in SQLite. For review management\nwe moved things much more towards `pandas` and used CMT as the central\nrepository of reviews, exporting them on a daily basis.\n\nA lot of communication is required between CMT through imports and exports. Some\nof the links used for CMT exports are available [here](http://nbviewer.ipython.org/github/lawrennd/conference/blob/master/notebooks/Useful Links.ipynb).\nThe various tasks are structured in IPython notebooks below. The code used was\nfirst written for the NIPS 2014 conference, but ideas were based on experience\nfrom using CMT for AISTATS 2012 and some preliminary code written then (for\nexample for importing the XML formatted version of Excel that CMT uses).\n\nRight from the start it was felt that being able to imp",
    "url": "https://github.com/sods/neurips2014",
    "last_updated": "2024-09-11T19:35:37+00:00"
  },
  {
    "full_name": "ludwig-ai/ludwig",
    "name": "ludwig",
    "description": "Low-code framework for building custom LLMs, neural networks, and other AI models",
    "language": "Python",
    "topics": [
      "deep-learning",
      "deeplearning",
      "deep",
      "learning",
      "machine-learning",
      "machinelearning",
      "natural-language-processing",
      "natural-language",
      "computer-vision",
      "data-centric",
      "data-science",
      "pytorch",
      "neural-network",
      "ml",
      "llm",
      "llm-training",
      "fine-tuning",
      "llama",
      "mistral",
      "llama2"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://ludwig.ai\">\n    <img src=\"https://github.com/ludwig-ai/ludwig-docs/raw/master/docs/images/ludwig_hero_smaller.jpg\" height=\"150\">\n  </a>\n</p>\n\n<div align=\"center\">\n\n_Declarative deep learning framework built for scale and efficiency._\n\n[![PyPI version](https://badge.fury.io/py/ludwig.svg)](https://badge.fury.io/py/ludwig)\n[![Discord](https://dcbadge.vercel.app/api/server/CBgdrGnZjy?style=flat&theme=discord-inverted)](https://discord.gg/CBgdrGnZjy)\n[![DockerHub](https://img.shields.io/docker/pulls/ludwigai/ludwig.svg)](https://hub.docker.com/r/ludwigai)\n[![Downloads](https://pepy.tech/badge/ludwig)](https://pepy.tech/project/ludwig)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/ludwig-ai/ludwig/blob/master/LICENSE)\n[![X](https://img.shields.io/twitter/follow/ludwig_ai.svg?style=social&logo=twitter)](https://twitter.com/ludwig_ai)\n\n</div>\n\n> \\[!IMPORTANT\\]\n> Our community has moved to [Discord](https://discord.gg/CBgdrGnZjy) -- please join us there!\n\n# 📖 What is Ludwig?\n\nLudwig is a **low-code** framework for building **custom** AI models like **LLMs** and other deep neural networks.\n\nKey features:\n\n- 🛠 **Build custom models with ease:** a declarative YAML configuration file is all you need to train a state-of-the-art LLM on your data. Support for multi-task and multi-modality learning. Comprehensive config validation detects invalid parameter combinations and prevents runtime failures.\n- ⚡ **Optimized for scale and efficiency:** automatic batch size selection, distributed training ([DDP](https://pytorch.org/tutorials/beginner/ddp_series_theory.html), [DeepSpeed](https://github.com/microsoft/DeepSpeed)), parameter efficient fine-tuning ([PEFT](https://github.com/huggingface/peft)), 4-bit quantization (QLoRA), paged and 8-bit optimizers, and larger-than-memory datasets.\n- 📐 **Expert level control:** retain full control of your models down to the activation functions. Support for hyperpar",
    "url": "https://github.com/ludwig-ai/ludwig",
    "last_updated": "2025-09-01T16:20:59+00:00"
  },
  {
    "full_name": "atlanhq/rLandsat",
    "name": "rLandsat",
    "description": "R Package to make Landsat8 data accessible",
    "language": "R",
    "topics": [
      "r",
      "landsat8",
      "landsat-data",
      "espa",
      "satellite-imagery",
      "api-wrapper",
      "rstats"
    ],
    "readme": "# rLandsat <img src=\"https://i.imgur.com/btZP6vS.png\" align=\"right\" />\nAcquire Landsat 8 Data: R Package to make Landsat 8 data accessible and help unlock its mysteries.\n\n## Overview\n\nrLandsat makes it easy to search for Landsat8 product IDs, place an order on USGS-ESPA and download the data along with the meta information in the perfect format from R. Internally, it uses a combination of sat-api, espa-api and AWS S3 Landsat 8 metadata.\n\n<img src=\"https://i.imgur.com/cmjtegG.png\" align=\"centre\" />\n\n  - `landsat_search()`: search product IDs (and AWS/Google download links) based on time and geography\n  - `espa_product()`: get list of available products for product IDs\n  - `espa_order()`: place an order for product IDs on ESPA\n  - `espa_status()`: get the status of the order\n  - `landsat_download()`: download the Landsat 8 scenes using ESPA URLs\n \nTo run any of the functions starting with `espa_`, you need valid login credentials from [ESPA-LSRD](https://espa.cr.usgs.gov/) and you need to input them in your environment with `espa_creds(username, password)` for the functions to work properly.\n\n**You should also check the demo script** (which downloads all the Landsat 8 data for India for January 2018) in the demo folder, or run `demo(\"india_landsat\")` in R after loading this library.\n\n## Installation\n\n``` r\n# Install the latest dev version from GitHub:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"atlanhq/rLandsat\")\n\n# Load the library\nlibrary(rLandsat)\n```\n\nIf you encounter a bug, please file an issue with steps to reproduce it on [Github](https://github.com/atlanhq/rLandsat/issues). Please use the same for any feature requests, enhancements or suggestions.\n\n## Additional Details\n### About Landsat 8 ###\nLandsat 8 Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) images consist of nine spectral bands with a spatial resolution of 30 meters for Bands 1 to 7 and 9. The ultra blue Band 1 is useful for coastal and aerosol studies. Band 9 is useful fo",
    "url": "https://github.com/atlanhq/rLandsat",
    "last_updated": "2025-03-22T08:14:27+00:00"
  },
  {
    "full_name": "frnsys/broca",
    "name": "broca",
    "description": "rapid nlp prototyping",
    "language": "Python",
    "topics": [],
    "readme": "# Broca\n### Various useful NLP algos and utilities\n\nThere is some Python 2 support scattered throughout but the library has not been fully tested against it.\n\n**This library is in development -- APIs may change and features may be unstable.**\n\n\n## Overview\n\n`broca` is a NLP library for experimenting with various approaches. So everything in this library is somewhat experimental and meant for rapid prototyping of NLP methods.\n\nWhen I implement a new method, often from a paper or another source, I add it here so that it can be re-applied elsewhere.\nEventually I hope that `broca` can become a battery of experimental NLP methods which can easily be thrown at a new problem.\n\n`broca` is structured like so:\n\n- `common`: misc utilities and classes reused across the whole library. Also includes shared objects.\n- `distance`: for measuring string distance. This should probably be renamed though, since \"distance\" means a lot more than just string distance.\n- `tokenize`: various tokenization methods\n    - `keyword`: keyword-based tokenization methods (i.e. keyword extraction methods)\n- `vectorize`: various ways of representing documents as vectors\n- `similarity`: various ways of computing similarity\n    - `term`: for computing similarity between two terms\n    - `doc`: for computing similarity matrices for sets of documents\n- `preprocess`: for preprocessing text, i.e. cleaning\n- `knowledge`: tools for preparing or incorporating external knowledge sources, such as Wikipedia or IDF on auxiliary corpora\n- `pipeline`: for easily chaining `broca` classes into pipelines - useful for rapid prototyping\n\n\n## Installation\n\n`broca` is available through pypi, but the library is under active development, so it's recommended to install via git:\n\n    $ pip install git+ssh://git@github.com/ftzeng/broca.git\n\nOr, if adding to a `requirements.txt`, add the line:\n\n    git+ssh://git@github.com/ftzeng/broca.git\n\nIf developing, you can clone the repo and from within the repo directory, install via `pip",
    "url": "https://github.com/frnsys/broca",
    "last_updated": "2025-02-16T16:44:34+00:00"
  },
  {
    "full_name": "etai83/lstm_stock_prediction",
    "name": "lstm_stock_prediction",
    "description": "This is an LSTM stock prediction using Tensorflow with Keras on top.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "",
    "url": "https://github.com/etai83/lstm_stock_prediction",
    "last_updated": "2025-06-25T13:05:25+00:00"
  },
  {
    "full_name": "stephens999/ashr",
    "name": "ashr",
    "description": "An R package for adaptive shrinkage",
    "language": "R",
    "topics": [],
    "readme": "[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/ashr)](https://cran.r-project.org/package=ashr)\n[![Build Status](https://travis-ci.org/stephens999/ashr.svg)](https://travis-ci.org/stephens999/ashr)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/stephens999/ashr?branch=master&svg=true)](https://ci.appveyor.com/project/stephens999/ashr)\n[![Coverage Status](https://coveralls.io/repos/github/stephens999/ashr/badge.svg?branch=master)](https://coveralls.io/github/stephens999/ashr?branch=master)\n[![Coverage Status](https://img.shields.io/codecov/c/github/stephens999/ashr/master.svg)](https://codecov.io/github/stephens999/ashr?branch=master)\n\nThis repository contains an R package for performing \"Adaptive Shrinkage.\"\n\nTo install the ashr package first you need to install devtools:\n\n```R\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(\"stephens999/ashr\")\n```\n\n\n## Running Adaptive Shrinkage\n\nThe main function in the ashr package is `ash`. To get minimal help:\n\n```R\nlibrary(ashr)\n?ash\n```\n\n## More background\n\nThe ashr (\"Adaptive SHrinkage\") package aims to provide simple,\ngeneric, and flexible methods to derive \"shrinkage-based\" estimates\nand credible intervals for unknown quantities\n$\\beta=(\\beta_1,\\dots,\\beta_J)$, given only estimates of those\nquantities ($\\hat\\beta=(\\hat\\beta_1,\\dots, \\hat\\beta_J)$) and their\ncorresponding estimated standard errors ($s=(s_1,\\dots,s_J)$).\n\nThe \"adaptive\" nature of the shrinkage is two-fold. First, the\nappropriate amount of shrinkage is determined from the data, rather\nthan being pre-specified. Second, the amount of shrinkage undergone by\neach $\\hat\\beta_j$ will depend on the standard error $s_j$:\nmeasurements with high standard error will undergo more shrinkage than\nmeasurements with low standard error.\n\n### Methods Outline\n\nThe methods are based on treating the vectors $\\hat\\beta$ and $s$ as\n\"observed data\", and then performing inference for $\\beta$ from these\nobserved data, using a stand",
    "url": "https://github.com/stephens999/ashr",
    "last_updated": "2025-07-24T15:42:21+00:00"
  },
  {
    "full_name": "swcarpentry/r-novice-inflammation",
    "name": "r-novice-inflammation",
    "description": "Programming with R",
    "language": "R",
    "topics": [
      "carpentries",
      "software-carpentry",
      "lesson",
      "r",
      "knitr",
      "rmarkdown",
      "data-visualisation",
      "data-wrangling",
      "data-visualization",
      "english",
      "programming",
      "stable",
      "open-educational-resources"
    ],
    "readme": "[![Build and Deploy Website](https://github.com/swcarpentry/r-novice-inflammation/workflows/01%20Build%20and%20Deploy%20Site/badge.svg)](https://github.com/swcarpentry/r-novice-inflammation/actions/workflows/sandpaper-main.yaml)\n[![Create a Slack Account with us](https://img.shields.io/badge/Create_Slack_Account-The_Carpentries-071159.svg)](https://slack-invite.carpentries.org/)\n[![Slack Status](https://img.shields.io/badge/Slack_Channel-swc--r--inflammation-E01563.svg)](https://carpentries.slack.com/messages/C9WDPCMUG)\n\n# r-novice-inflammation\n\nbuild-and-deploy-readme-badge\n\n[The Carpentries](https://carpentries.org/) teach foundational coding, and data science skills to\nresearchers worldwide. This GitHub repository generates the Software Carpentry lesson website\n\"Introduction to R for non-programmers using inflammation data.\" The [lesson website can be viewed\nhere][online]. Making changes in this GitHub repository\nallows us to change the content of the lesson website.\n\nThe following people are maintainers for this lesson, and are responsible for determining which\nchanges to incorporate into the lesson:\n\n- Thomas Cason (@tecason)\n- Rohit Goswami (@haozeke)\n- Hugo Gruson (@Bisaloo)\n- Katie O'Mahony (@aforestsomewhere)\n\nAlumni:\n\n- Daniel Chen (@chendaniely)\n- Diya Das (@diyadas)\n- Isaac Jennings (@isaac-jennings)\n- Katrin Leinweber (@katrinleinweber)\n\nThe goal of this lesson is to teach novice programmers to write modular code to\nperform a data analysis. R is used to teach these skills because it is a\ncommonly used programming language in many scientific disciplines. However, the\nemphasis is not on teaching every aspect of R, but instead on\nlanguage agnostic principles like automation with loops and encapsulation with\nfunctions (see [Best Practices for Scientific Computing][best-practices] to\nlearn more). This lesson is a translation of the [Python version][py],\nand is also available in [MATLAB].\n\nThe example used in this lesson analyzes a set of 12 data files with\ni",
    "url": "https://github.com/swcarpentry/r-novice-inflammation",
    "last_updated": "2025-09-02T06:33:29+00:00"
  },
  {
    "full_name": "gergness/srvyr",
    "name": "srvyr",
    "description": "R package to add 'dplyr'-like Syntax for Summary Statistics of Survey Data",
    "language": "R",
    "topics": [
      "r",
      "survey"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# srvyr <img src=\"man/figures/logo.png\" align=\"right\" height=\"149\" width=\"149\"/>\n\n<!-- badges: start -->\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/srvyr)](https://CRAN.R-project.org/package=srvyr)\n[![R build\nstatus](https://github.com/gergness/srvyr/workflows/R-CMD-check/badge.svg)](https://github.com/gergness/srvyr/actions)\n[![Codecov test\ncoverage](https://codecov.io/gh/gergness/srvyr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/gergness/srvyr?branch=main)\n<!-- badges: end -->\n\nsrvyr brings parts of [dplyr’s](https://github.com/tidyverse/dplyr/)\nsyntax to survey analysis, using the\n[survey](https://CRAN.R-project.org/package=survey) package.\n\nsrvyr focuses on calculating summary statistics from survey data, such\nas the mean, total or quantile. It allows for the use of many dplyr\nverbs, such as `summarize`, `group_by`, and `mutate`, the convenience of\npipe-able functions, rlang’s style of non-standard evaluation and more\nconsistent return types than the survey package.\n\nYou can try it out:\n\n``` r\ninstall.packages(\"srvyr\")\n# or for development version\n# remotes::install_github(\"gergness/srvyr\")\n```\n\n## Example usage\n\nFirst, describe the variables that define the survey’s structure with\nthe function `as_survey()`with the bare column names of the names that\nyou would use in functions from the survey package like\n`survey::svydesign()`, `survey::svrepdesign()` or `survey::twophase()`.\n\n``` r\nlibrary(srvyr, warn.conflicts = FALSE)\ndata(api, package = \"survey\")\n\ndstrata <- apistrat %>%\n   as_survey_design(strata = stype, weights = pw)\n```\n\nNow many of the dplyr verbs are available.\n\n- `mutate()` adds or modifies a variable.\n\n``` r\ndstrata <- dstrata %>%\n  mutate(api_diff = api00 - api99)\n```\n\n- `summarise()` calculates summary statistics such as mean, total,\n  quantile or ratio.\n\n``` r\ndstrata %>% \n  summarise(api_diff = survey_mean(api_diff, vartype = \"ci\"))\n#> # A tibbl",
    "url": "https://github.com/gergness/srvyr",
    "last_updated": "2025-08-03T15:38:21+00:00"
  },
  {
    "full_name": "client9/ipcat",
    "name": "ipcat",
    "description": "Categorization of IP Addresses",
    "language": "Go",
    "topics": [],
    "readme": "**ipcat**: datasets for categorizing IP addresses.\n\nArchived in 2023. Please fork and edit as you wish. It's MIT now.  Onward -- nickg\n\n---\n\nThis is a list of IPv4 addresses that correspond to datacenters,\nco-location centers, shared and virtual webhosting providers.  In\nother words, ip addresses that end web consumers should not be using.\n\nStatistics\n------------------------\n\nCheck out the new [datacenter stats](/datacenters-stats.csv)\n\nWhat is the file format?\n-------------------------\n\n\n\nStandard CSV with ip-start, ip-end (inclusive, in dot-notation),\nname of provider, url of provider.  IP ranges are non-overlapping,\nand in sorted order.\n\nWhy is hosting provider XXX is missing?\n---------------------------------------\n\nIt might not be.  Many providers are resellers of another and will be\nincluded under another name or ip range.\n\nAlso, as of 16-Oct-2011, many locations from Africa, Latin\nAmerica, Korea and Japan are missing.\n\nOr, it might just be missing.  Please let us know!\n\nWhy GitHub + CSV?\n-------------------------\n\nThe goal of the file format and the use of github was designed to make\nit really easy for other to send patches or additions.  It also provides\nan easy way of keeping track of changes.\n\nHow is this generated?\n-------------------------\n\nManually from users like you, and automatically via proprietary\ndiscovery algorithms.\n\nWho made this?\n-------------------------\n\nNick Galbreath.  See more at http://www.client9.com/\n\n",
    "url": "https://github.com/client9/ipcat",
    "last_updated": "2025-08-26T23:08:07+00:00"
  },
  {
    "full_name": "graphql-editor/graphql-editor",
    "name": "graphql-editor",
    "description": "📺 Visual Editor & GraphQL IDE. ",
    "language": "TypeScript",
    "topics": [
      "graphql",
      "playground",
      "ide",
      "tool",
      "tools",
      "visualisation"
    ],
    "readme": "<p align=\"center\">\n  <img src=\"assets/logo.gif\">\n</p>\n<h3 align=\"center\">Graph sorcery, that makes reading GraphQL schemas easier!</h3>\n<div align=\"center\">\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=for-the-badge&label=license)](https://www.apache.org/licenses/LICENSE-2.0.html)\n[![stars](https://img.shields.io/github/stars/graphql-editor/graphql-editor?style=for-the-badge&label=stars)](https://github.com/apache/incubator-streampark/stargazers)\n[![npm](https://img.shields.io/npm/v/graphql-editor.svg?style=for-the-badge)](https://www.npmjs.com/package/graphql-editor)\n[![npm downloads](https://img.shields.io/npm/dt/graphql-editor.svg?style=for-the-badge)](https://www.npmjs.com/package/graphql-editor)\n[![Twitter](https://img.shields.io/twitter/follow/GraphQLEditor?label=follow&logo=twitter&style=for-the-badge)](https://twitter.com/GraphQLEditor)\n\n**[Website](https://graphqleditor.com)**&nbsp;&nbsp;|&nbsp;&nbsp;**[Documentation](https://graphqleditor.com/docs)**&nbsp;&nbsp;|&nbsp;&nbsp;**[Discord](https://discord.gg/wVcZdmd)**\n\n![graphql-editor-gif](https://user-images.githubusercontent.com/779748/217845783-0f3c5cc3-d74d-4589-bfcb-79b49664935c.gif)\n\n</div>\n\n<br />\n\nGraphQLEditor makes it easier to understand GraphQL schemas. Create a schema by using visual blocks system. GraphQL Editor will transform them into code.\n\nWith GraphQL Editor you can create visual diagrams without writing any code or present your schema in a nice way!\n\n<h4>GraphQL Editor is Graph based system for reading and designing the GraphQL schema</h4>\n\n> GraphQL Editor is a GraphQL visualizer and designer. It allows you to create and display GraphQL schemas as a visual graph.\n\n<br />\n\n## Table of contents\n\n- [How it works](#how-it-works)\n- [💡 What is GraphQL Editor?](#-what-is-graphql-editor)\n- [🚀 Features](#-features)\n- [Table of contents](#table-of-contents)\n- [License](#license)\n- [Installation](#installation)\n- [GraphQL SDL Editor](#graphql-sdl-editor)\n  - [Usage](#usage)\n",
    "url": "https://github.com/graphql-editor/graphql-editor",
    "last_updated": "2025-08-29T08:47:04+00:00"
  },
  {
    "full_name": "jennybc/send-email-with-r",
    "name": "send-email-with-r",
    "description": "How to send a bunch of email from R",
    "language": "R",
    "topics": [],
    "readme": "\n-   [How to send a bunch of emails from R](#how-to-send-a-bunch-of-emails-from-r)\n    -   [FAQ: Can't I \"just\" do this with sendmail?](#faq-cant-i-just-do-this-with-sendmail)\n    -   [Prep work related to Gmail and the `gmailr` package](#prep-work-related-to-gmail-and-the-gmailr-package)\n    -   [Dry run](#dry-run)\n    -   [Compose and send your emails](#compose-and-send-your-emails)\n\nHow to send a bunch of emails from R\n====================================\n\nWe send a fair amount of email in [STAT 545](http://stat545.com). For example, it's how we inform students of their marks on weekly homework and peer review. We use R for essentially all aspects of the course and this is no exception.\n\nIn this repo I describe our workflow, with Lord of the Rings characters playing the role of our students.\n\nMust haves:\n\n-   A Google account with an associated Gmail email address.\n-   The [`gmailr` R package](http://cran.r-project.org/web/packages/gmailr/index.html) by Jim Hester, which wraps the Gmail API (development on [GitHub](https://github.com/jimhester/gmailr)).\n\nNice to haves:\n\n-   A project in Google Developers Console to manage your use of the Gmail API. Optional but recommended once your use `gmailr` is more than casual, as mine is.\n-   The [`readr`](http://cran.r-project.org/web/packages/readr/index.html), [`dplyr`](http://cran.r-project.org/web/packages/dplyr/index.html) and [`purrr`](https://cran.r-project.org/web/packages/purrr/) packages for data wrangling and iteration. \"Past me\" used `plyr` instead of `purrr` and you could certainly do all of this with base R if you prefer.\n-   [`addresses.csv`](addresses.csv) a file containing email addresses, identified by a **key**. In our case, student names.\n-   [`marks.csv`](marks.csv) a file containing the variable bits of the email you plan to send, including the same identifying **key** as above. In our case, the homework marks.\n-   The script [`send-email-with-r.R`](send-email-with-r.R) that\n    -   joins email addres",
    "url": "https://github.com/jennybc/send-email-with-r",
    "last_updated": "2025-08-22T18:12:47+00:00"
  },
  {
    "full_name": "skranz/EconJournalData",
    "name": "EconJournalData",
    "description": "Shiny App to Search for Economics Articles with Data Supplements",
    "language": "R",
    "topics": [],
    "readme": "This is the source code of my Shiny app to search among economic articles with data and code supplement. To use it, just press on this link:\n\n<a href=\"http://econ.mathematik.uni-ulm.de:3200/ejd/\" target=\"_blank\">http://econ.mathematik.uni-ulm.de:3200/ejd/</a>\n\nI did not include the code that has built and updates the underlying database via webscrapping. Instead you can simply download the zipped SQLite databases with the collected information about articles and data supplements from my server:\n\n<ul>\n<li>\nMain database (should suffices for most analyses):<br><a href=\"http://econ.mathematik.uni-ulm.de/ejd/articles.zip\">http://econ.mathematik.uni-ulm.de/ejd/articles.zip</a>\n</li><li>\n Large database containing names and sizes of all files in the data and code supplements:<br><a href=\"http://econ.mathematik.uni-ulm.de/ejd/articles.zip\">http://econ.mathematik.uni-ulm.de/ejd/files.zip</a> \n</li>\n</ul>\n\nI plan to regularly update the databases.",
    "url": "https://github.com/skranz/EconJournalData",
    "last_updated": "2025-01-14T15:07:21+00:00"
  },
  {
    "full_name": "jupyterlab/jupyterlab",
    "name": "jupyterlab",
    "description": "JupyterLab computational environment.",
    "language": "TypeScript",
    "topics": [
      "jupyterlab",
      "jupyter"
    ],
    "readme": "**[Installation](#installation)** |\n**[Documentation](https://jupyterlab.readthedocs.io)** |\n**[Contributing](#contributing)** |\n**[License](#license)** |\n**[Team](#team)** |\n**[Getting help](#getting-help)** |\n\n# [JupyterLab](https://jupyterlab.readthedocs.io)\n\n[![PyPI version](https://badge.fury.io/py/jupyterlab.svg)](https://badge.fury.io/py/jupyterlab)\n[![Downloads](https://static.pepy.tech/badge/jupyterlab/month)](https://pepy.tech/project/jupyterlab)\n[![Build Status](https://github.com/jupyterlab/jupyterlab/workflows/Linux%20Tests/badge.svg)](https://github.com/jupyterlab/jupyterlab/actions?query=branch%3Amain+workflow%3A%22Linux+Tests%22)\n[![Build Status](https://github.com/jupyterlab/jupyterlab/workflows/Windows%20Tests/badge.svg)](https://github.com/jupyterlab/jupyterlab/actions?query=branch%3Amain+workflow%3A%22Windows+Tests%22)\n[![Documentation Status](https://readthedocs.org/projects/jupyterlab/badge/?version=stable)](http://jupyterlab.readthedocs.io/en/stable/)\n[![Crowdin](https://badges.crowdin.net/jupyterlab/localized.svg)](https://crowdin.com/project/jupyterlab)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8675/badge)](https://www.bestpractices.dev/projects/8675)\n[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/jupyterlab/jupyterlab/badge)](https://scorecard.dev/viewer/?uri=github.com/jupyterlab/jupyterlab)\n[![GitHub](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/jupyterlab/jupyterlab/issues)\n[![Discourse](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.jupyter.org/c/jupyterlab)\n[![Zulip](https://img.shields.io/badge/social_chat-zulip-blue.svg)](https://jupyter.zulipchat.com/#narrow/channel/469762-jupyterlab)\n[![Gitpod](https://img.shields.io/badge/gitpod_editor-open-blue.svg)](https://gitpod.io/#https://github.com/jupyterlab/jupyterlab)\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/HEAD?u",
    "url": "https://github.com/jupyterlab/jupyterlab",
    "last_updated": "2025-09-02T09:19:06+00:00"
  },
  {
    "full_name": "scalaz/scalaz",
    "name": "scalaz",
    "description": "Principled Functional Programming in Scala",
    "language": "Scala",
    "topics": [
      "scalaz",
      "scala",
      "functional-programming",
      "scala-native",
      "scalajs"
    ],
    "readme": "# Scalaz\n\nScalaz is a Scala library for functional programming.\n\nIt provides purely functional data structures to complement those from the Scala standard library.\nIt defines a set of foundational type classes (e.g. `Functor`, `Monad`) and corresponding instances for\na large number of data structures.\n\n[![IRC](https://img.shields.io/badge/chat-on%20libera-brightgreen.svg)](irc://irc.libera.chat/scalaz)\n\n## Getting Scalaz\n\nThe current stable version is 7.3.8, which is cross-built against Scala 2.12.x, 2.13.x, 3.x and Scala.js, scala-native.\n\nIf you're using SBT, add the following line to your build file:\n\n```scala\nlibraryDependencies += \"org.scalaz\" %% \"scalaz-core\" % \"7.3.8\"\n```\n\nFor Maven and other build tools, you can visit [search.maven.org](https://search.maven.org/search?q=g:org.scalaz%20AND%20v:7.3.8).\n(This search will also list all available modules of scalaz.)\n\nTo get sample configurations, click on the version of the module you are interested in.\nYou can also find direct download links at the bottom of that page. Choose the file ending in `7.3.8.jar`.\n\n## Quick Start\n\n```scala\nimport scalaz._\nimport std.option._, std.list._ // functions and type class instances for Option and List\n\nscala> Apply[Option].apply2(some(1), some(2))((a, b) => a + b)\nres0: Option[Int] = Some(3)\n\nscala> Traverse[List].traverse(List(1, 2, 3))(i => some(i))\nres1: Option[List[Int]] = Some(List(1, 2, 3))\n```\n\nUse of the `Ops` classes, defined under `scalaz.syntax`.\n\n```scala\nimport scalaz._\nimport std.list._ // type class instances for List\nimport syntax.bind._ // syntax for the Bind type class (and its parents)\n\nscala> List(List(1)).join\nres0: List[Int] = List(1)\n\nscala> List(true, false).ifM(List(0, 1), List(2, 3))\nres1: List[Int] = List(0, 1, 2, 3)\n```\n\nWe've gone to great lengths to give you an *a-la-carte* importing experience, but if you prefer an all-you-can-eat\nbuffet, you're in luck:\n\n```scala\nimport scalaz._\nimport Scalaz._\n\nscala> NonEmptyList(1, 2, 3).cojoin\nres0: scalaz.N",
    "url": "https://github.com/scalaz/scalaz",
    "last_updated": "2025-08-26T07:29:19+00:00"
  },
  {
    "full_name": "MangoTheCat/keras-workshop",
    "name": "keras-workshop",
    "description": "Data and scripts for keras course",
    "language": "R",
    "topics": [],
    "readme": "# Keras for R Workshop\n\n## Environment\n\nYou can either use the RStudio Cloud project or use your laptop. It can be a mixed experience configuring everything so the cloud is a safe fallback.\n\n### RStudio Cloud\n\nAn environment with this repo checked out is available at: https://rstudio.cloud/project/489173\n\nYou will need to setup and account on RStudio Cloud. Afterwards you should be able to deploy the project (this can take a minute or two). Then create a copy in your own space:\n\n![save permanent copy](material/save-perm.png)\n\nTest it's working with:\n\n```r\nlibrary(keras)\nis_keras_available()\n[1] TRUE\n```\n\nThis takes a minute the first time and will be quick from then on.\n\n### Local (laptop)\n\nAll of the models we're building will work on a laptop so if you want to follow along on your own machine then please follow the steps below:\n\n* Install R from https://cran.r-project.org/\n* RStudio desktop from https://www.rstudio.com\n* Install Anaconda from https://www.anaconda.com/download/\n\nInstall the following R packages from CRAN in the usual way: \n\n```r\ninstall.packages(c(\"tidyverse\", \"rsample\", \"recipes\", \"keras\"))\n```\n\nIn an R session install the keras/tensorflow python libraries by running:\n\n```r\nlibrary(keras)\ninstall_keras()\n```\n\nThis takes a while as it will install the various python packages that are required. For further instructions please see https://keras.rstudio.com/ and follow the instructions there.\n\nIf it worked you should get:\n\n```r\nlibrary(keras)\nis_keras_available()\n[1] TRUE\n```\n",
    "url": "https://github.com/MangoTheCat/keras-workshop",
    "last_updated": "2024-12-26T10:39:51+00:00"
  },
  {
    "full_name": "koaning/memo",
    "name": "memo",
    "description": "Decorators that logs stats.",
    "language": "Python",
    "topics": [
      "simulation",
      "grid",
      "json-blobs"
    ],
    "readme": "![](https://github.com/koaning/memo/raw/main/docs/header.png)\n\n> Looking for the programming language? You want [memo-lang](https://github.com/kach/memo) instead. \n\n## Installation\n\n```\npip install memo\n```\n\n## Documentation\n\nThe documentation can be found [here](https://koaning.github.io/memo/).\n\nThe quickstart guide is found [here](https://koaning.github.io/memo/getting-started.html).\n\n## Usage\n\nHere's an example of utility functions provided by our library.\n\n```python\nimport numpy as np\nfrom memo import memlist, memfile, grid, time_taken\n\ndata = []\n\n@memfile(filepath=\"results.jsonl\")\n@memlist(data=data)\n@time_taken()\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return {\"est_proba\": proba}\n\nfor settings in grid(class_size=[5, 10, 20, 30], n_sim=[1000, 1_000_000]):\n    birthday_experiment(**settings)\n```\n\nThe decorators `memlist` and `memfile` are making sure that the keyword arugments and\ndictionary output of the `birthday_experiment` are logged. The contents of the `results.jsonl`-file\nand the `data` variable looks like this;\n\n```\n{\"class_size\": 5, \"n_sim\": 1000, \"est_proba\": 0.024, \"time_taken\": 0.0004899501800537109}\n{\"class_size\": 5, \"n_sim\": 1000000, \"est_proba\": 0.027178, \"time_taken\": 0.19407916069030762}\n{\"class_size\": 10, \"n_sim\": 1000, \"est_proba\": 0.104, \"time_taken\": 0.000598907470703125}\n{\"class_size\": 10, \"n_sim\": 1000000, \"est_proba\": 0.117062, \"time_taken\": 0.3751380443572998}\n{\"class_size\": 20, \"n_sim\": 1000, \"est_proba\": 0.415, \"time_taken\": 0.0009679794311523438}\n{\"class_size\": 20, \"n_sim\": 1000000, \"est_proba\": 0.411571, \"time_taken\": 0.7928380966186523}\n{\"class_size\": 30, \"n_sim\": 1000, \"est_proba\": 0.703, \"time_taken\": 0.0018239021301269531}\n{\"class_size\": 30, \"n_sim\": 1",
    "url": "https://github.com/koaning/memo",
    "last_updated": "2025-07-12T13:25:16+00:00"
  },
  {
    "full_name": "szilard/benchm-ml",
    "name": "benchm-ml",
    "description": "A minimal benchmark for scalability, speed and accuracy of commonly used open source implementations (R packages, Python scikit-learn, H2O, xgboost, Spark MLlib etc.) of the top machine learning algorithms for binary classification (random forests, gradient boosted trees, deep neural networks etc.).",
    "language": "R",
    "topics": [
      "machine-learning",
      "data-science",
      "r",
      "python",
      "gradient-boosting-machine",
      "random-forest",
      "deep-learning",
      "xgboost",
      "h2o",
      "spark"
    ],
    "readme": "\n## Simple/limited/incomplete benchmark for scalability, speed and accuracy of machine learning libraries for classification\n\n_**All benchmarks are wrong, but some are useful**_\n\nThis project aims at a *minimal* benchmark for scalability, speed and accuracy of commonly used implementations\nof a few machine learning algorithms. The target of this study is binary classification with numeric and categorical inputs (of \nlimited cardinality i.e. not very sparse) and no missing data, perhaps the most common problem in business\napplications (e.g. credit scoring, fraud detection or churn prediction). If the input matrix is of *n* x *p*, *n* is \nvaried as 10K, 100K, 1M, 10M, while *p* is ~1K (after expanding the categoricals into dummy \nvariables/one-hot encoding). This particular type of data structure/size (the largest) stems from this author's interest in \nsome particular business applications.\n\n**A large part of this benchmark was done in 2015, with a number of updates later on as things have changed. Make sure you read \nat the [end](https://github.com/szilard/benchm-ml#summary) of this repo a summary of how the focus has changed over time,\nand why instead of updating this benchmark I started a new one (and where to find it).**\n\nThe algorithms studied are \n- linear (logistic regression, linear SVM)\n- random forest\n- boosting \n- deep neural network\n\nin various commonly used open source implementations like \n- R packages\n- Python scikit-learn\n- Vowpal Wabbit\n- H2O \n- xgboost\n- lightgbm (added in 2017)\n- Spark MLlib.\n\n**Update (June 2015):** It turns out these are the [most popular tools](https://github.com/szilard/list-ml-tools)\nused for machine learning indeed. If your software tool of choice is not here, you can do a minimal benchmark\nwith little work with the [following instructions](z-other-tools).\n\nRandom forest, boosting and more recently deep neural networks are the algos expected to perform the best on the structure/sizes\ndescribed above (e.g. vs alternatives such ",
    "url": "https://github.com/szilard/benchm-ml",
    "last_updated": "2025-08-27T20:51:53+00:00"
  },
  {
    "full_name": "minimaxir/video-to-gif-osx",
    "name": "video-to-gif-osx",
    "description": "A set of utilities that allow the user to easily convert video files to very-high-quality GIFs on OS X.",
    "language": "Shell",
    "topics": [],
    "readme": "# Convert Video to GIF on OSX\n\nThis repository contains a set of utilities that allow the user to easily convert video files to high-quality, low file size GIFs on OS X. This post is a complement to my blog post [Making a Video-to-GIF Right-Click Menu Item in OS X](http://minimaxir.com/2015/08/gif-to-video-osx/)\n\n![](http://minimaxir.com/img/video-to-gif-osx/convert_to_gif.gif)\n\nThe primary intent for this tool is used with Quicktime Movie files obtained from using the Screen Recording feature on OS X or iOS input.\n\nThe tool comes in three forms, depending on user needs:\n\n* **[Convert Video to GIF](https://github.com/minimaxir/video-to-gif-osx/raw/master/Convert%20Video%20to%20GIF.zip)** - A OS X Service which adds a \"Convert Video to GIF\" right-click menu item to all video files, which outputs a GIF of the video in the same folder of the source(s). (file must be unzipped on desktop)\n* **[video_to_gif_osx.sh](https://raw.githubusercontent.com/minimaxir/video-to-gif-osx/master/video_to_gif_osx.sh)** - A Shell script which accepts video(s) as parameter(s) and outputs a GIF of the video in the same folder of the source(s).\n* **[Convert Video to GIF App](https://github.com/minimaxir/video-to-gif-osx/raw/master/Convert%20Video%20to%20GIF%20App.zip)** - An Application which prompts the user for video(s) and outputs a GIF of the video in the same folder of the source(s). (file must be unzipped on desktop; when prompted for security warning go to \"Security and Privacy\" in System Preference and allow Convert Video to GIF App to run)\n\nThere is also a special fourth tool, [Convert Video to GIF App Custom](https://github.com/minimaxir/video-to-gif-osx/raw/master/Convert%20Video%20to%20GIF%20App%20Custom.zip) app, which is less streamlined but also allows you to specify a custom start time, end time, maximum width, frame rate, and # of colors. For example, here's a GIF of [Freedom Planet](https://en.wikipedia.org/wiki/Freedom_Planet) from a test video of 2:00 to 2:05, a max widt",
    "url": "https://github.com/minimaxir/video-to-gif-osx",
    "last_updated": "2025-01-31T18:29:13+00:00"
  },
  {
    "full_name": "altomator/EN-data_mining",
    "name": "EN-data_mining",
    "description": "Data Mining Historical Newspaper Metadata (METS/ALTO formats)",
    "language": "HTML",
    "topics": [
      "perl-script",
      "metadata",
      "basex",
      "alto",
      "digital-library",
      "data-mining",
      "mets-xml",
      "alto-xml",
      "xml",
      "digital-humanities",
      "digital-libraries",
      "ocr"
    ],
    "readme": "## EN-data_mining\n*Data Mining Historical Newspapers Metadata (Europeana Newspaper Project)*\n\n### Synopsis\nNewspapers from European digital librabries collections are part of the data set OLR’ed (Optical Layout Recognition) by the project Europeana Newspapers (www.europeana-newspapers.eu). The OLR refinement consists of the description of the structure of each issue and articles (spatial extent, title and subtitle, classification of content types) using the METS/ALTO formats.\n\nFrom each digital document is derived a set of bibliographical metadata (date of publication, title) and quantitative metadata related to content and layout (number of pages, articles, words, illustrations, etc.). Shell and XSLT or Perl scripts are used to extract some metadata from METS manifest or from ALTO files.\n\n[Detailled presentation](http://altomator.github.io/EN-data_mining/)\n\n---\n \n### Installation\nYou can use a XSLT stylesheet (called with DOS scripts) or a Perl script (faster).\n\nSample documents are stored in the \"DOCS\" folder. The scripts have been designed for the [CCS](https://content-conversion.com/wp-content/uploads/2014/09/CCS-METS-ALTO-Info_basic_20140909.pdf) METS/ALTO profil, but this can be easily fixed.\n\nThe metadata are generated in a \"STATS\" folder.\n\n#### XSLT\nTwo DOS shell scripts :\n- batch-EN.bat\n- xslt.cmd\n\nTwo XSLT stylesheets:\n- analyseAltosCCS.xsl\n- calculeStatsMETS_CSV.xsl\n\nThe XSLT are runned with Xalan-Java. Path to the Java binary must be set in xslt.cmd.\n\nFor each document, its metadata are stored in the STATS folder under two formats :\n- XML (raw metadata, with detailled values for each page)\n- CSV (metadata at the issue level)\n\nAn aggregated file (metadata.csv) contains all the CSV metadata.\n\n\n##### Test\n1. Open a DOS terminal.\n2. Change dir to the batch folder\n3. >batch-EN.bat \n\n#### Perl script\nFaster and richer (more metadata) than the XSLT scripts.\n\n- One Perl script: extractMD.pl \n- One shell script (Bash): batch.sh (runs the Perl script and packages ",
    "url": "https://github.com/altomator/EN-data_mining",
    "last_updated": "2025-02-04T17:56:58+00:00"
  },
  {
    "full_name": "manasRK/word2vec-recommender",
    "name": "word2vec-recommender",
    "description": "Recommendation engine based on contextual word embeddings",
    "language": "Python",
    "topics": [],
    "readme": "# word2vec-recommender\n[![GitHub license](https://img.shields.io/pypi/l/pyzipcode-cli.svg)](https://img.shields.io/pypi/l/pyzipcode-cli.svg) \n\n[Talk Submission at Pycon India 2016](https://in.pycon.org/cfp/2016/proposals/creating-a-recommendation-engine-based-on-contextual-word-embeddings~aOZGe/)\n\n## Index\n- [What it is ??](#what-it-is?) \n- [How it is done?](#how-it-is-done?)\n- [Technologies used](#technologies-used)\n- [Data and Models](#data-and-models)\n- [Installation](#installation)\n- [What is there inside the box :package: ?](#What-is-there-inside-the-box?)\n- [Contributors](#contributors)\n- [Issues :bug:](#issues)\n\n## What it is?\nHow can we create a recommendation engine that is based both on user browsing history and product reviews? Can I create recommendations purely based on the 'intent' and 'context' of the search?\n\nThis talk will showcase how a recommendation engine can be built with user browser history and user-generated reviews using a state of the art technique - word2vec. We will create something that not only matches the existing recommender systems deployed by websites, but goes one step ahead - incorporating context to generate valid and innovative recommendations. The beauty of such a framework is that not only does it support online learning, but is also sensitive to minor changes in user tone and behavior.\n\n## How it is done?\n\nThe trick/secret sauce is - How do we account for the 'context' and build it in our systems? The talk will answer these questions and showcase effectiveness of such a recommender system.\n* ## First Milestone :tada:\n    Subset of the engine's functionality was completed during a project undertaken at IASNLP 2016 held by Language Technology Research Center (LTRC), IIIT Hyderabad\n## Technologies used\n\n* Google's Word2vec\n* Gensim\n* Numpy\n* Flask, Redis.\n\n\n## Data and Models\n   * Rest of the models (User & Metadata) can be downloaded from https://s3.amazonaws.com/iasnlp-models/output_models.tar\n   * Amazon review data will be ",
    "url": "https://github.com/manasRK/word2vec-recommender",
    "last_updated": "2025-01-17T13:20:12+00:00"
  },
  {
    "full_name": "databricks/spark-deep-learning",
    "name": "spark-deep-learning",
    "description": "Deep Learning Pipelines for Apache Spark",
    "language": "Python",
    "topics": [],
    "readme": "Deep Learning Pipelines for Apache Spark\n============================================================\n[![Build Status][pkg-build-badge]][pkg-build-link] [![Coverage][pkg-cov-badge]][pkg-cov-link]\n\n  [pkg-build-badge]: https://github.com/databricks/spark-deep-learning/actions/workflows/test.yml/badge.svg?branch=master\n  [pkg-build-link]: https://github.com/databricks/spark-deep-learning/actions/workflows/test.yml\n  [pkg-cov-badge]: https://codecov.io/gh/databricks/spark-deep-learning/coverage.svg?branch=master\n  [pkg-cov-link]: https://codecov.io/gh/databricks/spark-deep-learning/branch/master\n\nThe repo only contains HorovodRunner code for local CI and API docs. To use HorovodRunner for distributed training, please use Databricks Runtime for Machine Learning,\nVisit databricks doc [HorovodRunner: distributed deep learning with Horovod](https://docs.databricks.com/applications/machine-learning/train-model/distributed-training/horovod-runner.html) for details.\n\nTo use the previous release that contains Spark Deep Learning Pipelines API, please go to [Spark Packages page](https://spark-packages.org/package/databricks/spark-deep-learning).\n\n\n## API Documentation\n\n### class sparkdl.HorovodRunner(\\*, np, driver_log_verbosity='all')\nBases: `object`\n\nHorovodRunner runs distributed deep learning training jobs using Horovod.\n\nOn Databricks Runtime 5.0 ML and above, it launches the Horovod job as a distributed Spark job.\nIt makes running Horovod easy on Databricks by managing the cluster setup and integrating with\nSpark.\nCheck out Databricks documentation to view end-to-end examples and performance tuning tips.\n\nThe open-source version only runs the job locally inside the same Python process,\nwhich is for local development only.\n\n**NOTE**: Horovod is a distributed training framework developed by Uber.\n\n* **Parameters**\n\n\n    * **np** - number of parallel processes to use for the Horovod job.\n        This argument only takes effect on Databricks Runtime 5.0 ML and above.\n        ",
    "url": "https://github.com/databricks/spark-deep-learning",
    "last_updated": "2025-08-25T17:05:03+00:00"
  },
  {
    "full_name": "gitpitch/in-60-seconds",
    "name": "in-60-seconds",
    "description": "GitPitch In 60 Seconds - A Very Short Tutorial",
    "language": "Python",
    "topics": [],
    "readme": "[![GitPitch](https://gitpitch.com/assets/badge.svg)](https://gitpitch.com/gitpitch/in-60-seconds)\n\n# GitPitch In 60 Seconds\n\n- [In 60 Seconds Slide Deck](https://gitpitch.com/gitpitch/in-60-seconds)\n- [What's New in GitPitch 4.0](https://docs.gitpitch.com/#/whats-new-in-40)\n- [GitPitch 4.0 Quickstart Guide](https://docs.gitpitch.com/#/quickstart)\n\n",
    "url": "https://github.com/gitpitch/in-60-seconds",
    "last_updated": "2025-08-07T03:04:13+00:00"
  },
  {
    "full_name": "scoopr/vectorial",
    "name": "vectorial",
    "description": "Vector math library with NEON/SSE support",
    "language": "C++",
    "topics": [],
    "readme": "\n    Vectorial - vector math library\n\n\n\n  Motivation\n\n    I couldn't find an open source math library that was usable and\n    supported simd - especially the ARM NEON variant.\n\n\n  Features\n\n    Supports NEON, SSE, scalar and generic gcc vector extension.\n    Most basic vector and matrix math is available, but not quite\n    yet full featured.\n\n\n  Design\n\n    Vectorial consists of two main parts, pure-C wrapper around\n    platform-specific vector instructions in the simd*.h files\n    and C++ classes for common uses, the vec*.h and mat*.h\n\n    The config.h autodetects approriate vector instructions to use.\n\n    The platform-specific support is done with intrisincs only,\n    allowing the compiler to have a full view of the code, hopefully\n    resulting in better optimizations especially with reordering etc.\n\n\n  Installation / Usage\n\n    Add vectorial/include to your include path\n\n    #include \"vectorial/simd4f.h\"  \n    for C-only simd wrapper, using it looks like this:\n      simd4f v = simd4f_normalize( simd4f_add( simd4f_create(1,2,3,4), y) );\n      float z = simd4f_get_z(v);\n\n    #include \"vectorial/vectorial.h\"\n    for C++ classes. They reside in vectorial namespace, you might\n    want to alias them to your own namespace\n      namespace myproject {\n        using namespace ::vectorial;\n        // if you like different name: typedef vec3f Vector3;\n      }\n      using myproject::vec4f;\n      \n      vec4f v = normalize( vec4f(1,2,3,4) + y );\n      float z = v.z();\n\n\n  License\n\n    2-clause BSD. See LICENSE\n\n\n\n\n",
    "url": "https://github.com/scoopr/vectorial",
    "last_updated": "2025-05-21T12:16:57+00:00"
  },
  {
    "full_name": "berkeley-scf/tutorial-parallel-distributed",
    "name": "tutorial-parallel-distributed",
    "description": "Tutorial on parallelization tools for distributed computing (multiple computers or cluster nodes) in R, Python, Matlab, and C.",
    "language": "HTML",
    "topics": [],
    "readme": "# tutorial-parallel-distributed\nTutorial on parallelization tools for distributed computing (multiple computers or cluster nodes) in R, Python, Matlab, and C.\n\nPlease see [the parallel-dist.html file](https://htmlpreview.github.io/?https://github.com/berkeley-scf/tutorial-parallel-distributed/blob/master/parallel-dist.html), which is generated dynamically from the underlying Markdown and various code files. \n",
    "url": "https://github.com/berkeley-scf/tutorial-parallel-distributed",
    "last_updated": "2024-05-07T03:18:29+00:00"
  },
  {
    "full_name": "ivan-bilan/The-NLP-Pandect",
    "name": "The-NLP-Pandect",
    "description": "A comprehensive reference for all topics related to Natural Language Processing",
    "language": "Python",
    "topics": [
      "nlp",
      "naturallanguageprocessing",
      "awesome-list",
      "deeplearning",
      "natural-language-processing",
      "pandect"
    ],
    "readme": "![The-NLP-Pandect](./Resources/Images/pandect.png)\n\n<p align=\"center\">\nThis pandect (πανδέκτης is Ancient Greek for encyclopedia) was created to help you find almost anything related to Natural Language Processing that is available online.\n</p>\n\n> __Note__\n> Quick legend on available resource types:\n> \n> ⭐ - open source project, usually a GitHub repository with its number of stars\n> \n> 📙 - resource you can read, usually a blog post or a paper\n> \n> 🗂️ - a collection of additional resources\n> \n> 🔱 - non-open source tool, framework or paid service\n> \n> 🎥️ - a resource you can watch\n> \n> 🎙️ - a resource you can listen to\n\n### <p align=\"center\"><b>Table of Contents</b></p>\n\n| 📇 Main Section  | 🗃️ Sub-sections Sample |\n| ------------- | ------------- |\n| [NLP Resources](https://github.com/ivan-bilan/The-NLP-Pandect#)  | [Paper Summaries](https://github.com/ivan-bilan/The-NLP-Pandect#papers-and-paper-summaries), [Conference Summaries](https://github.com/ivan-bilan/The-NLP-Pandect#conference-summaries), [NLP Datasets](https://github.com/ivan-bilan/The-NLP-Pandect#nlp-datasets) |\n| [NLP Podcasts](https://github.com/ivan-bilan/The-NLP-Pandect#-1)  | [NLP-only Podcasts](https://github.com/ivan-bilan/The-NLP-Pandect#nlp-only-podcasts), [Podcasts with many NLP Episodes](https://github.com/ivan-bilan/The-NLP-Pandect#many-nlp-episodes) |\n| [NLP Newsletters](https://github.com/ivan-bilan/The-NLP-Pandect#-2)  | -  |\n| [NLP Meetups](https://github.com/ivan-bilan/The-NLP-Pandect#-3)  | -  |\n| [NLP YouTube Channels](https://github.com/ivan-bilan/The-NLP-Pandect#-4)  | -  |\n| [NLP Benchmarks](https://github.com/ivan-bilan/The-NLP-Pandect#-5)  | [General NLU](https://github.com/ivan-bilan/The-NLP-Pandect#general-nlu), [Question Answering](https://github.com/ivan-bilan/The-NLP-Pandect#question-answering), [Multilingual](https://github.com/ivan-bilan/The-NLP-Pandect#multilingual-and-non-english-benchmarks)  |\n| [Research Resources](https://github.com/ivan-bilan/The-NLP-Pandect#-6) | [Resou",
    "url": "https://github.com/ivan-bilan/The-NLP-Pandect",
    "last_updated": "2025-08-31T15:09:58+00:00"
  },
  {
    "full_name": "rdrr1990/bigKRLS",
    "name": "bigKRLS",
    "description": "Now on CRAN, bigKRLS combines bigmemory & RcppArmadillo (C++) for speed into a new Kernel Regularized Least Squares algorithm. Slides:",
    "language": "R",
    "topics": [
      "rstats",
      "rcpp",
      "bigmemory",
      "rcpparmadillo",
      "kernel",
      "cran",
      "regularization"
    ],
    "readme": "# bigKRLS\n[![Travis-CI Build Status](https://travis-ci.org/rdrr1990/bigKRLS.svg?branch=master)](https://travis-ci.org/rdrr1990/bigKRLS)\n[![Coverage Status](https://img.shields.io/codecov/c/github/rdrr1990/bigKRLS/master.svg)](https://codecov.io/github/rdrr1990/bigKRLS?branch=master) [![cran checks](https://cranchecks.info/badges/summary/bigKRLS)](https://cranchecks.info/pkgs/bigKRLS) [![Rdoc](http://www.rdocumentation.org/badges/version/bigKRLS)](http://www.rdocumentation.org/packages/bigKRLS) ![](http://cranlogs.r-pkg.org/badges/grand-total/bigKRLS)\n \nKernel Regularized Least Squares (KRLS) is a kernel-based, complexity-penalized method developed by [Hainmueller and Hazlett (2013)](http://pan.oxfordjournals.org/content/22/2/143), and designed to minimize parametric assumptions while maintaining interpretive clarity. Here, we introduce `bigKRLS`, an updated version of the original [KRLS R package](https://CRAN.R-project.org/package=KRLS) with algorithmic and implementation improvements designed to optimize speed and memory usage. These improvements allow users to straightforwardly estimate pairwise regression models with KRLS once *N > 2500*. Since April 15, 2017, `bigKRLS` has been available on  [CRAN](http://www.rdocumentation.org/badges/version/bigKRLS). You may also be interested in our [working paper](https://web.stanford.edu/~pmohanty/mohanty_shaffer_workingpaper.pdf), which has been accepted by *Political Analysis*, and which demonstrates the utility of `bigKRLS` by analyzing the 2016 US presidential election. Our replication materials can be found on [Dataverse](https://doi.org/10.7910/DVN/CYYLOK) and our [Github repo](https://github.com/rdrr1990/bigKRLS/tree/master/examples) contains examples too.\n\n# Major Updates found in bigKRLS\n\n1. C++ integration. We re-implement most major computations in the model in `C++` via [Rcpp](https://CRAN.R-project.org/package=Rcpp) and [RcppArmadillo](https://CRAN.R-project.org/package=RcppArmadillo). These changes produce up",
    "url": "https://github.com/rdrr1990/bigKRLS",
    "last_updated": "2024-07-28T20:03:04+00:00"
  },
  {
    "full_name": "FoundrySH/zipcodes",
    "name": "zipcodes",
    "description": "An API for zip code geolocation",
    "language": "JavaScript",
    "topics": [],
    "readme": "\n# Zipcodes\n\nAn API for zip code geolocation.\n\n## Docs\n\nhttps://docs.foundry.sh/zipcodes.html\n\n## HTML Interface\n\nhttps://zipcodes.foundry.sh/\n\n## License\n\nCopyright (c) 2018 Foundry\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n",
    "url": "https://github.com/FoundrySH/zipcodes",
    "last_updated": "2025-07-17T02:05:41+00:00"
  },
  {
    "full_name": "colearendt/tidyjson",
    "name": "tidyjson",
    "description": "Tidy your JSON data in R with tidyjson",
    "language": "R",
    "topics": [],
    "readme": "tidyjson\n================\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/tidyjson)](https://cran.r-project.org/package=tidyjson)\n[![Build\nStatus](https://travis-ci.org/colearendt/tidyjson.svg?branch=master)](https://travis-ci.org/colearendt/tidyjson)\n[![Coverage\nStatus](https://codecov.io/github/colearendt/tidyjson/coverage.svg?branch=master)](https://codecov.io/github/colearendt/tidyjson?branch=master)\n\n[![CRAN\nActivity](http://cranlogs.r-pkg.org/badges/tidyjson)](https://cran.r-project.org/package=tidyjson/index.html)\n[![CRAN\nHistory](http://cranlogs.r-pkg.org/badges/grand-total/tidyjson)](https://cran.r-project.org/package=tidyjson/index.html)\n\n![tidyjson\ngraphs](https://cloud.githubusercontent.com/assets/2284427/18217882/1b3b2db4-7114-11e6-8ba3-07938f1db9af.png)\n\ntidyjson provides tools for turning complex [json](http://www.json.org/)\ninto\n[tidy](https://cran.r-project.org/package=tidyr/vignettes/tidy-data.html)\ndata.\n\n## Installation\n\nGet the released version from CRAN:\n\n``` r\ninstall.packages(\"tidyjson\")\n```\n\nor the development version from github:\n\n``` r\ndevtools::install_github(\"colearendt/tidyjson\")\n```\n\n## Examples\n\nThe following example takes a character vector of 500 documents in the\n`worldbank` dataset and spreads out all objects.  \nEvery JSON object key gets its own column with types inferred, so long\nas the key does not represent an array. When `recursive=TRUE` (the\ndefault behavior), `spread_all` does this recursively for nested objects\nand creates column names using the `sep` parameter (i.e. `{\"a\":{\"b\":1}}`\nwith `sep='.'` would generate a single column: `a.b`).\n\n``` r\nlibrary(dplyr)\nlibrary(tidyjson)\n\nworldbank %>% spread_all\n#> # A tbl_json: 500 x 9 tibble with a \"JSON\" attribute\n#>    ..JSON        docum…¹ board…² closi…³ count…⁴ proje…⁵ regio…⁶ total…⁷ _id.$…⁸\n#>    <chr>           <int> <chr>   <chr>   <chr>   <chr>   <chr>     <dbl> <chr>  \n#>  1 \"{\\\"_id\\\":{\\…  ",
    "url": "https://github.com/colearendt/tidyjson",
    "last_updated": "2025-08-18T11:05:48+00:00"
  },
  {
    "full_name": "rednote-hilab/dots.ocr",
    "name": "dots.ocr",
    "description": "Multilingual Document Layout Parsing in a Single Vision-Language Model",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png\" width=\"300\"/>\n<p>\n\n<h1 align=\"center\">\ndots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model\n</h1>\n\n[![Blog](https://img.shields.io/badge/Blog-View_on_GitHub-333.svg?logo=github)](https://github.com/rednote-hilab/dots.ocr/blob/master/assets/blog.md)\n[![HuggingFace](https://img.shields.io/badge/HuggingFace%20Weights-black.svg?logo=HuggingFace)](https://huggingface.co/rednote-hilab/dots.ocr)\n\n\n<div align=\"center\">\n  <a href=\"https://dotsocr.xiaohongshu.com\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>🖥️ Live Demo</strong></a> | \n  <a href=\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/wechat.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>💬 WeChat</strong></a> | \n  <a href=\"https://www.xiaohongshu.com/user/profile/683ffe42000000001d021a4c\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>📕 rednote</strong></a> | \n  <a href=\"https://x.com/rednotehilab\" target=\"_blank\" rel=\"noopener noreferrer\"><strong>🐦 X</strong></a>\n</div>\n\n</div>\n\n\n\n## Introduction\n\n**dots.ocr** is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance.\n\n1. **Powerful Performance:** **dots.ocr** achieves SOTA performance for text, tables, and reading order on [OmniDocBench](https://github.com/opendatalab/OmniDocBench), while delivering formula recognition results comparable to much larger models like Doubao-1.5 and gemini2.5-pro.\n2. **Multilingual Support:** **dots.ocr** demonstrates robust parsing capabilities for low-resource languages, achieving decisive advantages across both layout detection and content recognition on our in-house multilingual documents benchmark.\n3. **Unified and Simpl",
    "url": "https://github.com/rednote-hilab/dots.ocr",
    "last_updated": "2025-09-02T09:34:49+00:00"
  },
  {
    "full_name": "AaronJackson/vrn",
    "name": "vrn",
    "description": ":man:  Code for \"Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression\"",
    "language": "MATLAB",
    "topics": [
      "computervision",
      "deeplearning",
      "3d",
      "reconstruction",
      "face",
      "torch7",
      "computer-vision"
    ],
    "readme": "* Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric Regression\n\n*Aaron S. Jackson, Adrian Bulat, Vasileios Argyriou and Georgios Tzimiropoulos*\n\n*Try out the code without running it!* Check out our online demo [[https://vrn.aaronsplace.co.uk][here]]. Alternatively,\npull the DockerHub image `asjackson:vrn`, see docs in the [[https://github.com/AaronJackson/vrn-docker][vrn-docker]] repo. \n\n[[http://aaronsplace.co.uk/papers/jackson2017recon/preview.png]]\n\nPlease visit our [[http://aaronsplace.co.uk/papers/jackson2017recon/][project webpage]] for a link to the paper and an\nexample video run on 300VW. This code is licenses under the MIT\nLicense, as described in the LICENSE file.\n\nThis is an unguided version of the Volumetric Regression Network (VRN)\nfor 3D face reconstruction from a single image. This method approaches\nthe problem of reconstruction as a segmentation problem, producing a\n3D volume, spatially aligned with the input image. A mesh can then be\nobtained by taking the isosurface of this volume.\n\nSeveral example images are included in the examples folder. Most of\nthese are AFLW images taken from [[http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm][3DDFA]].\n\nIf you are running the code to calculate error for a potential\npublication, please use the MATLAB version, as this is what was used\nto compute the error for the paper.\n\n** Prebuilt Docker Image for CPU version\n\nI have released an image on Docker Hub which has everything to get the\nCPU version running under Docker. I'll extend this to have CUDA\nsupport at some point.\n\n#+BEGIN_SRC\ndocker pull asjackson/vrn:latest\ndocker run -v $(pwd)/data:/data:Z vrn /runner/run.sh /data/turing.jpg\n#+END_SRC\n\nThe repo holding this is available at [[https://github.com/AaronJackson/vrn-docker][vrn-docker]] and the models (which\nhave been cast to CPU floats already) were stored in Git LFS but took too much space. If you\nhave an issue with this docker container, please use the vrn-docker\nis",
    "url": "https://github.com/AaronJackson/vrn",
    "last_updated": "2025-08-23T04:14:13+00:00"
  },
  {
    "full_name": "nteract/papermill",
    "name": "papermill",
    "description": "📚 Parameterize, execute, and analyze notebooks",
    "language": "Python",
    "topics": [
      "jupyter",
      "notebooks",
      "notebook-generator",
      "nteract",
      "publishing",
      "pipeline",
      "notebook",
      "python",
      "r",
      "julia",
      "scala"
    ],
    "readme": "# <a href=\"https://github.com/nteract/papermill\"><img src=\"https://media.githubusercontent.com/media/nteract/logos/master/nteract_papermill/exports/images/png/papermill_logo_wide.png\" height=\"48px\" /></a>\n\n<!---(binder links generated at https://mybinder.readthedocs.io/en/latest/howto/badges.html and compressed at https://tinyurl.com) -->\n\n[![CI](https://github.com/nteract/papermill/actions/workflows/ci.yml/badge.svg)](https://github.com/nteract/papermill/actions/workflows/ci.yml)\n[![CI](https://github.com/nteract/papermill/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/nteract/papermill/actions/workflows/ci.yml)\n[![image](https://codecov.io/github/nteract/papermill/coverage.svg?branch=main)](https://codecov.io/github/nteract/papermill?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/papermill/badge/?version=latest)](http://papermill.readthedocs.io/en/latest/?badge=latest)\n[![badge](https://tinyurl.com/ybwovtw2)](https://mybinder.org/v2/gh/nteract/papermill/main?filepath=binder%2Fprocess_highlight_dates.ipynb)\n[![badge](https://tinyurl.com/y7uz2eh9)](https://mybinder.org/v2/gh/nteract/papermill/main?)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/papermill)](https://pypi.org/project/papermill/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n[![papermill](https://snyk.io/advisor/python/papermill/badge.svg)](https://snyk.io/advisor/python/papermill)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/papermill/badges/downloads.svg)](https://anaconda.org/conda-forge/papermill)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/nteract/papermill/main.svg)](https://results.pre-commit.ci/latest/github/nteract/papermill/main)\n\n**papermill** is a tool for parameterizing, executing, and analyzing\nJupyter Notebooks.\n\nPapermill lets you:\n\n- **parameterize** notebooks\n- **execute** notebooks\n\nThis opens up new opportunities for",
    "url": "https://github.com/nteract/papermill",
    "last_updated": "2025-09-02T08:36:33+00:00"
  },
  {
    "full_name": "dinedal/textql",
    "name": "textql",
    "description": "Execute SQL against structured text like CSV or TSV",
    "language": "Go",
    "topics": [],
    "readme": "# TextQL\n\n[![Build Status](https://travis-ci.org/dinedal/textql.svg)](https://travis-ci.org/dinedal/textql) [![Go Report Card](http://goreportcard.com/badge/dinedal/textql)](http://goreportcard.com/report/dinedal/textql)\n\nAllows you to easily execute SQL against structured text like CSV or TSV.\n\nExample session:\n\n![textql_usage_session](https://raw.github.com/dinedal/textql/master/textql_usage.gif)\n\n## Major changes!\n\nIn the time since the initial release of textql, I've made some improvements as well as made the project much more modular. There've also been additional performance tweaks and added functionality, but this comes at the cost of breaking the original command-line flags and changing the install command.\n\n### Changes since v1\n\nAdditions:\n\n- Numeric values are automatically recognized in more cases.\n- Date / Time / DateTime values are automatically recognized in reasonable formats. See [Time Strings](https://www.sqlite.org/lang_datefunc.html) for a list for accepted formats, and how to convert from other formats.\n- Added join support! Multiple files / directories can be loaded by listing them at the end of the command.\n- Directories are read by reading each file inside, and this is non-recursive.\n- You can list as many files / directories as you like.\n- Added flag '-output-file' to save output directly to a file.\n- Added flag '-output-dlm' to modify the output delimiter.\n- Added \"short SQL\" syntax.\n  - For the case of a single table, the `FROM [table]` can be dropped from the query.\n  - For simple selects, the `SELECT` keyword can be dropped from the query.\n  - This means the v1 command `textql -sql \"select * from tbl\" -source some_file.csv` can be shortened to `textql -sql \"*\" some_file.csv`\n\nChanges:\n\n- The flag '-outputHeader' was renamed to '-output-header'.\n\nRemovals:\n\n- Dropped the ability to override table names. This makes less sense after the automatic tablename generation based on filename, joins, and shorter SQL syntax changes.\n- Removed '-sourc",
    "url": "https://github.com/dinedal/textql",
    "last_updated": "2025-09-01T00:36:46+00:00"
  },
  {
    "full_name": "TaddyLab/gamlr",
    "name": "gamlr",
    "description": "Gamma lasso regression",
    "language": "C",
    "topics": [],
    "readme": "The Gamma Lasso    \n==\n\nThis package implements the gamma lasso algorithm for regularization paths corresponding to a range of non-convex cost functions between L0 and L1 norms.  As much as possible, usage is analogous to that for the glmnet package (which does the same thing for penalization between L1 and L2 norms).     \n\nThe CRAN page is at https://CRAN.R-project.org/package=gamlr.    \n\nIf you use gamlr, please cite Taddy (2017), One-step estimator paths for concave regularization, Journal of Computational and Graphical Statistics.  \n\n",
    "url": "https://github.com/TaddyLab/gamlr",
    "last_updated": "2024-12-18T00:37:29+00:00"
  },
  {
    "full_name": "eddelbuettel/math",
    "name": "math",
    "description": "Stan Math Library",
    "language": "C++",
    "topics": [],
    "readme": "The <b>Stan Math Library</b> is a C++, reverse-mode automatic differentiation library designed to be usable, extensive and extensible, efficient, scalable, stable, portable, and redistributable in order to facilitate the construction and utilization of algorithms that utilize derivatives.\n\nLicensing\n---------\nThe Stan Math Library is licensed under the [new BSD license](https://raw.githubusercontent.com/stan-dev/math/develop/licenses/math-license.txt).\n\nRequired Libraries\n------------------\nStan Math depends on two libraries:\n\n- Boost (version 1.58): [Boost Home Page](http://www.boost.org)\n- Eigen (version 3.24): [Eigen Home Page](http://eigen.tuxfamily.org/index.php?title=Main_Page)\n\nThese are distributed under the `lib/` subdirectory. Only these two versions of the dependent libraries have been tested with Stan Math.\n\nInstallation\n------------\nThe Stan Math Library is a header-only C++ library.\n\nA simple hello world program using Stan Math is as follows:\n\n```\n#include <stan/math.hpp>\n#include <iostream>\n\nint main() {\n  std::cout << \"log normal(1 | 2, 3)=\" \n            << stan::math::normal_log(1, 2, 3) \n            << std::endl;\n}\n```\n\nIf this is in the file `/path/to/foo/foo.cpp`, then you can compile and run this with something like this, with the `path/to` business replaced with actual paths:\n\n```\n> cd /path/to/foo\n> clang++ -I /path/to/stan -I /path/to/Eigen -I /path/to/boost foo.cpp\n> ./a.out\nlog normal(1 | 2, 3)=-2.07311\n```\n\nThe `-I` includes point to the three necessary includes:\n\n* the Stan Math library:  the path is to the source directory that contains `stan` as a subdirectory\n* the Eigen C++ matrix library:  this path is to the source directory that contains `Eigen` as a subdirectory\n* the Boost C++ library: this path is to the source directory that contains `boost` as a subdirectory\n\nNote that the paths should *not* include the final directories `stan`, `Eigen`, or `boost` on the paths.  An example of a real instantiation:\n\n```\nclang++ -I ~/stan-dev/m",
    "url": "https://github.com/eddelbuettel/math",
    "last_updated": "2015-08-12T15:19:21+00:00"
  },
  {
    "full_name": "jeroenjanssens/rush",
    "name": "rush",
    "description": "R One-Liners from the Shell",
    "language": "R",
    "topics": [
      "r",
      "cli",
      "plot",
      "packages",
      "shell",
      "one-liner"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# rush\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/jeroenjanssens/rush/workflows/R-CMD-check/badge.svg)](https://github.com/jeroenjanssens/rush/actions)\n<!-- badges: end -->\n\n## Installation\n\nYou can install the development version of rush from with:\n\n``` r\nremotes::install_github(\"jeroenjanssens/rush\")\n```\n\n## Code of Conduct\n\nPlease note that the rush project is released with a [Contributor Code\nof\nConduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html).\nBy contributing to this project, you agree to abide by its terms.\n",
    "url": "https://github.com/jeroenjanssens/rush",
    "last_updated": "2025-07-22T07:12:30+00:00"
  },
  {
    "full_name": "pridiltal/staplr",
    "name": "staplr",
    "description": "PDF Toolkit. :paperclip: :hammer: :wrench: :scissors:  :bookmark_tabs: :file_folder::paperclip: :bookmark: :construction: :construction_worker:",
    "language": "Java",
    "topics": [
      "r",
      "pdftk",
      "pdf",
      "toolkit"
    ],
    "readme": "\n# staplr <img src=\"man/figures/logo.png\" align=\"right\" height=\"150\"/>\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![Licence](https://img.shields.io/badge/licence-GPL--3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0.en.html)\n[![Build\nStatus](https://travis-ci.org/pridiltal/staplr.svg?branch=master)](https://travis-ci.org/pridiltal/staplr)\n\n-----\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/staplr)](https://cran.r-project.org/web/packages/staplr/index.html)\n[![](http://cranlogs.r-pkg.org/badges/staplr)](http://cran.rstudio.com/web/packages/staplr/index.html)\n\n-----\n\n[![Last-changedate](https://img.shields.io/badge/last%20change-2020--07--21-yellowgreen.svg)](/commits/master)\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# staplr\n\nThis package provides functions to manipulate PDF files:\n\n  - fill out PDF forms: get\\_fields() and set\\_fields()\n  - merge multiple PDF files into one: staple\\_pdf()\n  - remove selected pages from a file: remove\\_pages()\n  - rename multiple files in a directory: rename\\_files()\n  - rotate entire pdf document: rotate\\_pdf()\n  - rotate selected pages of a pdf file: rotate\\_pages()\n  - Select pages from a file: select\\_pages()\n  - splits single input PDF document into individual pages: split\\_pdf()\n  - splits single input PDF document into parts from given points:\n    split\\_from()\n\nThis package is still under development and this repository contains a\ndevelopment version of the R package *staplr*.\n\n## Installation\n\nstaplr requires a Java installation on your system. You can get the\nlatest version of java from [here](https://www.java.com/en/download/).\n[OpenJDK](https://openjdk.java.net/) also works.\n\nYou can install the stable version from CRAN.\n\n``` r\ninstall.packages('staplr', dependencies = TRUE)\n```\n\nYou can install staplr from github with:\n\n",
    "url": "https://github.com/pridiltal/staplr",
    "last_updated": "2025-08-26T06:22:58+00:00"
  },
  {
    "full_name": "sarahromanes/multiDA",
    "name": "multiDA",
    "description": "High Dimensional Discriminant Analysis in R  :sparkles:",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "machine-learning",
      "discriminant-analysis",
      "classification",
      "r-package",
      "rpackage"
    ],
    "readme": "multiDA [![Build Status](https://travis-ci.org/sarahromanes/multiDA.svg?branch=master)](https://travis-ci.org/sarahromanes/multiDA)\n[![](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing) \n  <img src=\"man/figures/test_logo.png\" align=\"right\"  height=\"250\" width=\"250\"/>\n======================================================\n\nHigh Dimensional Discriminant Analysis using Multiple Hypothesis Testing\n\nOverview\n--------\n\n**multiDA** is a Discriminant Analysis (DA) algorithm capable for use in high dimensional datasets, providing feature selection through multiple hypothesis testing. This algorithm has minimal tuning parameters, is easy to use, and offers improvement in speed compared to existing DA classifiers.\n\n**Publication to appear in JCGS. See our preprint - available on arXiv, [here](https://arxiv.org/pdf/1807.01422).**\n\nThis package is part of a suite of discriminant analysis packages we have authored for large-scale/complex datasets. See also our package [genDA](http://github.com/sarahromanes/genDA), a statistical ML method for Multi-distributional Discriminant Analysis using Generalised Linear Latent Variable Modelling.\n\nInstallation\n--------\n\n```r\n# Install the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"sarahromanes/multiDA\")\n\n```\n\n\nUsage\n-----\n\nThe following example trains the multiDA classifier using the SRBCT dataset, and finds the resubstitution error rate. \n\n```r\ny   <- SRBCT$y\nX   <- SRBCT$X\nres  <- multiDA(X, y, penalty=\"EBIC\", equal.var=TRUE, set.options=\"exhaustive\")\nvals <- predict(res, newdata=X)$y.pred          #y.pred returns class labels\nrser <- sum(vals!=y)/length(y)\n\n```\n\nA case study and overview of the statistical processes behind multiDA can be found [here](https://sarahromanes.github.io/multiDA/articles/multiDAvignette_caseStudy.html).\n\n## Authors\n\n* **Sarah Romanes**  - [@sarah_romanes](https://twitter.com/sarah_romanes)\n* **John Ormerod**  ",
    "url": "https://github.com/sarahromanes/multiDA",
    "last_updated": "2019-07-31T17:31:37+00:00"
  },
  {
    "full_name": "imgurbot12/pydhcp",
    "name": "pydhcp",
    "description": "Simple Python DHCP Library. DHCP Packet-Parsing/Client/Server",
    "language": "Python",
    "topics": [
      "dhcp",
      "python",
      "python3",
      "dhcp-client",
      "dhcp-server",
      "dhcpv4"
    ],
    "readme": "pydhcp\n-------\n\n[![PyPI version](https://img.shields.io/pypi/v/pydhcp3?style=for-the-badge)](https://pypi.org/project/pydhcp3/)\n[![Python versions](https://img.shields.io/pypi/pyversions/pydhcp3?style=for-the-badge)](https://pypi.org/project/pydhcp3/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://github.com/imgurbot12/pydhcp/blob/master/LICENSE)\n[![Made with Love](https://img.shields.io/badge/built%20with-%E2%99%A5-orange?style=for-the-badge)](https://github.com/imgurbot12/pydhcp)\n\nSimple Python DHCP Library. DHCP Packet-Parsing/Client/Server\n\n### Installation\n\n```\npip install pydhcp3\n```\n\n### DHCPv4 Examples\n\nPacket Parsing\n\n```python\nfrom pydhcp.v4 import Message\n\nhex = \\\n    '0101060000003d1d0000000000000000000000000000000000000000000b8201fc4200' +\\\n    '0000000000000000000000000000000000000000000000000000000000000000000000' +\\\n    '0000000000000000000000000000000000000000000000000000000000000000000000' +\\\n    '0000000000000000000000000000000000000000000000000000000000000000000000' +\\\n    '0000000000000000000000000000000000000000000000000000000000000000000000' +\\\n    '0000000000000000000000000000000000000000000000000000000000000000000000' +\\\n    '0000000000000000000000000000000000000000000000000000638253633501013d07' +\\\n    '01000b8201fc4232040000000037040103062aff00000000000000'\n\nraw     = bytes.fromhex(hex)\nmessage = Message.unpack(raw)\nprint(message)\n```\n\nClient\n\n```python\nfrom pydhcp.v4 import Message\nfrom pydhcp.v4.client import Client, new_message_id\n\nmac    = 'aa:bb:cc:dd:ee:ff'\nclient = Client(interface=None)\n\n# send crafted messages\nid       = new_message_id()\nhwaddr   = bytes.fromhex(mac.replace(':', ''))\nrequest  = Message.discover(id, hwaddr)\nresponse = client.request(request)\nprint(response)\n\n# or simplify the standard network assignment request process\nrecord = client.request_assignment(mac)\nprint(record)\n```\n\nServer\n\n```python\nimport logging\nfrom ipaddress import IPv4Address, IPv4Network\n\nfrom py",
    "url": "https://github.com/imgurbot12/pydhcp",
    "last_updated": "2025-08-16T21:59:02+00:00"
  },
  {
    "full_name": "jjallaire/TensorFlow-v1",
    "name": "TensorFlow-v1",
    "description": "",
    "language": "C",
    "topics": [],
    "readme": "",
    "url": "https://github.com/jjallaire/TensorFlow-v1",
    "last_updated": "2022-04-21T15:55:58+00:00"
  },
  {
    "full_name": "dirkschumacher/ompr",
    "name": "ompr",
    "description": "R package to model Mixed Integer Linear Programs",
    "language": "R",
    "topics": [
      "integer-programming",
      "linear-programming",
      "r",
      "mip",
      "milp",
      "optimization"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# Mixed integer linear programming in R\n\n<!-- badges: start -->\n\n[![R build\nstatus](https://github.com/dirkschumacher/ompr/workflows/R-CMD-check/badge.svg)](https://github.com/dirkschumacher/ompr/actions)\n[![CRAN\nStatus](https://www.r-pkg.org/badges/version/ompr)](https://cran.r-project.org/package=ompr)\n[![Codecov test\ncoverage](https://codecov.io/gh/dirkschumacher/ompr/branch/master/graph/badge.svg)](https://app.codecov.io/gh/dirkschumacher/ompr?branch=master)\n<!-- badges: end -->\n\nOMPR (Optimization Modeling Package) is a DSL to model and solve Mixed\nInteger Linear Programs. It is inspired by the excellent Jump project in\nJulia.\n\nHere are some problems you could solve with this package:\n\n-   What is the cost minimal way to visit a set of clients and return\n    home afterwards?\n-   What is the optimal conference time table subject to certain\n    constraints (e.g. availability of a projector)?\n-   [Sudokus](https://github.com/dirkschumacher/r-sudoku)\n\nThe [Wikipedia](https://en.wikipedia.org/wiki/Integer_programming)\narticle gives a good starting point if you would like to learn more\nabout the topic.\n\nI am always happy to get bug reports or feedback.\n\n## Install\n\n### CRAN\n\n``` r\ninstall.packages(\"ompr\")\ninstall.packages(\"ompr.roi\")\n```\n\n### Development version\n\nTo install the current development version use devtools:\n\n``` r\nremotes::install_github(\"dirkschumacher/ompr\")\nremotes::install_github(\"dirkschumacher/ompr.roi\")\n```\n\n## Available solver bindings\n\n-   [ompr.roi](https://github.com/dirkschumacher/ompr.roi) - Bindings to\n    ROI (GLPK, Symphony, CPLEX etc.)\n\n## A simple example:\n\n``` r\nsuppressPackageStartupMessages(library(dplyr, quietly = TRUE)) \nsuppressPackageStartupMessages(library(ROI))\nlibrary(ROI.plugin.glpk)\nlibrary(ompr)\nlibrary(ompr.roi)\n\nresult <- MIPModel() |>\n  add_variable(x, type = \"integer\") |>\n  add_variable(y, type = \"continuous\", lb = 0) |>\n  set_bounds(x, lb = 0) |>\n ",
    "url": "https://github.com/dirkschumacher/ompr",
    "last_updated": "2025-07-31T19:10:16+00:00"
  },
  {
    "full_name": "intel-machine-learning/DistML",
    "name": "DistML",
    "description": "DistML provide a supplement to mllib to support model-parallel on Spark",
    "language": "Java",
    "topics": [],
    "readme": "# DistML (Distributed Machine Learning platform)\n\n  DistML is a machine learning tool which allows traing very large models on Spark, it's fully compatible with Spark (tested on 1.2 or above).\n  \n  <img src=https://github.com/intel-machine-learning/DistML/blob/master/doc/architect.png>\n  \n  Reference paper: [Large Scale Distributed Deep Networks](http://research.google.com/archive/large_deep_networks_nips2012.html)\n  \n  \n  Runtime view:\n  \n  <img src=https://github.com/intel-machine-learning/DistML/blob/master/doc/runtime.png>\n  \n  DistML provides several algorithms (LR, LDA, Word2Vec, ALS) to demonstrate its scalabilites, however, you may need to write your own algorithms based on DistML APIs(Model, Session, Matrix, DataStore...), generally, it's simple to extend existed algorithms to DistML, here we take LR as an example: [How to implement logistic regression on DistML](https://github.com/intel-machine-learning/DistML/tree/master/doc/lr-implementation.md).\n\n### User Guide\n  1. [Download and build DistML](https://github.com/intel-machine-learning/DistML/tree/master/doc/build.md).\n  2. [Typical options](https://github.com/intel-machine-learning/DistML/tree/master/doc/options.md).\n  3. [Run Sample - LR](https://github.com/intel-machine-learning/DistML/tree/master/doc/sample_lr.md).\n  4. [Run Sample - MLR](https://github.com/intel-machine-learning/DistML/tree/master/doc/sample_mlr.md).\n  5. [Run Sample - LDA](https://github.com/intel-machine-learning/DistML/tree/master/doc/sample_lda.md).\n  6. [Run Sample - Word2Vec](https://github.com/intel-machine-learning/DistML/tree/master/doc/sample_word2vec.md).\n  7. [Run Sample - ALS](https://github.com/intel-machine-learning/DistML/tree/master/doc/sample_als.md).\n  8. [Benchmarks](https://github.com/intel-machine-learning/DistML/tree/master/doc/benchmarks.md).\n  9. [FAQ](https://github.com/intel-machine-learning/DistML/tree/master/doc/faq.md).\n\n### API Document\n  1. [Source Tree](https://github.com/intel-machine-learning/DistM",
    "url": "https://github.com/intel-machine-learning/DistML",
    "last_updated": "2025-06-07T14:15:47+00:00"
  },
  {
    "full_name": "rashidakamal/foia-online",
    "name": "foia-online",
    "description": "Analysis related to article on FOIA Online Database.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Analysis related to FOIA Online Database. \n\n[CJR - What makes a good FOIA request? We studied 33,000 to find out.](http://www.cjr.org/analysis/foia-request-how-to-study.php)\n\n+ Code by Nic & Rashida\n\n**1/31/2017**: We're hoping to have all of our code up tonight. Important note: the current \"analysis\" notebook is not what the conclusions of our article are based on, but rather, simpler \"gut-checks\" to poke through the data, guided by some of the terms that came up in our text analysis and decision tree analysis. That code will be posted shortly! \n\n---\n\nWe scraped all requests for years 2011 through 2016 available at [https://foiaonline.regulations.gov/](https://foiaonline.regulations.gov/) and filtered out requests that were still under agency review.\n\nTwo categories of features were considered: ones we derived ourselves and others indicating the presence of specific word in a request. We stemmed words using a lemmatizer before generating a document-term matrix. Below is a complete list of our derived features:\n\n+ Character count\n+ Word count\n+ Sentence count\n+ Average sentence length\n+ Whether the request mentioned the Freedom of Information Act\n+ Whether the request mentioned fulfillment fees\n+ Whether the request included a phone number\n+ Whether the request included a hyperlink\n+ Whether the request included an email address\n+ Whether the request included a date\n+ Whether the request made a reference to place\n+ A readability score derived from character, word and sentence counts\n+ Whether the request mentioned strings associated with data, like data formats\n+ A specificity score, which amounts to the number of groups of nouns\n+ Whether the request included an EPA identification number\n\nWe used decision trees as a first step in our search for important features. \n\nTwo methods of classifying requests were used. One limited our definition of a successful request to full grants. Another additionally considered partial grants to be successes. We did not observe a s",
    "url": "https://github.com/rashidakamal/foia-online",
    "last_updated": "2024-09-01T20:17:43+00:00"
  },
  {
    "full_name": "JaidedAI/EasyOCR",
    "name": "EasyOCR",
    "description": "Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.",
    "language": "Python",
    "topics": [
      "ocr",
      "deep-learning",
      "crnn",
      "pytorch",
      "lstm",
      "machine-learning",
      "scene-text",
      "scene-text-recognition",
      "optical-character-recognition",
      "cnn",
      "data-mining",
      "image-processing",
      "python",
      "easyocr",
      "information-retrieval"
    ],
    "readme": "# EasyOCR\n\n[![PyPI Status](https://badge.fury.io/py/easyocr.svg)](https://badge.fury.io/py/easyocr)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/JaidedAI/EasyOCR/blob/master/LICENSE)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.to/easyocr)\n[![Tweet](https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR)\n[![Twitter](https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat)](https://twitter.com/JaidedAI)\n\nReady-to-use OCR with 80+ [supported languages](https://www.jaided.ai/easyocr) and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.\n\n[Try Demo on our website](https://www.jaided.ai/easyocr)\n\nIntegrated into [Huggingface Spaces 🤗](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/tomofi/EasyOCR)\n\n\n## What's new\n- 24 September 2024 - Version 1.7.2\n    - Fix several compatibilities\n\n- [Read all release notes](https://github.com/JaidedAI/EasyOCR/blob/master/releasenotes.md)\n\n## What's coming next\n- Handwritten text support\n\n## Examples\n\n![example](examples/example.png)\n\n![example2](examples/example2.png)\n\n![example3](examples/example3.png)\n\n\n## Installation\n\nInstall using `pip`\n\nFor the latest stable release:\n\n``` bash\npip install easyocr\n```\n\nFor the latest development release:\n\n``` bash\npip install git+https://github.com/JaidedAI/EasyOCR.git\n```\n\nNote 1: For Windows, please install torch and torchvision first by following the official instructions here https://pytorch.org. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, selec",
    "url": "https://github.com/JaidedAI/EasyOCR",
    "last_updated": "2025-09-02T09:19:02+00:00"
  },
  {
    "full_name": "ytdl-org/youtube-dl",
    "name": "youtube-dl",
    "description": "Command-line program to download videos from YouTube.com and other video sites",
    "language": "Python",
    "topics": [],
    "readme": "[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI)\n\n\nyoutube-dl - download videos from youtube.com or other video platforms\n\n- [INSTALLATION](#installation)\n- [DESCRIPTION](#description)\n- [OPTIONS](#options)\n- [CONFIGURATION](#configuration)\n- [OUTPUT TEMPLATE](#output-template)\n- [FORMAT SELECTION](#format-selection)\n- [VIDEO SELECTION](#video-selection)\n- [FAQ](#faq)\n- [DEVELOPER INSTRUCTIONS](#developer-instructions)\n- [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)\n- [BUGS](#bugs)\n- [COPYRIGHT](#copyright)\n\n# INSTALLATION\n\nTo install it right away for all UNIX users (Linux, macOS, etc.), type:\n\n    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n    sudo chmod a+rx /usr/local/bin/youtube-dl\n\nIf you do not have curl, you can alternatively use a recent wget:\n\n    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl\n    sudo chmod a+rx /usr/local/bin/youtube-dl\n\nWindows users can [download an .exe file](https://yt-dl.org/latest/youtube-dl.exe) and place it in any location on their [PATH](https://en.wikipedia.org/wiki/PATH_%28variable%29) except for `%SYSTEMROOT%\\System32` (e.g. **do not** put in `C:\\Windows\\System32`).\n\nYou can also use pip:\n\n    sudo -H pip install --upgrade youtube-dl\n\nThis command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.\n\nmacOS users can install youtube-dl with [Homebrew](https://brew.sh/):\n\n    brew install youtube-dl\n\nOr with [MacPorts](https://www.macports.org/):\n\n    sudo port install youtube-dl\n\nAlternatively, refer to the [developer instructions](#developer-instructions) for how to check out and work with the git repository. For further options, including PGP signatures, see the [youtube-dl Download Page](https://ytdl-org.github.io/youtube-dl/download.html).\n\n# DESCRIPTION\n**y",
    "url": "https://github.com/ytdl-org/youtube-dl",
    "last_updated": "2025-09-02T09:34:29+00:00"
  },
  {
    "full_name": "dpp/lawyersongithub",
    "name": "lawyersongithub",
    "description": "A Telegram site for the people who are lawyers and also have GitHub accounts",
    "language": "",
    "topics": [],
    "readme": "# Lawyers on GitHub\n\nA club full of lawyers who also have GitHub accounts.\nAll you have to do to join is create a [pull request](https://github.com/dpp/lawyersongithub) with\ninformation about your bar membership.\n",
    "url": "https://github.com/dpp/lawyersongithub",
    "last_updated": "2025-08-26T01:05:17+00:00"
  },
  {
    "full_name": "hadley/15-state-of-the-union",
    "name": "15-state-of-the-union",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# Interactive graphics in R: A state of the union\n\nPresented at JSM 2015.\n\n## Installation tips\n\nI found most everything fairly easy to install on the mac, using brew to get package prereqs. YMMV.\n\n* [cranvas](http://cranvas.org/download.html#install-qt-under-mac-os-x)\n* [rggobi](http://www.ggobi.org/rggobi/): `brew install ggobi` + `install.packages(\"rggobi\", type = \"source\")`\n* [loon](https://github.com/waddella): currently not publicly available.\n* [Acinonyx](https://rforge.net/Acinonyx/): `devtools::install_url(\"https://rforge.net/src/contrib/Acinonyx_3.0-0.tar.gz\")`\n",
    "url": "https://github.com/hadley/15-state-of-the-union",
    "last_updated": "2024-09-02T07:05:19+00:00"
  },
  {
    "full_name": "sckott/request",
    "name": "request",
    "description": "http requests DSL for R",
    "language": "R",
    "topics": [
      "http",
      "r",
      "r-stats",
      "curl",
      "curl-library",
      "api"
    ],
    "readme": "request\n=======\n\n\n\n[![cran checks](https://cranchecks.info/badges/worst/request)](https://cranchecks.info/pkgs/request)\n[![Build Status](https://travis-ci.org/sckott/request.svg)](https://travis-ci.org/sckott/request)\n[![codecov.io](https://codecov.io/github/sckott/request/coverage.svg?branch=master)](https://codecov.io/github/sckott/request?branch=master)\n[![rstudio mirror downloads](http://cranlogs.r-pkg.org/badges/request?color=F3B1FF)](https://github.com/metacran/cranlogs.app)\n[![cran version](http://www.r-pkg.org/badges/version/request)](https://cran.r-project.org/package=request)\n\n`request` is DSL for http requests for R, and is inspired by the CLI tool [httpie](https://github.com/jakubroztocil/httpie).\n\n`request` is built on `httr`, though may allow using the R packages `RCurl` or `curl` as optional backends at some point.\n\nI gave a poster at User2016, its in my [talks repo](https://github.com/sckott/talks/blob/gh-pages/user2016/request.pdf)\n\n## Philosophy\n\n* The web is increasingly a JSON world, so we assume `applications/json` by default, but give back other types if not\n* The workflow follows logically, or at least should, from, _hey, I got this url_, to _i need to add some options_, to _execute request_\n* Whenever possible, we transform output to data.frame's - facilitating downstream manipulation via `dplyr`, etc.\n* We do `GET` requests by default. Specify a different type if you don't want `GET`\n* You can use non-standard evaluation to easily pass in query parameters without worrying about `&`'s, URL escaping, etc. (see `api_query()`)\n* Same for body params (see `api_body()`)\n\nAll of the defaults just mentioned can be changed.\n\n## Auto execute http requests with pipes\n\nWhen using pipes, we autodetect that a pipe is being used within the function calls, and automatically do the appropriate http request on the last piped function call. When you call a function without using pipes, you have to use the `http()` function explicitly to make the http request.\n",
    "url": "https://github.com/sckott/request",
    "last_updated": "2022-03-23T20:47:15+00:00"
  },
  {
    "full_name": "rushter/MLAlgorithms",
    "name": "MLAlgorithms",
    "description": "Minimal and clean examples of machine learning algorithms implementations",
    "language": "Python",
    "topics": [
      "machine-learning",
      "deep-learning",
      "neural-networks",
      "machine-learning-algorithms",
      "python"
    ],
    "readme": "# Machine learning algorithms\nA collection of minimal and clean implementations of machine learning algorithms.\n\n### Why?\nThis project is targeting people who want to learn internals of ml algorithms or implement them from scratch.  \nThe code is much easier to follow than the optimized libraries and easier to play with.  \nAll algorithms are implemented in Python, using numpy, scipy and autograd.  \n\n### Implemented:\n* [Deep learning (MLP, CNN, RNN, LSTM)](mla/neuralnet)\n* [Linear regression, logistic regression](mla/linear_models.py)\n* [Random Forests](mla/ensemble/random_forest.py)\n* [Support vector machine (SVM) with kernels (Linear, Poly, RBF)](mla/svm)\n* [K-Means](mla/kmeans.py)\n* [Gaussian Mixture Model](mla/gaussian_mixture.py)\n* [K-nearest neighbors](mla/knn.py)\n* [Naive bayes](mla/naive_bayes.py)\n* [Principal component analysis (PCA)](mla/pca.py)\n* [Factorization machines](mla/fm.py)\n* [Restricted Boltzmann machine (RBM)](mla/rbm.py)\n* [t-Distributed Stochastic Neighbor Embedding (t-SNE)](mla/tsne.py)\n* [Gradient Boosting trees (also known as GBDT, GBRT, GBM, XGBoost)](mla/ensemble/gbm.py)\n* [Reinforcement learning (Deep Q learning)](mla/rl)\n\n\n### Installation\n```sh\n        git clone https://github.com/rushter/MLAlgorithms\n        cd MLAlgorithms\n        pip install scipy numpy\n        python setup.py develop\n```\n### How to run examples without installation\n```sh\n        cd MLAlgorithms\n        python -m examples.linear_models\n```\n### How to run examples within Docker\n```sh\n        cd MLAlgorithms\n        docker build -t mlalgorithms .\n        docker run --rm -it mlalgorithms bash\n        python -m examples.linear_models\n```\n### Contributing\n\nYour contributions are always welcome!  \nFeel free to improve existing code, documentation or implement new algorithm.  \nPlease open an issue to propose your changes if they are big enough.  \n",
    "url": "https://github.com/rushter/MLAlgorithms",
    "last_updated": "2025-09-01T00:05:29+00:00"
  },
  {
    "full_name": "Tufte-LaTeX/tufte-latex",
    "name": "tufte-latex",
    "description": "A Tufte-inspired LaTeX class for producing handouts, papers, and books",
    "language": "TeX",
    "topics": [
      "tufte-latex",
      "tex",
      "latex",
      "latex-template",
      "latex-class",
      "tufte",
      "tufte-style",
      "template-latex",
      "template",
      "templates",
      "bookdesigns",
      "document-design"
    ],
    "readme": "Hi,\n\nWelcome to the beginnings of Tufte LaTeX package to help you\nproduce Tufte-style handouts, reports, and notes.\n\n## Quick Start\n\nTry typesetting `sample-handout.tex` with the following sequence\nof commands,\n\n    pdflatex sample-handout\n    bibtex   sample-handout\n    pdflatex sample-handout\n    pdflatex sample-handout\n\nThe result should look like `sample-handout.pdf`.\n\nThe sample book can be compiled with the following:\n\n    pdflatex sample-book\n    bibtex sample-book\n    texindy --language english sample-book.idx\n    # or makeindex sample-book.idx\n    pdflatex sample-book\n    pdflatex sample-book\n    pdflatex sample-book\n\n## Troubleshooting\n\nIf you encounter errors of the form,\n\n    ! LaTeX Error: File `paralist.sty' not found.\n\nyou will need to obtain missing packages from [CTAN](http://ctan.org).\nFor package installation instructions and answers to many other\nquestions, see the [UK TeX FAQ](http://www.tex.ac.uk/faq/) or search the [`comp.text.tex` group](http://groups.google.com/group/comp.text.tex).\n\nThe following packages are required:\n\n * chngpage or changepage\n * fancyhdr\n * fontenc\n * geometry\n * hyperref\n * natbib and bibentry\n * optparams\n * paralist\n * placeins\n * ragged2e\n * setspace\n * textcase\n * textcomp\n * titlesec\n * titletoc\n * xcolor\n * xifthen\n\nThe following packages are optional and will be automatically used if installed:\n\n * beramono\n * helvet\n * ifpdf\n * ifxetex\n * letterspace (in the microtype package)\n * mathpazo\n * soul\n\n## Bugs/Features/Support\n\nFor kudos, feature requests, patches, or support requests that you\nfeel are _particular_ to this Tufte-LaTeX package, i.e., not a general\nLaTeX issue, please use this project's issue tracker available at <https://github.com/Tufte-LaTeX/tufte-latex/issues>.\n\n## Contributing\n\nPatches and pull requests are most welcome via the issue tracker!  Submit a series of high quality patches, and you'll find yourself a developer on this project.\n\n## License\n\nCopyright 2007–2015 by Kevin Godby, Bil Kleb, an",
    "url": "https://github.com/Tufte-LaTeX/tufte-latex",
    "last_updated": "2025-08-29T15:31:43+00:00"
  },
  {
    "full_name": "rstudio/webinars",
    "name": "webinars",
    "description": "Code and slides for RStudio webinars",
    "language": "HTML",
    "topics": [],
    "readme": "RStudio Webinars\n================\n\nThis repository contains materials that have been used in RStudio webinars.\n\nYou can either clone this repository with git, or download the entire content as a zip file by clicking on the \"Download ZIP\" button on the right.\n\nRStudio Process\n================\n1. We start the webinars approximately 20 minutes prior to the launch for the public in case there are questions or concerns. \n2. All webinars are recorded and shortened to show only the content and questions.\n3. All recordings are posted on RStudio.com within 48 hours.\n4. A host will introduce the speaker and the topic and then hand control to the main presenter.\n5. The presenter will then share their screen and the host will confirm that they can see it.\n6. Once the presentation is complete the presenter will open the questions panel and review the red flagged questions. Questions are flagged by others from the company when they think it would benefit the entire audience to hear.\n7. Once questions are complete the host will take back control and close out the webinar.\n\n\nWebinar Guide\n================\nSo you want to make a great webinar? Use this helpful checklist.\n\nTitle Card\n=======\nMake your first slide a title card that contains:\n1. the webinar title\n2. a subtitle (optional)\n3. your name\n4. the date\n5. links to any resources\n6. a copyright (CC-BY-4.0 https://creativecommons.org/licenses/by/4.0/). \n\nThis will let the viewer know they are in the right place, and will help us edit the webinar for Youtube, etc.\n\nResources and Links\n================\nMake the resources that you use in the webinar available online, preferrably here at https://github.com/rstudio/webinars.\n\nPlease use the same naming convention for you files.\n\nUse a Rebrandly rstd.io URL shortener to create your links. This has two benefits:\n1. You can update the location of the resources at a later time, without needing to update the link in the video (videos are hard to edit). \n2. RStudio can track the download st",
    "url": "https://github.com/rstudio/webinars",
    "last_updated": "2025-09-01T00:04:33+00:00"
  },
  {
    "full_name": "leeper/tttable",
    "name": "tttable",
    "description": "Defining a grammar of tables",
    "language": "",
    "topics": [],
    "readme": "# A grammar of tables\n\n(c) Thomas J. Leeper (2018), licensed [CC-BY](LICENSE.md)\n\n*The Grammar of Graphics* introduced data analysts to an abstract and generalized \"grammar\" to translate raw data into data visualizations. **ggplot2** implemented those ideas in R. **dplyr** provides a similar grammar for data transformations. These packages have showcased the value of an abstract grammar as an approach to translating a data object into some kind of output and, in particular, have highlighted the value of working with \"tidy\" data frames as a fundamental unit of data analysis. But lacking from the tidy verse is a grammar of *tables*, the common, rectangular data structure that summarizes a dataset into cells representing subsets of the data, arranged as rows, columns, and facets. This document outlines a grammar of tables and provides a provisional R implementation.\n\n## Why a grammar of tables?\n\n**ggplot2** is successful in part because it separates the content of visualization being created (\"geoms\" and \"aesthetics\") and the styling of that visualization (\"themes\"). A grammar of tables would do the same for tabular presentations of data and it is something that has already been discussed in [various](http://stats.stackexchange.com/questions/3542/what-is-a-good-resource-on-table-design) [places](https://github.com/yihui/knitr/issues/53). This would represent a significant advance over existing approaches to tabular construction which closely couple content and styling (e.g., `xtable()`) and styling/rendering tools (e.g., `stargazer()`) which must be customized to work with every possible input format. R's tabular functions are also needlessly complex, such as the common expression `prop.table(table(x))` which summarizes a *table* rather than conveniently summarizing the original data.\n\nOther statistical software also merges content and styling inflexibly, such as the `tab var1, summarize(var2)` and `tabstat var2, s(min max mean median) by(var1)` approaches in Stata or ",
    "url": "https://github.com/leeper/tttable",
    "last_updated": "2024-02-12T15:35:20+00:00"
  },
  {
    "full_name": "dwillis/house-jobs",
    "name": "house-jobs",
    "description": "PDF and text version of House of Representatives Job and Internship Announcements",
    "language": "Python",
    "topics": [],
    "readme": "# house-jobs\nPDF files of House of Representatives Job and Internship Announcements\n\nThis repository contains tools for extracting, processing, and structuring job listings from the U.S. House of Representatives. See related blog post: https://thescoop.org/archives/2025/02/28/turning-congressional-job-listings-into-data/index.html\n\n## Overview\n\nThe House of Representatives regularly distributes PDF files containing job and internship announcements from members and committees. This project:\n\n1. Collects these PDF files\n2. Extracts the text content\n3. Processes the text using Large Language Models (LLMs)\n4. Produces structured JSON files with detailed information about each job listing\n\nThis is a public archive of those files.\n\n## Process Flow\n\n```\nPDF Files → Text Extraction → LLM Processing → Structured JSON\n```\n\n### 1. PDF Collection\n\nWeekly emails from the House of Representatives include PDF attachments with job and internship listings. These are archived in this repository.\n\n### 2. Text Extraction\n\nThe PDFs are converted to plain text files using `pdftotext`, triggered by GitHub Actions. The extracted text files are stored in the `output` directory with filenames that include the date of the listing.\n\n### 3. LLM Processing\n\nTwo parser implementations are provided:\n\n#### Parser 1 (`parser.py`)\n\nThe first approach:\n- Processes entire text files at once\n- Uses the Gemini 1.5 Flash model\n- Formats output as a JSON array of job listings\n- Handles basic text normalization\n\n#### Parser 2 (`parser2.py`)\n\nThe improved implementation:\n- Splits text into chunks based on job IDs (MEM-xxx-yy pattern)\n- Processes each chunk separately\n- Uses the Gemini 2.0 Flash model\n- Implements more robust error handling\n- Provides more comprehensive field extraction\n- Includes better handling of non-ASCII characters and formatting\n\n### 4. JSON Output\n\nThe final output is stored in JSON files with a structured format. Each job listing contains fields such as:\n\n- `id`: Job ID in MEM-XXX-YY ",
    "url": "https://github.com/dwillis/house-jobs",
    "last_updated": "2025-08-04T20:55:30+00:00"
  },
  {
    "full_name": "Hack-with-Github/Awesome-Hacking",
    "name": "Awesome-Hacking",
    "description": "A collection of various awesome lists for hackers, pentesters and security researchers",
    "language": "",
    "topics": [
      "hacking",
      "security",
      "bug-bounty",
      "awesome",
      "android",
      "fuzzing",
      "penetration-testing",
      "pentesting-windows",
      "reverse-engineering"
    ],
    "readme": "![Awesome Hacking](awesome_hacking.jpg)\n\n# [Awesome Hacking](https://github.com/Hack-with-Github/Awesome-Hacking) [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Awesome%20Hacking%20-%20a%20collection%20of%20awesome%20lists%20for%20hackers%20and%20pentesters%20by%20@HackwithGithub&url=https://github.com/Hack-with-Github/Awesome-Hacking&hashtags=security,hacking)\n\n**A collection of awesome lists for hackers, pentesters & security researchers.**\n\nYour [contributions](contributing.md) are always welcome !\n\n## Awesome Repositories\n\nRepository | Description\n---- | ----\n[Android Security](https://github.com/ashishb/android-security-awesome) \t\t\t| Collection of Android security related resources\n[AppSec](https://github.com/paragonie/awesome-appsec)\t\t\t\t\t\t\t\t| Resources for learning about application security\n[Asset Discovery](https://github.com/redhuntlabs/Awesome-Asset-Discovery)    | List of resources which help during asset discovery phase of a security assessment engagement\n[Bug Bounty](https://github.com/djadmin/awesome-bug-bounty) \t\t\t\t\t\t| List of Bug Bounty Programs and write-ups from the Bug Bounty hunters\n[Capsulecorp Pentest](https://github.com/r3dy/capsulecorp-pentest) \t\t\t\t\t\t| Vagrant+Ansible virtual network penetration testing lab. Companion to \"The Art of Network Penetration Testing\" by Royce Davis \n[Celluar Hacking](https://github.com/W00t3k/Awesome-Cellular-Hacking)    | This is a list of hacking research in the 3G/4G/5G cellular security space. \n[CTF](https://github.com/apsdehal/awesome-ctf) \t\t\t\t\t\t\t\t\t\t| List of CTF frameworks, libraries, resources and softwares\n[Cyber Skills](https://github.com/joe-shenouda/awesome-cyber-skills) | Curated list of hacking environments where you can train your cyber skills legally and safely\n[DevSecOps](https://github.com/devsecops/awesome-devsecops) \t\t\t\t\t\t| List of awesome DevSecOps tools with the help from community experiments and contributions\n[Embedded and",
    "url": "https://github.com/Hack-with-Github/Awesome-Hacking",
    "last_updated": "2025-09-02T09:21:28+00:00"
  },
  {
    "full_name": "shlbatra/TriggerWakeWordDetection",
    "name": "TriggerWakeWordDetection",
    "description": "Project to identify wake work in Speech",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "<h1 style=\"text-align: center;\">Wake Word Detector</h1>\n<img src=\"images/wake_word_detect.png\">\n\n# Table of Contents\n- [Background](#background)\n- [Introduction](#introduction)\n- [Related Work](#related-work)\n- [Implementation](#implementation)\n    - [Preparing labelled dataset](#preparing-labelled-dataset)\n    - [Word Alignment](#word-alignment)\n    - [Fix data imbalance](#fix-data-imbalance)\n    - [Extract audio features](#extract-audio-features)\n    - [Audio transformations](#audio-transformations)\n    - [Define model architecture](#define-model-architecture)\n    - [Train model](#train-model)\n    - [Test Model](#test-model)\n    - [Inference](#inference)\n        - [Using Pyaudio](#using-pyaudio)\n        - [Using web sockets](#using-web-sockets)\n        - [Using onnx](#using-onnx)\n        - [Using tensorflowjs](#using-tensorflowjs)\n        - [Using tflite](#using-tflite)\n- [Demo](#demo)\n- [Slides](#slides)\n- [Conclusion](#conclusion)\n- [Enhancements](#enhancements)\n\n# Background\nPersonal Assistant devices like Google Home, Alexa and Apple Homepod, will be constantly listening for specific set of wake words like “Ok, Google” or “Alexa” or “Hey Siri”, and once these sequence of words are detected it would prompt to user for next commands and respond to them appropriately.\n\n# Introduction\nTo create a open-source custom wake word detector, which will take audio as input and once the sequence of words are detected then prompt to the user. <br>\n\nGoal is to provide configurable custom detector so that anyone can use it on their own application to perform operations, once configured wake words are detected.\n\n# Related Work\n- Firefox Voice \n    - Model was trained using Mozilla Common Voice dataset, used Pytorch (refer paper [Howl](https://arxiv.org/abs/2008.09606)) library to extract audio features and to train model on res8. Custom logic MeydaMelSpectrogram was used to train the model. \n    - Used [Meyda: an audio feature extraction library for the Web\nAudio API](http://d",
    "url": "https://github.com/shlbatra/TriggerWakeWordDetection",
    "last_updated": "2021-12-19T23:45:40+00:00"
  },
  {
    "full_name": "mpatacchiola/Y-AE",
    "name": "Y-AE",
    "description": "Official Tensorflow implementation of the paper \"Y-Autoencoders: disentangling latent representations via sequential-encoding\", Pattern Recognition Letters (2020)",
    "language": "Python",
    "topics": [
      "autoencoders",
      "mnist",
      "tensorflow",
      "disentangled-representations",
      "paper",
      "y-autoencoder",
      "generative-model"
    ],
    "readme": "\n<p align=\"center\">\n<img src=\"etc/img/fig_samples.png\" width=\"800\">\n</p>\n\nThis repository contains the official implementation of the paper: \n\n*\"Y-Autoencoders: disentangling latent representations via sequential-encoding\"*, Massimiliano Patacchiola, Patrick Fox-Roberts, Edward Rosten. Pattern Recognition Letters, vol. 140, pp. 59-65. [[download paper from arxiv]](https://arxiv.org/pdf/1907.10949.pdf)\n\nPlease, cite this paper if you use the code in this repository as part of a published research project:\n\n```\n@article{patacchiola2020yautoencoders,\n  title={Y-Autoencoders: disentangling latent representations via sequential-encoding},\n  author={Patacchiola, Massimiliano and Fox-Roberts, Patrick and Rosten, Edward},\n  journal={Pattern Recognition Letters},\n  volume={140},\n  pages={59--65},\n  year={2020}\n}\n```\n\nThis code allows reproducing the quantitative results reported in the paper. In particular the ablation experiment (Section 3.2) and the comparison against baselines experiment (Section 3.3).\n\nRequirements\n============\n\nThe code has been tested on `Ubuntu 18.04.2 LTS`, with `Python 2.7` , `Tensorflow 1.10` and `Numpy 1.15.1`. In the test phase `OpenCV 3.4.3.18` and `Scikit-image 0.14.2` have been used to manipulate and visualize the samples. We reccommend the use of a virtual environment (e.g. [conda](https://docs.conda.io/en/latest/)) to setup all the packages. Even though training and test have been performend on the GeForce GTX 1060, a GPU is not strictly required, but a decent amount of RAM may be necessary in order to load the model and the dataset. To better visualize this README offline the use of `grip` is suggested:\n\n```\npip install grip\ngrip ./README.md\n```\n\nThis will allow to visualize the README offline directly from the browser.\n\nTo install Tensorflow GPU version 1.10 run this command from `pip`:\n```\npip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl\n```\n\nOtherwise you can install the CPU",
    "url": "https://github.com/mpatacchiola/Y-AE",
    "last_updated": "2025-01-02T17:48:25+00:00"
  },
  {
    "full_name": "WinVector/rquery",
    "name": "rquery",
    "description": "Data Wrangling and Query Generating Operators for R. Distributed under choice of GPL-2 or GPL-3 license.",
    "language": "HTML",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/rquery)](https://cran.r-project.org/package=rquery)\n[![status](https://tinyverse.netlify.com/badge/rquery)](https://CRAN.R-project.org/package=rquery)\n\n# `rquery`\n\n[`rquery`](https://winvector.github.io/rquery/) is a piped query\ngenerator based on [Codd’s relational\nalgebra](https://en.wikipedia.org/wiki/Relational_algebra) (updated to\nreflect lessons learned from working with\n[`R`](https://www.r-project.org),\n[`SQL`](https://en.wikipedia.org/wiki/SQL), and\n[`dplyr`](https://CRAN.R-project.org/package=dplyr) at big data scale in\nproduction).\n\n## Introduction\n\n[`rquery`](https://github.com/WinVector/rquery) is a data wrangling\nsystem designed to express complex data manipulation as a series of\nsimple data transforms. This is in the spirit of `R`’s\n`base::transform()`, or `dplyr`’s `dplyr::mutate()` and uses a pipe in\nthe style popularized in `R` with `magrittr`. The operators themselves\nfollow the selections in Codd’s relational algebra, with the addition of\nthe traditional `SQL` “window functions.” More on the background and\ncontext of `rquery` can be found\n[here](https://github.com/WinVector/rquery/blob/master/Examples/old_readme/README.md).\n\nThe `R`/`rquery` version of this introduction is\n[here](https://github.com/WinVector/rquery/blob/main/Examples/Introduction/rquery_Introduction.md),\nand the `Python`/`data_algebra` version of this introduction is\n[here](https://github.com/WinVector/data_algebra/blob/main/Examples/Introduction/data_algebra_Introduction.ipynb).\n\nIn transform formulations data manipulation is written as\ntransformations that produce new `data.frame`s, instead of as\nalterations of a primary data structure (as is the case with\n`data.table`). Transform system *can* use more space and time than\nin-place methods. However, in our opinion, transform systems have a\nnumber of pedagogical advantages.\n\nIn `rquery`’s case the pri",
    "url": "https://github.com/WinVector/rquery",
    "last_updated": "2025-01-02T20:51:44+00:00"
  },
  {
    "full_name": "mtdvio/every-programmer-should-know",
    "name": "every-programmer-should-know",
    "description": "A collection of (mostly) technical things every software developer should know about",
    "language": "",
    "topics": [
      "cc-by",
      "computer-science",
      "educational",
      "novice",
      "collection"
    ],
    "readme": "> *[Join our community](https://metadevelopment.io/)* for professional Software Developers and get more control over your life and career! \n\n----\n\n# Every Programmer Should Know :thinking:\nA collection of (mostly) technical things every software developer should know.\n\n:point_up: *These are resources I can recommend to every programmer regardless of their skill level or tech stack*\n\nHighly opinionated :bomb:. Not backed by science.\nComes in no particular order :recycle:\n\nU like it? :star: it and [share](https://twitter.com/mr_mig_by/status/900735231552098306) with a friendly developer!\nU don't like it? [Watch the doggo](https://twitter.com/RespectfulMemes/status/900147758845308930) :dog:\n\n*P.S. You [don't need to know](https://xkcd.com/1050/) all of that by heart to be a programmer.\nBut knowing the stuff will help you become better! :muscle:*\n\n*P.P.S. [Contributions](CONTRIBUTING.md) are welcome!*\n\n----\n\n### Introduction\n- :movie_camera: [Map of Computer Science](https://www.youtube.com/watch?v=SzJ46YA_RaA)\n- :movie_camera: [40 Key Computer Science Concepts Explained In Layman’s Terms](http://carlcheo.com/compsci)\n- :page_facing_up: [Computer Science Roadmap](https://roadmap.sh/computer-science)\n\n### Falsehoods\n- [Awesome Falsehoods](https://github.com/kdeldycke/awesome-falsehood)\n  💊 Curated list of falsehoods programmers believe in.\n  Check for things you do not know about Strings, Addresses, Names, Numbers, Emails, Timezones and Dates and more.\n\n### Algorithms\n- [Big O Cheatsheet](http://bigocheatsheet.com/)\n- :book: [Computer Science Distilled](https://www.goodreads.com/book/show/34189798-computer-science-distilled)\n- :book: [Grokking Algorithms](https://www.goodreads.com/book/show/22847284-grokking-algorithms-an-illustrated-guide-for-programmers-and-other-curio)\n- :book: [Introduction to Algorithms](https://www.goodreads.com/book/show/108986.Introduction_to_Algorithms?from_search=true&from_srp=true&qid=8mUglV9uZ1&rank=1)\n- [Algorithms Visualization](https://www",
    "url": "https://github.com/mtdvio/every-programmer-should-know",
    "last_updated": "2025-09-02T08:51:07+00:00"
  },
  {
    "full_name": "jvns/classificator",
    "name": "classificator",
    "description": "",
    "language": "Go",
    "topics": [],
    "readme": "# classificator\n\na tool for taking text responses and classifying them by hand. I just made it\nfor myself and it has a lot of problems (very messy CSS, lots of UI issues) but open sourcing it anyway.\n\nIt stores everything in a SQLite database.\n\n### how to use it\n\nthese instructions should work:\n\n```\n$ sqlite3 comments.db < schema.sql\n$ go run .\n```\n\nThen open http://localhost:8080\n\n### a screenshot\n\nHere's a screenshot of what the main interface looks like:\n\n![Screenshot 2025-02-05 at 2 37 22 PM](https://github.com/user-attachments/assets/31fe3d3c-f4b4-49b0-a768-fb05ad2686ad)\n",
    "url": "https://github.com/jvns/classificator",
    "last_updated": "2025-08-24T19:09:51+00:00"
  },
  {
    "full_name": "PAIR-code/facets",
    "name": "facets",
    "description": "Visualizations for machine learning datasets",
    "language": "Jupyter Notebook",
    "topics": [
      "machine-learning",
      "data-visualization"
    ],
    "readme": "# Introduction\n\nThe facets project contains two visualizations for understanding and analyzing machine learning datasets: Facets Overview and Facets Dive.\n\nThe visualizations are implemented as [Polymer](https://www.polymer-project.org) web components, backed by [Typescript](https://www.typescriptlang.org) code and can be easily embedded into Jupyter notebooks or webpages.\n\nLive demos of the visualizations can be found on the [Facets project description page](https://pair-code.github.io/facets/).\n\n## Facets Overview\n\n![Overview visualization of UCI census data](/img/overview-census.png \"Overview visualization of UCI census data -  Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml/datasets/Census+Income]. Irvine, CA: University of California, School of Information and Computer Science\")\n\nOverview gives a high-level view of one or more data sets. It produces a visual feature-by-feature statistical analysis, and can also be used to compare statistics across two or more data sets. The tool can process both numeric and string features, including multiple instances of a number or string per feature.\n\nOverview can help uncover issues with datasets, including the following:\n\n* Unexpected feature values\n* Missing feature values for a large number of examples\n* Training/serving skew\n* Training/test/validation set skew\n\nKey aspects of the visualization are outlier detection and distribution comparison across multiple datasets.\nInteresting values (such as a high proportion of missing data, or very different distributions of a feature across multiple datasets) are highlighted in red.\nFeatures can be sorted by values of interest such as the number of missing values or the skew between the different datasets.\n\nThe python code to generate the statistics for visualization can be installed through `pip install facets-overview`.\nAs of version 1.1.0, the `facets-overview` package requires a version of `protobuf` at version 3.20.0 or later.\n\nDetails abou",
    "url": "https://github.com/PAIR-code/facets",
    "last_updated": "2025-08-22T11:50:38+00:00"
  },
  {
    "full_name": "tectonic-typesetting/tectonic",
    "name": "tectonic",
    "description": "A modernized, complete, self-contained TeX/LaTeX engine, powered by XeTeX and TeXLive.",
    "language": "C",
    "topics": [
      "tex",
      "xetex",
      "tex-engine",
      "texlive",
      "typesetting",
      "tex-typesetting",
      "rust"
    ],
    "readme": "[![Build Status](https://dev.azure.com/tectonic-typesetting/tectonic/_apis/build/status/tectonic-typesetting.tectonic?branchName=master)](https://dev.azure.com/tectonic-typesetting/tectonic/_build/latest?definitionId=11&branchName=master)\n[![](http://meritbadge.herokuapp.com/tectonic)](https://crates.io/crates/tectonic)\n[![codecov](https://codecov.io/gh/tectonic-typesetting/tectonic/branch/master/graph/badge.svg)](https://codecov.io/gh/tectonic-typesetting/tectonic)\n\n# Tectonic\n\nTectonic is a modernized, complete, self-contained\n[TeX](https://en.wikipedia.org/wiki/TeX)/[LaTeX](https://www.latex-project.org/)\nengine, powered by [XeTeX](http://xetex.sourceforge.net/) and\n[TeXLive](https://www.tug.org/texlive/).\n\n## Read this first\n\nIf you just want to compile TeX documents, you should probably **click through\nto [the main Tectonic website](https://tectonic-typesetting.github.io/)**. This\npage is primarily aimed at folks interested in how Tectonic works \"under the\nhood.\" If you want to build the [`tectonic`][crate] Rust crate, check out [its\nREADME](./CARGO_README.md).\n\n[crate]: https://crates.io/crates/tectonic\n\n## Developer dashboard\n\n<a href=\"https://repology.org/metapackage/tectonic\">\n    <img src=\"https://repology.org/badge/vertical-allrepos/tectonic.svg\" alt=\"Packaging status\" align=\"right\">\n</a>\n\n- [User website](https://tectonic-typesetting.github.io/)\n- [Community discussion forum](https://github.com/tectonic-typesetting/tectonic/discussions)\n- [Installation](https://tectonic-typesetting.github.io/book/latest/installation/)\n- [Build instructions](https://tectonic-typesetting.github.io/book/latest/howto/build-tectonic/)\n- [API documentation](https://docs.rs/tectonic/)\n- [Issues](https://github.com/tectonic-typesetting/tectonic/issues/)\n- [Changelog](https://github.com/tectonic-typesetting/tectonic/blob/release/CHANGELOG.md)\n\n## Technical ecosystem\n\nIf you’re interested in Tectonic as a software tool, you might also want to check out:\n\n- One of the following [Gi",
    "url": "https://github.com/tectonic-typesetting/tectonic",
    "last_updated": "2025-09-01T14:14:09+00:00"
  },
  {
    "full_name": "nvbn/thefuck",
    "name": "thefuck",
    "description": "Magnificent app which corrects your previous console command.",
    "language": "Python",
    "topics": [
      "python",
      "shell"
    ],
    "readme": "# The Fuck [![Version][version-badge]][version-link] [![Build Status][workflow-badge]][workflow-link] [![Coverage][coverage-badge]][coverage-link] [![MIT License][license-badge]](LICENSE.md)\n\n*The Fuck* is a magnificent app, inspired by a [@liamosaur](https://twitter.com/liamosaur/)\n[tweet](https://twitter.com/liamosaur/status/506975850596536320),\nthat corrects errors in previous console commands.\n\n\nIs *The Fuck* too slow? [Try the experimental instant mode!](#experimental-instant-mode)\n\n[![gif with examples][examples-link]][examples-link]\n\nMore examples:\n\n```bash\n➜ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\n➜ fuck\nsudo apt-get install vim [enter/↑/↓/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...\n```\n\n```bash\n➜ git push\nfatal: The current branch master has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin master\n\n\n➜ fuck\ngit push --set-upstream origin master [enter/↑/↓/ctrl+c]\nCounting objects: 9, done.\n...\n```\n\n```bash\n➜ puthon\nNo command 'puthon' found, did you mean:\n Command 'python' from package 'python-minimal' (main)\n Command 'python' from package 'python3' (main)\nzsh: command not found: puthon\n\n➜ fuck\npython [enter/↑/↓/ctrl+c]\nPython 3.4.2 (default, Oct  8 2014, 13:08:17)\n...\n```\n\n```bash\n➜ git brnch\ngit: 'brnch' is not a git command. See 'git --help'.\n\nDid you mean this?\n    branch\n\n➜ fuck\ngit branch [enter/↑/↓/ctrl+c]\n* master\n```\n\n```bash\n➜ lein rpl\n'rpl' is not a task. See 'lein help'.\n\nDid you mean this?\n         repl\n\n➜ fuck\nlein repl [enter/↑/↓/ctrl+c]\nnREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848\nREPL-y 0.3.1\n...\n```\n\nIf you're not afraid of blindly running corrected commands, the\n`require_confirmation` [settings](#settings) option can be disabled:\n\n```bash\n➜ apt-get install vim\nE: Could not open l",
    "url": "https://github.com/nvbn/thefuck",
    "last_updated": "2025-09-02T09:28:34+00:00"
  },
  {
    "full_name": "assertpy/assertpy",
    "name": "assertpy",
    "description": "Simple assertion library for unit testing in python with a fluent API",
    "language": "Python",
    "topics": [],
    "readme": "# assertpy\n\nSimple assertions library for unit testing in Python with a nice fluent API.  Supports both Python 2 and 3.\n\n[![Build Status](https://travis-ci.org/assertpy/assertpy.svg?branch=main)](https://travis-ci.org/assertpy/assertpy)\n[![Coverage Status](https://coveralls.io/repos/github/assertpy/assertpy/badge.svg?branch=main)](https://coveralls.io/github/assertpy/assertpy?branch=main)\n\n\n## Usage\n\nJust import the `assert_that` function, and away you go...\n\n```py\nfrom assertpy import assert_that\n\ndef test_something():\n    assert_that(1 + 2).is_equal_to(3)\n    assert_that('foobar').is_length(6).starts_with('foo').ends_with('bar')\n    assert_that(['a', 'b', 'c']).contains('a').does_not_contain('x')\n```\n\nOf course, `assertpy` works best with a python test runner like [pytest](http://pytest.org/) (our favorite) or [Nose](http://nose.readthedocs.org/).\n\n\n## Installation\n\n### Install via pip\n[![PyPI Badge](https://badge.fury.io/py/assertpy.svg)](https://pypi.org/project/assertpy/)\n\nThe `assertpy` library is available via [PyPI](https://pypi.org/project/assertpy/).\nJust install with:\n\n```\npip install assertpy\n```\n\n### Install via conda\n\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/assertpy.svg)](https://anaconda.org/conda-forge/assertpy)\n[![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/assertpy.svg)](https://anaconda.org/conda-forge/assertpy)\n\nOr, if you are a big fan of [conda](https://conda.io/) like we are, there is an [assertpy-feedstock](https://github.com/conda-forge/assertpy-feedstock) for [Conda-Forge](https://conda-forge.org/) that you can use:\n\n```\nconda install assertpy --channel conda-forge\n```\n\n\n## Docs\n\nThe fluent API of `assertpy` is designed to create compact, yet readable tests.\nThe API has been modeled after other fluent testing APIs, especially the awesome\n[AssertJ](https://assertj.github.io/doc/) assertion library for Java.  Of course, in the `assertpy` library everything is fully pythonic and designed to take full adva",
    "url": "https://github.com/assertpy/assertpy",
    "last_updated": "2025-07-27T20:26:18+00:00"
  },
  {
    "full_name": "ourresearch/depsy",
    "name": "depsy",
    "description": "Track the impact of research software.",
    "language": "Python",
    "topics": [],
    "readme": "\n*the Depsy project has [successfully concluded,](https://twitter.com/depsy_org/status/970376969782149120)\nand is no longer being maintained. We're now working on a followup project to track software impact, \n[funded by the Sloan Foundation.](http://blog.impactstory.org/collaborating-635k-grant-improve-credit-research-software/)*\n\n## We need to value the software that powers science\n\nToday's cutting-edge science is built on an array of specialist research software. This research software is often as important as traditional scholarly papers--[but it's not treated that way when it comes to funding and tenure](http://sciencecodemanifesto.org/). There, the traditional publish-or-perish, show-me-the-Impact-Factor system still rules.\n\nWe need to fix that. We need to provide meaningful incentives for the [scientist-developers](http://dirkgorissen.com/2012/03/26/the-researcher-programmer-a-new-species/) who make important research software, so that we can keep doing important, software-driven science.\n\n## Depsy helps value research software\n\n[Lots of things have to happen](http://rev.oxfordjournals.org/content/early/2015/07/26/reseval.rvv014.full) to support this change. Depsy is a shot at making one of those things happen: a system that tracks the impact of software in *software-native ways*.\n\nThat means not just counting up citations to a hastily-written paper *about* the software, but actual mentions of *the software itself* in the literature. It means looking how software gets reused by other software, even when it's not cited at all. And it means understanding the full complexity of software authorship, where one project can involve hundreds of contributors in multiple roles that don't map to traditional paper authorship.\n\n## This is just the beginning\n\nDepsy doesn't do any of these things perfectly, and it's not supposed to. Instead, Depsy is a proof-of-concept to show that we can do them at all. The data and tools are there. We *can* measure and reward software impa",
    "url": "https://github.com/ourresearch/depsy",
    "last_updated": "2025-08-18T09:27:32+00:00"
  },
  {
    "full_name": "tidymodels/parsnip",
    "name": "parsnip",
    "description": "A tidy unified interface to models",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# parsnip <a href=\"https://parsnip.tidymodels.org/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"a drawing of a parsnip on a beige background\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/tidymodels/parsnip/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/parsnip/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidymodels/parsnip/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/parsnip?branch=main)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/parsnip)](https://CRAN.R-project.org/package=parsnip)\n[![Downloads](https://cranlogs.r-pkg.org/badges/parsnip)](https://CRAN.R-project.org/package=parsnip)\n[![lifecycle](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidymodels/parsnip/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/parsnip)\n<!-- badges: end -->\n\n## Introduction\n\nThe goal of parsnip is to provide a tidy, unified interface to models\nthat can be used to try a range of models without getting bogged down in\nthe syntactical minutiae of the underlying packages.\n\n## Installation\n\n``` r\n# The easiest way to get parsnip is to install all of tidymodels:\ninstall.packages(\"tidymodels\")\n\n# Alternatively, install just parsnip:\ninstall.packages(\"parsnip\")\n\n# Or the development version from GitHub:\n# install.packages(\"pak\")\npak::pak(\"tidymodels/parsnip\")\n```\n\n## Getting started\n\nOne challenge with different modeling functions available in R *that do\nthe same thing* is that they can have different interfaces and\narguments. For example, to fit a random forest regression model, we\nmight have:\n\n``` r\n# From randomForest\nrf_1 <- randomForest(\n  y ~ ., \n  data = dat, \n  mtry = 10, \n  ntree = 2000, \n  importance = TRUE\n)\n\n# From ranger\nrf_2 <- ranger(\n  y ~ ., \n  d",
    "url": "https://github.com/tidymodels/parsnip",
    "last_updated": "2025-08-31T20:18:14+00:00"
  },
  {
    "full_name": "tellform/tellform",
    "name": "tellform",
    "description": "✏️ Free Opensource Alternative to TypeForm or Google Forms ⛺",
    "language": "JavaScript",
    "topics": [
      "typeform",
      "forms",
      "self-hosted",
      "mean-stack",
      "form-builder",
      "google-forms",
      "nodejs"
    ],
    "readme": "TellForm 2.1.0\n========\n\nDEPRECATION WARNING UNTIL FURTHER NOTICE.\nThere are many oudated and vulnerable dependencies within this project and I recommend that you use this code repository for internal testing and development only.\n\nThere were too many impassable hurdles to really continue forward at the pace that I was hoping with TellForm @leopere~ If you want to follow my progress on an alternative in the mean time check out https://OhMyForm.com or our Discord server.  We managed to get the base Docker image fixed before forking the code so you can give this a try however not much has changed at the moment.\n<!--\n[![Code Shelter](https://www.codeshelter.co/static/badges/badge-flat.svg)](https://www.codeshelter.co/)\n[![Build Status](https://travis-ci.org/tellform/tellform.svg?branch=master)](https://travis-ci.org/tellform/tellform)\n![Project Status](https://img.shields.io/badge/status-2.1.0-green.svg)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/3491e86eb7194308b8fc80711d736ede)](https://www.codacy.com/app/david-baldwin/tellform?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=tellform/tellform&amp;utm_campaign=Badge_Grade)\n-->\n\nTo Join the fork's community please follow this Discord button here.\n\n![Discord](https://img.shields.io/discord/595773457862492190.svg?label=Discord%20Chat)\n\n## Readme and Issues\nThe README.md is still effectively in tact however it's all been commented out so that it's no longer visible on the main github repository page.  You may visit it by navigating through the repositories files themselves.  \n\nNo new or old issues will be tended to so the Issues Board has been closed.  We don't recommend using this repositories codebase as its no longer maintained and is only intended for reference code.  If you wish to use the fork which should remain backwards compatible feel free to explore [https://ohmyform.com](https://ohmyform.com/) or its GitHub repository at [https://github.com/ohmyform/ohmyform/](https://github.com/",
    "url": "https://github.com/tellform/tellform",
    "last_updated": "2025-08-25T16:31:19+00:00"
  },
  {
    "full_name": "hadley/rminds",
    "name": "rminds",
    "description": "Sample R code for visualising models (especially models in data space)",
    "language": "",
    "topics": [],
    "readme": "",
    "url": "https://github.com/hadley/rminds",
    "last_updated": "2025-03-22T11:17:36+00:00"
  },
  {
    "full_name": "slifty/opened-captions",
    "name": "opened-captions",
    "description": "A SocketIO API for live TV closed captions",
    "language": "JavaScript",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/slifty/opened-captions.png)](https://travis-ci.org/slifty/opened-captions)\n\nOpenedCaptions\n=============\n> A distributed API for live TV closed captions.\n\nThis package can be easily incorproated into any node project in order to write code that knows what is happening on live television.\n\nPlease be warned that the project is early in its development, which means the API and code base are generally unstable for the short term.\n\nInitially, this package is intended to power front-end applications, but eventually it will support server side events as well\n\nNew in 1.0\n=============\nOpened Captions used to be a standalone node app; you could download it and run it, which was fine for a quick hack but didn't make it particularly easy to use in your own code base.  Now Opened Captions is an NPM package.  See below for the new installation instructions.  You can also use the [example server](https://github.com/slifty/opened-captions-example) to get started.\n\nThe architecture has been redone to make it easy to create new stream types.\nPayloads also include information about the stream itself, thus supporting multiple channels from a single server.\n\nInstalling Opened Captions\n=============\n\n```shell\nnpm install opened-captions\n```\n\n\nCreating an Opened Captions Stream\n=============\n\nCreating an opened captions server is easy:\n\n```\n\n# Create the Server\nvar OpenedCaptions = require('opened-captions');\nvar oc = new OpenedCaptions();\n\n# Add a Stream\noc.addStream('random');\n\n# Add a Better Stream\noc.addStream('server', {\n  host: 'https://openedcaptions.com',\n  port: 443,\n  description: \"CSPAN\"\n});\n\n```\n\nYou also can use [this example server](https://github.com/slifty/opened-captions-example) to get started.\n\n\nTools\n=============\nTests are written in Mocha.\n\n\nLicensing\n=============\nThe MIT License (MIT)\n-------------\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files ",
    "url": "https://github.com/slifty/opened-captions",
    "last_updated": "2025-03-09T19:36:05+00:00"
  },
  {
    "full_name": "notnews/notnews",
    "name": "notnews",
    "description": "classifiers for soft news based on the story text and the url structure for both the US and UK news media.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "notnews: predict soft news using story text and the url structure\n=================================================================\n\n.. image:: https://github.com/notnews/notnews/workflows/test/badge.svg\n    :target: https://github.com/notnews/notnews/actions?query=workflow%3Atest\n.. image:: https://img.shields.io/pypi/v/notnews.svg\n    :target: https://pypi.python.org/pypi/notnews\n.. image:: https://readthedocs.org/projects/notnews/badge/?version=latest\n    :target: http://notnews.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n.. image:: https://static.pepy.tech/badge/notnews\n    :target: https://pepy.tech/project/notnews\n\nThe package provides classifiers for soft news based on the story text and the url structure for both the US and UK news media. We provide also provide a way to infer the 'kind' of news---Arts, Books, Science, Sports, Travel, etc.---for the US news media.\n\nStreamlit App: https://notnews-notnews-streamlitstreamlit-app-u8j3a6.streamlit.app/\n\nQuick Start\n-----------\n\n::\n\n    >>> import pandas as pd\n    >>> from notnews import *\n\n    >>> # Get help\n    >>> help(soft_news_url_cat_us)\n\n    Help on method soft_news_url_cat in module notnews.soft_news_url_cat:\n\n    soft_news_url_cat(df, col='url') method of builtins.type instance\n        Soft News Categorize by URL pattern.\n\n        Using the URL pattern to categorize the soft/hard news of the input\n        DataFrame.\n\n        Args:\n            df (:obj:`DataFrame`): Pandas DataFrame containing the URL\n                column.\n            col (str or int): Column's name or location of the URL in\n                DataFrame (default: url).\n\n        Returns:\n            DataFrame: Pandas DataFrame with additional columns:\n                - `soft_lab` set to 1 if URL match with soft news URL pattern.\n                - `hard_lab` set to 1 if URL match with hard news URL pattern.\n\n    >>> # Load data\n    >>> df = pd.read_csv('./notnews/tests/sample_us.csv')\n    >>> df\n                src  ",
    "url": "https://github.com/notnews/notnews",
    "last_updated": "2024-12-20T18:26:39+00:00"
  },
  {
    "full_name": "karthik/ggplot-lecture",
    "name": "ggplot-lecture",
    "description": "My lecture on ggplot at Cal (spring 2013)",
    "language": "TeX",
    "topics": [],
    "readme": "# Introduction to ggplot2\n\nAuthor: [Karthik Ram](mailto:karthik.ram+ggplot2@gmail.com)\n\nHere are my [slides](https://github.com/karthikram/ggplot-lecture/blob/master/ggplot.pdf?raw=true) for the Spring 2013 R seminar in Integrative Biology at UC Berkeley. The source file is `ggplot.Rnw`. Feel free to modify and reuse as necessary.\n\n[![slides](slides.png)](https://github.com/karthikram/ggplot-lecture/blob/master/ggplot.pdf?raw=true)\n\nTo generate the pdf presentation locally:\n\n```r\nknitr('ggplot.Rnw')\nsystem(\"pdflatex ggplot.tex\")\n```\n\n* [R code from the deck](https://github.com/karthikram/ggplot-lecture/blob/master/ggplot.R)\n\n## Suggested readings\n* [Elegant graphics for data analysis](http://www.amazon.com/ggplot2-Ele gant-Graphics-Data-Analysis/dp/0387981403/)  \n*  [R Graphics Cookbook](http://www.amazon.com/R-Graphics-Cookbook-Winston-Chang/dp/1449316956)\n\n# License  \n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/2.0/\">Creative Commons Attribution 2.0 Generic License</a>.\n",
    "url": "https://github.com/karthik/ggplot-lecture",
    "last_updated": "2024-09-27T19:59:16+00:00"
  },
  {
    "full_name": "robjhyndman/cricketdata",
    "name": "cricketdata",
    "description": "International cricket data for men and women, Tests, ODIs and T20s",
    "language": "R",
    "topics": [
      "r",
      "cricket-data",
      "cricket",
      "ozunconf17",
      "r-package",
      "rstats",
      "unconf"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# cricketdata <img src=\"man/figures/cricketdata.png\" width=\"15%\" align=\"right\" />\n\n<!-- badges: start -->\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/cricketdata)](https://cran.r-project.org/package=cricketdata)\n[![Downloads](https://cranlogs.r-pkg.org/badges/cricketdata)](https://cran.r-project.org/package=cricketdata)\n[![Licence](https://img.shields.io/badge/licence-GPL--3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0.en.html)\n[![R-CMD-check](https://github.com/robjhyndman/cricketdata/workflows/R-CMD-check/badge.svg)](https://github.com/robjhyndman/cricketdata/actions)\n[![R-CMD-check](https://github.com/robjhyndman/cricketdata/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/robjhyndman/cricketdata/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\nFunctions for downloading data on international and other major cricket\nmatches from [ESPNCricinfo](https://www.espncricinfo.com) and\n[Cricsheet](https://cricsheet.org). This package provides some functions\nto download the data into tibbles ready for analysis.\n\nPlease respect the terms of use for each website:\n[ESPNCricinfo](https://www.espncricinfo.com/ci/content/site/company/terms_use.html),\n[Cricsheet](https://cricsheet.org/register/).\n\n## Installation\n\nYou can install the **stable** version from\n[CRAN](https://cran.r-project.org/package=cricketdata).\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"cricketdata\")\n```\n\nYou can install the **development** version\n[Github](https://github.com/robjhyndman/cricketdata):\n\n``` r\npak::pak(\"robjhyndman/cricketdata\")\n```\n\n## License\n\nThis package is free and open source software, licensed under GPL-3.\n",
    "url": "https://github.com/robjhyndman/cricketdata",
    "last_updated": "2025-08-01T20:10:14+00:00"
  },
  {
    "full_name": "cleanzr/record-linkage-tutorial",
    "name": "record-linkage-tutorial",
    "description": "A tutorial on entity resolution (record linkage or de-duplication)",
    "language": "TeX",
    "topics": [],
    "readme": "# Some of Entity Resolution\n\n## Speakers/Contributors\n\nRebecca C. Steorts, Assistant Professor, Duke University \nhttps://resteorts.github.io/\n\nBrenda Betancourt, Assistant Professor, University of Florida\n(Former Foerster-Bernstein Postdoctoral Fellow, Duke University)\nhttps://www.brendabc.net/\n\nAndee Kaplan, Assistant Professor, Colorado State University \n(Former Postdoctoral Scholar, Duke University)\nhttp://andeekaplan.com/\n\nNeil Marchant, PhD Student, University of Melbourne \nhttps://github.com/ngmarchant\n\nBeidi Chen, PhD Student, Rice University\nhttp://rush.rice.edu/team.html\n\nAcknowledgements: We would like to thank Olivier Binette (PhD student, Duke University) for identifying typos and improving the quality of this tutorial. \n\n## Abstract\n\nVery often information about social entities is scattered across multiple databases.  Combining that information into one database can result in enormous benefits for analysis, resulting in richer and more reliable conclusions.  Among the types of questions that have been, and can be, addressed by combining information include: How accurate are census enumerations for minority groups? How many of the elderly are at high risk for sepsis in different parts of the country? How many people were victims of war crimes in recent conflicts in Syria? In most practical applications, however, analysts cannot simply link records across databases based on unique identifiers, such as social security numbers, either because they are not a part of some databases or are not available due to privacy concerns.  In such cases, analysts need to use methods from statistical and computational science known as entity resolution (record linkage or de-duplication) to proceed with analysis.  Entity resolution is not only a crucial task for social science and industrial applications, but is a challenging statistical and computational problem itself. In this short course, we first provide an overview and introduction to entity resolution. Second, we pr",
    "url": "https://github.com/cleanzr/record-linkage-tutorial",
    "last_updated": "2025-05-15T00:46:32+00:00"
  },
  {
    "full_name": "rockita/criticalML",
    "name": "criticalML",
    "description": "Toward ethical, transparent and fair AI/ML: a critical reading list for engineers, designers, and policy makers",
    "language": "",
    "topics": [],
    "readme": "# Toward ethical, transparent and fair AI/ML: a critical reading list for engineers, designers, and policy makers\n\n\nIn the past 5 years there’s been a lot of enthusiasm about AI and specifically machine learning and deep learning. As we continuously deploy AI models in the wild we are forced to re-examine what are the effects of knowledge symbolisation, generalisation and classification on the historical, political and social conditions of human life. We also need to remind ourselves that algorithms don’t exercise their power over us. People do. \n\nThis reading list is made for engineers, scientists, designers, policy makers and those interested in machine learning and AI. It’s an open ended document that examines machine learning as a sociotechnical system and contextualises its critical discourse. For suggestions and comments please tweet @irinimalliaraki or drop me an email at e.malliaraki16@imperial.ac.uk\n\nThese sections aren't in any particular order. There's overlap and interaction between these topics that you can jump around as much as you want; Reading \"out of order\" could lead to interesting connections. \n\n\n## CRITICAL AI\n\n![alt text](https://i.imgur.com/TTpIGvo.jpg)\n\n##### Must\n* [Manufacturing an Artificial Intelligence Revolution](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3078224), Yarden Katz, 2017 *(paper)*\n* [The critical engineering manifesto](https://criticalengineering.org/), Julian Oliver, Gordan Savičić, Danja Vasiliev\n* [Resisting Reduction: Designing our Complex Future with Machines](https://pubpub.ito.com/pub/resisting-reduction), Joi Ito , 2017 *(article)*\n* [The Seven Deadly Sins of Predicting the Future of AI](http://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/), Rodney Brooks, 2017 *(article)*\n* [Remarks on the Hole of Representation in Computer ‘Vision’](https://www.youtube.com/watch?v=VSAaGcPsim0&t=1361s), Benjamin Bratton, 2017 *(video)*\n##### Optional\n* [Critical Studies in Machine Intelligence](http",
    "url": "https://github.com/rockita/criticalML",
    "last_updated": "2025-07-29T11:50:58+00:00"
  },
  {
    "full_name": "jennybc/row-oriented-workflows",
    "name": "row-oriented-workflows",
    "description": "Row-oriented workflows in R with the tidyverse",
    "language": "R",
    "topics": [],
    "readme": "# Row-oriented workflows in R with the tidyverse\n\nMaterials for [RStudio webinar](https://resources.rstudio.com/webinars/thinking-inside-the-box-you-can-do-that-inside-a-data-frame-april-jenny-bryan) *recording available at this link!*:\n\nThinking inside the box: you can do that inside a data frame?!  \nJenny Bryan  \nWednesday, April 11 at 1:00pm ET / 10:00am PT  \n[rstd.io/row-work](https://rstd.io/row-work) *<-- shortlink to this repo*  \nSlides available [on SpeakerDeck](https://speakerdeck.com/jennybc/row-oriented-workflows-in-r-with-the-tidyverse)\n\n## Abstract\n\nThe data frame is a crucial data structure in R and, especially, in the tidyverse. Working on a column or a variable is a very natural operation, which is great. But what about row-oriented work? That also comes up frequently and is more awkward. In this webinar I’ll work through concrete code examples, exploring patterns that arise in data analysis. We’ll discuss the general notion of \"split-apply-combine\", row-wise work in a data frame, splitting vs. nesting, and list-columns.\n\n## Code examples\n\nBeginner --> intermediate --> advanced  \nNot all are used in webinar\n\n  * **Leave your data in that big, beautiful data frame.** [`ex01_leave-it-in-the-data-frame`](ex01_leave-it-in-the-data-frame.md) Show the evil of creating copies of certain rows of certain variables, using Magic Numbers and cryptic names, just to save some typing.\n  * **Adding or modifying variables.** [`ex02_create-or-mutate-in-place`](ex02_create-or-mutate-in-place.md) `df$var <- ...` versus `dplyr::mutate()`. Recycling/safety, `df`'s as data mask, aesthetics.\n  * **Are you SURE you need to iterate over rows?** [`ex03_row-wise-iteration-are-you-sure`](ex03_row-wise-iteration-are-you-sure.md) Don't fixate on most obvious generalization of your pilot example and risk overlooking a vectorized solution. Features a `paste()` example, then goes out with some glue glory.\n  * **Working with non-vectorized functions.** [`ex04_map-example`](ex04_map-ex",
    "url": "https://github.com/jennybc/row-oriented-workflows",
    "last_updated": "2025-08-22T18:12:42+00:00"
  },
  {
    "full_name": "soodoku/wait",
    "name": "wait",
    "description": "Waiting Times at CA DMV",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## Out of Line: Waiting Times at the DMV\n \nThe quality of public services matters a lot. If you have to stand in a long line to [pay your electricity bill](https://www.bbc.com/news/world-asia-india-38088385), as is the norm in some third-world countries, it takes away from the time you have to earn money or the time you have for leisure. If there is trash on the streets, it is not just an eyesore but also potentially a breeding ground for disease. \n\nIn this note, we shed light on an important aspect of quality for one such public service---waiting times at the DMV. We scrape waiting time data from [CA DMV](https://www.dmv.ca.gov/). The DMV site provides average wait times by hour of the day for all the hours for which a facility is open. We use the data to estimate the median average wait time (median of the hourly averages), variation in wait times by day, hour, and the relationship with local sociodemographics. \n\n### Data\n\n#### DMV Data\n\nCA DMV today provides services in various ways: on the phone, the Internet, partner locations, and field offices. You need to go to the local DMV office for only a small set of important services, or if you have trouble using other methods, e.g., the Internet.    \n\nIn all, there are 175 DMV field offices in CA. (There are [178 DMV offices](data/yogov_dmv_list.txt) listed on https://yogov.org/dmv/california/california-dmv-locations/. However, three have closed since yogov compiled their list.)\n\nFor each DMV field office, we collected:\n\n1. **name and location:** \"name\", \"street\", \"locality\", \"region\", and \"zip\".\n2. **wait time (minutes) by hour:** wait time for the Monday 2 pm hour is stored in the column \"M14.\"\n3. **services offered:** \"title transfers\", \"licensing services\", \"replace lost/stolen/damaged\", \"plates permits & placards\", \"testing\", \"records\", \"registration\", \"request for miscellaneous original documents.\"\n\nThe final dataset can be downloaded [here](https://github.com/soodoku/wait/blob/master/data/dmv_data_output_12_14",
    "url": "https://github.com/soodoku/wait",
    "last_updated": "2023-11-05T20:00:39+00:00"
  },
  {
    "full_name": "wrathematics/RparallelGuide",
    "name": "RparallelGuide",
    "description": "A guide to parallel R, including OpenMP.   Kind of.",
    "language": "HTML",
    "topics": [],
    "readme": "# [Parallelism, R, and OpenMP](https://wrathematics.github.io/RparallelGuide)\n\nDo you want to know about parallelism? With R? And possibly OpenMP?\nThen this is the guide for you maybe.\n\n* Text is licensed CC BY-SA\n* Code in the text is public domain or MIT, your choice (some legal\nentities do not recognize the public domain).\n* The files `custom.css` and `headers.js` are licensed \n[GamahCode v1.2](http://gamahgpl.org/)\n\nMany thanks to Karl Broman for catching a ton of typos.\n",
    "url": "https://github.com/wrathematics/RparallelGuide",
    "last_updated": "2025-01-16T18:52:14+00:00"
  },
  {
    "full_name": "yuchang0321/IVQR",
    "name": "IVQR",
    "description": "Instrumental Variable Quantile Regression",
    "language": "R",
    "topics": [],
    "readme": "# IVQR\nR Package: Instrumental Variable Quantile Regression\n\nPlease find a demonstration of the package below:\nhttps://www.dropbox.com/s/j4fzlksgdpyznyu/ivqr_demo.pdf?dl=0\n\nThis is a preliminary version. Any comment or bug report is appreciated.\nEmail: yuc391@ucsd.edu\n",
    "url": "https://github.com/yuchang0321/IVQR",
    "last_updated": "2025-03-05T08:34:57+00:00"
  },
  {
    "full_name": "pablobarbera/social-media-workshop",
    "name": "social-media-workshop",
    "description": "Workshop: Collecting and Analyzing Social Media Data with R",
    "language": "HTML",
    "topics": [],
    "readme": "# Workshop: Collecting and Analyzing Social Media Data with R\n\nThis repository contains the materials for a short workshop entitled \"Collecting and Analyzing Social Media Data with R,\" versions of which I have taught at Georgetown University, European University Institute, Pompeu Fabra University, University of Southern California, and University of Cologne. It will next be offered at the London School of Economics on January 2018.\n\nThe worskhop website is at http://pablobarbera.com/social-media-workshop/\n\nThis website is built with [Rmarkdown](http://rmarkdown.rstudio.com/rmarkdown_websites.html#overview).\n\nThe layout for this website was designed by [Jeffrey Arnold](http://www.jrnold.me/) (thanks!).\n\n## Usage\n\nTo build the site,\n```rconsole\n> source(\"build.R\")\n```\n\nTo serve the site locally and rebuild it dynamically with changes, run:\n```rconsole\n> source(\"serve.R\")\n```\n",
    "url": "https://github.com/pablobarbera/social-media-workshop",
    "last_updated": "2025-08-01T20:24:38+00:00"
  },
  {
    "full_name": "Textualize/textual",
    "name": "textual",
    "description": "The lean application framework for Python.  Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and a web browser.",
    "language": "Python",
    "topics": [
      "terminal",
      "python",
      "tui",
      "rich",
      "cli",
      "framework"
    ],
    "readme": "\n\n[![Discord](https://img.shields.io/discord/1026214085173461072)](https://discord.gg/Enf6Z3qhVr)\n[![Supported Python Versions](https://img.shields.io/pypi/pyversions/textual/1.0.0)](https://pypi.org/project/textual/)\n[![PyPI version](https://badge.fury.io/py/textual.svg?)](https://badge.fury.io/py/textual)\n![OS support](https://img.shields.io/badge/OS-macOS%20Linux%20Windows-red)\n\n\n\n![textual-splash](https://github.com/user-attachments/assets/4caeb77e-48c0-4cf7-b14d-c53ded855ffd)\n\n# Textual\n\n<img align=\"right\" width=\"250\" alt=\"clock\" src=\"https://github.com/user-attachments/assets/63e839c3-5b8e-478d-b78e-cf7647eb85e8\" />\n\nBuild cross-platform user interfaces with a simple Python API. Run your apps in the terminal *or* a web browser.\n\nTextual's API combines modern Python with the best of developments from the web world, for a lean app development experience.\nDe-coupled components and an advanced [testing](https://textual.textualize.io/guide/testing/) framework ensure you can maintain your app for the long-term.\n\nWant some more examples? See the [examples](https://github.com/Textualize/textual/tree/main/examples) directory.\n\n```python\n\"\"\"\nAn App to show the current time.\n\"\"\"\n\nfrom datetime import datetime\n\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Digits\n\n\nclass ClockApp(App):\n    CSS = \"\"\"\n    Screen { align: center middle; }\n    Digits { width: auto; }\n    \"\"\"\n\n    def compose(self) -> ComposeResult:\n        yield Digits(\"\")\n\n    def on_ready(self) -> None:\n        self.update_clock()\n        self.set_interval(1, self.update_clock)\n\n    def update_clock(self) -> None:\n        clock = datetime.now().time()\n        self.query_one(Digits).update(f\"{clock:%T}\")\n\n\nif __name__ == \"__main__\":\n    app = ClockApp()\n    app.run()\n```\n\n> [!TIP]\n> Textual is an asynchronous framework under the hood. Which means you can integrate your apps with async libraries &mdash; if you want to.\n> If you don't want or need to use async, Textual won't force it o",
    "url": "https://github.com/Textualize/textual",
    "last_updated": "2025-09-02T08:17:37+00:00"
  },
  {
    "full_name": "sekilab/RoadDamageDetector",
    "name": "RoadDamageDetector",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# RoadDamageDetector\n\n********\n\n# News\n[2025-01-25]: **ORDDC'2024 - Summary**: The IEEE Big Data Cup, Optimized Road Damage Detection Challenge (ORDDC'2024) culminated successfully. Check out the ORDDC article summarizing details of winners and proposed solutions [here](https://www.researchgate.net/publication/388092879_ORDDC'2024_State_of_the_art_Solutions_for_Optimized_Road_Damage_Detection)!\n\n[2024-05-15]: **ORDDC'2024 - Announcement**: Following the success of GRDDC'2020 and CRDDC'2022, another BigData Cup in the form of [road damage detection challenge, ORDDC'2024](https://orddc2024.sekilab.global/), is open now! Associated conference: IEEE BigData'2024. Venue: Washington, DC, USA!\n\n[2024-03-11]: **CRDDC'2022 Detailed Review**: Curious about -- What can we learn from Cross-country collaborations and Winning Strategies? Check out our latest article [From global challenges to local solutions: A review of cross-country collaborations and winning strategies in road damage detection](https://doi.org/10.1016/j.aei.2024.102388) providing complete details. Use this [link](https://authors.elsevier.com/a/1ijMG5FA1k9d5h) for free access till April 26, 2024!\n\n[2023-09-29]: **CRDDC'2022 Winners and Proposed Solutions**: Check out the CRDDC article summarizing details of winners and proposed solutions [here](https://www.researchgate.net/publication/367456896_Crowdsensing-based_Road_Damage_Detection_Challenge_CRDDC'2022)!\n\n[2022-12-18]: **CRDDC'2022 culminated successfully!** [New leaderboards available on the website](https://crddc2022.sekilab.global/leaderboard/) can still be utilized to perform more experiments. \n\n[2022-09-29]: **Data Article for RDD2022**: The article for data released through CRDDC'2022 can be accessed [here](https://www.researchgate.net/publication/363668453_RDD2022_A_multi-national_image_dataset_for_automatic_Road_Damage_Detection)!\n\n[2022-09-29]: **CRDDC'2022**: Deadline for Phase 3 and 4 has been extended! Submissions will be accepted till **Oct 5, 2",
    "url": "https://github.com/sekilab/RoadDamageDetector",
    "last_updated": "2025-09-02T07:17:43+00:00"
  },
  {
    "full_name": "yihui/xaringan",
    "name": "xaringan",
    "description": "Presentation Ninja 幻灯忍者 · 写轮眼",
    "language": "CSS",
    "topics": [
      "ninja",
      "rstudio",
      "slideshow",
      "naruto",
      "rmarkdown",
      "remarkjs",
      "presentation",
      "presentation-ninja",
      "r",
      "markdown"
    ],
    "readme": "# xaringan\n\n<img src=\"https://user-images.githubusercontent.com/163582/45438104-ea200600-b67b-11e8-80fa-d9f2a99a03b0.png\" align=\"right\" alt=\"Sharingan\" width=\"180\" />\n\n[ʃaː.'riŋ.ɡan]\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/yihui/xaringan/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/yihui/xaringan/actions/workflows/R-CMD-check.yaml)\n[![CRAN release](https://www.r-pkg.org/badges/version/xaringan)](https://cran.r-project.org/package=xaringan)\n[![Codecov test coverage](https://codecov.io/gh/yihui/xaringan/branch/master/graph/badge.svg)](https://app.codecov.io/gh/yihui/xaringan?branch=master)\n<!-- badges: end -->\n\nAn R package for creating slideshows with [remark.js](https://remarkjs.com) through R Markdown. The package name **xaringan** comes from [Sharingan](https://naruto.fandom.com/wiki/Sharingan), a dōjutsu in Naruto with two abilities: the \"Eye of Insight\" and the \"Eye of Hypnotism\". A presentation ninja should have these basic abilities, and I think remark.js may help you acquire these abilities, even if you are not a member of the Uchiha clan.\n\nPlease see the full documentation as a [presentation here](https://slides.yihui.org/xaringan/) ([中文版在此](https://slides.yihui.org/xaringan/zh-CN.html)). The [remark.js](https://remarkjs.com) website provides a quick introduction to the underlying syntax upon which **xaringan** builds. If you prefer reading a book, **xaringan** is also documented in [the R Markdown book (Chapter 7)](https://bookdown.org/yihui/rmarkdown/xaringan.html). You can use **remotes** to install the package:\n\n```r\nremotes::install_github('yihui/xaringan')\n```\n\nIf you use RStudio, it is easy to get started from the menu `File -> New File -> R Markdown -> From Template -> Ninja Presentation`, and you will see an R Markdown example. Press the `Knit` button to compile it, or use the RStudio Addin `Infinite Moon Reader` to live preview the slides (every time you update and save the Rmd document, the slides will be au",
    "url": "https://github.com/yihui/xaringan",
    "last_updated": "2025-08-22T19:19:14+00:00"
  },
  {
    "full_name": "bearloga/tinydensR",
    "name": "tinydensR",
    "description": "A set of RStudio add-ins for playing with distribution parameters and visualizing the resulting probability density and mass functions.",
    "language": "R",
    "topics": [],
    "readme": "# tinydensR\n\nAn [RStudio add-in](https://shiny.rstudio.com/articles/gadgets.html) for playing with distribution parameters and visualizing the resulting probability density and mass functions.\n\n## Installation\n\n```R\n# install.packages(\"remotes\")\nremotes::install_github(\"bearloga/tinydensR\")\n```\n\n## Distributions\n\n- Univariate\n  - Discrete\n    - [x] Binomial\n    - [x] Hypergeometric\n    - [x] Poisson\n  - Continuous\n    - [x] Beta\n    - [x] Cauchy\n    - [x] Chi-squared\n    - [x] Exponential\n    - [x] Gamma\n    - [x] Inverse-gamma\n    - [x] Normal\n    - [x] Log-Normal\n    - [x] Student-t\n    - [x] Weibull\n    - [x] [Exponentiated Weibull](https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution)\n- Multivariate\n  - Discrete\n    - [ ] Multinomial\n  - Continuous\n    - [ ] Bivariate Normal\n\n## Code of Conduct\n\nPlease note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.\n",
    "url": "https://github.com/bearloga/tinydensR",
    "last_updated": "2024-08-25T10:40:53+00:00"
  },
  {
    "full_name": "bnosac/image",
    "name": "image",
    "description": "Computer Vision and Image Recognition algorithms for R users",
    "language": "C++",
    "topics": [
      "image-recognition",
      "darknet",
      "computer-vision",
      "dlib",
      "r",
      "r-package",
      "contours",
      "surf",
      "hog-features",
      "otsu",
      "harris-corners",
      "harris-interest-point-detector",
      "canny-edge-detection",
      "f9",
      "openpano",
      "image-algorithms"
    ],
    "readme": "# image -  Computer Vision and Image Recognition algorithms for R users \n\nThis repository contains a suite of R packages which perform image algorithms currently not available in other R packages like [magick](https://CRAN.R-project.org/package=magick), [imager](https://CRAN.R-project.org/package=imager) or [EBImage](https://bioconductor.org/packages/release/bioc/html/EBImage.html). \n\nThese algorithms are put into different packages because of license differences. Currently the following R packages are available:\n\n| Package           | Functionality                          | License     | Details|\n|-------------------|----------------------------------------|-------------|--------|\n| **image.CornerDetectionF9**    | FAST-9 corner detection for images     | BSD-2   | [Details](image.CornerDetectionF9)       |\n| **image.CornerDetectionHarris**| Harris corner detection for images     | BSD-2   | [Details](image.CornerDetectionHarris)   |\n| **image.LineSegmentDetector**  | Line Segment Detector (LSD) for images | AGPL-3  | [Details](image.LineSegmentDetector)     |\n| **image.ContourDetector**      | Unsupervised Smooth Contour Line Detection for images | AGPL-3  | [Details](image.ContourDetector)     |\n| **image.CannyEdges**           | Canny Edge Detector for Images         | GPL-3   | [Details](image.CannyEdges)              |\n| **image.Otsu**                 | Otsu's Image Segmentation Method       | MIT     | [Details](image.Otsu)                    |\n| **image.dlib**                 | Speeded up robust features (SURF) and histogram of oriented gradients (HOG) features | AGPL-3 | [Details](image.dlib)     |\n| **image.libfacedetection**     | CNN for Face Detection                 | BSD-3   | [Details](image.libfacedetection)        |\n| **image.darknet**              | Image classification using darknet with deep learning models AlexNet, Darknet, VGG-16, Extraction (GoogleNet) and Darknet19. As well object detection using the state-of-the art YOLO detection system |",
    "url": "https://github.com/bnosac/image",
    "last_updated": "2025-07-14T14:17:47+00:00"
  },
  {
    "full_name": "sattosan/sql2mermaid-cli",
    "name": "sql2mermaid-cli",
    "description": "CLI tool to generate mermaid from sql query.",
    "language": "Python",
    "topics": [
      "convertor",
      "mermaid",
      "python",
      "sql"
    ],
    "readme": "<img src=\"https://raw.githubusercontent.com/sattosan/sql2mermaid-cli/master/img/top-image.png\" width=\"1200px\">\n\n---\n\n![PyPI - License](https://img.shields.io/pypi/l/sql2mermaid)\n![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/nkato/sql2mermaid/python-tox.yml?event=push&label=pytest%20with%20py38)\n![PyPI](https://img.shields.io/pypi/v/sql2mermaid)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/sql2mermaid)\n\n## sql2mermaid-cli\n`sql2mermaid-cli` is a CLI tool that converts SQL query into [mermaid.js](https://mermaid.js.org/) style!.\n\n![output-image](https://user-images.githubusercontent.com/20574756/235055268-3ecf0ec7-a3b7-45c3-93d9-fb032b14b4f6.gif)\n\n## Required\n\nPython >=3.8.1\n\n## Installation\n\nTo install sql2mermaid-cli, use the following command:\n\n```bash\n$ pip install sql2mermaid-cli\n```\n\n## Getting Started\n\nAs a preparation, create a sample SQL file.\n\n```bash\n$ echo \"with bar as (select * from baz)\\n\\\nselect * from foo inner join bar on foo.id = bar.id\\n\"> input.sql\n```\n\nThe basic usage of `sql2mermaid-cli` is as follows:\n\n```bash\n$ sql2mermaid-cli -i input.sql\n```\n\nThis will output the mermaid diagram to the console:\n\n```bash\ngraph LR\n\nbar([bar])\nroot([root])\n\nbaz[(baz)]\nfoo[(foo)]\n\nbar --> baz\nroot --> foo\nroot --> bar\n```\n\nThe Mermaid diagram that is outputted can be visualized on the Mermaid Live Editor website.\n\n[Mermaid Live Editor](https://mermaid.live/)\n\n\n## Options\n\nTo save the output to a file, use the -o option followed by the path to the output file:\n\n```bash\n$ sql2mermaid-cli -i input.sql -o output.txt\n```\n\nBy default, the output format is plain text. To output the mermaid diagram in markdown format, use the -m option:\n\n```bash\n$ sql2mermaid-cli -i input.sql -o output.md -m\n```\n\nYou can also specify either \"upper\" or \"lower\" after the -d option to display the join type of SQL in the mermaid diagram.\n\n```bash\n$ sql2mermaid-cli -i input.sql -d upper\n```\n\nThis will output the mermaid diagram ",
    "url": "https://github.com/sattosan/sql2mermaid-cli",
    "last_updated": "2025-04-27T12:04:16+00:00"
  },
  {
    "full_name": "jseabold/538model",
    "name": "538model",
    "description": "538 Election Forecasting Model",
    "language": "Python",
    "topics": [],
    "readme": "This is a Python script that replicates some features of Nate Silver's 538 Election Forecasting Model. It was constructed from reading the methodology posts on the [old site](http://www.fivethirtyeight.com/2008/03/frequently-asked-questions-last-revised.html) and the new one at the [New York Times](http://fivethirtyeight.blogs.nytimes.com/). This is my interpretation of these posts. Any and all errors are, of course, mine. Furthermore, this code should be considered as more of an example of how to conduct data analysis in Python using pandas and statsmodels rather than a \"real\" model. You can consider it a starting point for doing more complex analyses with Python rather than a real forecasting model. Or better yet, consider a fun way to learn some Python data tricks.\n\nThe polling data is up to date as of 10/2/2012. It is all publicly available from [Real Clear Politics](http://www.realclearpolitics.com/). For some reason Real Clear Politics stopped allowing directory access to their servers, so if you want to update the polling data, you'll have to update the script to walk the links on their site or do it by hand. This should be trivial, I just don't have the time. Historical polling data was obtained from [Electoral Vote](electoral-vote.com).\n\nThe pollster reliability/weighting data is also very out of date, and I did not attempt to replicate the calculation of these. I simply used old weights.\n\nPull requests are welcome. Suggestions and comments on anything from the programming to the modeling are also welcome.\n\nThe third-party packages used are \n\n* [matplotlib](http://matplotlib.org/)\n* [numpy](http://numpy.org/)\n* [pandas](http://pandas.pydata.org/)\n* [scikit-learn](http://scikit-learn.org/stable/)\n* [scipy](http://www.scipy.org/)\n* [statsmodels](http://statsmodels.sourceforge.net/)\n\nand, of course, [IPython](http://ipython.org/) is used for the notebooks.\n",
    "url": "https://github.com/jseabold/538model",
    "last_updated": "2025-08-24T15:28:56+00:00"
  },
  {
    "full_name": "eddelbuettel/bh",
    "name": "bh",
    "description": "R package providing Boost Header files",
    "language": "C++",
    "topics": [
      "r",
      "c-plus-plus",
      "boost",
      "rcpp",
      "r-package"
    ],
    "readme": "## bh: Boost Headers for R\n\n[![CI](https://github.com/eddelbuettel/bh/workflows/ci/badge.svg)](https://github.com/eddelbuettel/bh/actions?query=workflow%3Aci)\n[![License](https://img.shields.io/badge/license-BSL--1.0-brightgreen.svg?style=flat)](https://www.boost.org/users/license.html)\n[![CRAN](https://www.r-pkg.org/badges/version/BH)](https://cran.r-project.org/package=BH)\n[![Dependencies](https://tinyverse.netlify.app/badge/BH)](https://cran.r-project.org/package=BH)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/bh)](https://github.com/eddelbuettel/bh)\n[![Downloads (monthly)](https://cranlogs.r-pkg.org/badges/BH?color=brightgreen)](https://www.r-pkg.org:443/pkg/BH)\n[![Downloads (total)](https://cranlogs.r-pkg.org/badges/grand-total/BH?color=brightgreen)](https://www.r-pkg.org:443/pkg/BH)\n[![CRAN use](https://jangorecki.gitlab.io/rdeps/BH/CRAN_usage.svg?sanitize=true)](https://cran.r-project.org/package=BH)\n[![BioConductor use](https://jangorecki.gitlab.io/rdeps/BH/BioC_usage.svg?sanitize=true)](https://cran.r-project.org/package=BH)\n\n### Synopsis\n\nThis package provides [R](https://www.r-project.org) with access to\n[Boost](https://www.boost.org/) header files.  [Boost](https://www.boost.org/)\nprovides free peer-reviewed portable C++ source libraries.  A large part of\n[Boost](https://www.boost.org/) is provided as C++ template code which is\nresolved entirely at compile-time without linking.  \n\nThis package aims to provide the most useful subset of\n[Boost](https://www.boost.org/) libraries for template use among CRAN\npackages. By placing these libraries in this package, we offer a more\nefficient distribution system for CRAN as replication of this code in the\nsources of other packages is avoided.\n\nIt can be used via the `LinkingTo:` field in the `DESCRIPTION` field of an R\npackage --- and the R package infrastructure tools will then know how to set\ninclude flags correctly on all architectures supported by R.\n\nNote that this can be used solely",
    "url": "https://github.com/eddelbuettel/bh",
    "last_updated": "2025-06-18T13:06:51+00:00"
  },
  {
    "full_name": "kateto/R-Network-Visualization-Workshop",
    "name": "R-Network-Visualization-Workshop",
    "description": "Static and dynamic network visualization with R - code and tutorial from Sunbelt 2019 workshop.",
    "language": "HTML",
    "topics": [],
    "readme": "# R-Network-Visualization-Workshop\n\n<i> [Updated Jne 2021] </i><br>\nStatic and dynamic network visualization with R - <b>new</b> code and tutorial from my 2021 Sunbelt workshop.<br>\nThe examples use packages <i>igraph</i>, <i>network</i>, <i>visNetwork</i>, and <i>ndtv</i>, among others.\n\n<h3>Network visualization with R</h3>\n<a href=\"http://kateto.net\">Katherine Ognyanova</a>, <a href=\"http://comminfo.rutgers.edu/\">Rutgers University</a><br>\n\nThis workshop covers network visualization using the R language for statistical computing (cran.r-project.org) and RStudio (rstudio.com). Participants should have some prior knowledge of R and network concepts. The session will provide a brief overview of network formats, focusing on their structure and representation in key R packages. Attendees will also receive an introduction to major principles of graphics used in the R environment. <br>\nThe workshop will provide a step-by-step guide describing (through series of examples) the path from raw data to graph visualization in the igraph and Statnet frameworks.  The advanced portion of the workshop will touch on dynamic visualization for longitudinal networks and combining networks with geographic maps. We will also discuss ways of converting graphs in R to interactive JavaScript/d3-based visualizations for the Web.\n\n",
    "url": "https://github.com/kateto/R-Network-Visualization-Workshop",
    "last_updated": "2025-03-31T08:40:17+00:00"
  },
  {
    "full_name": "michaelherold/pyIsEmail",
    "name": "pyIsEmail",
    "description": "Simple, robust email validation",
    "language": "Python",
    "topics": [
      "python",
      "email-validation",
      "hacktoberfest"
    ],
    "readme": "pyIsEmail\n=========\n\n|pypi| |ci| |coveralls| |downloads|\n\nGetting Started\n---------------\n\npyIsEmail is a no-nonsense approach for checking whether that\nuser-supplied email address could be real. Sick of not being able to use\n`email address tagging`_ to sort through your `Bacn`_? We can fix that.\n\nRegular expressions are cheap to write, but often require maintenance when\nnew top-level domains come out or don't conform to email addressing\nfeatures that come back into vogue. pyIsEmail allows you to validate an\nemail address -- and even check the domain, if you wish -- with one simple\ncall, making your code more readable and faster to write. When you want to\nknow why an email address doesn't validate, we even provide you with\na diagnosis.\n\n.. _email address tagging: http://en.wikipedia.org/wiki/Email_address#Address_tags\n.. _Bacn: http://en.wikipedia.org/wiki/Bacn\n\nInstall\n-------\n\nInstall from PyPI using `pip`_, a package manager for Python.\n\n.. code-block:: bash\n\n    $ pip install pyIsEmail\n\nDon't have pip installed? Try installing it by running this from the\ncommand line:\n\n.. code-block:: bash\n\n    $ curl https://raw.github.com/pypa/pip/master/contrib/get-pip.py | python\n\nOr you can `download the source code (zip)`_ for ``pyIsEmail`` and then\nrun:\n\n.. code-block:: bash\n\n    $ python setup.py install\n\nYou may need to run the above commands with ``sudo``.\n\n.. _pip: http://www.pip-installer.org/en/latest/\n.. _download the source code (zip): https://github.com/michaelherold/pyIsEmail/zipball/master\n\nUsage\n-----\n\nFor the simplest usage, import and use the ``is_email`` function:\n\n.. code-block:: python\n\n    from pyisemail import is_email\n\n    address = \"test@example.com\"\n    bool_result = is_email(address)\n    detailed_result = is_email(address, diagnose=True)\n\nYou can also check whether the domain used in the email is a valid domain\nand whether or not it has a valid MX record:\n\n.. code-block:: python\n\n    from pyisemail import is_email\n\n    address = \"test@example.com\"\n ",
    "url": "https://github.com/michaelherold/pyIsEmail",
    "last_updated": "2025-08-20T18:01:46+00:00"
  },
  {
    "full_name": "r-dbi/DBI",
    "name": "DBI",
    "description": "A database interface (DBI) definition for communication between R and RDBMSs",
    "language": "HTML",
    "topics": [
      "r",
      "database",
      "interface"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# DBI\n\n<!-- badges: start -->\n\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)\n[![rcc](https://github.com/r-dbi/DBI/workflows/rcc/badge.svg)](https://github.com/r-dbi/DBI/actions)\n[![Coverage\nStatus](https://codecov.io/gh/r-dbi/DBI/branch/main/graph/badge.svg)](https://app.codecov.io/github/r-dbi/DBI?branch=main)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/DBI)](https://cran.r-project.org/package=DBI)\n[![CII Best\nPractices](https://bestpractices.coreinfrastructure.org/projects/1882/badge)](https://bestpractices.coreinfrastructure.org/projects/1882)\n<!-- badges: end -->\n\nThe DBI package helps connecting R to database management systems\n(DBMS). DBI separates the connectivity to the DBMS into a “front-end”\nand a “back-end”. The package defines an interface that is implemented\nby *DBI backends* such as:\n\n- [RPostgres](https://rpostgres.r-dbi.org),\n- [RMariaDB](https://rmariadb.r-dbi.org),\n- [RSQLite](https://rsqlite.r-dbi.org),\n- [odbc](https://github.com/r-dbi/odbc),\n- [bigrquery](https://github.com/r-dbi/bigrquery),\n\nand many more, see the [list of\nbackends](https://github.com/r-dbi/backends#readme). R scripts and\npackages use DBI to access various databases through their DBI backends.\n\nThe interface defines a small set of classes and methods similar in\nspirit to Perl’s [DBI](https://dbi.perl.org/), Java’s JDBC, Python’s\n[DB-API](https://www.python.org/dev/peps/pep-0249/), and Microsoft’s\n[ODBC](https://en.wikipedia.org/wiki/ODBC). It supports the following\noperations:\n\n- connect/disconnect to the DBMS\n- create and execute statements in the DBMS\n- extract results/output from statements\n- error/exception handling\n- information (meta-data) from database objects\n- transaction management (optional)\n\n## Installation\n\nMost users who want to access a database do not need to install DBI\ndirectly.",
    "url": "https://github.com/r-dbi/DBI",
    "last_updated": "2025-08-28T21:26:13+00:00"
  },
  {
    "full_name": "datamade/probablepeople",
    "name": "probablepeople",
    "description": ":family: a python library for parsing unstructured western names into name components.",
    "language": "Python",
    "topics": [
      "parse",
      "names",
      "python"
    ],
    "readme": "probablepeople\n=================\n[![Build Status](https://travis-ci.org/datamade/probablepeople.svg?branch=master)](https://travis-ci.org/datamade/probablepeople)\n\nprobablepeople is a python library for parsing unstructured romanized name or company strings into components, using advanced NLP methods. This is based off [usaddress](https://github.com/datamade/usaddress), a python library for parsing addresses.\n\nTry it out on our [web interface](https://parserator.datamade.us/probablepeople)! For those who aren't python developers, we also have an [API](https://parserator.datamade.us/api-docs).\n\n**What this can do:** Using a probabilistic model, it makes (very educated) guesses in identifying name or corporation components, even in tricky cases where rule-based parsers typically break down.\n\n**What this cannot do:** It cannot identify components with perfect accuracy, nor can it verify that a given name/company is correct/valid.\n\nprobablepeople learns how to parse names/companies through a body of training data. If you have examples of names/companies that stump this parser, please send them over! By adding more examples to the training data, probablepeople can continue to learn and improve.\n\n## How to use the probablepeople python library\n1. Install probablepeople with [pip](https://pip.readthedocs.io/en/latest/quickstart.html), a tool for installing and managing python packages ([beginner's guide here](http://www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/))\n\n   In the terminal,\n   \n    ```\n    pip install probablepeople  \n    ```  \n2. Parse some names/companies!\n   \n   ![pp](https://cloud.githubusercontent.com/assets/1406537/7870535/966f0956-054f-11e5-8312-4d392f79ff75.gif)\n    \n   Note that `parse` and `tag` are differet methods:\n   ```python\n   import probablepeople as pp\n   name_str='Mr George \"Gob\" Bluth II'\n   corp_str='Sitwell Housing Inc'\n  \n   # The parse method will split your string into components, and label each component.\n   pp.parse(na",
    "url": "https://github.com/datamade/probablepeople",
    "last_updated": "2025-08-12T17:53:48+00:00"
  },
  {
    "full_name": "briatte/neta",
    "name": "neta",
    "description": "Archived code. See the `parlement` repo for the updated version:",
    "language": "R",
    "topics": [],
    "readme": "This repository contains draft R code to plot legislative cosponsorship networks in the French Parliament. Its preliminary results were presented at [Open Legislative Data Politics](http://www.lafabriquedelaloi.fr/conference/) I (2012) and II (2014) in Paris.\n\n- [new version of the code](https://github.com/briatte/parlement)\n- [interactive visualization](http://f.briatte.org/parlviz/parlement/)\n- [more countries](https://github.com/briatte/parlnet)\n\n![](preview.png)\n",
    "url": "https://github.com/briatte/neta",
    "last_updated": "2023-01-28T18:00:39+00:00"
  },
  {
    "full_name": "khalil-research/PyEPO",
    "name": "PyEPO",
    "description": "A PyTorch-based End-to-End Predict-then-Optimize Library for Linear and Integer Programming",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# PyEPO: A PyTorch-based End-to-End Predict-then-Optimize Tool\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![GitHub Release](https://img.shields.io/github/v/release/khalil-research/PyEPO)\n[![PyPI version](https://badge.fury.io/py/pyepo.svg)](https://badge.fury.io/py/pyepo)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/pyepo)\n\n<p align=\"center\"><img width=\"100%\" src=\"images/logo1.png\" /></p>\n\n\n## Learning Framework\n\n<p align=\"center\"><img width=\"100%\" src=\"images/learning_framework_e2e.png\" /></p>\n\n\n## Publication\n\nThis repository is the official implementation of the paper:\n[PyEPO: A PyTorch-based End-to-End Predict-then-Optimize Library for Linear and Integer Programming](https://link.springer.com/article/10.1007/s12532-024-00255-x) (Accepted to Mathematical Programming Computation (MPC))\n\nCitation:\n```\n@article{tang2024,\n  title={PyEPO: a PyTorch-based end-to-end predict-then-optimize library for linear and integer programming},\n  author={Tang, Bo and Khalil, Elias B},\n  journal={Mathematical Programming Computation},\n  issn={1867-2957},\n  doi={10.1007/s12532-024-00255-x},\n  year={2024},\n  month={July},\n  publisher={Springer}\n}\n```\n\n\n## Introduction\n\n``PyEPO`` (PyTorch-based End-to-End Predict-then-Optimize Tool) is a Python-based, open-source software that supports modeling and solving predict-then-optimize problems with the linear objective function. The core capability of ``PyEPO`` is to build optimization models with [GurobiPy](https://www.gurobi.com/), [Pyomo](http://www.pyomo.org/), [MPAX](https://github.com/MIT-Lu-Lab/MPAX) or any other solvers and algorithms, then embed the optimization model into an artificial neural network for the end-to-end training. For this purpose, ``PyEPO`` implements various methods as [PyTorch](https://pytorch.org/) autograd modules.\n\n\n## Documentation\n\nThe official ``PyEPO`` docs can be found at [https://khalil-research.github.io/PyEPO](https://khalil-research",
    "url": "https://github.com/khalil-research/PyEPO",
    "last_updated": "2025-08-21T20:41:44+00:00"
  },
  {
    "full_name": "robertjwilson/ggplot2-theme",
    "name": "ggplot2-theme",
    "description": "a ggplot2 designed to make it easier to create publication quality figures",
    "language": "R",
    "topics": [],
    "readme": "# ggplot2-theme\na ggplot2 designed to make it easier to create publication quality figures\n",
    "url": "https://github.com/robertjwilson/ggplot2-theme",
    "last_updated": "2025-07-09T15:04:12+00:00"
  },
  {
    "full_name": "deepspeedai/DeepSpeed-MII",
    "name": "DeepSpeed-MII",
    "description": "MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.",
    "language": "Python",
    "topics": [
      "deep-learning",
      "inference",
      "pytorch"
    ],
    "readme": "[![Formatting](https://github.com/deepspeedai/DeepSpeed-MII/actions/workflows/formatting.yml/badge.svg?branch=main)](https://github.com/deepspeedai/DeepSpeed-MII/actions/workflows/formatting.yml)\n[![nv-v100-legacy](https://github.com/deepspeedai/DeepSpeed-MII/actions/workflows/nv-v100-legacy.yml/badge.svg?branch=main)](https://github.com/deepspeedai/DeepSpeed-MII/actions/workflows/nv-v100-legacy.yml)\n[![nv-a6000-fastgen](https://github.com/deepspeedai/DeepSpeed-MII/actions/workflows/nv-a6000-fastgen.yml/badge.svg?branch=main)](https://github.com/deepspeedai/DeepSpeed-MII/actions/workflows/nv-a6000-fastgen.yml)\n[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed-mii.svg)](https://pypi.org/project/deepspeed-mii/)\n<!-- [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest) -->\n\n<div align=\"center\">\n <img src=\"docs/images/mii-white.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/images/mii-dark.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n\n* [2024/01] [DeepSpeed-FastGen: Introducting Mixtral, Phi-2, and Falcon support with major performance and feature enhancements.](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-fastgen/2024-01-19)\n* [2023/11] [DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-fastgen)\n* [2022/11] [Stable Diffusion Image Generation under 1 second w. DeepSpeed MII](mii/legacy/examples/benchmark/txt2img)\n* [2022/10] [Announcing DeepSpeed Model Implementations for Inference (MII)](https://www.deepspeed.ai/2022/10/10/mii.html)\n\n# Contents\n\n<!-- toc -->\n\n- [DeepSpeed-MII](#deepspeed-mii)\n- [Key Technologies](#key-technologies)\n- [How does MII work?](#how-does-mii-work)\n- [Suppo",
    "url": "https://github.com/deepspeedai/DeepSpeed-MII",
    "last_updated": "2025-08-28T17:13:13+00:00"
  },
  {
    "full_name": "dvolgyes/zenodo_get",
    "name": "zenodo_get",
    "description": "Zenodo_get: Downloader for Zenodo records",
    "language": "Python",
    "topics": [],
    "readme": "zenodo_get: a downloader for Zenodo records\n===========================================\n\nAppVeyor:[![Build status](https://ci.appveyor.com/api/projects/status/f6hw96rhdl104ch9?svg=true)](https://ci.appveyor.com/project/dvolgyes/zenodo-get)\nCircleCI:[![Build status](https://circleci.com/gh/dvolgyes/zenodo_get.svg?style=svg)](https://app.circleci.com/pipelines/github/dvolgyes/zenodo_get?branch=master)\n\nCoveralls:[![Coverage Status](https://img.shields.io/coveralls/github/dvolgyes/zenodo_get/master)](https://coveralls.io/github/dvolgyes/zenodo_get?branch=master)\nCodecov:[![codecov](https://codecov.io/gh/dvolgyes/zenodo_get/branch/master/graph/badge.svg)](https://codecov.io/gh/dvolgyes/zenodo_get)\n\n\nThis is a Python3 tool that can mass-download files from Zenodo records.\n\n[![pyversion](https://img.shields.io/pypi/pyversions/zenodo_get.svg)](https://pypi.org/project/zenodo-get/)\n[![PyPI - License](https://img.shields.io/pypi/l/zenodo_get.svg)](https://github.com/dvolgyes/zenodo_get/raw/master/LICENSE.txt)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1261812.svg)](https://doi.org/10.5281/zenodo.1261812)\n\nSource code\n-----------\n\nThe code is hosted at Github.\n\nInstallation\n------------\n\nIt is recommended to use `uv` for managing Python environments and installing this package.\n`zenodo-get` requires **Python 3.10 or newer**.\n\n0. The most simple way is to use it as a tool, no installation is needed:\n    ```bash\n    uv tool run zenodo_get RECORD_ID_OR_DOI\n    ```\n\n1.  Install `uv` (if you haven't already):\n    ```bash\n    # On macOS and Linux\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    # On Windows\n    powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n    ```\n2.  Create a virtual environment and install `zenodo-get`:\n    *   From PyPI:\n        ```bash\n        uv venv\n        uv pip install zenodo-get\n        source .venv/bin/activate # Or .venv\\Scripts\\activate on Windows\n        ```\n    *   Or from a local source checkout:\n        ```bash\n        ",
    "url": "https://github.com/dvolgyes/zenodo_get",
    "last_updated": "2025-08-26T03:57:51+00:00"
  },
  {
    "full_name": "matterport/Mask_RCNN",
    "name": "Mask_RCNN",
    "description": "Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow",
    "language": "Python",
    "topics": [
      "mask-rcnn",
      "tensorflow",
      "object-detection",
      "instance-segmentation",
      "keras"
    ],
    "readme": "# Mask R-CNN for Object Detection and Segmentation\n\nThis is an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870) on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.\n\n![Instance Segmentation Sample](assets/street.png)\n\nThe repository includes:\n* Source code of Mask R-CNN built on FPN and ResNet101.\n* Training code for MS COCO\n* Pre-trained weights for MS COCO\n* Jupyter notebooks to visualize the detection pipeline at every step\n* ParallelModel class for multi-GPU training\n* Evaluation on MS COCO metrics (AP)\n* Example of training on your own dataset\n\n\nThe code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released [Matterport3D](https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/) dataset useful as well.\nThis dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples [here](https://matterport.com/gallery/).\n\n# Getting Started\n* [demo.ipynb](samples/demo.ipynb) Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images.\nIt includes code to run object detection and instance segmentation on arbitrary images.\n\n* [train_shapes.ipynb](samples/shapes/train_shapes.ipynb) shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.\n\n* ([model.py](mrcnn/model.py), [utils.py](mrcnn/utils.py), [config.py](mrcnn/config.py)): These files contain the main Mask RCNN implementation. \n\n\n* [inspect_data.ipynb](samples/coco/inspect_data.ipynb). This notebook visualizes the different pre-processing step",
    "url": "https://github.com/matterport/Mask_RCNN",
    "last_updated": "2025-09-02T09:33:59+00:00"
  },
  {
    "full_name": "dfreelon/unspooler",
    "name": "unspooler",
    "description": "Research-grade URL expansion for Python.",
    "language": "Python",
    "topics": [],
    "readme": "# unspooler\n**Research-grade URL expansion for Python.**\n\n```unspooler``` is a Python module that expands shortened URLs (e.g. by [bit.ly](https://bit.ly), [goo.gl](https://goo.gl), [tinyurl.com](http://tinyurl.com), etc.) quickly and efficiently. It extracts each URL in a dataset once and only once, decreasing execution time and bandwidth usage. It can also resume partially complete jobs so you don't have to start from scratch if something interrupts execution.\n\nSystem requirements\n-------------------\n* Python 3\n* [requests](http://docs.python-requests.org/en/master/)\n\nInstallation\n------------\n```python\npip install unspooler\n```\n\nQuick start\n-----------\n```python\nfrom unspooler import *\nshortlinks = ['Here is one shortlink: http://tinyurl.com/2unsh',\n'And here is another: http://bit.ly/1dNVPAW'] #examples from http://www.getlinkinfo.com\nunspooled = unspool_easy(shortlinks)\nprint(unspooled['urls'])\n```\n\nDetailed documentation\n----------------------\n```unspooler``` offers two primary functions: ```unspool_easy``` (easy) and ```unspool``` (advanced). The former is, as its name implies, easier to use, but has the distinct disadvantage of not allowing resumption of interrupted jobs. ```unspool``` is slightly more complex to use but allows resuming. The longer your job, the more I recommend using ```unspool``` over ```unspool_easy```.\n\nHere's how you'd run the above job with ```unspool```:\n\n```python\nfrom unspooler import *\nshortlinks = ['Here is one shortlink: http://tinyurl.com/2unsh',\n'And here is another: http://bit.ly/1dNVPAW'] #examples from http://www.getlinkinfo.com\nunspooled = {}\nfor i in unspool(shortlinks):\n    unspooled.update(i)\nprint(unspooled['urls'])\n```\n\nIf this job is interrupted, the ```unspooled``` variable will contain all unshortened data up to the point of interruption. You can then simply re-run the above ```unspool``` loop with the ```resume_dict=unspooled``` parameter set and it will resume at the last unprocessed string. You can even reuse ```",
    "url": "https://github.com/dfreelon/unspooler",
    "last_updated": "2025-03-22T19:59:18+00:00"
  },
  {
    "full_name": "notnews/msnbc_transcripts",
    "name": "msnbc_transcripts",
    "description": "MSNBC Transcripts: 2003--2022",
    "language": "Jupyter Notebook",
    "topics": [
      "cable-news",
      "msnbc",
      "news",
      "transcripts"
    ],
    "readme": "### MSNBC Transcripts: 2010--2022\n\nWe scraped https://www.msnbc.com/transcripts to get all the transcripts from 2010--2021. \n\n```\nyear\t n_transcripts\n2010      43\n2011     115\n2012     205\n2013     175\n2014     217\n2015     986\n2016     907\n2017    1185\n2018    1468\n2019    1475\n2020    1286\n2021    1476\n2022     131\n```\n\nWhen I scraped in 03/2025, I got the following (so essentially 2022)\n\n```\nyear\n2017       2\n2020     703\n2021    1479\n2022    1156\n2023      52\n2024      48\n2025      11\n```\n\n### Scripts\n\n1. [Scrape](scripts/msnbc.py)\n2. [Quick Peek](scripts/peek_file.ipynb)\n3. [Upload to Dataverse][scripts/upload_to_dataverse.ipynb]\n\n### Data\n\nThe final data posted on the Harvard Dataverse includes 16k scripts spanning 2003--2014 that were scraped earlier. The data scraped in 2025 is stored under `msnbc_transcripts_2022.csv.gz` \n\nThe data are posted at:\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FUPJDE1\n",
    "url": "https://github.com/notnews/msnbc_transcripts",
    "last_updated": "2025-03-28T05:28:27+00:00"
  },
  {
    "full_name": "kjhealy/vissoc.co",
    "name": "vissoc.co",
    "description": "Code for DataViz course website",
    "language": "R",
    "topics": [],
    "readme": "# Data Visualization Short Course\n\n### `@kjhealy`\n\nNotes and code samples from Fall 2015 Data Visualization Mini-Course, as a little website rather than slides.\n",
    "url": "https://github.com/kjhealy/vissoc.co",
    "last_updated": "2018-06-26T16:24:11+00:00"
  },
  {
    "full_name": "TaddyLab/talks",
    "name": "talks",
    "description": "public talks",
    "language": "TeX",
    "topics": [],
    "readme": "# talks\npublic talks\n",
    "url": "https://github.com/TaddyLab/talks",
    "last_updated": "2016-11-21T18:29:43+00:00"
  },
  {
    "full_name": "muan/mojibar",
    "name": "mojibar",
    "description": ":tangerine: Emoji searcher but as a menubar app.",
    "language": "JavaScript",
    "topics": [
      "emoji",
      "menubar",
      "electron"
    ],
    "readme": "# Mojibar [![Travis CI build status](https://img.shields.io/travis/muan/mojibar.svg)](https://travis-ci.org/muan/mojibar) [![JS Standard Style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg?style=flat)](https://github.com/feross/standard) [![Greenkeeper badge](https://badges.greenkeeper.io/muan/mojibar.svg)](https://greenkeeper.io/)\n\nA menubar app adaptation of [Emoji searcher](http://emoji.muan.co).\n\n![screenshot](https://cloud.githubusercontent.com/assets/1153134/12583324/7756a38a-c485-11e5-9388-3b5c61743905.gif)\n\n## Install\n\n### OSX\n\n#### :triangular_flag_on_post: Download and drag\n\n[Download the latest version for Mac on the releases page](https://github.com/muan/mojibar/releases) (and drag into your apps folder.)\n\n#### :triangular_flag_on_post: Install using [Homebrew Cask](https://caskroom.github.io/)\n\n```bash\n# Make sure homebrew knows about the latest mojibar\n$ brew update && brew upgrade brew-cask\n# Install mojibar\n$ brew cask install mojibar\n```\n\nAfter installation, find Mojibar in your apps folder or search Mojibar in spotlight. Mojibar will appear in your tray at the top right corner of your screen.\n\nTo upgrade mojibar installed from brew-cask to a newer version:\n\n```\n$ brew cask reinstall mojibar\n```\n\n### Linux\n\n#### :triangular_flag_on_post: Download and drag\n\n[Download the latest version for Linux on the releases page](https://github.com/muan/mojibar/releases) (and drag into your apps folder.)\n\nYou can use it without install any font, but the not all emoji will work, to get all emoji list you can try these approach:\n\n1. **Color** – Follow [these instructions](http://stdio.tumblr.com/post/114082931782)\n1. **Black and White** – Download this [emoji font](https://github.com/eosrei/emojione-color-font)\n\n## Usage\n\n<kbd>control + shift + space</kbd><br>\nOpen app.\n\n<kbd>command/control + ,</kbd><br>\nOpen preference (while window is open).\n\n<kbd>👆/👇/👈/👉</kbd><br>\nNavigate between emojis.\n\n<kbd>enter</kbd><br>\nCopy emoji unicode char and e",
    "url": "https://github.com/muan/mojibar",
    "last_updated": "2025-08-17T21:09:16+00:00"
  },
  {
    "full_name": "jadianes/spark-r-notebooks",
    "name": "spark-r-notebooks",
    "description": " R on Apache Spark (SparkR) tutorials for Big Data analysis and Machine Learning as IPython / Jupyter notebooks",
    "language": "Jupyter Notebook",
    "topics": [
      "sparkr",
      "notebook",
      "jupyter-notebook",
      "exploratory-data-analysis",
      "jupyter",
      "r",
      "data-science",
      "data-analysis",
      "big-data",
      "bigdata"
    ],
    "readme": "# SparkR Notebooks  \n\n[![Join the chat at https://gitter.im/jadianes/spark-r-notebooks](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jadianes/spark-r-notebooks?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nThis is a collection of [Jupyter](https://jupyter.org/) \nnotebooks intended to train the reader on different [Apache Spark](http://spark.apache.org/) concepts, from \nbasic to advanced, by using the **R** language.  \n\nIf your are interested in being introduced to some basic Data Science Engineering concepts and applications, you might find [these series of tutorials](https://github.com/jadianes/data-science-your-way) interesting. There we explain different concepts and applications \nusing Python and R. Additionally, if you are interested in using Python with Spark, you can have a look at our [pySpark notebooks]().    \n\n## Instructions  \n\nFor these series of notebooks, we have used [Jupyter](https://jupyter.org/) with the [IRkernel](http://irkernel.github.io/) R kernel. You can find installation instructions for you specific setup [here](http://irkernel.github.io/installation/). Have also a look at [Andrie de Vries](https://twitter.com/RevoAndrie) post [Using R with Jupyter Notebooks](http://blog.revolutionanalytics.com/2015/09/using-r-with-jupyter-notebooks.html) that includes instructions for installing Jupyter and IRkernel together.   \n\nA good way of using these notebooks is by first cloning the repo, and then \nstarting your [Jupyter](https://jupyter.org/) in **pySpark mode**. For example, \nif we have a *standalone* Spark installation running in our `localhost` with a \nmaximum of 6Gb per node assigned to IPython:  \n\n    MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.5.0-bin-hadoop2.6/bin/pyspark\n\nNotice that the path to the `pyspark` command will depend on your specific \ninstallation. So as requirement, you need to have\n[Spark installed](https://spark.apache",
    "url": "https://github.com/jadianes/spark-r-notebooks",
    "last_updated": "2025-04-24T03:08:34+00:00"
  },
  {
    "full_name": "rich-iannone/splitr",
    "name": "splitr",
    "description": "Use the HYSPLIT model from inside R and do more with it",
    "language": "R",
    "topics": [
      "modeling",
      "atmosphere",
      "trajectory-model",
      "dispersion-model"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# splitr <img src=\"man/figures/logo.svg\" align=\"right\" height=\"250px\" />\n\n<!-- badges: start -->\n\n[![Travis build\nstatus](https://travis-ci.org/rich-iannone/splitr.svg?branch=master)](https://travis-ci.org/rich-iannone/splitr)\n[![DOI](https://zenodo.org/badge/20543/rich-iannone/SplitR.svg)](https://zenodo.org/badge/latestdoi/20543/rich-iannone/SplitR)\n<!-- badges: end -->\n\n**splitr** is an **R** package for conducting trajectory and dispersion\nmodeling with **HYSPLIT**. We can determine, from one or more receptor\nsites, where arriving air masses originated. Conversely, it’s possible\nto model trajectories of air masses from receptor sites. Forward and\nbackward modeling of gas-phase or particulate matter can also be\nconducted from defined sites. It’s a means to help explain how, where,\nand when chemicals and materials are atmospherically transported,\ndispersed, and deposited.\n\nThis model has many applications. Some have modeled the atmospheric\ntransport of moisture to determine probable extreme rainfall locations\nleading to flood events ([Gustafsson et\nal., 2010](http://tellusa.net/index.php/tellusa/article/view/15715)).\nSimilarly, [Creamean et\nal., 2013](http://science.sciencemag.org/content/339/6127/1572.full)\nhave presented a direct link between long-range transported dust and\nbiological aerosols affecting cloud ice formation and precipitation\nprocesses in western United States.\n\nOthers have successfully improved understanding of invasive species\ndispersal abilities to inform conservation and landscape management\n([Lander et\nal., 2014](http://onlinelibrary.wiley.com/doi/10.1002/ece3.1206/abstract)).\nAlong similar lines, the long-distance transport of high-risk plant\npathogens can be modeled with **HYSPLIT** to assist with plant disease\nmanagement decisions, such as applications of fungicide or pesticide to\npotentially-affected agricultural areas ([Schmale and\nRoss, 2015](http://www.annualrevie",
    "url": "https://github.com/rich-iannone/splitr",
    "last_updated": "2025-06-05T17:19:21+00:00"
  },
  {
    "full_name": "sakrejda/stan-workshop",
    "name": "stan-workshop",
    "description": "Examples of working and non-working models, initially for ODCS 2016 in Boston.",
    "language": "R",
    "topics": [],
    "readme": "# stan-workshop\nExamples of working and non-working models, initially for ODCS 2016 in Boston.\n\nTo make the best use of your time at the workshop, please look over the scripts in this repo and:\n\n## Install Stan\nInstalling Stan is straightforward. Installation instructions are interface language and platform specific and can be found [here](http://mc-stan.org/interfaces/). Our examples will be done using R, but you should be able to follow along if you are using other languages like Python or Julia.\n\n## Download a Copy of the Stan Manual\n[Stan manual](https://github.com/stan-dev/stan/releases/download/v2.9.0/stan-reference-2.9.0.pdf) is full of usefull information and is part Bayesian textbook, part language reference. In particular, take a look at Section 3, Page 34 -- Data Types.\n\n## If You Are New to Bayes\nIf you are not familiar with the Bayesian paradigm, we will do a (very) quick introduction at the beginning of the workshop, but not enough to make people who had never seen Bayes comfortable with the subject. For those people, we recommend:\n\n* [Chapters 1 and 12](http://xcelab.net/rmpubs/rethinking/Statistical_Rethinking_sample.pdf) of Richard McElreath's Statistical Rethinking book.\n\n\n\n\n\n",
    "url": "https://github.com/sakrejda/stan-workshop",
    "last_updated": "2019-12-27T02:11:07+00:00"
  },
  {
    "full_name": "crflynn/skgrf",
    "name": "skgrf",
    "description": "scikit-learn compatible Python bindings for grf (generalized random forests) C++ random forest library",
    "language": "Python",
    "topics": [
      "machine-learning",
      "random-forest",
      "scikit-learn",
      "generalized-random-forest"
    ],
    "readme": "skgrf\n=====\n\n|build| |wheels| |rtd| |pypi| |pyversions|\n\n.. |build| image:: https://github.com/crflynn/skgrf/workflows/build_and_test/badge.svg\n    :target: https://github.com/crflynn/skgrf/actions\n\n.. |wheels| image:: https://github.com/crflynn/skgrf-wheels/workflows/release/badge.svg\n    :target: https://github.com/crflynn/skgrf/actions\n\n.. |rtd| image:: https://img.shields.io/readthedocs/skgrf.svg\n    :target: http://skgrf.readthedocs.io/en/latest/\n\n.. |pypi| image:: https://img.shields.io/pypi/v/skgrf.svg\n    :target: https://pypi.python.org/pypi/skgrf\n\n.. |pyversions| image:: https://img.shields.io/pypi/pyversions/skgrf.svg\n    :target: https://pypi.python.org/pypi/skgrf\n\n``skgrf`` provides `scikit-learn <https://scikit-learn.org/stable/index.html>`__ compatible Python bindings to the C++ random forest implementation, `grf <https://github.com/grf-labs/grf>`__, using `Cython <https://cython.readthedocs.io/en/latest/>`__.\n\nThe latest release of ``skgrf`` uses version `2.1.0 <https://github.com/grf-labs/grf/releases/tag/v2.1.0>`__ of ``grf``.\n\n``skgrf`` is still in development. Please create issues for any discrepancies or errors. PRs welcome.\n\nDocumentation\n-------------\n\n* `stable <https://skgrf.readthedocs.io/en/stable/>`__\n* `latest <https://skgrf.readthedocs.io/en/latest/>`__\n\nInstallation\n------------\n\n``skgrf`` is available on `pypi <https://pypi.org/project/skgrf>`__ and can be installed via pip:\n\n.. code-block:: bash\n\n    pip install skgrf\n\nEstimators\n----------\n\n* GRFForestCausalRegressor\n* GRFForestInstrumentalRegressor\n* GRFForestLocalLinearRegressor\n* GRFForestQuantileRegressor\n* GRFForestRegressor\n* GRFBoostedForestRegressor\n* GRFForestSurvival\n\nUsage\n-----\n\nGRFForestRegressor\n~~~~~~~~~~~~~~~~~~\n\nThe ``GRFForestRegressor`` predictor uses ``grf``'s RegressionPredictionStrategy class.\n\n.. code-block:: python\n\n    from sklearn.datasets import load_boston\n    from sklearn.model_selection import train_test_split\n    from skgrf.ensemble import GRFForestReg",
    "url": "https://github.com/crflynn/skgrf",
    "last_updated": "2025-05-26T16:58:37+00:00"
  },
  {
    "full_name": "Netflix/vectorflow",
    "name": "vectorflow",
    "description": "",
    "language": "D",
    "topics": [],
    "readme": "<img src=\"http://ae.nflximg.net/vectorflow/vectorflow_logo.png\" width=\"200\">\n\n**Vectorflow** is a minimalist neural network library optimized for sparse data and single machine environments.\n\nOriginal blog post [here](https://medium.com/@NetflixTechBlog/introducing-vectorflow-fe10d7f126b8).\n\n[![Build Status](https://travis-ci.com/Netflix/vectorflow.svg?branch=master)](https://travis-ci.com/Netflix/vectorflow)\n\n### Installation\n\n#### dub package\nThe library is distributed as a [`dub`](https://code.dlang.org/) package. Add `vectorflow` to the `dependencies` section of your `dub.json`:\n```\n\"vectorflow\": \"~>1.0.2\"\n```\n\nThe library itself doesn't have any dependencies. All you need is a recent D compiler.\n\n**`LDC` is the recommended compiler** for the fastest runtime speed. \n\nTested on:\n- Linux, OSX\n- LDC version: >= 1.1.1\n- DMD version: >= 2.073.1\n\n#### Setting up a D environment \nIf you're new to [D](http://dlang.org/), keep reading. You will need `dub` (the D package manager) and `LDC` (the LLVM-based D compiler).\n##### macOS\n```\nbrew install dub\nbrew install ldc\n```\n##### Ubuntu\n```\napt-get install -y curl xz-utils\ncurl -fsS https://dlang.org/install.sh | bash -s ldc\nsource ~/dlang/ldc-{VERSION}/activate\n```\n\n### Examples\nTo run the RCV1 example (sparse logistic regression):\n```\ncd examples && ./compile_run.sh rcv1.d\n```\n\n### Tests\nTo run the tests:\n```\ndub test\n```\n\n### Documentation\n`vectorflow` uses [ddoc](https://dlang.org/spec/ddoc.html).\nOne way of building and serving the documentation locally (you will need `libevent` for serving) is:\n```\ndub build -b ddox && dub run -b ddox\n```\nOr use your favorite DDOC compiler.\n\nPlease also refer to the repo wiki.\n",
    "url": "https://github.com/Netflix/vectorflow",
    "last_updated": "2025-08-25T13:42:49+00:00"
  },
  {
    "full_name": "TalAter/annyang",
    "name": "annyang",
    "description": "💬 Speech recognition for your site",
    "language": "JavaScript",
    "topics": [
      "speech-recognition",
      "speech",
      "speech-to-text",
      "voice"
    ],
    "readme": "# annyang!\n\nA tiny JavaScript Speech Recognition library that lets your users control your site with voice commands.\n\n**annyang** has no dependencies, weighs just 2 KB, and is free to use and modify under the MIT license.\n\n## Demo and Tutorial\n\n[Play with some live speech recognition demos](https://www.talater.com/annyang)\n\n## FAQ, Technical Documentation, and API Reference\n\n- [annyang Frequently Asked Questions](https://github.com/TalAter/annyang/blob/master/docs/FAQ.md)\n- [annyang API reference](https://github.com/TalAter/annyang/blob/master/docs/README.md)\n- [annyang tutorial](https://www.talater.com/annyang)\n\n## Hello World\n\nIt's as easy as adding [one javascript file](//cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js) to your document and defining the commands you want.\n\n````html\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/annyang/2.6.1/annyang.min.js\"></script>\n<script>\nif (annyang) {\n  // Let's define a command.\n  const commands = {\n    'hello': () => { alert('Hello world!'); }\n  };\n\n  // Add our commands to annyang\n  annyang.addCommands(commands);\n\n  // Start listening.\n  annyang.start();\n}\n</script>\n````\n\n**Check out some [live speech recognition demos and advanced samples](https://www.talater.com/annyang), then read the full [API Docs](https://github.com/TalAter/annyang/blob/master/docs/README.md).**\n\n## Adding a GUI\n\nYou can easily add a GUI for the user to interact with Speech Recognition using [Speech KITT](https://github.com/TalAter/SpeechKITT).\n\nSpeech KITT makes it easy to add a graphical interface for the user to start or stop Speech Recognition and see its current status. KITT also provides clear visual hints to the user on how to interact with your site using their voice, providing instructions and sample commands.\n\nSpeech KITT is fully customizable and comes with many different themes, and instructions on how to create your own designs.\n\n[![Speech Recognition GUI with Speech KITT](https://raw.githubusercontent.com/TalAter/SpeechKITT",
    "url": "https://github.com/TalAter/annyang",
    "last_updated": "2025-08-19T13:01:33+00:00"
  },
  {
    "full_name": "CannyLab/tsne-cuda",
    "name": "tsne-cuda",
    "description": "GPU Accelerated t-SNE for CUDA with Python bindings",
    "language": "Cuda",
    "topics": [
      "cuda",
      "gpu",
      "mnist",
      "tsne",
      "tsne-algorithm",
      "data-visualization",
      "data-analysis",
      "barnes-hut",
      "barnes-hut-tsne",
      "fit-tsne",
      "multithreading",
      "tsne-cuda",
      "python"
    ],
    "readme": "# TSNE-CUDA\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/CannyLab/tsne-cuda)\n![Conda](https://img.shields.io/conda/pn/CannyLab/tsnecuda)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tsnecuda)\n![CUDA versions](https://img.shields.io/badge/cuda-10.1%20%7C%2010.2%20%7C%2011.0%20%7C%2011.1%20%7C%2011.2%20%7C%2011.3%20%7C%2012.2-blue)\n![Conda](https://img.shields.io/conda/dn/cannylab/tsnecuda)\n\nThis repo is an optimized CUDA version of [FIt-SNE algorithm](https://github.com/KlugerLab/FIt-SNE) with associated python modules. We find that our implementation of t-SNE can be up to 1200x faster than Sklearn, or up to 50x faster than Multicore-TSNE when used with the right GPU. The paper describing our approach, as well as the results below, is available at [https://arxiv.org/abs/1807.11824](https://arxiv.org/abs/1807.11824).\n\nYou can install binaries with anaconda for CUDA version 10.1 and 10.2 using `conda install tsnecuda -c conda-forge`. Tsnecuda supports CUDA versions 9.0 and later through source installation, check out the wiki for up to date installation instructions. [https://github.com/CannyLab/tsne-cuda/wiki/](https://github.com/CannyLab/tsne-cuda/wiki/)\n\n# Benchmarks\n### Simulated Data\n![](docs/simulated_speedup.png)\n\nTime taken compared to other state of the art algorithms on synthetic datasets with 50 dimensions and four clusters for varying numbers of points. Note the log scale on both the points and time axis, and that the scale of the x-axis is in thousands of points (thus, the values on the x-axis range from 1K to 10M points. Dashed lines on SkLearn, BH-TSNE, and MULTICORE-4 represent projected times. Projected scaling assumes an O(nlog(n)) implementation.\n\n### MNIST\n![](docs/mnist_speedup.png)\n\nThe performance of t-SNE-CUDA compared to other state-of-the-art implementations on the MNIST dataset. t-SNE-CUDA runs on the raw pixels of the MNIST dataset (60000 images x 768 dimensions) in under 7 seconds.\n\n### CIFAR",
    "url": "https://github.com/CannyLab/tsne-cuda",
    "last_updated": "2025-08-30T00:43:48+00:00"
  },
  {
    "full_name": "r-lib/rex",
    "name": "rex",
    "description": "Friendly regular expressions for R.",
    "language": "R",
    "topics": [],
    "readme": "# Rex\n\n<!-- badges: start -->\n[![Codecov test coverage](https://codecov.io/gh/kevinushey/rex/branch/master/graph/badge.svg)](https://app.codecov.io/gh/kevinushey/rex?branch=main)\n[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![R-CMD-check](https://github.com/kevinushey/rex/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/kevinushey/rex/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n### Friendly Regular Expressions\n\nRegular expressions are very powerful feature, however they are often difficult\nto interpret. Rex allows you to build complex regular expressions from human\nreadable expressions.  So instead of writing (and later trying to decipher)\n```r\nr <- \"^(?:(((?:[^:])+)://))?((?:(?:(?!:/).)*)+)(?:(:([[:digit:]]+)))?(?:(/.*))?$\"\n```\n\nYou can write\n\n```r\nr <- rex(\n\n  start,\n\n  ## match the protocol -- may exist or may not\n  maybe(capture(\n      capture(except_some_of(\":\")),\n      \"://\"\n      )),\n\n  ## match the path\n  capture(one_or_more(not(\":/\"))),\n\n  ## get the port\n  maybe(capture(\":\", capture(numbers))),\n\n  ## and the rest\n  maybe(capture(\"/\", anything)),\n\n  end\n\n)\n```\n\nWhile these expressions are a bit longer than their corresponding regular\nexpression, they are much more readable and maintainable.\n\n## Installation\n\n```r\ninstall.packages(\"rex\")\n```\n\n## Usage\n\nThe vignettes have longer form usage examples.\n\n- [URL Validation](http://rpubs.com/jimhester/rex-url_parsing)\n- [Webserver Log Parsing](http://rpubs.com/jimhester/rex-log_parsing)\n\nEach `rex()` function call can include a number of functions and shortcuts.\nFor a full list of the functions available please see `?rex` and `?shortcuts`.\n\n### Rex Mode\n\nRex functions are not exported because they are only useful within `rex()`\ncalls, but they can be temporarily attached using `rex_mode()` which allows\nthem to be auto-completed.\n\n### Using Rex in other packages\n\nUsing `rex` in other packages wi",
    "url": "https://github.com/r-lib/rex",
    "last_updated": "2025-04-07T22:38:11+00:00"
  },
  {
    "full_name": "mxenoph/cheat_sheets",
    "name": "cheat_sheets",
    "description": "",
    "language": "TeX",
    "topics": [],
    "readme": "This repository contains a cheat sheet for GNU Make. It also contains a Makefile for building the PDF from the LaTeX source files.\n\nThe LaTeX source files in this directory were based on the [Deedy-Resume][] template from [Debarghya Das][] and were extensively modified.\n\n[Deedy-Resume]: https://github.com/deedydas/Deedy-Resume\n[Debarghya Das]: https://github.com/deedydas\n",
    "url": "https://github.com/mxenoph/cheat_sheets",
    "last_updated": "2024-12-17T15:56:00+00:00"
  },
  {
    "full_name": "slanglab/phrasemachine",
    "name": "phrasemachine",
    "description": "Quickly extract multi-word phrases from a corpus",
    "language": "Python",
    "topics": [],
    "readme": "Have you ever tried using word counts to analyze a collection of documents?\nLots of important concepts get missed, since they don't appear as single words\n(unigrams).  For example, the words \"social\" and \"security\" don't fully\nrepresent the concept \"social security\"; the words \"New\" and \"York\" don't\nreally represent \"New York.\" Phrasemachine identifies these sort of multiword\nphrases automatically so you can use them in text analysis. Here's how it works in Python.\n\n    >>> import phrasemachine\n    >>> text = \"Barack Obama supports expanding social security.\"\n    >>> phrasemachine.get_phrases(text)\n    {'num_tokens': 7, 'counts': Counter({'barack obama': 1, 'social security': 1})}\n\nFor more details, see our paper: [Bag of What?](http://brenocon.com/handler2016phrases.pdf), or [this slidedeck](http://brenocon.com/oconnor_textasdata2016.pdf).  By default, this package uses the (FilterFSA, k=8, SimpleNP) method from the paper.\n\nThe software only supports English texts.\n\n#### Installation\n\nWe have implementations in both R and Python.  For Python, install with:\n\n    pip install phrasemachine\n\nFor the R version, [see the R vignette here](R/phrasemachine/vignettes/getting_started_with_phrasemachine.Rmd).\n\n#### Near duplicates and merging\n\nYou might notice that phrasemachine sometimes extracts nested phrases. For instance,  \n\n    text = \"The Omnibus Crime Control and Safe Streets Act of 1968 was signed into law by President Lyndon B. Johnson\"\n    phrasemachine.get_phrases(text)\n\nextracts 'lyndon b. johnson' and 'b. johnson'.\n\nThis is intentional: phrasemachine tries to extract **all** phrases that might be useful for downstream analysis. In some cases, you might want to try to merge similar, overlapping or cofererent terms. For strategies, see section 4.3.1 from our paper: [Bag of What?](http://brenocon.com/handler2016phrases.pdf)\n\n#### Can I use `phrasemachine` with spaCy or CoreNLP? \n \nYep! By default, phrasemachine depends on [NLTK](http://www.nltk.org/) for part-of-spe",
    "url": "https://github.com/slanglab/phrasemachine",
    "last_updated": "2025-08-25T14:27:01+00:00"
  },
  {
    "full_name": "tednaleid/ganda",
    "name": "ganda",
    "description": "fast cmd-line app that quickly requests millions of urls and can save/echo the results",
    "language": "Go",
    "topics": [
      "command-line",
      "curl",
      "http-client",
      "golang",
      "pipe",
      "shell",
      "http",
      "https",
      "macos",
      "linux",
      "web-scraping"
    ],
    "readme": "\n# ganda - High-Performance HTTP Request CLI\n\n## Overview\n\n`ganda` lets you make HTTP/HTTPS requests to hundreds to millions of URLs in just a few minutes.\nIt's designed with the Unix philosophy of [\"do one thing well\"](https://en.wikipedia.org/wiki/Unix_philosophy#Do_One_Thing_and_Do_It_Well) and wants to be used in a chain of command line pipes to make its requests in parallel. \nBy default, it will echo all response bodies to standard out but can optionally save the results of each request in a directory for later analysis.\n\n### Key Features\n\n* **Parallel Request Processing:** Handle thousands of URLs simultaneously with customizable worker counts.\n* **Flexible Output Options:** Output responses to stdout, save to a directory, or format as JSON for easy parsing.\n* **Integrate with CLI Tools:** Works well with tools like jq, awk, sort, and more for powerful data transformations.\n\n### Why use `ganda` over `curl` (or `wget`, `httpie`, `postman-cli`, ...)?\n\nAll existing CLI tools for making HTTP requests are oriented around making a single request at a time.  They're great\nat starting a pipe of commands (ex: `curl <url> | jq .`) but they're awkward to use beyond a few requests.\n\nThe easiest way to use them is in a bash `for` loop or with something like `xargs`.  This is slow and expensive as they open up a new HTTP connection on every request.\n\n`ganda` makes many requests in parallel and can maintain context between the request and response.  It's designed to\nbe used in a pipeline of commands and can be used to make hundreds of thousands of requests in just a few minutes.\n\n`ganda` will reuse HTTP connections and can specify how many \"worker\" threads should be used to tightly control parallelism.\n\nThe closest CLIs I've found to `ganda` are load-testing tools like `vegeta`.  They're able to make many requests in\nparallel, but they're not designed to only call each URL once, don't maintain context between the request and response,\nand don't have the same flexibility in h",
    "url": "https://github.com/tednaleid/ganda",
    "last_updated": "2025-06-22T20:28:55+00:00"
  },
  {
    "full_name": "komoot/photon",
    "name": "photon",
    "description": "an open source geocoder for openstreetmap data",
    "language": "Java",
    "topics": [
      "elasticsearch",
      "java",
      "openstreetmap",
      "photon",
      "geocoding",
      "geocoder",
      "search",
      "reverse-geocoding"
    ],
    "readme": "# About photon\n\n[![Continuous Integration](https://github.com/komoot/photon/workflows/CI/badge.svg)](https://github.com/komoot/photon/actions)\n\n_photon_ is an open source geocoder built for\n[OpenStreetMap](https://openstreetmap.org) data. It is based on\n[elasticsearch](http://elasticsearch.org/)/[OpenSearch](https://opensearch.org/) -\nan efficient, powerful and highly scalable search platform.\n\n_photon_ was started by [komoot](http://www.komoot.de) who also provide the\npublic demo server at [photon.komoot.io](https://photon.komoot.io).\n\n## Features\n\n- high performance\n- highly scalability\n- search-as-you-type\n- multilingual search\n- location bias\n- typo tolerance\n- filter by osm tag and value\n- filter by bounding box\n- reverse geocode a coordinate to an address\n- OSM data import (built upon [Nominatim](https://nominatim.org)) inclusive continuous updates\n- import and export dumps in concatenated JSON format\n\n## Demo server\n\nYou can try out photon via the demo server at\n[https://photon.komoot.io](http://photon.komoot.io). You are welcome to use\nthe API for your project as long as the number of requests stay in a reasonable\nlimit. Extensive usage will be throttled or completely banned. We do not\ngive guarantees for availability and reserve the right to implement changes\nwithout notice.\n\n*If you have a larger number of requests to make, please consider setting up\nyour own private instance. It is as simple as downloading two files and starting\nthe server. See instructions below.*\n\n# Installation and Usage\n\n## photon ElasticSearch vs. photon OpenSearch\n\nphoton was originally built on ElasticSearch. For technical reasons, we are\nstuck with a very old an unsupported version 5.6 of ElasticSearch. Over the\nlast year, photon has been ported to the latest version of OpenSearch. This\nversion can now be considered stable. When setting up a new instance of\nphoton, please use **photon-opensearch**. This will become the default\nand only available version soon.\n\n## Requirements\n\npho",
    "url": "https://github.com/komoot/photon",
    "last_updated": "2025-09-02T02:55:40+00:00"
  },
  {
    "full_name": "soodoku/epic_children",
    "name": "epic_children",
    "description": "Sex Ratio of Children of Key Characters in Epics",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## Epic Children: Sex Ratio of Children of Key Characters in Epics\n\n[Data](epic_children.csv) on the number and sex of children of key characters in epics.\n\nIn Ramayan and Mahabharat, across 117 characters, the median number of children is 2 and the average is ~16. The sex ratio is nearly 25 and if you take out people with 50 or more kids, it drops to ~5.\n",
    "url": "https://github.com/soodoku/epic_children",
    "last_updated": "2025-01-26T20:18:37+00:00"
  },
  {
    "full_name": "great-northern-diver/loon",
    "name": "loon",
    "description": "A Toolkit for Interactive Statistical Data Visualization",
    "language": "Tcl",
    "topics": [
      "loon",
      "interactive-visualizations",
      "statistical-analysis",
      "high-dimensional-data",
      "tcl-extension",
      "tk",
      "data-visualization",
      "statistical-graphics",
      "exploratory-data-analysis",
      "exploratory-analysis",
      "interactive-graphics",
      "statistics",
      "data-analysis",
      "data-science",
      "python"
    ],
    "readme": "# loon\n\n[![Total downloads](https://cranlogs.r-pkg.org/badges/grand-total/loon?color=blue)](https://cran.r-project.org/package=loon) \n[![Monthly downloads](https://cranlogs.r-pkg.org/badges/loon)](https://cran.r-project.org/package=loon)\n\n`loon` is a toolkit for interactive data visualization and\nexploration. Currently, `loon` is embedded in `R` and `Tcl`.\n\nTo get more on either system, go look at the `README` in the corresponding directory above, `R` or `Tcl`.\n\nWe also have two documentation websites\n\n* `Tcl` and `R` documentation [here](https://great-northern-diver.github.io/loon/l_help)\n* `R` only manual style documentation [here](https://great-northern-diver.github.io/loon/)\n\n\n`loon` is joint work of the [Great Northern Diver](https://github.com/great-northern-diver) team.\n\n\n## On the name\n\nThe software is named after a large aquatic bird known as the [**common loon**](https://en.wikipedia.org/wiki/Common_loon) in Canada and the United States, or the **great northern diver** in the United Kingdom.  (Hence the name of the github organization: [https://github.com/great-northern-diver](https://github.com/great-northern-diver)) \n\nThe loon is a visual predator, diving deep beneath the surface, there chasing its prey with speed and remarkable maneuvrability. Once apprehended, the prey\nare either swallowed immediately or, when large, at least brought to the surface to be dealt with there. \n\nThis seemed an excellent metaphor for an agile, interactive, and exploratory visualization system; one which empowered an analyst to chase, and perhaps discover, whatever features might be revealed in the data by quickly diving below its surface.  The goal is to provide such a system in loon.\n\nAs an acronym \"loon\" does not fare so well (**l**east **o**bvious **o**stensive **n**ame) and perhaps its synonym \"diver\" would be better  (**d**irect **i**nteractive **v**isual **e**xploration in **R**).  But, then,  \"loon\" is funnier ...  **diveR** is the name of a collection of \"loon\" relat",
    "url": "https://github.com/great-northern-diver/loon",
    "last_updated": "2025-08-01T19:13:26+00:00"
  },
  {
    "full_name": "facebookresearch/faiss",
    "name": "faiss",
    "description": "A library for efficient similarity search and clustering of dense vectors.",
    "language": "C++",
    "topics": [],
    "readme": "# Faiss\n\nFaiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's [Fundamental AI Research](https://ai.facebook.com/) group.\n\n## News\n\nSee [CHANGELOG.md](CHANGELOG.md) for detailed information about latest features.\n\n## Introduction\n\nFaiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.\n\nSome of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server. Other methods, like HNSW and NSG add an indexing structure on top of the raw vectors to make searching more efficient.\n\nThe GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace `IndexFlatL2` with `GpuIndexFlatL2`) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported.\n\n## Installing\n\nFaiss comes with precompiled libraries for Anaconda in Python, see ",
    "url": "https://github.com/facebookresearch/faiss",
    "last_updated": "2025-09-02T09:25:32+00:00"
  },
  {
    "full_name": "soodoku/python-workshop",
    "name": "python-workshop",
    "description": "Introduction to Python, Data structures, scraping, APIs, pre-processing text data",
    "language": "Python",
    "topics": [],
    "readme": "## Python Workshop\n\nData structures, scraping, APIs, and some text processing\n\nThe workshop covers the following scripts:\n\n1. [Basics of Python](py_scripts/learn_py.py)\n   \n2. [Scraping webpages](py_scripts/get_html.py)\n   \n3. [Scraping pdf pages](py_scripts/get_pdf.py)\n   Uses pdf files in [ad_data](ad_data/).\n\n4. [Using NY Times Articles API using nytimesarticle package](py_scripts/nyt_v1.py)\n   \n5. [Using NY Times Articles API](py_scripts/nyt_v2.py)\n    \n6. [Basic Text Pre-processing](py_scripts/pre_process.py)\n   \n",
    "url": "https://github.com/soodoku/python-workshop",
    "last_updated": "2021-05-25T18:03:35+00:00"
  },
  {
    "full_name": "eddelbuettel/rinside",
    "name": "rinside",
    "description": "Seamless embedding of R in C++ programs",
    "language": "C++",
    "topics": [
      "r",
      "c-plus-plus",
      "r-package",
      "cran",
      "embedded"
    ],
    "readme": "## RInside: Easy embedding of R inside C++ (and C)\n\n[![CI](https://github.com/eddelbuettel/rinside/workflows/ci/badge.svg)](https://github.com/eddelbuettel/rinside/actions?query=workflow%3Aci)\n[![License](https://img.shields.io/badge/license-GPL%20%28%3E=%202%29-brightgreen.svg?style=flat)](https://www.gnu.org/licenses/gpl-2.0.html)\n[![CRAN](https://www.r-pkg.org/badges/version/RInside)](https://cran.r-project.org/package=RInside)\n[![Dependencies](https://tinyverse.netlify.app/badge/RInside)](https://cran.r-project.org/package=RInside)\n[![Debian package](https://img.shields.io/debian/v/r-cran-rinside/sid?color=brightgreen)](https://packages.debian.org/sid/r-cran-rinside)\n[![Downloads](https://cranlogs.r-pkg.org/badges/RInside?color=brightgreen)](https://cran.r-project.org/package=RInside)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/rinside)](https://github.com/eddelbuettel/rinside)\n\n### About\n\nThe RInside package provides a few classes for seamless embedding of [R](https://www.r-project.org) inside of\nC++ applications by relying on [Rcpp](https://www.rcpp.org/).\n\n### Examples\n\nProvided with the package itself are nine subdirectories with examples: from more than a dozen basic command-line examples (in directory\n`standard`) to graphical user-interfaces (using both [Qt](https://www.qt.io/) and Wt), linear algebra with\n[Armadillo](https://arma.sourceforge.net/) and [Eigen](https://eigen.tuxfamily.org/index.php?title=Main_Page), parallel computing with MPI to a\nsandboxed server, and (since release 0.2.16) a simple (and more limited) interface for embedding insice C applications.\n\nThe simplest example (modulo its header) is [examples/standard/rinside_sample0.cpp](inst/examples/standard/rinside_sample0.cpp)\n\n```c++\n#include <RInside.h>                    // for the embedded R via RInside\n\nint main(int argc, char *argv[]) {\n\n    RInside R(argc, argv);              // create an embedded R instance\n\n    R[\"txt\"] = \"Hello, world!\\n\";\t    // assign a",
    "url": "https://github.com/eddelbuettel/rinside",
    "last_updated": "2025-08-22T21:53:56+00:00"
  },
  {
    "full_name": "DevashishPrasad/CascadeTabNet",
    "name": "CascadeTabNet",
    "description": "This repository contains the code and implementation details of the CascadeTabNet paper \"CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents\"",
    "language": "Python",
    "topics": [
      "table-recognition",
      "table-structure-recognition",
      "table-detection",
      "table-detection-using-deep-learning"
    ],
    "readme": "# CascadeTabNet\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/cascadetabnet-an-approach-for-end-to-end/table-detection-on-icdar2013-1)](https://paperswithcode.com/sota/table-detection-on-icdar2013-1?p=cascadetabnet-an-approach-for-end-to-end)\n[![PWC](https://img.shields.io/badge/PyTorch-v1.4-blue)](https://pytorch.org/)\n[![PWC](https://img.shields.io/badge/%20mmdetection%20-v1.2-blue)](https://github.com/open-mmlab/mmdetection)\n\n> **CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents**<br>\n> [Devashish Prasad](https://github.com/DevashishPrasad),\n> [Ayan Gadpal](https://github.com/ayangadpal),\n> [Kshitij Kapadni](https://github.com/kshitijkapadni),\n> [Manish Visave](https://github.com/ManishDV),\n> <br>\n> [CVPR Link of Paper](http://openaccess.thecvf.com/content_CVPRW_2020/papers/w34/Prasad_CascadeTabNet_An_Approach_for_End_to_End_Table_Detection_and_CVPRW_2020_paper.pdf)<br>\n> [arXiv Link of Paper](https://arxiv.org/abs/2004.12629)<br>\n> <a href=\"results.pdf\">Supplementary file</a> <br>\n> The paper was presented (Orals) at [CVPR 2020 Workshop on Text and Documents in the Deep Learning Era](https://cvpr2020text.wordpress.com/)<br>\n\n\n> Virtual Oral Presentation [YOUTUBE VIDEO](https://www.youtube.com/watch?v=6rovEyWKZw8)\n> <br> Cascadetabnet Demo by Bhavesh Bhatt [YOUTUBE VIDEO](https://www.youtube.com/watch?v=bFahxVX9Ka0)\n<img align=\"right\" src=\"imgs/CVPR Teaser.gif\" />\n\n## 1. Introduction\nCascadTabNet is an automatic table recognition method for interpretation of tabular data in document images. We present an improved deep learning-based end to end approach for solving both problems of table detection and structure recognition using a single Convolution Neural Network (CNN) model. CascadeTabNet is a Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet) based model that detects the regions of tables and recognizes the structural body cells from the",
    "url": "https://github.com/DevashishPrasad/CascadeTabNet",
    "last_updated": "2025-08-25T04:28:33+00:00"
  },
  {
    "full_name": "chiphuyen/stanford-tensorflow-tutorials",
    "name": "stanford-tensorflow-tutorials",
    "description": "This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. ",
    "language": "Python",
    "topics": [
      "tensorflow",
      "deep-learning",
      "tutorial",
      "nlp",
      "natural-language-processing",
      "chatbot",
      "machine-learning",
      "stanford",
      "course-materials",
      "python"
    ],
    "readme": "[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Join the https://gitter.im/stanford-tensorflow-tutorials](https://badges.gitter.im/tflearn/tflearn.svg)](https://gitter.im/stanford-tensorflow-tutorials)\n\n# stanford-tensorflow-tutorials\nThis repository contains code examples for the course CS 20: TensorFlow for Deep Learning Research. <br>\nIt will be updated as the class progresses. <br>\nDetailed syllabus and lecture notes can be found [here](http://cs20.stanford.edu).<br>\nFor this course, I use python3.6 and TensorFlow 1.4.1.\n\nFor the code and notes of the previous year's course, please see the folder 2017 and the website https://web.stanford.edu/class/cs20si/2017\n\nFor setup instruction and the list of dependencies, please see the setup folder of this repository.",
    "url": "https://github.com/chiphuyen/stanford-tensorflow-tutorials",
    "last_updated": "2025-08-31T10:20:05+00:00"
  },
  {
    "full_name": "bart6114/sparklines",
    "name": "sparklines",
    "description": "A sparkline htmlwidget for R using jQuery Sparklines",
    "language": "R",
    "topics": [],
    "readme": "__Update 13-04-2015:__ I've noticed that there is already a __very__ similar package available at https://github.com/htmlwidgets/sparkline. Please refer there for all your _sparkline_ needs (and consider this repository as archived).\n\n-- __ARCHIVED__ --\n\nPlease see http://bart6114.github.io/sparklines/ for documentation.\n",
    "url": "https://github.com/bart6114/sparklines",
    "last_updated": "2025-04-30T21:45:00+00:00"
  },
  {
    "full_name": "soodoku/aea_rct_reg",
    "name": "aea_rct_reg",
    "description": "Analyzing the AEA RCT Registry",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## Analyzing the AEA RCT Registry\n\n### Start Date of Exp.\n\n![Cumulative Rows](cumulative_rows_by_start_date.png)\n\n### Where are the experiments taking place?\n\n```\nCountry\nUnited States of America                                       0.223356\nGermany                                                        0.078851\nIndia                                                          0.058889\nChina                                                          0.038816\nUnited Kingdom of Great Britain and Northern Ireland           0.031163\n                                                                 ...   \nAzerbaijan                                                     0.000111\nIndia, Norway                                                  0.000111\nGermany, Denmark, Estonia, Spain, Finland, Italy, Lithuania    0.000111\nSerbia, Turkey, Ukraine                                        0.000111\nNetherlands, United States of America                          0.000111\n```\n\n### Sample size\n\n```\ncount         5,657.00\nmean        182,168.00\nstd       6,577,887.00\nmin               1.00\n25%             499.00\n50%           1,280.00\n75%           3,543.00\nmax     372,000,000.00\n```\n\nMany of the ones < 50 are coding errors so the average sample size is pretty healthy.\n",
    "url": "https://github.com/soodoku/aea_rct_reg",
    "last_updated": "2025-03-13T06:13:45+00:00"
  },
  {
    "full_name": "floleuerer/fastai_ulmfit",
    "name": "fastai_ulmfit",
    "description": "fastai ulmfit - Pretraining the Language Model, Fine-Tuning and training a Classifier",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# fast.ai ULMFiT with SentencePiece from pretraining to deployment\n\n\n\n**Motivation:**\nWhy even bother with a non-BERT / Transformer language model? Short answer: you can train a state of the art text classifier with ULMFiT with limited data and affordable hardware. The whole process (preparing the Wikipedia dump, pretrain the language model, fine tune the language model and training the classifier) takes about 5 hours on my workstation with a RTX 3090. The training of the model with FP16 requires less than 8 GB VRAM - so you can train the model on affordable GPUs.\n\nI also saw this paper on the roadmap for fast.ai 2.3 [Single Headed Attention RNN: Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423) which could improve the performance further. \n\nThis Repo is based on: \n- https://github.com/fastai/fastai\n- [ULMFiT Paper](https://arxiv.org/abs/1801.06146)\n- the fast.ai NLP-course repository: https://github.com/fastai/course-nlp\n\n## Pretrained models\n\n| Language | (local) | code  | Perplexity | Vocab Size | Tokenizer | Download (.tgz files) |\n|---|---|---|---|---|---|---|\n| German | Deutsch  | de  | 16.1 | 15k | SP | https://bit.ly/ulmfit-dewiki |\n| German | Deutsch  | de  | 18.5 | 30k | SP | https://bit.ly/ulmfit-dewiki-30k |\n| Dutch | Nederlands | nl  | 20.5  | 15k | SP | https://bit.ly/ulmfit-nlwiki |\n| Russian | Русский | ru  | 29.8  | 15k | SP | https://bit.ly/ulmfit-ruwiki |\n| Portuguese | Português | pt  | 17.3 | 15k | SP | https://bit.ly/ulmfit-ptwiki |\n| Vietnamese | Tiếng Việt | vi  | 18.8 | 15k | SP | https://bit.ly/ulmfit-viwiki |\n| Japanese | 日本語 | ja  | 42.6 | 15k | SP | https://bit.ly/ulmfit-jawiki |\n| Italian | Italiano | it  | 23.7 | 15k | SP |https://bit.ly/ulmfit-itwiki |\n| Spanish | Español | es  | 21.9 | 15k | SP |https://bit.ly/ulmfit-eswiki |\n| Korean | 한국어 | ko  | 39.6 | 15k | SP |https://bit.ly/ulmfit-kowiki |\n| Thai | ไทย | th  | 56.4 | 15k | SP |https://bit.ly/ulmfit-thwiki |\n| Hebrew | עברית | he  | 46.3 | 15k | SP |https://bit.ly/",
    "url": "https://github.com/floleuerer/fastai_ulmfit",
    "last_updated": "2024-11-20T03:18:43+00:00"
  },
  {
    "full_name": "gojiplus/tubern",
    "name": "tubern",
    "description": "R Client for the Youtube Analytics and Reporting API",
    "language": "R",
    "topics": [
      "youtube-analytics",
      "youtube",
      "youtube-api"
    ],
    "readme": "## tubern: R Client for the YouTube Analytics and Reporting API\n\n[![CRAN_Status_Badge](https://www.r-pkg.org:443/badges/version/tubern)](https://www.r-pkg.org:443/badges/version/tubern)\n![](https://cranlogs.r-pkg.org/badges/grand-total/tubern)\n[![R-CMD-check](https://github.com/gojiplus/tubern/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/gojiplus/tubern/actions)\n\ntubern provides an R interface to the [YouTube Analytics API v2](https://developers.google.com/youtube/analytics), allowing you to retrieve YouTube Analytics data for channels and content owners. The package supports authentication via OAuth 2.0 and provides functions for getting analytics reports and managing channel groups.\n\n## Features\n\n- **Analytics Reports**: Retrieve detailed analytics data including views, likes, dislikes, watch time, and more\n- **OAuth 2.0 Authentication**: Secure authentication with YouTube Analytics API\n- **Group Management**: Create, list, update, and delete channel groups\n- **Flexible Querying**: Support for dimensions, filters, sorting, and date ranges\n- **Content Owner Support**: Access analytics for channels you manage as a content owner\n\n## Installation\n\n### From CRAN\n```r\ninstall.packages(\"tubern\")\n```\n\n### Development version from GitHub\n```r\n# install.packages(\"devtools\")\ndevtools::install_github(\"soodoku/tubern\", build_vignettes = TRUE)\n```\n\n## Quick Start\n\n### 1. Authentication\n\nFirst, you need to set up OAuth 2.0 authentication. You'll need to create a Google Cloud project and obtain OAuth 2.0 credentials:\n\n```r\nlibrary(tubern)\n\n# Set up authentication (you'll need your own client ID and secret)\nyt_oauth(app_id = \"your_client_id.apps.googleusercontent.com\",\n         app_secret = \"your_client_secret\")\n```\n\n### 2. Get Analytics Reports\n\n```r\n# Get basic view statistics for your channel\nreport <- get_report(\n  ids = \"channel==MINE\",\n  metrics = \"views,likes,dislikes,shares\",\n  start_date = \"2024-01-01\",\n  end_date = \"2024-12-31\"\n)\n\n# Get data with dim",
    "url": "https://github.com/gojiplus/tubern",
    "last_updated": "2025-08-29T10:11:55+00:00"
  },
  {
    "full_name": "daroczig/CEU-R-intro",
    "name": "CEU-R-intro",
    "description": "Data Analysis 1a: Foundation of Data management in R @ CEU",
    "language": "HTML",
    "topics": [],
    "readme": "This is the R script repository of the \"[Coding 3: Introduction to R](https://ceu.studyguide.timeedit.net/modules/ECBS5155?type=CORE)\" course of the 2024/2025 Winter term, part of the [MSc in Business Analytics](https://courses.ceu.edu/programs/ms/master-science-business-analytics) at CEU. In the previous years, most of these materials were part of the \"Data Analysis 1a: Exploration\" course that you can find in the [2015/2016 Winter](https://github.com/daroczig/CEU-R-lab/tree/2016), [2016/2017 Fall](https://github.com/daroczig/CEU-R-lab/tree/2017), [2017/2018 Fall](https://github.com/daroczig/CEU-R-lab/tree/2018), [2018/2019 Fall](https://github.com/daroczig/CEU-R-lab/tree/2018-fall), and [2023/2024 Winter](https://github.com/daroczig/CEU-R-lab/tree/2023-winter) branches.\n\n## Table of Contents\n\n* [Schedule](#schedule)\n* [Syllabus](#syllabus)\n* [Technical Prerequisites](#technical-prerequisites)\n* [Class Schedule](#class-schedule)\n* [Homework](#homework)\n* [Contact](#contacts)\n\n## Schedule\n\n2 x 300 mins on Jan 8 and 15:\n\n* 13:30 - 15:10 session 1\n* 15:10 - 15:40 break\n* 15:40 - 17:20 session 2\n* 17:20 - 17:40 break\n* 17:40 - 19:20 session 3\n\n## Location\n\nIn-person at the Vienna campus (QS B-421).\n\n## Syllabus\n\nPlease find in the `syllabus` folder of this repository.\n\n## Technical Prerequisites\n\nPlease bring your own laptop* and make sure to install the below items **before** attending the first class:\n\n0. Join the Teams channel dedicated to the class at `ba-r-intro-2024` with the `o3c4ngs` team code\n1. Install `R` from https://cran.r-project.org\n2. Install `RStudio Desktop` (Open Source License) from https://posit.co/download/rstudio-desktop/\n3. Enter the following commands in the R console (bottom left panel of RStudio) and make sure you see a plot in the bottom right panel and no errors in the R console:\n\n```r\ninstall.packages('ggplot2')\nlibrary(ggplot2)\nggplot(diamonds) +\n  aes(x = price, fill = cut) +\n  geom_density(alpha = 0.5) + facet_wrap(~ color) +\n  xlab('')",
    "url": "https://github.com/daroczig/CEU-R-intro",
    "last_updated": "2025-01-15T20:38:22+00:00"
  },
  {
    "full_name": "lampepfl/progfun-wiki",
    "name": "progfun-wiki",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "Changes need to be published manually to coursera.\n\n  - Install jekyll\n  - Run \"jekyll serve\" on your machine\n  - Copy the generated HTML and paste it into the corresponding page on the Coursera class site\n",
    "url": "https://github.com/lampepfl/progfun-wiki",
    "last_updated": "2025-09-02T01:52:23+00:00"
  },
  {
    "full_name": "BishrGhalil/stitchtoon",
    "name": "stitchtoon",
    "description": "A powerful package for stitching and cutting webtoons/manhwa/manhua raws.",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n  <h1>Stitch Toon</h1>\n  <p>\n    A powerful package for stitching and cutting webtoons/manhwa/manhua raws.\n  </p>\n  <p>\n    A GUI version is available at <a href=\"https://github.com/BishrGhalil/stitchtoon-gui\"><b>Stitchtoon GUI</b></a>\n\n <a href=\"https://discord.gg/vgWyc3tNnK\">Discord Server</a>.\n\n  </p>\n  <a href=\"https://github.com/BishrGhalil/stitchtoon/releases/latest\">\n    <img src=\"https://img.shields.io/github/v/release/BishrGhalil/stitchtoon\">\n  </a>\n  <a href=\"https://github.com/BishrGhalil/stitchtoon/releases/latest\">\n    <img src=\"https://img.shields.io/github/release-date/BishrGhalil/stitchtoon\">\n  </a>\n  <a href=\"https://github.com/BishrGhalil/stitchtoon/tree/dev\">\n    <img src=\"https://img.shields.io/github/last-commit/BishrGhalil/stitchtoon\">\n  </a>\n  <a href=\"https://github.com/BishrGhalil/stitchtoon/blob/dev/LICENSE\">\n    <img src=\"https://img.shields.io/github/license/BishrGhalil/stitchtoon\">\n  </a>\n  </div>\n\n## Install\nInstall with pip\n```\npip install stitchtoon\n```\n\nInstall from source\n```\ngit clone https://github.com/BishrGhalil/stitchtoon\ncd stitchtoon\npip instal --user requirements.txt\npip install .\n```\n\n## Basic usage\n```\nstitchtoon -i <input-path> -s <split-height> -o <output-path>\n```\nOptions\n```\nusage: __main__.py [-h] [-V] -i INPUT -o OUTPUT -s SIZE -n IMAGES_NUMBER\n                   [-t {png,jpeg,jpg,webp,psb,psd}] [-r] [-a] [-p]\n                   [-w {none,auto,copyright}] [-d {none,pixel}] [-e [0-100]]\n                   [-q [1-100]] [-g IGNORABLE_PIXELS] [-l [1-100]]\n                   [--write-metadata] [--slice-to-metadata]\n                   [--log-level {error,debug,info}] [--log-file LOG_FILE]\n\noptions:\n  -h, --help            show this help message and exit\n  -V, --version         show program's version number and exit\n  -i INPUT, --input INPUT\n                        Sets the path of Input Folder\n  -o OUTPUT, --output OUTPUT\n                        Saves at specified output path\n  -s SIZE, --size SIZE  Se",
    "url": "https://github.com/BishrGhalil/stitchtoon",
    "last_updated": "2025-08-05T07:33:18+00:00"
  },
  {
    "full_name": "BlitzKraft/saythanks.io",
    "name": "saythanks.io",
    "description": "Spreading Thankfulness in Open Source. ",
    "language": "Jinja",
    "topics": [
      "inbox",
      "saythanks",
      "thanks",
      "thankfulness",
      "opensource"
    ],
    "readme": "# ☼ The 'Say Thanks' Project\n\n[![saythanks](https://img.shields.io/badge/say-thanks-modal.svg)](https://saythanks.io/to/lifebalance&saythanks)\n\n## Spreading Thankfulness in Open Source™\n\n[**saythanks.io**](https://saythanks.io/) will provide a button/link for use by open source projects, to encourage users to send a simple _thank you_ note to the creator (or creators) of that project.\n\nThis simple button/link can be added to READMEs and project documentation.\n\nThe author can then enjoy a nice inbox (ideally) filled with very small, thoughtful messages from the happy users of the software they enjoy to toil over.\n\n## Recent Improvements\n\n- **Email Delivery Provider Updated:** Migrated transactional email delivery from SendGrid to MailerSend for improved deliverability, modern API support, and more reliable service. This change addresses recent deliverability issues and leverages MailerSend’s robust features for transactional messaging.\n  - Further explore this [here](https://github.com/mailersend/mailersend-python?tab=readme-ov-file#send-a-template-based-email)\n- **Versatile Markdown Editor:** Added a powerful and user-friendly markdown editor (Toast UI Editor) for thank you note writing, featuring live preview and enhanced formatting options for users.\n- Codebase has been prettified and refactored for maintainability.\n- Improved CSRF protection and message inbox aggregation for users/projects.\n- Hosting migrated to [KGiSL](https://www.kgisl.com) (was Heroku); CloudFlare continues for SSL termination.\n- Unnecessary files and legacy configs cleaned up.\n- UI and UX improvements, including updated button colors.\n- Enhanced documentation and developer setup instructions.\n- Ongoing improvements to authentication (Auth0, OAuth2).\n\n## Implementation Concepts\n\n### ☤ The Basics\n\n- Email when a new message of thankfulness is submitted ([csrf](https://en.wikipedia.org/wiki/Cross-site_request_forgery) enabled).\n- Inbox page for each user/project with simple aggregation of messag",
    "url": "https://github.com/BlitzKraft/saythanks.io",
    "last_updated": "2025-08-19T02:05:25+00:00"
  },
  {
    "full_name": "wiseio/paratext",
    "name": "paratext",
    "description": "A library for reading text files over multiple cores.",
    "language": "C++",
    "topics": [],
    "readme": "ParaText\n========\n\n<a href=\"https://travis-ci.org/wiseio/paratext.svg?branch=master\"><img src=\"https://travis-ci.org/wiseio/paratext.svg?branch=master\" qlt=\"Paratext Travis Status\"></a>\n<a href=\"https://github.com/wiseio/paratext/blob/master/README.md\"><img src=\"https://img.shields.io/github/license/wiseio/paratext.svg\" qlt=\"Paratext License\"></a>\n\nParaText is a C++ library to read text files in parallel on multi-core\nmachines. The alpha release includes a CSV reader and Python bindings.\nThe library itself has no dependencies other than the standard library.\n\nDepedencies\n-----------\nParaText has the following dependencies:\n\n   - a fully C++11-compliant C++ compiler (gcc 4.8 or above, clang 3.4 or above)\n   - SWIG 2.0.7 or above (Python 2 bindings)\n   - SWIG 3.0.8 or above (Python 3 bindings)\n   - Python 2.7 or 3.5\n   - setuptools\n   - numpy\n\nPandas is required only if using ParaText to read CSV files into\nPandas. The SWIG available from Ubuntu 14.04 does not work with Python 3.\n\nAnaconda packages the latest version of SWIG that works properly\nwith Python 3. You can install it as follows:\n\n```\nconda install swig\n```\n\nBuilding Python\n---------------\n\nFirst, go into the `python` directory:\n\n```\n   cd python/\n```\n\nThen run `setup.py`:\n\n```\n   python setup.py build install\n```\n\nUse the `--prefix` option if you prefer to install ParaText to a\ndifferent location:\n\n```\n   cd python/\n   python setup.py build install --prefix=/my/prefix/dir\n```\n\n\nUsing ParaText in Python\n========================\n\nFirst, import the `paratext` Python package.\n\n```\n   import paratext\n```\n\nLoading into Pandas\n-------------------\n\nA CSV file can be loaded into Pandas in just one line of code using\nthe `load_csv_to_pandas` function.\n\n```\ndf = paratext.load_csv_to_pandas(\"hepatitis.csv\")\n```\n\nThe data frame looks something like this:\n\n```\nIn [1]: print df.head()\n   AGE     SEX STEROID ANTIVIRALS FATIGUE MALAISE ANOREXIA LIVER_BIG  \\\n0   30    male      no         no      no      no       no        n",
    "url": "https://github.com/wiseio/paratext",
    "last_updated": "2025-03-22T11:21:53+00:00"
  },
  {
    "full_name": "kbroman/RStudioConf2017Slides",
    "name": "RStudioConf2017Slides",
    "description": "Links to slides for talks at the 2017 rstudio::conf",
    "language": "",
    "topics": [],
    "readme": "# Links to slides from rstudio::conf 2017\n\nLinks to slides to talks at the 2017\n[rstudio::conf](https://www.rstudio.com/conference/)\n\nSchedule: <https://www.rstudio.com/conference/#speakers>\n\nPull requests welcome! Or add an issue or email\n[Karl Broman](http://kbroman.org).\n\n[Videos](https://blog.rstudio.org/2017/02/15/rstudioconf-2017-session-recordings-are-now-available/) now available!\n\nAlso see\n[Stephen Turner's list](https://github.com/stephenturner/rstudioconf_purrr_listcols), [RWeekly's list](https://github.com/rweekly/conferences/blob/master/rstudioconf2017.csv), and [Sharon Machlis's tips and takeways](http://www.computerworld.com/article/3157004/data-analytics/best-tips-and-takeaways-from-rstudio-conference.html).\n\n---\n\n## Wednesday 2017-01-11 and Thursday 2017-01-12\n\n- [Joe Cheng](https://github.com/jcheng5), [@jcheng](https://twitter.com/jcheng),\n  [Intermediate Shiny Workshop](https://github.com/jcheng5/rstudio2017-shiny-workshop)\n\n- [Garrett Grolemund](https://github.com/garrettgman), [@StatGarrett](https://twitter.com/statgarrett),\n  [Master the Tidyverse Workshop](https://github.com/garrettgman/MasterTheTidyverse)\n\n## Friday 2017-01-13\n\n- [Charlotte Wickham](http://cwick.co.nz/), [@cvwickham](https://twitter.com/cvwickham),\n  [Happy R Users Purrr](https://www.dropbox.com/sh/062xjv35izc2a92/AAAnC-nzToR1rPekDZipRJSLa?dl=0&preview=slides.pdf)\n  \\[[Source](https://www.dropbox.com/sh/062xjv35izc2a92/AAAnC-nzToR1rPekDZipRJSLa?dl=0)\\]\n\n- [Winston Chang](https://github.com/wch), [@winston_chang](https://twitter.com/winston_chang),\n  [Building dashboards with Shiny](https://github.com/jcheng5/dashtutorial)\n\n- Javier Luraschi,\n  [@javierluraschi](https://twitter.com/javierluraschi),\n  [R and Spark](https://beta.rstudioconnect.com/content/2371/r-and-spark.html#1)\n\n- [Bárbara Borges Ribeiro](http://www.barbara-borgesribeiro.com/),  [Dynamic Shiny Interfaces](https://docs.google.com/presentation/d/1wMxhVIWsFfkCZjEJ6VlfEZpMWmiA_umOavcuwGVIQcg/edit)[Source](https:/",
    "url": "https://github.com/kbroman/RStudioConf2017Slides",
    "last_updated": "2024-09-09T16:40:44+00:00"
  },
  {
    "full_name": "abbyy/ocrsdk.com",
    "name": "ocrsdk.com",
    "description": "ABBYY Cloud OCR SDK",
    "language": "Java",
    "topics": [],
    "readme": "# Cloud OCR SDK \n\nABBYY Cloud OCR SDK provides Web API that can be easily used in C#, Java, Python, or any other development tool supporting communication over network. \n\nThis repo contains a set of samples in different programming languages showing how to create a simple client application using API V1 for processing image with the specified parameters and exporting the result.\n\nFind the [API V1 reference](https://www.ocrsdk.com/documentation/api-reference/process-image-method/) in the ABBYY Cloud OCR SDK [documentation](https://www.ocrsdk.com/documentation/).\n\nFor more information about the product visit [ABBYY Cloud OCR SDK website](https://www.ocrsdk.com/). \n\n## Features\n\n- Text recognition\n  - full-page and zonal OCR (printed text) for 200+ languages\n  - ICR (hand-printed text)\n- Document conversion\n  - convert image/PDF to searchable PDF, PDF/A and Microsoft Word, Excel, PowerPoint\n- Data extraction\n  - Barcode recognition \n  - Business card recognition\n  - ID recognition (MRZ)\n\n## Quick start guide\n\nTo observe and use the samples fo image recognition, do the following:\n\n1. [Register](https://cloud.ocrsdk.com/Account/Register) on the ABBYY Cloud OCR SDK website and create your Application. You will need the Application ID and Application Password to run any of the code samples.\n2. Download the samples from this repo.\n3. Observe the API V1 and implement your application using ABBYY OCR and capturing technologies.\n\n## Web API versions \n\nCurrently ABBYY Cloud OCR SDK provides 2 Web API versions:\n* V1 (XML response format)\n* V2 (JSON response format)\n\nFind the [full V1 and V2 difference list](https://www.ocrsdk.com/documentation/faq/#v1v2diff) in the documentation.\n\nThis repo contains samples, supporting v1 version only.\n\nInvestigate the [cloudsdk-client-dotnet](https://github.com/abbyysdk/cloudsdk-client-dotnet) repo for the client library and sample using the Web API (v2). \n\n\n## Supported export formats\n\nYou can export recognized text to the following formats:\n-",
    "url": "https://github.com/abbyy/ocrsdk.com",
    "last_updated": "2025-08-30T10:43:27+00:00"
  },
  {
    "full_name": "matheusfacure/python-causality-handbook",
    "name": "python-causality-handbook",
    "description": "Causal Inference for the Brave and True. A light-hearted yet rigorous approach to learning about impact estimation and causality. ",
    "language": "Jupyter Notebook",
    "topics": [
      "causal-inference",
      "python",
      "causality",
      "data-science",
      "econometrics",
      "impact-estimation",
      "harmless-econometrics"
    ],
    "readme": "# Causal Inference for The Brave and True\n\n![img](./causal-inference-for-the-brave-and-true/data/img/brave-and-true.png)\n\n[![DOI](https://zenodo.org/badge/255903310.svg)](https://zenodo.org/badge/latestdoi/255903310)\n\nA light-hearted yet rigorous approach to learning impact estimation and sensitivity analysis. All in Python and with as many memes as I could find.\n\n[Check out the book here!](https://matheusfacure.github.io/python-causality-handbook/landing-page.html)\n\nIf you want to read the book in Brazilian Portuguese, @rdemarqui made this awesome translation:  \n[Inferência Causal para os Corajosos e Verdadeiros](https://github.com/rdemarqui/python-causality-handbook-ptbr)\n\nIf you want to read the book in French, Arthur Mello put a lot of effort into this beautiful translation:  \n[L'Inférence Causale pour les Courageux et les Vrais](https://github.com/arthurmello/python-causality-handbook)\n\nIf you want to read the book in Chinese, @xieliaing was very kind to make a translation (Chapters 1-21):  \n[因果推断：从概念到实践](https://github.com/xieliaing/CausalInferenceIntro)\n\nThere is also a more recent Chinese tralsation by 黄文喆（Wenzhe Huang) and 许文立（Wenli Xu) (All Chapters):  \n[因果推断：献给求真敢为者](https://github.com/Wenzhe-Huang/python-causality-handbook-zh)\n\nIf you want to read the book in Spanish, @donelianc was very kind to make a translation:  \n[Inferencia Causal para los Valientes y Verdaderos](https://github.com/donelianc/introduccion-inferencia-causal)\n\nIf you want to read it in Korean, @jsshin2019 has put up a team to make the that translation possible:  \n[Python으로 하는 인과추론 : 개념부터 실습까지](https://github.com/TeamCausality/Causal-Inference-with-Python)\n\nAlso, some really kind folks (@vietecon, @dinhtrang24 and @anhpham52) also translated this content into Vietnamese:  \n[Nhân quả Python](https://github.com/vietecon/NhanQuaPython)\n\n\n\n\n\n\nI like to think of this entire series as a tribute to Joshua Angrist, Alberto Abadie and Christopher Walters for their amazing Econometrics class. Mos",
    "url": "https://github.com/matheusfacure/python-causality-handbook",
    "last_updated": "2025-09-02T09:35:27+00:00"
  },
  {
    "full_name": "freemindcore/sdwebuiapi",
    "name": "sdwebuiapi",
    "description": "Python API client for AUTOMATIC1111/stable-diffusion-webui, thanks mix1009/sdwebuiapi for the great project!",
    "language": "Python",
    "topics": [],
    "readme": "# easyai-sdwebui-api\nAPI client for AUTOMATIC1111/stable-diffusion-webui\n\nTested on AUTOMATIC1111/stable-diffusion-webui v1.2.x/v1.4.x and Mikubill/sd-webui-controlnet v1.1.232\n\n* Supports txt2img, img2img, extra-single-image, extra-batch-images API calls.\n\n* API support have to be enabled from webui. Add --api when running webui.\nIt's explained [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API).\n\n* You can use --api-auth user1:pass1,user2:pass2 option to enable authentication for api access.\n(Since it's basic http authentication the password is transmitted in cleartext)\n\n* API calls are (almost) direct translation from http://127.0.0.1:7860/docs as of 2022/11/21.\n\n# Install\n\n```\npip install easyai-sdwebui-api\n```\n\n# Usage\n\neasyai_demo.ipynb contains example code with original images. Images are compressed as jpeg in this document.\n\n## create API client\n```\nimport easyai\n\n# create API client\napi = easyai.EasyAPI()\n\n# create API client with custom host, port\n#api = easyai.EasyAPI(host='127.0.0.1', port=7860)\n\n# create API client with custom host, port and https\n#api = easyai.EasyAPI(host='webui.example.com', port=443, use_https=True)\n\n# create API client with default sampler, steps.\n#api = easyai.EasyAPI(sampler='Euler a', steps=20)\n\n# optionally set username, password when --api-auth is set on webui.\napi.set_auth('username', 'password')\n\n# check controlnet version\napi.controlnet_version()\n\n# list all controlnet models\n# api.controlnet_module_list()\n# api.controlnet_model_list()\n\n```\n\n## txt2img\n```\nresult1 = api.txt2img(prompt=\"cute squirrel\",\n                    negative_prompt=\"ugly, out of frame\",\n                    seed=1003,\n                    styles=[\"anime\"],\n                    cfg_scale=7,\n#                      sampler_index='DDIM',\n#                      steps=30,\n#                      enable_hr=True,\n#                      hr_scale=2,\n#                      hr_upscaler=easyai.HiResUpscaler.Latent,\n#                      hr_second",
    "url": "https://github.com/freemindcore/sdwebuiapi",
    "last_updated": "2025-01-15T15:29:59+00:00"
  },
  {
    "full_name": "microsoft/VFSForGit",
    "name": "VFSForGit",
    "description": "Virtual File System for Git: Enable Git at Enterprise Scale",
    "language": "C#",
    "topics": [],
    "readme": "# VFS for Git\r\n\r\n**Notice:** With the release of VFS for Git 2.32, VFS for Git is in maintenance mode. Only required updates as a reaction to critical security vulnerabilities will prompt a release.\r\n\r\n|Branch|Unit Tests|Functional Tests|Large Repo Perf|Large Repo Build|\r\n|:--:|:--:|:--:|:--:|:--:|\r\n|**master**|[![Build status](https://dev.azure.com/gvfs/ci/_apis/build/status/CI%20-%20Windows?branchName=master)](https://dev.azure.com/gvfs/ci/_build/latest?definitionId=7&branchName=master)|[![Build status](https://dev.azure.com/gvfs/ci/_apis/build/status/CI%20-%20Windows%20-%20Full%20Functional%20Tests?branchName=master)](https://dev.azure.com/gvfs/ci/_build/latest?definitionId=6&branchName=master)|[![Build status](https://dev.azure.com/mseng/AzureDevOps/_apis/build/status/GVFS/GitHub%20VFSForGit%20Large%20Repo%20Perf%20Tests?branchName=master)](https://dev.azure.com/mseng/AzureDevOps/_build/latest?definitionId=7179&branchName=master)|[![Build status](https://dev.azure.com/mseng/AzureDevOps/_apis/build/status/GVFS/GitHub%20VFSForGit%20Large%20Repo%20Build?branchName=master)](https://dev.azure.com/mseng/AzureDevOps/_build/latest?definitionId=7180&branchName=master)|\r\n|**shipped**|[![Build status](https://dev.azure.com/gvfs/ci/_apis/build/status/CI%20-%20Windows?branchName=releases%2Fshipped)](https://dev.azure.com/gvfs/ci/_build/latest?definitionId=7&branchName=releases%2Fshipped)|[![Build status](https://dev.azure.com/gvfs/ci/_apis/build/status/CI%20-%20Windows%20-%20Full%20Functional%20Tests?branchName=releases%2Fshipped)](https://dev.azure.com/gvfs/ci/_build/latest?definitionId=6&branchName=releases%2Fshipped)|[![Build status](https://dev.azure.com/mseng/AzureDevOps/_apis/build/status/GVFS/GitHub%20VFSForGit%20Large%20Repo%20Perf%20Tests?branchName=releases%2Fshipped)](https://dev.azure.com/mseng/AzureDevOps/_build/latest?definitionId=7179&branchName=releases%2Fshipped)|[![Build status](https://dev.azure.com/mseng/AzureDevOps/_apis/build/status/GVFS/GitHub%20VFSFor",
    "url": "https://github.com/microsoft/VFSForGit",
    "last_updated": "2025-08-31T03:48:31+00:00"
  },
  {
    "full_name": "notnews/stanford_tv_news",
    "name": "stanford_tv_news",
    "description": "Stanford Cable TV News Dataset",
    "language": "Jupyter Notebook",
    "topics": [
      "cable-news",
      "news"
    ],
    "readme": "### Stanford Cable TV News Dataset\n\nData from https://tvnews.stanford.edu/data\n\n* Scripts\n  * [Get Screentime for each person](https://github.com/notnews/stanford_tv_news/blob/main/01_get_people_screentime.ipynb)\n  * [Get Video Metadata](https://github.com/notnews/stanford_tv_news/blob/main/02_get_video_meta.ipynb)\n  * [Merge](https://github.com/notnews/stanford_tv_news/blob/main/03_merge_people_video.ipynb)\n* [Data](data/)\n\nWith archive_id, you can crosswalk to https://github.com/notnews/archive_news_cc\n\n  \n\n\n",
    "url": "https://github.com/notnews/stanford_tv_news",
    "last_updated": "2025-03-28T06:01:08+00:00"
  },
  {
    "full_name": "notnews/working_women_on_tv",
    "name": "working_women_on_tv",
    "description": "Employment Status of Female Characters on Indian Television Soaps",
    "language": "R",
    "topics": [
      "indian-television-soaps",
      "female-characters"
    ],
    "readme": "### Working Women on Indian TV\n\nTallying the number of fictional female characters who work on Indian television evening soaps. We find that 36.6% of the female characters of working age work. The proportion varies considerably across shows, spanning 0 and 1. The standard deviation is .30.\n\n#### Data\n\nThe [data](data/working_women_on_tv) were collected between September and November, 2015. For now, we have data on 172 characters covering 20 shows. If you see an inconsistency in the data, or have a suggestion, or some data that you would like to contribute to the project, please create a pull request.\n\n#### Analysis, Write-up, And Figures\n\n[Script](scripts/tv_women.R)\n\n#### Authors\n\nAsha Sood and Gaurav Sood \n\n#### License\n\nScripts, figures, and writing are released under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/). ",
    "url": "https://github.com/notnews/working_women_on_tv",
    "last_updated": "2025-04-16T22:27:54+00:00"
  },
  {
    "full_name": "aws/aws-parallelcluster",
    "name": "aws-parallelcluster",
    "description": "AWS ParallelCluster is an AWS supported Open Source cluster management tool to deploy and manage HPC clusters in the AWS cloud.",
    "language": "Python",
    "topics": [],
    "readme": "AWS ParallelCluster - HPC for the Cloud\n=======================================\n\n[![PyPI Version](https://img.shields.io/pypi/v/aws-parallelcluster)](https://pypi.org/project/aws-parallelcluster/)\n[![Spack Version](https://img.shields.io/spack/v/aws-parallelcluster)](https://spack.readthedocs.io/en/latest/package_list.html#aws-parallelcluster)\n[![Conda Verseion](https://img.shields.io/conda/vn/conda-forge/aws-parallelcluster)](https://anaconda.org/conda-forge/aws-parallelcluster)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![codecov](https://codecov.io/gh/aws/aws-parallelcluster/branch/develop/graph/badge.svg)](https://codecov.io/gh/aws/aws-parallelcluster)\n[![ParallelCluster CI](https://github.com/aws/aws-parallelcluster/workflows/ParallelCluster%20CI/badge.svg)](https://github.com/aws/aws-parallelcluster/actions)\n\nAWS ParallelCluster is an AWS supported Open Source cluster management tool that makes it easy for you to deploy and\nmanage High Performance Computing (HPC) clusters in the AWS cloud.\nBuilt on the Open Source CfnCluster project, AWS ParallelCluster enables you to quickly build an HPC compute environment in AWS.\nIt automatically sets up the required compute resources and a shared filesystem and offers a variety of batch schedulers such as AWS Batch and Slurm.\nAWS ParallelCluster facilitates both quick start proof of concepts (POCs) and production deployments.\nYou can build higher level workflows, such as a Genomics portal that automates the entire DNA sequencing workflow, on top of AWS ParallelCluster.\n\nQuick Start\n-----------\n**IMPORTANT**: you will need an **Amazon EC2 Key Pair** to be able to complete the following steps.\nPlease see the [Official AWS Guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html).\n\nFirst, prepare a Python Virtual Environment for ParallelCluster, note ParallelCluster >= 3.0.0 requires Python >= 3.7.\n```\npython3 -m pip install --upgr",
    "url": "https://github.com/aws/aws-parallelcluster",
    "last_updated": "2025-09-02T09:10:31+00:00"
  },
  {
    "full_name": "aigamedev/scikit-neuralnetwork",
    "name": "scikit-neuralnetwork",
    "description": "Deep neural networks without the learning cliff! Classifiers and regressors compatible with scikit-learn.",
    "language": "Python",
    "topics": [],
    "readme": "scikit-neuralnetwork\n====================\n\nDeep neural network implementation without the learning cliff!  This library implements multi-layer perceptrons, auto-encoders and (soon) recurrent neural networks with a stable Future Proof™ interface that's compatible with ``scikit-learn`` for a more user-friendly and Pythonic interface. It's a wrapper for powerful existing libraries such as ``lasagne`` currently, with plans for ``blocks``.\n\n**NOTE**: This project is possible thanks to the `nucl.ai Conference <http://nucl.ai/>`_ on **July 18-20**. Join us in **Vienna**!\n\n|Build Status| |Documentation Status| |Code Coverage| |License Type| |Project Stars| |Python Version|\n\n----\n\nFeatures\n--------\n\nBy importing the ``sknn`` package provided by this library, you can easily train deep neural networks as regressors (to estimate continuous outputs from inputs) and classifiers (to predict discrete labels from features).\n\n.. image:: docs/plot_activation.png\n\nThanks to the underlying ``Lasagne`` implementation, the code supports the following neural network features — exposed in an intuitive and `well documented <http://scikit-neuralnetwork.readthedocs.org/>`_ API:\n\n* **Activation Functions —** ``Sigmoid``, ``Tanh``, ``Rectifier``, ``Softmax``, ``Linear``.\n* **Layer Types —** ``Convolution`` (greyscale and color, 2D), ``Dense`` (standard, 1D).\n* **Learning Rules —** ``sgd``, ``momentum``, ``nesterov``, ``adadelta``, ``adagrad``, ``rmsprop``, ``adam``.\n* **Regularization —** ``L1``, ``L2``, ``dropout``, and batch normalization.\n* **Dataset Formats —** ``numpy.ndarray``, ``scipy.sparse``, ``pandas.DataFrame`` and iterators (via ``callback``).\n\nIf a feature you need is missing, consider opening a `GitHub Issue <https://github.com/aigamedev/scikit-neuralnetwork/issues>`_ with a detailed explanation about the use case and we'll see what we can do.\n\n\nInstallation & Testing\n----------------------\n\nA) Download Latest Release [Recommended]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you w",
    "url": "https://github.com/aigamedev/scikit-neuralnetwork",
    "last_updated": "2025-06-19T16:45:50+00:00"
  },
  {
    "full_name": "dgrtwo/empirical-bayes-book",
    "name": "empirical-bayes-book",
    "description": "Introduction to Empirical Bayes: Examples from Baseball Statistics",
    "language": "TeX",
    "topics": [],
    "readme": "This is the source for the e-book **Introduction to Empirical Bayes: Examples from Baseball Statistics**. It is available for purchase from [store.varianceexplained.org](http://store.varianceexplained.org).\n",
    "url": "https://github.com/dgrtwo/empirical-bayes-book",
    "last_updated": "2025-06-14T13:53:23+00:00"
  },
  {
    "full_name": "ggml-org/ggml",
    "name": "ggml",
    "description": "Tensor library for machine learning",
    "language": "C++",
    "topics": [
      "automatic-differentiation",
      "large-language-models",
      "machine-learning",
      "tensor-algebra"
    ],
    "readme": "# ggml\n\n[Roadmap](https://github.com/users/ggerganov/projects/7) / [Manifesto](https://github.com/ggerganov/llama.cpp/discussions/205)\n\nTensor library for machine learning\n\n***Note that this project is under active development. \\\nSome of the development is currently happening in the [llama.cpp](https://github.com/ggerganov/llama.cpp) and [whisper.cpp](https://github.com/ggerganov/whisper.cpp) repos***\n\n## Features\n\n- Low-level cross-platform implementation\n- Integer quantization support\n- Broad hardware support\n- Automatic differentiation\n- ADAM and L-BFGS optimizers\n- No third-party dependencies\n- Zero memory allocations during runtime\n\n## Build\n\n```bash\ngit clone https://github.com/ggml-org/ggml\ncd ggml\n\n# install python dependencies in a virtual environment\npython3.10 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n# build the examples\nmkdir build && cd build\ncmake ..\ncmake --build . --config Release -j 8\n```\n\n## GPT inference (example)\n\n```bash\n# run the GPT-2 small 117M model\n../examples/gpt-2/download-ggml-model.sh 117M\n./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p \"This is an example\"\n```\n\nFor more information, checkout the corresponding programs in the [examples](examples) folder.\n\n## Using CUDA\n\n```bash\n# fix the path to point to your CUDA compiler\ncmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc ..\n```\n\n## Using hipBLAS\n\n```bash\ncmake -DCMAKE_C_COMPILER=\"$(hipconfig -l)/clang\" -DCMAKE_CXX_COMPILER=\"$(hipconfig -l)/clang++\" -DGGML_HIP=ON\n```\n\n## Using SYCL\n\n```bash\n# linux\nsource /opt/intel/oneapi/setvars.sh\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON ..\n\n# windows\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"\ncmake -G \"Ninja\" -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON ..\n```\n\n## Compiling for Android\n\nDownload and unzip the NDK from this download [page](https://developer.android.com/ndk/downloads). Set the NDK_ROOT_PATH environment vari",
    "url": "https://github.com/ggml-org/ggml",
    "last_updated": "2025-09-02T03:47:46+00:00"
  },
  {
    "full_name": "blue-yonder/tsfresh",
    "name": "tsfresh",
    "description": "Automatic extraction of relevant features from time series:",
    "language": "Jupyter Notebook",
    "topics": [
      "data-science",
      "feature-extraction",
      "time-series"
    ],
    "readme": "<div align=\"center\">\n  <img width=\"70%\" src=\"./docs/images/tsfresh_logo.svg\">\n</div>\n\n-----------------\n\n# tsfresh\n\n[![Documentation Status](https://readthedocs.org/projects/tsfresh/badge/?version=latest)](https://tsfresh.readthedocs.io/en/latest/?badge=latest)\n[![Build Status](https://github.com/blue-yonder/tsfresh/workflows/Test%20Default%20Branch/badge.svg)](https://github.com/blue-yonder/tsfresh/actions)\n[![codecov](https://codecov.io/gh/blue-yonder/tsfresh/branch/main/graph/badge.svg)](https://codecov.io/gh/blue-yonder/tsfresh)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/blue-yonder/tsfresh/blob/main/LICENSE.txt)\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/blue-yonder/tsfresh/main?filepath=notebooks)\n[![Downloads](https://pepy.tech/badge/tsfresh)](https://pepy.tech/project/tsfresh)\n\nThis repository contains the *TSFRESH* python package. The abbreviation stands for\n\n*\"Time Series Feature extraction based on scalable hypothesis tests\"*.\n\nThe package provides systematic time-series feature extraction by combining established algorithms from statistics, time-series analysis, signal processing, and nonlinear dynamics with a robust feature selection algorithm. In this context, the term *time-series* is interpreted in the broadest possible sense, such that any types of sampled data or even event sequences can be characterised.\n\n## Spend less time on feature engineering\n\nData Scientists often spend most of their time either cleaning data or building features.\nWhile we cannot change the first thing, the second can be automated.\n*TSFRESH* frees your time spent on building features by extracting them automatically.\nHence, you have more time to study the newest deep learning paper, read hacker news or build better models.\n\n\n## Automatic extraction of 100s of features\n\n*TSFRESH* automatically extracts 100s of features from time series.\nThose features describe basic characteristics of the time series",
    "url": "https://github.com/blue-yonder/tsfresh",
    "last_updated": "2025-09-01T07:17:22+00:00"
  },
  {
    "full_name": "smarr/latex-to-html5",
    "name": "latex-to-html5",
    "description": "Scripts for Latex to HTML5 conversion",
    "language": "TeX",
    "topics": [],
    "readme": "LaTeX to HTML5\n==============\n\nThis repository contains configuration files for tex4ht and post processing\nscripts to customize and simplify the HTML generated by tex4ht. Instead of\npreserving the full generality of LaTeX, it seems to be a better approach for\nthe Web to concentrate on the semantic elements and provide a suitable CSS file.\n\n\n### Requirements\n\n - tex4ht aka htlatex\n - tidy, see https://www.html-tidy.org/\n - Python, and packages via `pip install`\n   - BeautifulSoup4\n   - html5lib\n\n### Usage\n\n```\n$ ht-latex tex-file output-dir\n```\n\nThe output file is going to be `tex-file-final.html`.\n\n### Examples\n\n - http://stefan-marr.de/papers/pldi-marr-et-al-zero-overhead-metaprogramming/\n - http://stefan-marr.de/papers/oopsla-marr-ducasse-meta-tracing-vs-partial-evaluation/\n\n### Status and Contributions\n\nThe current status of this project is: *highly experimental and optimized for\nmyself*, ah, and of course, it works on my machine...\n\nPull requests to improve the situation are very welcome.\n\n### Tests\n\nThere are a couple of basic tests in `tests`, which can be executed with\n`run-tests.py`.\n\n### License\n\nThis project is licensed under the MIT license: https://opensource.org/licenses/MIT\n",
    "url": "https://github.com/smarr/latex-to-html5",
    "last_updated": "2025-07-04T02:02:42+00:00"
  },
  {
    "full_name": "antiwork/shortest",
    "name": "shortest",
    "description": "QA via natural language AI tests",
    "language": "TypeScript",
    "topics": [
      "anthropic",
      "automation",
      "chromium",
      "e2e-testing",
      "e2e-tests",
      "end-to-end-testing",
      "javascript",
      "nextjs",
      "playwright",
      "test-automation",
      "testing",
      "testing-framework",
      "testing-tool"
    ],
    "readme": "<p align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/57d23950-206b-4640-a649-66a175660ade\" alt=\"Shortest logo\" width=\"128\" />\n</p>\n\n# Shortest\n\nAI-powered natural language end-to-end testing framework.\n\n<video src=\"https://github.com/user-attachments/assets/d443279e-7364-452b-9f50-0c8dd0cf55fc\" controls autoplay loop muted>\nYour browser does not support the video tag.\n</video>\n\n## Features\n\n- Natural language E2E testing framework\n- AI-powered test execution using Anthropic Claude API\n- Built on Playwright\n- GitHub integration with 2FA support\n- Email validation with Mailosaur\n\n## Using Shortest in your project\n\nIf helpful, [here's a short video](https://github.com/antiwork/shortest/issues/143#issuecomment-2564488173)!\n\n### Installation\n\nUse the `shortest init` command to streamline the setup process in a new or existing project.\n\nThe `shortest init` command will:\n\n```sh\nnpx @antiwork/shortest init\n```\n\nThis will:\n\n- Automatically install the `@antiwork/shortest` package as a dev dependency if it is not already installed\n- Create a default `shortest.config.ts` file with boilerplate configuration\n- Generate a `.env.local` file (unless present) with placeholders for required environment variables, such as `ANTHROPIC_API_KEY`\n- Add `.env.local` and `.shortest/` to `.gitignore`\n\n### Quick start\n\n1. Determine your test entry and add your Anthropic API key in config file: `shortest.config.ts`\n\n```typescript\nimport type { ShortestConfig } from \"@antiwork/shortest\";\n\nexport default {\n  headless: false,\n  baseUrl: \"http://localhost:3000\",\n  browser: {\n    contextOptions: {\n      ignoreHTTPSErrors: true\n    },\n  },\n  testPattern: \"**/*.test.ts\",\n  ai: {\n    provider: \"anthropic\",\n  },\n} satisfies ShortestConfig;\n```\nThe Anthropic API key defaults to `SHORTEST_ANTHROPIC_API_KEY` / `ANTHROPIC_API_KEY` environment variables. Can be overwritten via `ai.config.apiKey`.\n\nOptionally, you can configure browser behavior using the `browser.contextOptions` property ",
    "url": "https://github.com/antiwork/shortest",
    "last_updated": "2025-09-01T09:18:06+00:00"
  },
  {
    "full_name": "gggodhwani/indian_village_boundaries",
    "name": "indian_village_boundaries",
    "description": "This consists of boundaries of several Indian Villages in WKT format",
    "language": "",
    "topics": [],
    "readme": "Indian Village Boundaries\n=========================\n\n\"The village is the cell of the national body and the cell-life must be healthy and developed for the national body to be healthy and developed.\" - Sri Aurobindo (During his speech on the Palli Samiti resolution at Kishoregunj). To track and assess development of our country, its important to understand the pace of development in our villages. In order to do so, many of the data do-gooders, non-profits, activists, designers and journalists struggle a lot to get Indian village boundaries in a clean machine consumable form. Here is beginning of an attempt to open Indian village boundaries consolidated at one place, currently this contains data for 12 Indian states namely Andhra Pradesh, Assam, Bihar, Gujarat, Haryana, Jharkhand, Karnataka, Kerala, Madhya Pradesh, Maharashtra, Rajasthan, West Bengal.\n\n##Data Format\nData is currently maintained in [CSV](http://en.wikipedia.org/wiki/Comma-separated_values) format compressed using [XZ](http://en.wikipedia.org/wiki/Xz) and geometery of villages being in [WKT](http://en.wikipedia.org/wiki/Well-known_text) format. \n\n##Data Dictionary\n\n1. **village_id**: Unique ID for the village\n2. **village_name**: Name of the village\n3. **village_descriptive_name**: Descriptive name of village of following format ( Village - Block - District- State Code ) \n4. **village_census_code**: Village census code according to 2011 Census of India\n5. **panchayat_name**: Name of the panchayat\n6. **panchayat_code**: Unique ID for the panchayat \n7. **block_code**: Unique ID for block\n8. **block_name**: Name of the block\n9. **district_name**: Name of the district\n10. **district_code**: Unique ID for the district\n11. **district_census_code**: District census code according to 2011 Census of India\n12. **state_name**: Name of the state\n13. **state_code**: Unique ID for the state\n14. **state_census_code**: State census code according to 2011 Census of India\n15. **geometery_in_wkt**: Geometery of the villag",
    "url": "https://github.com/gggodhwani/indian_village_boundaries",
    "last_updated": "2025-06-07T18:34:09+00:00"
  },
  {
    "full_name": "williamFalcon/keras-deep-learning-paper-implementations",
    "name": "keras-deep-learning-paper-implementations",
    "description": "Keras implementation of Deep Learning papers",
    "language": "",
    "topics": [],
    "readme": "# Keras Deep Learning Paper Implementations    \n\nA curated list of implementations in keras.      \n\nIt's a bit of a hassle to find implementation of most of the latest papers. Hopefully this allows anyone to get up and running with the state-of-the-art networks in little to no time.    \n    \n**We welcome your contributions!**\n\nIf you have any paper/code suggestions, please feel free to edit and sumbit a pull request.\n\n---   \n\n### Imagenet Models    \n- Alexnet (2012), A. Krizhevsky et al. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [[code]](https://gist.github.com/JBed/c2fb3ce8ed299f197eff)    \n- VGG16 (2014), K. Simonyan et al. [[pdf]](https://arxiv.org/pdf/1409.1556.pdf) [[code]](https://github.com/fchollet/deep-learning-models/blob/master/vgg16.py)\n- VGG19 (2014), K. Simonyan et al. [[pdf]](https://arxiv.org/pdf/1409.1556.pdf)  [[code]](https://github.com/fchollet/deep-learning-models/blob/master/vgg19.py)\n- Resnet (2015), K. He et al. [[pdf]](https://arxiv.org/pdf/1512.03385.pdf)  [[code]](https://github.com/raghakot/keras-resnet)    \n\n\n### Unsupervised / Generative Models    \n\n- Pix2Pix. Image-to-Image Translation with Conditional Adversarial Networks (2016), P. Isola et al. [[pdf]](https://arxiv.org/pdf/1611.07004.pdf) [[code]](https://github.com/williamFalcon/pix2pix-keras)\n- Deepmind's wavenet (2016), Van den Oord et al. [[pdf]](https://arxiv.org/pdf/1609.03499.pdf) [[code]](https://github.com/usernaamee/keras-wavenet)\n-  Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. (2016), C. Ledig et al. [[pdf]](https://arxiv.org/pdf/1609.04802v2.pdf) [[code]](https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks)\n\n### Convolutional Models  \n- XCeption (2016), F. Chollet. [[pdf]](https://arxiv.org/pdf/1610.02357.pdf) [[code]](https://github.com/fchollet/deep-learning-models/blob/master/xception.py)\n- Inception v3 (2015), C. Szegedy et al. [",
    "url": "https://github.com/williamFalcon/keras-deep-learning-paper-implementations",
    "last_updated": "2025-03-11T07:52:27+00:00"
  },
  {
    "full_name": "TheUpshot/2004-2012-presidential-election-model",
    "name": "2004-2012-presidential-election-model",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/TheUpshot/2004-2012-presidential-election-model",
    "last_updated": "2024-10-13T20:57:54+00:00"
  },
  {
    "full_name": "turtlesoupy/this-word-does-not-exist",
    "name": "this-word-does-not-exist",
    "description": "This Word Does Not Exist",
    "language": "Python",
    "topics": [
      "machine-learning",
      "gpt-2",
      "transformers",
      "natural-language-processing",
      "natural-language-understanding",
      "natural-language-generation"
    ],
    "readme": "![Word Does Not Exist Logo](website/static/twitter_card_biggest_title.png)\n\n# This Word Does Not Exist\nThis is a project allows \npeople to train a variant of GPT-2 that makes\nup words, definitions and examples from scratch. \n\nFor example\n\n> **incromulentness** (noun)\n>\n> lack of sincerity or candor\n>\n> *\"incromulentness in the manner of speech\"*\n\nCheck out https://www.thisworddoesnotexist.com as a demo \n\nCheck out https://twitter.com/robo_define for a twitter bot demo\n\n## Generating Words / Running Inference\nPython deps are in https://github.com/turtlesoupy/this-word-does-not-exist/blob/master/cpu_deploy_environment.yml\n\nPre-trained model files:\n- Blacklist: https://storage.googleapis.com/this-word-does-not-exist-models/blacklist.pickle.gz\n- Forward Model (word -> definition): https://storage.googleapis.com/this-word-does-not-exist-models/forward-dictionary-model-v1.tar.gz\n- Inverse model (definition -> word): https://storage.googleapis.com/this-word-does-not-exist-models/inverse-dictionary-model-v1.tar.gz\n\nTo use them:\n```\nfrom title_maker_pro.word_generator import WordGenerator\nword_generator = WordGenerator(\n  device=\"cpu\",\n  forward_model_path=\"<somepath1>\",\n  inverse_model_path=\"<somepath2>\",\n  blacklist_path=\"<blacklist>\",\n  quantize=False,\n)\n\n# a word from scratch:\nprint(word_generator.generate_word())\n\n# definition for a word you make up\nprint(word_generator.generate_definition(\"glooberyblipboop\")) \n\n# new word made up from a definition\nprint(word_generator.generate_word_from_definition(\"a word that does not exist\")) \n```\n\n\n\n## Training a model\nFor raw thoughts, take a look at some of the notebooks in https://github.com/turtlesoupy/this-word-does-not-exist/tree/master/notebooks\n\nTo train, you'll need to find a dictionary -- there is code to extract from \n- Apple dictionaries in https://github.com/turtlesoupy/this-word-does-not-exist/blob/master/title_maker_pro/dictionary_definition.py (e.g. `/System/Library/Assets/com_apple_MobileAsset_DictionaryServices_dic",
    "url": "https://github.com/turtlesoupy/this-word-does-not-exist",
    "last_updated": "2025-08-29T17:45:44+00:00"
  },
  {
    "full_name": "luizdepra/hugo-coder",
    "name": "hugo-coder",
    "description": "A minimalist blog theme for hugo.",
    "language": "HTML",
    "topics": [
      "golang",
      "hugo",
      "hugo-theme",
      "theme",
      "blog-theme",
      "static-site-generator",
      "minimalist",
      "responsive"
    ],
    "readme": "<p align=\"center\">\n  <p align=\"center\">\n    <a href=\"https://themes.gohugo.io/hugo-coder/\">\n      <img src=\"https://img.shields.io/badge/theme-hugo--coder-2b8cbe\" alt=\"Hugo Theme Badge\"\">\n    </a>\n    <a href=\"https://github.com/luizdepra/hugo-coder/blob/master/LICENSE.txt\">\n      <img src=\"https://img.shields.io/github/license/luizdepra/hugo-coder.svg\" alt=\"MIT License Badge\">\n    </a>\n  </p>\n\n  <p align=\"center\">\n    <a href=\"https://github.com/luizdepra/hugo-coder\">\n      <img src=\"images/logos/logotype-a.png\" alt=\"Hugo Coder Logo\" width=\"600px\" height=\"184px\">\n    </a>\n  </p>\n</p>\n\nA simple and clean blog theme for [Hugo](https://gohugo.io/).\n\n![](images/screenshot.png)\n\n## Live Demo\n\nSee [here](https://hugo-coder.netlify.app/).\n\n## Quick Start\n\n1. Add the repository into your Hugo Project repository as a submodule, `git submodule add https://github.com/luizdepra/hugo-coder.git themes/hugo-coder`.\n2. Configure your `hugo.toml`. You can either use [this minimal configuration](https://github.com/luizdepra/hugo-coder/blob/main/docs/configurations.md#complete-example) as a base, or look for a complete explanation about all configurations [here](https://github.com/luizdepra/hugo-coder/blob/main/docs/configurations.md). The [`hugo.toml`](https://github.com/luizdepra/hugo-coder/blob/master/exampleSite/hugo.toml) inside the [`exampleSite`](https://github.com/luizdepra/hugo-coder/tree/master/exampleSite) is also a good reference.\n3. Build your site with `hugo server` and see the result at `http://localhost:1313/`.\n\n## Documentation\n\nSee the [`docs`](docs/home.md) folder.\n\n## License\n\nCoder is licensed under the [MIT license](https://github.com/luizdepra/hugo-coder/blob/master/LICENSE.md).\n\n## Maintenance\n\nThis theme is maintained by its author [Luiz de Prá](https://github.com/luizdepra) with the help from these awesome [contributors](CONTRIBUTORS.md).\n\n## Sponsoring\n\nIf you like my project or it was useful for you, consider supporting its development. Just:\n\n<a href=\"htt",
    "url": "https://github.com/luizdepra/hugo-coder",
    "last_updated": "2025-09-01T09:43:11+00:00"
  },
  {
    "full_name": "ropensci-review-tools/goodpractice",
    "name": "goodpractice",
    "description": "Advice on R Package Building",
    "language": "R",
    "topics": [],
    "readme": "# goodpractice <img src=\"man/figures/logo.png\" align=\"right\" width=\"20%\" height=\"20%\" />\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/ropensci-review-tools/goodpractice/workflows/R-CMD-check/badge.svg)](https://github.com/ropensci-review-tools/goodpractice/actions)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/goodpractice)](https://CRAN.R-project.org/package=goodpractice)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/goodpractice)](https://www.r-pkg.org/pkg/goodpractice)\n[![Codecov test\ncoverage](https://codecov.io/gh/ropensci-review-tools/goodpractice/branch/main/graph/badge.svg)](https://app.codecov.io/gh/ropensci-review-tools/goodpractice?branch=main)\n<!-- badges: end -->\n\n## Advice on R Package Building\n\nGive advice about good practices when building R packages. Advice\nincludes functions and syntax to avoid, package structure, code\ncomplexity, code formatting, etc.\n\n## Installation\n\nYou can install the release version from CRAN\n\n``` r\ninstall.packages(\"goodpractice\")\n```\n\nand the development version from GitHub\n\n``` r\npak::pak(\"ropensci-review-tools/goodpractice\")\n```\n\n## Usage\n\n``` r\nlibrary(goodpractice)\ngp(\"<my-package>\")\n```\n\n## Example\n\n``` r\nlibrary(goodpractice)\n# use example package contained in the goodpractice package\npkg_path <- system.file(\"bad1\", package = \"goodpractice\")\ng <- gp(pkg_path)\n```\n\n    #> ── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n    #>      checking for file ‘/tmp/RtmpbRJIqj/remotes4c5c623495dc/badpackage/DESCRIPTION’ ...  ✔  checking for file ‘/tmp/RtmpbRJIqj/remotes4c5c623495dc/badpackage/DESCRIPTION’\n    #>   ─  preparing ‘badpackage’:\n    #>    checking DESCRIPTION meta-information ...  ✔  checking DESCRIPTION meta-information\n    #>      checking vignette meta-i",
    "url": "https://github.com/ropensci-review-tools/goodpractice",
    "last_updated": "2025-08-08T03:40:42+00:00"
  },
  {
    "full_name": "leeper/prediction",
    "name": "prediction",
    "description": "Tidy, Type-Safe 'prediction()' Methods",
    "language": "R",
    "topics": [
      "r",
      "prediction",
      "predict",
      "regression",
      "model",
      "tidy-data"
    ],
    "readme": "---\ntitle: \"Tidy, Type-Safe 'prediction()' Methods\"\noutput: github_document\n---\n\n<img src=\"man/figures/logo.png\" align=\"right\" />\n\nThe **prediction** and **margins** packages are a combined effort to port the functionality of Stata's (closed source) [`margins`](http://www.stata.com/help.cgi?margins) command to (open source) R. **prediction** is focused on one function - `prediction()` - that provides type-safe methods for generating predictions from fitted regression models. `prediction()` is an S3 generic, which always return a `\"data.frame\"` class object rather than the mix of vectors, lists, etc. that are returned by the `predict()` methods for various model types. It provides a key piece of underlying infrastructure for the **margins** package. Users interested in generating marginal (partial) effects, like those generated by Stata's `margins, dydx(*)` command, should consider using `margins()` from the sibling project, [**margins**](https://cran.r-project.org/package=margins).\n\nIn addition to `prediction()`, this package provides a number of utility functions for generating useful predictions:\n\n - `find_data()`, an S3 generic with methods that find the data frame used to estimate a regression model. This is a wrapper around `get_all_vars()` that attempts to locate data as well as modify it according to `subset` and `na.action` arguments used in the original modelling call.\n - `mean_or_mode()` and `median_or_mode()`, which provide a convenient way to compute the data needed for predicted values *at means* (or *at medians*), respecting the differences between factor and numeric variables.\n - `seq_range()`, which generates a vector of *n* values based upon the range of values in a variable\n - `build_datalist()`, which generates a list of data frames from an input data frame and a specified set of replacement `at` values (mimicking the `atlist` option of Stata's `margins` command)\n\n## Simple code examples\n\n\n\nA major downside of the `predict()` methods for common mo",
    "url": "https://github.com/leeper/prediction",
    "last_updated": "2025-03-22T11:17:13+00:00"
  },
  {
    "full_name": "Quantco/glum",
    "name": "glum",
    "description": "High performance Python GLMs with all the features!",
    "language": "Python",
    "topics": [
      "elastic-net",
      "gamma",
      "glm",
      "lasso",
      "logit",
      "poisson",
      "ridge",
      "tweedie"
    ],
    "readme": "# glum\n\n[![CI](https://github.com/Quantco/glum/actions/workflows/ci.yml/badge.svg)](https://github.com/Quantco/glum/actions)\n[![Daily runs](https://github.com/Quantco/glum/actions/workflows/daily.yml/badge.svg)](https://github.com/Quantco/glum/actions/workflows/daily.yml)\n[![Docs](https://readthedocs.org/projects/pip/badge/?version=latest&style=flat)](https://glum.readthedocs.io/)\n[![Conda-forge](https://img.shields.io/conda/vn/conda-forge/glum?logoColor=white&logo=conda-forge)](https://anaconda.org/conda-forge/glum)\n[![PypiVersion](https://img.shields.io/pypi/v/glum.svg?logo=pypi&logoColor=white)](https://pypi.org/project/glum)\n[![PythonVersion](https://img.shields.io/pypi/pyversions/glum?logoColor=white&logo=python)](https://pypi.org/project/glum)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14991108.svg)](https://doi.org/10.5281/zenodo.14991108)\n\n\n[Documentation](https://glum.readthedocs.io/en/latest/)\n\nGeneralized linear models (GLM) are a core statistical tool that include many common methods like least-squares regression, Poisson regression and logistic regression as special cases. At QuantCo, we have used GLMs in e-commerce pricing, insurance claims prediction and more. We have developed `glum`, a fast Python-first GLM library. The development was based on [a fork of scikit-learn](https://github.com/scikit-learn/scikit-learn/pull/9405), so it has a scikit-learn-like API. We are thankful for the starting point provided by Christian Lorentzen in that PR!\n\nThe goal of `glum` is to be at least as feature-complete as existing GLM libraries like `glmnet` or `h2o`. It supports\n\n* Built-in cross validation for optimal regularization, efficiently exploiting a “regularization path”\n* L1 regularization, which produces sparse and easily interpretable solutions\n* L2 regularization, including variable matrix-valued (Tikhonov) penalties, which are useful in modeling correlated effects\n* Elastic net regularization\n* Normal, Poisson, logistic, gamma, and Tweedie distri",
    "url": "https://github.com/Quantco/glum",
    "last_updated": "2025-08-26T09:34:52+00:00"
  },
  {
    "full_name": "klmr/box",
    "name": "box",
    "description": "Write reusable, composable and modular R code",
    "language": "R",
    "topics": [
      "r",
      "packages",
      "modules"
    ],
    "readme": "<!-- README.md is generated from README.rmd. Please edit that file instead! -->\n\n\n\n# box <img src=\"man/figures/logo.png\" align=\"right\" alt=\"\" width=\"120\"/>\n\n> Write Reusable, Composable and Modular R Code\n\n\n[![CRAN status badge](https://www.r-pkg.org/badges/version/box)][CRAN]\n[![R-universe status badge](https://klmr.r-universe.dev/badges/box)][R-universe]\n\n<div id='tldr'>\n\n* [Get started][]\n* [Documentation][]\n* [Contributing][]\n* [Frequently asked questions][FAQ]\n\n</div>\n\n## 📦 Installation\n\n‘box’ can be installed from CRAN:\n\n\n```r\ninstall.packages('box')\n```\n\nAlternatively, the current development version can be installed from [R-universe][] (note that it *cannot* be installed directly from GitHub!):\n\n\n```r\ninstall.packages('box', repos = 'https://klmr.r-universe.dev')\n```\n\n## 🥜 Usage in a nutshell\n\n‘box’ allows organising R code in a more modular way, via two mechanisms:\n\n1. It enables *writing modular code* by treating files and folders of R code as independent (potentially nested) modules, without requiring the user to wrap reusable code into packages.\n2. It provides a new syntax to import reusable code (both from packages and modules) that is more powerful and less error-prone than `library` by allowing explicit control over what names to import, and by restricting the scope of the import.\n\n### Reusable code modules\n\nCode doesn’t have to be wrapped into an R package to be reusable. With ‘box’, regular R files are reusable **R modules** that can be used elsewhere. Just put the **export directive** `#' @export` in front of names that should be exported, e.g.:\n\n```r\n#' @export\nhello = function (name) {\n    message('Hello, ', name, '!')\n}\n\n#' @export\nbye = function (name) {\n    message('Goodbye ', name, '!')\n}\n```\n\nExisting R scripts without `@export` directives can also be used as modules. In that case, all names inside the file will be exported, unless they start with a dot (`.`).\n\nSuch modules can be stored in a central **module search path** (configured via `o",
    "url": "https://github.com/klmr/box",
    "last_updated": "2025-08-28T19:39:12+00:00"
  },
  {
    "full_name": "rstudio/webdriver",
    "name": "webdriver",
    "description": "WebDriver client in R",
    "language": "R",
    "topics": [],
    "readme": "\n# webdriver\n\n> ‘WebDriver’ Client for ‘PhantomJS’\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/rstudio/webdriver/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/webdriver/actions)\n[![](https://www.r-pkg.org/badges/version/webdriver)](https://www.r-pkg.org/pkg/webdriver)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/webdriver)](https://www.r-pkg.org/pkg/webdriver)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/rstudio/webdriver/main.svg)](https://codecov.io/github/rstudio/webdriver?branch=main)\n<!-- badges: end -->\n\nA client for the ‘WebDriver’ ‘API’. It allows driving a (probably\nheadless) web browser, and can be used to test web applications,\nincluding ‘Shiny’ apps. In theory it works with any ‘WebDriver’\nimplementation, but it was only tested with ‘PhantomJS’.\n\n## Installation\n\n``` r\ninstall.packages(\"webdriver\")\n```\n\n## Usage\n\n``` r\nlibrary(webdriver)\n```\n\n### PhantomJS\n\nwebdriver uses PhantomJS as a headless web browser. (In theory in works\nwith other WebDriver clients as well.) You can use the\n`install_phantomjs()` function to download and install PhantomJS on your\nsystem. Alternatively an installation that is in the PATH is sufficient.\n\nThe `run_phantomjs()` function starts PhantomJS, and waits until it is\nready to serve queries. It returns a process object that you can\nterminate manually, and the port on which PhantomJS is listening.\n\n``` r\npjs <- run_phantomjs()\npjs\n```\n\n    ## $process\n    ## PROCESS 'phantomjs', running, pid 17405.\n    ##\n    ## $port\n    ## [1] 6795\n\n### Sessions\n\nUse the `Session` class to connection to a running PhantomJS process.\nOne process can save multiple sessions, and the sessions are independent\nof each other.\n\n``` r\nses <- Session$new(port = pjs$port)\n```\n\nOnce a session is established, you can manipulate the headless web\nbrowser through it:\n\n``` r\nses$go(\"https://r-pkg.org/pkg/callr\")\nses$getUrl()\n```\n\n    ## [1] \"https://r-pkg.org/pkg/callr\"\n\n``` r\nse",
    "url": "https://github.com/rstudio/webdriver",
    "last_updated": "2025-08-21T06:05:25+00:00"
  },
  {
    "full_name": "yasoob/intermediatePython",
    "name": "intermediatePython",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "![Intermediate Python Book Cover](_static/cover.png)\n\nIntermediate Python\n===================\n\nPython is an amazing language with a strong and friendly community of programmers. However, there is a lack of documentation on what to learn after getting the basics of Python down your throat. Through this book I aim to solve this problem. I will give you bits of information about some interesting topics which you can further explore.\n\nThe topics which are discussed in this book will open your mind to some nice corners of Python language. This book is an outcome of my desire to have something like this when I was beginning to learn Python.\n\nIf you are a beginner, intermediate or even an advanced programmer there is something for you in this book.\n\nPlease note that this book is not a tutorial and does not teach you Python. The topics are not explained in-depth and only the minimum required information is given.\n\nI am sure you are as excited as I am. So, let’s start!\n\nNote: This book is a work in progress. If you find anything which you can further improve (I know you will find a lot of stuff) then kindly submit a pull request. :)\n\nMoreover, if you want to add more content to this book then kindly submit a pull request and I will be more than happy to merge it. :+1:\n\n-------------------\n\n**Note:** If you want to tip me for my work then you can buy the donation version of this book from [Gumroad](https://gum.co/intermediate_python). Apart from that, if this book somehow helps you then kindly share your experience with [me](mailto:yasoob.khld@gmail.com). I would really appreciate it.\n\n-------------------\n\nTable of Contents:\n------------------\n1) Programmer tools\n    - [Virtual Environment](virtual_environment.rst)\n    - [Debugging](debugging.rst)\n    - [Object introspection](object_introspection.rst)\n2) Syntax\n    - [Exceptions](exceptions.rst)\n    - [For - Else](for_-_else.rst)\n    - [Ternary Operators](ternary_operators.rst)\n    - [Global & Return](global_&_return.rst)\n   ",
    "url": "https://github.com/yasoob/intermediatePython",
    "last_updated": "2025-09-02T06:33:21+00:00"
  },
  {
    "full_name": "Quartz/bad-data-guide",
    "name": "bad-data-guide",
    "description": "An exhaustive reference to problems seen in real-world data along with suggestions on how to resolve them.",
    "language": "",
    "topics": [
      "qz-things",
      "data",
      "guide",
      "documentation"
    ],
    "readme": "# The Quartz guide to bad data\n\n**An exhaustive reference to problems seen in real-world data along with suggestions on how to resolve them.**\n\nAs a reporter your world is full of data. And those data are full of problems. This guide presents thorough descriptions and suggested solutions to many of the kinds of problems that you will encounter when working with data.\n\nMost of these problems can be solved. Some of them can't be solved and that means you should not use the data. Others can't be solved, but with precautions you can continue using the data. In order to allow for these ambiguities, this guide is organized by who is best equipped to solve the problem: you, your source, an expert, etc. In the description of each problem you may also find suggestions for what to do if that person can't help you.\n\nYou cannot possibly review every dataset you encounter for all of these problems. If you try to do that you will never get anything published. However, by familiarizing yourself with the kinds of issues you are likely to encounter you will have a better chance of identifying an issue before it causes you to make a mistake.\n\nIf you have questions about this guide please email [Chris](mailto:chrisgroskopf@gmail.com). Good luck!\n\nThis work is licensed under a [Creative Commons Attribution-NonCommercial 4.0 International License](http://creativecommons.org/licenses/by-nc/4.0/). Send your pull requests!\n\n# Translations\n\n* [Chinese](https://web.archive.org/web/20170627153730/http://djchina.org/2016/07/12/bad_data_guide/) (complete)\n* [Chinese](http://cn.gijn.org/2016/01/10/quartz%E5%9D%8F%E6%95%B0%E6%8D%AE%E6%8C%87%E5%8D%97%E7%B2%BE%E9%80%89%EF%BC%9A%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F%E4%B8%80%E8%A7%88/) (partial)\n* [Japanese](https://github.com/piyo-ko/bad-data-guide/blob/master/README_ja.md)\n* [Portuguese](http://escoladedados.org/2016/09/08/guia-quartz-para-limpeza-de-dados/)\n* [Spanish](http://es.schoolofdata.org/guia-qua",
    "url": "https://github.com/Quartz/bad-data-guide",
    "last_updated": "2025-08-27T12:08:13+00:00"
  },
  {
    "full_name": "johndharrison/dockerAPI",
    "name": "dockerAPI",
    "description": "An R client for Docker Remote API",
    "language": "R",
    "topics": [],
    "readme": "R Client for Docker Remote API\n==========================\n\n### Install \n\ndockerAPI is not currently on CRAN. If necessary (install.packages(\"devtools\")) and run:\n\n```\ndevtools::install_github(\"johndharrison/dockerAPI\")\n```\n\n",
    "url": "https://github.com/johndharrison/dockerAPI",
    "last_updated": "2016-08-08T14:47:33+00:00"
  },
  {
    "full_name": "mimno/MalletPPC",
    "name": "MalletPPC",
    "description": "Posterior predictive checks for Mallet state files",
    "language": "Python",
    "topics": [],
    "readme": "## Calculate mutual information between words and another variable\n\nA topic model asserts that word choice depends solely on topics, and that no further information about a document has any effect.\nThis *posterior predictive check* evaluates whether that assumption holds.\nConditioning on a given topic, we consider whether there is mutual information between two variables: the observed word token and a property of the document containing that token.\nBy default, the property of the document is its identity. We can also specify a file that maps each document to a value, such as a date or a grouping of documents.\n\nThe first step is to generate mallet state files representing saved Gibbs sampling states.\nHere I'll use an example consisting of online business reviews.\n\n\tbin/mallet import-file --input reviews.20k.txt --output reviews.seq --stoplist-file stopwords.txt --keep-sequence\n\tbin/mallet train-topics --input reviews.seq --num-topics 100 --output-state state-1000.gz\n\tbin/mallet train-topics --input reviews.seq --num-topics 100 --input-state state-1000.gz --output-state state-1100.gz --num-iterations 100\n\tbin/mallet train-topics --input reviews.seq --num-topics 100 --input-state state-1100.gz --output-state state-1200.gz --num-iterations 100\n\nNow we have three saved sampling states. These files should be gzipped. The format for these files is one row per token, with the following fields separated by a single space:\ndocument order, an unused field, token position within the document, numeric word id, word string, topic id. Lines beginning with \"#\" are ignored.\nFor example, \n\n\t0 NA 0 0 lately 8\n\t0 NA 1 1 feeling 20\n\t0 NA 2 2 homesick 94\n\t0 NA 3 3 asian 52\n\t0 NA 4 4 hitting 71\n\nNow we can run a PPC. This will select topic 56, report scores for the top 20 words, and run 20 replications.\n\n\tpython mutual_info_ppc.py -t 56 -w 20 -r 20 state-*.gz > by-doc-56.tsv\n\nUsing the PPC file, we can now generate a PDF for the top words. This command will look for a file called `topic98",
    "url": "https://github.com/mimno/MalletPPC",
    "last_updated": "2019-09-01T19:03:42+00:00"
  },
  {
    "full_name": "justingrimmer/WUSTL",
    "name": "WUSTL",
    "description": "Text as Data Material for WashU Course",
    "language": "TeX",
    "topics": [],
    "readme": "",
    "url": "https://github.com/justingrimmer/WUSTL",
    "last_updated": "2024-12-04T08:28:19+00:00"
  },
  {
    "full_name": "tldr-pages/tldr",
    "name": "tldr",
    "description": "📚 Collaborative cheatsheets for console commands",
    "language": "Markdown",
    "topics": [
      "shell",
      "man-page",
      "tldr",
      "manpages",
      "documentation",
      "terminal",
      "command-line",
      "console",
      "examples",
      "help",
      "manual",
      "hacktoberfest",
      "cheatsheet",
      "cheatsheets",
      "android",
      "bsd",
      "linux",
      "macos",
      "osx",
      "windows"
    ],
    "readme": "<!-- markdownlint-disable MD041 -->\n\n<div align=\"center\">\n  <h1><a href=\"https://tldr.sh/\"><img alt=\"tldr-pages\" src=\"images/banner.png\" width=600/></a></h1>\n\n[![Build status][github-actions-image]][github-actions-url]\n[![Matrix chat][matrix-image]][matrix-url]\n[![Merged PRs][prs-merged-image]][prs-merged-url]\n[![GitHub contributors][contributors-image]][contributors-url]\n[![license][license-image]][license-url]\n[![Mastodon][mastodon-image]][mastodon-url]\n\n[github-actions-url]: https://github.com/tldr-pages/tldr/actions\n[github-actions-image]: https://img.shields.io/github/actions/workflow/status/tldr-pages/tldr/ci.yml?branch=main&label=Build\n[matrix-url]: https://matrix.to/#/#tldr-pages:matrix.org\n[matrix-image]: https://img.shields.io/matrix/tldr-pages:matrix.org?label=Chat+on+Matrix\n[prs-merged-url]: https://github.com/tldr-pages/tldr/pulls?q=is:pr+is:merged\n[prs-merged-image]: https://img.shields.io/github/issues-pr-closed-raw/tldr-pages/tldr.svg?label=Merged+PRs&color=green\n[contributors-url]: https://github.com/tldr-pages/tldr/graphs/contributors\n[contributors-image]: https://img.shields.io/github/contributors-anon/tldr-pages/tldr.svg?label=Contributors\n[license-url]: https://github.com/tldr-pages/tldr/blob/main/LICENSE.md\n[license-image]: https://img.shields.io/badge/license-CC_BY_4.0-blue.svg?label=License\n[mastodon-url]: https://fosstodon.org/@tldr_pages\n[mastodon-image]: https://img.shields.io/badge/Mastodon-6364FF?logo=mastodon&logoColor=fff\n</div>\n\n## What is tldr-pages?\n\nThe **tldr-pages** project is a collection of community-maintained help pages\nfor command-line tools, that aims to be a simpler, more approachable complement\nto traditional [man pages](https://en.wikipedia.org/wiki/Man_page).\n\nMaybe you're new to the command-line world. Perhaps you're just a little rusty or can't always recall the arguments for commands like `lsof`, or `tar`?\n\nIt certainly doesn't help that, in the past, the first option explained in `man tar` was:\n\n```console\n$ man tar",
    "url": "https://github.com/tldr-pages/tldr",
    "last_updated": "2025-09-02T09:04:07+00:00"
  },
  {
    "full_name": "r-lib/rcmdcheck",
    "name": "rcmdcheck",
    "description": "Run R CMD check from R and collect the results",
    "language": "R",
    "topics": [
      "r"
    ],
    "readme": "Run R CMD check from R and Capture Results\n================\n\n- [rcmdcheck](#rcmdcheck)\n  - [Installation](#installation)\n  - [Usage](#usage)\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# rcmdcheck\n\n> Run R CMD check from R and Capture Results\n\n<!-- badges: start -->\n\n[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![](https://www.r-pkg.org/badges/version/rcmdcheck)](https://www.r-pkg.org/pkg/rcmdcheck)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/rcmdcheck)](https://www.r-pkg.org/pkg/rcmdcheck)\n[![R-CMD-check](https://github.com/r-lib/rcmdcheck/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/rcmdcheck/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/rcmdcheck/graph/badge.svg)](https://app.codecov.io/gh/r-lib/rcmdcheck)\n<!-- badges: end -->\n\nRun R CMD check from R programmatically and capture the results of the\nindividual checks.\n\n## Installation\n\nInstall the released version from CRAN\n\n``` r\ninstall.packages(\"rcmdcheck\")\n```\n\nOr install the development version from GitHub:\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"r-lib/rcmdcheck\")\n```\n\n## Usage\n\n``` r\nlibrary(rcmdcheck)\nrcmdcheck(\"path/to/R/package\")\n```\n\nCall `rcmdcheck()` on a source R package `.tar.gz` file, or on a folder\ncontaining your R package. Supply `quiet = FALSE` if you want to omit\nthe output. The result of the check is returned, in a list with elements\n`errors`, `warnings`, and `notes`. Each element is a character vector,\nand one element of the character vectors is a single failure.\n\n<img width=\"1000\" src=\"https://cdn.jsdelivr.net/gh/r-lib/rcmdcheck@main/tools/rcmdcheck.svg\" alt=\"animated screenshot of a terminal window demonstrating example usage of the rcmdcheck function.\">\n\n### Programmatic usage\n\n`rcmdcheck()` returns an `rcmdcheck` object, which you can query and\nmanipulate.\n\n``` r\nlibrary(rcmdcheck)\nchk <",
    "url": "https://github.com/r-lib/rcmdcheck",
    "last_updated": "2025-06-12T15:52:01+00:00"
  },
  {
    "full_name": "rcc-uchicago/influenceR",
    "name": "influenceR",
    "description": " Software tools to quantify structural importance of nodes in a network",
    "language": "C",
    "topics": [],
    "readme": "# influenceR\n\ninfluenceR: Software tools to quantify structural importance of nodes in a network. Algorithms include Betweenness Centrality, Bridging, Constraint Index, Effective Network Size, and Key Players. Currently, algorithms are implemented only for undirected graphs; work on directed graphs is in progress.\n\n#### Installation\nThe current release version can be found on CRAN and installed with:\n\ninstall.packages(\"influenceR\")\n\n#### Citation\nIf using this package, please cite \nJacobs SD, Khanna AS (2015). *influenceR: Software tools to quantify structural importance of nodes in a network* R Package Version 1.0.0.2. URL https://github.com/rcc-uchicago/influenceR.\n\n#### Funding\nDevelopment of this software package was supported by NIH grant R01 DA033875.\n",
    "url": "https://github.com/rcc-uchicago/influenceR",
    "last_updated": "2023-05-17T16:19:22+00:00"
  },
  {
    "full_name": "XiaoHuiHui233/ETHHelper",
    "name": "ETHHelper",
    "description": "Asynchronous Geth node connection encapsulation based on httpx, websockets and web3.py. Geth node and Ethereum type extension based on pydantic.",
    "language": "Python",
    "topics": [
      "asyncio",
      "ethereum",
      "geth",
      "pydantic"
    ],
    "readme": "# ETHHelper\n\n[![Build Status](https://img.shields.io/github/actions/workflow/status/XiaoHuiHui233/ETHHelper/publish.yml)](https://github.com/XiaoHuiHui233/ETHHelper/actions)\n[![Documentation Status](https://readthedocs.org/projects/ethhelper/badge/?version=latest)](https://ethhelper.readthedocs.io/en/latest/?badge=latest)\n![Python Version](https://img.shields.io/pypi/pyversions/ethhelper)\n![Wheel Status](https://img.shields.io/pypi/wheel/ethhelper)\n[![Latest Version](https://img.shields.io/github/v/release/XiaoHuiHui233/ETHHelper)](https://github.com/XiaoHuiHui233/ETHHelper/releases)\n[![License](https://img.shields.io/github/license/XiaoHuiHui233/ETHHelper)](https://github.com/XiaoHuiHui233/ETHHelper/blob/main/LICENSE)\n\nAsynchronous Geth node connection encapsulation based on httpx, websockets and web3.py. Geth node and Ethereum type extension based on pydantic.\n\nQuickstart see [this](https://ethhelper.readthedocs.io/en/latest/quickstart.html).\n\n[中文](docs/README_cn.md) | English\n\n## Usage\n\n### pypi\n\nIf you prefer to use pypi to install this package, you can just run the following command:\n\n```bash\npip install ethhelper\n```\n\n### git\n\nThe project is managed by poetry. If you prefer to use git to install this package, you can use poetry to directly add a reference to the project's build package through git.\n\nThe command is as follow:\n\n```bash\npoetry add git+ssh://git@github.com:XiaoHuiHui233/ETHHelper.git\n```\n\n## Build\n\nYou can use poetry's tools to generate a build of this project (pure Python).\n\nThe command is as follow:\n\n```bash\npoetry build\n```\n\n## Author and License\n\nETHHelper was written by [XiaoHuiHui233](https://github.com/XiaoHuiHui233/), licensed under the Apache 2.0.\n",
    "url": "https://github.com/XiaoHuiHui233/ETHHelper",
    "last_updated": "2025-01-15T15:28:58+00:00"
  },
  {
    "full_name": "b-k/tweets-are-not-news",
    "name": "tweets-are-not-news",
    "description": "Remove headlines from news sites with titles including words like \"lambasts\", \"blasts\", \"jabs\"",
    "language": "JavaScript",
    "topics": [],
    "readme": "# tweets-are-not-news\nRemove headlines from news sites with titles including words like \"lambasts\", \"blasts\", \"jabs\"\n\nThis plugin checks headlines on the NY Times and Washington Post for a list of words including \"tweets\",\n\"jabs back at\", and so on---headlines about a tweet or one person insulting another. Those headlines disappear.\nDepending on positioning, this may leave a blank box on the page, or there may be no trace that an article was there.\n\nInstallation: the script uses the Greasemonkey add-on for Firefox, which is available from Firefox's usual add-on collection.\nAfter installing Greasemonkey, select 'new user script' from the Monkey's menu, paste in the script here, and click save.\n\nTweaking: the top of the script has a list of stop words that cause an article to be excluded, which is easy to modify if you\nfind it to be too restrictive or permissive.\n\nThere is also a self-explanatory `the_opinion_section_is_also_not_news` variable, which is by default set to `false`,\nbut also hides the opinon/perspective sections on the front page if set to `true`.\n",
    "url": "https://github.com/b-k/tweets-are-not-news",
    "last_updated": "2025-02-26T20:26:41+00:00"
  },
  {
    "full_name": "gojiplus/lta",
    "name": "lta",
    "description": "Group zipcodes to Build Local Television Areas",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## Local Television Area\n\n\"A media market, broadcast market, media region, designated market area (DMA), television market area, or simply market is a region where the population can receive the same (or similar) television and radio station offerings.\" --- [Wikipedia](https://en.wikipedia.org/wiki/Media_market)\n\nIn the Internet era, with an ever increasing proportion of households that have 'cut-the-cord,' media markets mean less than ever. But local television news continues to draw older people. And local network affiliates are still an important part of media 'diets' of older Americans. \n\nUsing [FCC DTV Maps](https://www.fcc.gov/media/engineering/dtvmaps), we first create a comprehensive database of all local channels per zip code. Then, we cluster zip codes two different ways: 1. using k-means based on overlap between tv stations, 2. using manhattan distance.\n\n### Data\n\nWe use the list of [zip codes](data/us_zipcodes.csv) and iterate over [FCC DTV Maps](https://www.fcc.gov/media/engineering/dtvmaps) and produce [a CSV](output/stations.csv) with the following columns:\n\n(each zipcode has multiple rows --- one row per channel)\n\n`zipcode, callsign, network, channel_number, band, ia, signal_strength (strong/moderate, weak, no signal), facility_id, city_of_license, rf_channel, rx_strength, tower_distance, repacked_channel, repacking_dates`\n\n![example](example.png)\n\nFor ~ 2,000 zip codes, the search came back empty. Here's the [log file](output/log.zip).\n\n#### Scraping Scripts\n\n1. [Scrape](scripts/01_get_data.py)\n2. [Generate CSV from Logs](scripts/02_generate_csv_from_logs.py)\n3. [Checks](scripts/03_generate_metatdata.py)\n\n### Clustering\n\n1. [group_zips](scripts/04_manhattan_distance.ipynb): clusters zip codes based on overlap between list of TV stations (with certain signal strength) and appends the grouping variable. We use deterministic (within a certain manhattan distance) multi-assignment (each zipcode can be part of multiple clusters) clustering. We run it for ",
    "url": "https://github.com/gojiplus/lta",
    "last_updated": "2025-04-16T22:15:19+00:00"
  },
  {
    "full_name": "norvig/pytudes",
    "name": "pytudes",
    "description": "Python programs, usually short, of considerable difficulty, to perfect particular skills.",
    "language": "Jupyter Notebook",
    "topics": [
      "python",
      "python-3",
      "programming",
      "practice",
      "demonstrate-skills"
    ],
    "readme": "\n<div align=\"right\" style=\"text-align:right\"><i>Peter Norvig\n<br><a href=\"https://github.com/norvig/pytudes/blob/main/LICENSE\">MIT License</a><br>2015-2022</i></div>\n\n# pytudes\n\n\"An ***étude*** (a French word meaning *study*) is an instrumental musical composition, usually short, of considerable difficulty, \nand designed to provide practice material for perfecting a particular musical skill.\" — [*Wikipedia*](https://en.wikipedia.org/wiki/%C3%89tude)\n\nThis project contains ***pytudes***—Python programs, usually short, for perfecting particular programming skills.\n\n# Who is this for?\n\nTo continue the musical analogy, some people think of programming like [Spotify](http://spotify.com): they want to know how to install the app, find a good playlist, and hit the \"play\" button; after that they don't want to think about it. There are plenty of other tutorials that will tell you how to do the equivalent of that for various programming tasks—this one won't help. But if you think of programming like playing the piano—a craft that can take [years](https://norvig.com/21-days.html) to perfect—then I hope this collection can help.\n\n\n# Index of Jupyter (IPython) Notebooks\n\nFor each notebook you can hover on the title to see a description, or click the title to view on github, or click one of the letters in the left column to launch the notebook on \n[**C**olab](https://colab.research.google.com),\n[**D**eepnote](https://deepnote.com),\n[**M**ybinder](https://mybinder.org),\n[**S**agemaker](https://studiolab.sagemaker.aws/), or\n[**N**BViewer](https://nbviewer.jupyter.org/).\n\n\n\n|Run|Year|New|\n|---|---|---|\n| [C](https://colab.research.google.com/github/norvig/pytudes/blob/main/ipynb/Advent-2024.ipynb) [D](https://beta.deepnote.org/launch?template=python_3.6&url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmain%2Fipynb%2FAdvent-2024.ipynb) [M](https://mybinder.org/v2/gh/norvig/pytudes/main?filepath=ipynb%2FAdvent-2024.ipynb) [N](https://nbviewer.jupyter.org/github/norvig/pytudes/",
    "url": "https://github.com/norvig/pytudes",
    "last_updated": "2025-09-01T23:29:29+00:00"
  },
  {
    "full_name": "llSourcell/How-to-Predict-Stock-Prices-Easily-Demo",
    "name": "How-to-Predict-Stock-Prices-Easily-Demo",
    "description": "How to Predict Stock Prices Easily - Intro to Deep Learning #7 by Siraj Raval on Youtube",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# How-to-Predict-Stock-Prices-Easily-Demo\nHow to Predict Stock Prices Easily - Intro to Deep Learning #7 by Siraj Raval on Youtube\n\n##Overview\n\nThis is the code for [this](https://youtu.be/ftMq5ps503w) video on Youtube by Siraj Raval part of the Udacity Deep Learning nanodegree. We use an [LSTM neural network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) to predict the closing price of the S&P 500 using a dataset of past prices.\n\n##Dependencies\n\n* keras\n* tensorflow\n\nInstall Keras from [here](https://keras.io/) and Tensorflow from [here](https://www.tensorflow.org/versions/r0.12/get_started/os_setup). \n\n##Usage\n\nRun this using [jupyter notebook](http://jupyter.readthedocs.io/en/latest/install.html). Just type `jupyter notebook` in the main directory and the code will pop up in a browser window. \n\n#Coding Challenge - Due Date, Thursday, March 2nd 2017 at 12 PM PST\n\nUse the price history AND two other metrics of your choice to predict the price of GOOGL stock with an LSTM network. You can find the CSV [here](https://www.google.com/finance/historical?q=NASDAQ%3AGOOGL&ei=Xu6wWKnDAcS1jAGX6a-ACg). Metrics could be sentiment analysis from Twitter of what people have said about Google, dividends, etc. \n\n##Credits\n\nCredits go to [jaungiers](https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction). I've merely created a wrapper to get people started.\n\n",
    "url": "https://github.com/llSourcell/How-to-Predict-Stock-Prices-Easily-Demo",
    "last_updated": "2025-08-25T22:35:50+00:00"
  },
  {
    "full_name": "soodoku/partisan_head",
    "name": "partisan_head",
    "description": "",
    "language": "TeX",
    "topics": [],
    "readme": "# Replicating \"The Partisans in our Heads\" (Ahler and Sood, 2015)\n\n## Summary\n\nThe folder contains data and scripts needed to replicate the results in \"The Partisans in our Heads\" by Doug Ahler and Gaurav Sood. For more information, please contact Doug Ahler (doug.ahler@gmail.com).\n\n## Replication Materials\n\nData:\n\n* mturk_guess.dta --- data from MTurk Study\n\nCodebook:\n* codebook.md\n\nScripts:\n* 01_sociodem.R --- Code for producing table 1 that compares socio-demographics and PID of the Mturk sample to more representative samples.\n* 02_guesses.R  --- Code for producing table 2 that gives the percentage of Democrats and Republicans guessing each category.\n",
    "url": "https://github.com/soodoku/partisan_head",
    "last_updated": "2023-06-17T23:54:34+00:00"
  },
  {
    "full_name": "lmfit/lmfit-py",
    "name": "lmfit-py",
    "description": "Non-Linear Least Squares Minimization, with flexible Parameter settings, based on scipy.optimize, and with many additional classes and methods for curve fitting. ",
    "language": "Python",
    "topics": [
      "python",
      "scipy",
      "curve-fitting",
      "least-squares"
    ],
    "readme": "LMfit-py\n========\n\n.. image:: https://dev.azure.com/lmfit/lmfit-py/_apis/build/status/lmfit.lmfit-py?branchName=master\n    :target: https://dev.azure.com/lmfit/lmfit-py/_build/latest?definitionId=1&branchName=master\n\n.. image:: https://codecov.io/gh/lmfit/lmfit-py/branch/master/graph/badge.svg\n  :target: https://codecov.io/gh/lmfit/lmfit-py\n\n.. image:: https://img.shields.io/pypi/v/lmfit.svg\n   :target: https://pypi.org/project/lmfit\n\n.. image:: https://img.shields.io/pypi/dm/lmfit.svg\n   :target: https://pypi.org/project/lmfit\n\n.. image:: https://img.shields.io/badge/docs-read-brightgreen\n   :target: https://lmfit.github.io/lmfit-py/\n\n.. image:: https://zenodo.org/badge/4185/lmfit/lmfit-py.svg\n   :target: https://doi.org/10.5281/zenodo.598352\n\n.. _LMfit google mailing list: https://groups.google.com/group/lmfit-py\n.. _Github Discussions: https://github.com/lmfit/lmfit-py/discussions\n.. _Github Issues: https://github.com/lmfit/lmfit-py/issues\n\n\n..\n   Note: the Zenodo target should be\n   https://zenodo.org/badge/latestdoi/4185/lmfit/lmfit-py\n   but see https://github.com/lmfit/lmfit-py/discussions/862\n\n\nOverview\n---------\n\nThe ``lmfit`` Python library provides tools for non-linear least-squares\nminimization and curve fitting.  The goal is to make these optimization\nalgorithms more flexible, more comprehensible, and easier to use, with the\nkey feature of casting variables in minimization and fitting routines as named\nparameters that can have many attributes beside just a current value.\n\nLMfit is a pure Python package -- built on top of Scipy and Numpy -- and is easy to\ninstall with ``pip install lmfit``.\n\nFor questions, comments, and suggestions, please use the `LMfit google mailing\nlist`_ or `Github discussions`_.  For software issues and bugs, use `Github\nIssues`_, but please read `Contributing.md <.github/CONTRIBUTING.md>`_ before\ncreating an Issue.\n\n\nParameters and Minimization\n------------------------------\n\nLMfit provides optimization routines similar to (and ba",
    "url": "https://github.com/lmfit/lmfit-py",
    "last_updated": "2025-09-01T19:13:44+00:00"
  },
  {
    "full_name": "LukasKrocek/single-consumer-queue-python",
    "name": "single-consumer-queue-python",
    "description": "A Python library that provides a high-performance single consumer queue implementation for asyncio applications",
    "language": "Python",
    "topics": [],
    "readme": "# Single Consumer Queue\nSingle Consumer Queue is a Python library that provides an alternative to the standard asyncio.Queue for single consumer scenarios. It consists of two classes: SingleConsumerQueue and SingleConsumerPriorityQueue. Both classes implement the AbstractSingleConsumerQueue abstract base class, which provides the basic functionality of a single consumer queue.\n\n## Why Single Consumer Queue?\nIn some scenarios, the standard asyncio.Queue can be slower than necessary. This is because asyncio.Queue is designed to be used with multiple consumers, which means that it has additional overhead to handle multiple concurrent accesses. If you only have one consumer, you can use SingleConsumerQueue or SingleConsumerPriorityQueue to reduce this overhead and improve performance.\n\n## How to use Single Consumer Queue\nInstallation\nYou can install Single Consumer Queue using pip:\n\n```pip install single-consumer-queue```\n\n## Usage\nHere's an example of how to use SingleConsumerQueue:\n\n```\nasync def consumer(queue: SingleConsumerQueue | SingleConsumerPriorityQueue):\n    async for item in queue.start_consuming():\n        print(item)\n```\n\nSingleConsumerQueue raises exception if you try to add multiple consumers:\n\n```\nasync def consumer(queue: SingleConsumerQueue | SingleConsumerPriorityQueue):\n    async for item in queue.start_consuming():\n        print(item)\n\nqueue = SingleConsumerQueue()\nasyncio.create_task(consumer(queue))\nawait asyncio sleep(0.1)\nawait queue.get()  # raises runtime error\n```\n\nLock is checked and acquired when consumer starts and every time that get is awaited.\n\nGet has considerate overhead because it has to use lock every time, so it is recommended to use `start_consuming` generator when you want to consume items in the loop.\n",
    "url": "https://github.com/LukasKrocek/single-consumer-queue-python",
    "last_updated": "2025-01-15T15:29:04+00:00"
  },
  {
    "full_name": "ObisoftDev/pyobigram",
    "name": "pyobigram",
    "description": "Python 3 library for telegram bots",
    "language": "Python",
    "topics": [],
    "readme": "# Pyobigram\nPython 3 library for making fast telegram bots\n# Description\nPyobigram is a fast library for telegram bots , mtproto update, author : Obisoft\n#INSTALL \n```\npip install pyobigram\n```\n# Features v2.1.0\n- Suport Handle Messages (all_messages,cmd_message,inline_message,callback_message)\n- Suport Send & Edit & Delete (Messages)\n- Suport Forward Messages\n- Suport Upload & Download Files MtProto (max 2gb)\n- Suport AnswerInline (InlineArticle,InlineDocument)\n- Suport Send Reply Narkup Buttons\n# Quickstart & Installation\nPyobigram requires an installation of Python 3.6 or greater, as well as pip. (Pip is typically bundled with Python \nTo install from the source with pip:\n```\npip install https://github.com/ObisoftDev/pyobigram/archive/master.zip\n```\n- Using pyobigram in a Python script\n```\nfrom pyobigram.client import ObigramClient\n\nbot_token = 'BOT_TOKEN'\napi_id = 'API_ID'\napi_hash = 'API_HASH'\n\ndef message_handle(update,bot:ObigramClient):\n\t#TODO message handle functions\n\tpass\n\nif __name__ == '__main__':\n    bot = ObigramClient(bot_token,api_id=api_id,api_hash=api_hash)\n    bot.onMessage(message_handle)\n    print('bot is runing!')\n    bot.run()\n```\n",
    "url": "https://github.com/ObisoftDev/pyobigram",
    "last_updated": "2025-01-23T08:57:26+00:00"
  },
  {
    "full_name": "paulgp/applied-methods-phd",
    "name": "applied-methods-phd",
    "description": "Repo for Yale Applied Empirical Methods PHD Course",
    "language": "HTML",
    "topics": [],
    "readme": "\n\n\n# Applied Empirical Methods \n## Course Pages\n1. https://github.com/paulgp/applied-methods-phd\n2. https://yale.instructure.com/courses/64286 [Yale students only]\n\nCourse videos are available on [Youtube](https://www.youtube.com/playlist?list=PLWWcL1M3lLlojLTSVf2gGYQ_9TlPyPbiJ)\n\n## Course Description\nThis course is primarily designed for graduate students interested in econometric methods used in empirical research. The goal of this class is to provide an overview of different empirical methods, with an emphasis on practical implementation.  I will provide a set of lecture slides and notes. There are additional background papers that are largely optional.\n\nMore generally, this is a course where I focus on providing my understanding and intuition of empirical methods, as they are used by practicioners. This means that this is not a course where we will spend a lot of time on the formal details (beyond what is necessary), but instead focus on the intuitive framework that guides these papers. I'll also do my best to communciate how any of these topics fit together.\n\nThis is a course very much focused on communication and artisanship. By the end of the term, my hope is for three things:\n\n1. You will have been exposed to a wide range of empirical methods, and have at least a passing familiarity with their pros and cons. Moreover, you will know where to go look if you decide to use these methods. \n2. Much of the terminology and jargon that we use in econometric methods will be less intimidating to you. When someone says \"I use semiparametric inference,\" now instead of intimidate you, it will bother you that they are not using clearer language.\n3. You will approach research papers with the desire to disentangle the underlying framework and \"experiment\" that drives their causal inferences.\n\n## Assignments\nThere will be problem sets most weeks (nine in total). These will involve both theoretical calculations and computer exercises in which you will be asked to analyze data ",
    "url": "https://github.com/paulgp/applied-methods-phd",
    "last_updated": "2025-09-02T09:34:12+00:00"
  },
  {
    "full_name": "giampaolo/psutil",
    "name": "psutil",
    "description": "Cross-platform lib for process and system monitoring in Python",
    "language": "Python",
    "topics": [
      "python",
      "monitoring",
      "ps",
      "top",
      "cpu",
      "memory",
      "freebsd",
      "osx",
      "windows",
      "netbsd",
      "openbsd",
      "linux",
      "disk",
      "sensors",
      "system-monitoring",
      "psutil"
    ],
    "readme": "|  |downloads| |stars| |forks| |contributors| |coverage|\n|  |version| |py-versions| |packages| |license|\n|  |github-actions-wheels|  |github-actions-bsd| |doc| |twitter| |tidelift|\n\n.. |downloads| image:: https://img.shields.io/pypi/dm/psutil.svg\n    :target: https://pepy.tech/project/psutil\n    :alt: Downloads\n\n.. |stars| image:: https://img.shields.io/github/stars/giampaolo/psutil.svg\n    :target: https://github.com/giampaolo/psutil/stargazers\n    :alt: Github stars\n\n.. |forks| image:: https://img.shields.io/github/forks/giampaolo/psutil.svg\n    :target: https://github.com/giampaolo/psutil/network/members\n    :alt: Github forks\n\n.. |contributors| image:: https://img.shields.io/github/contributors/giampaolo/psutil.svg\n    :target: https://github.com/giampaolo/psutil/graphs/contributors\n    :alt: Contributors\n\n.. |github-actions-wheels| image:: https://img.shields.io/github/actions/workflow/status/giampaolo/psutil/.github/workflows/build.yml.svg?label=Linux%2C%20macOS%2C%20Windows\n    :target: https://github.com/giampaolo/psutil/actions?query=workflow%3Abuild\n    :alt: Linux, macOS, Windows\n\n.. |github-actions-bsd| image:: https://img.shields.io/github/actions/workflow/status/giampaolo/psutil/.github/workflows/bsd.yml.svg?label=FreeBSD,%20NetBSD,%20OpenBSD\n    :target: https://github.com/giampaolo/psutil/actions?query=workflow%3Absd-tests\n    :alt: FreeBSD, NetBSD, OpenBSD\n\n.. |coverage| image:: https://coveralls.io/repos/github/giampaolo/psutil/badge.svg?branch=master\n    :target: https://coveralls.io/github/giampaolo/psutil?branch=master\n    :alt: Test coverage (coverall.io)\n\n.. |doc| image:: https://readthedocs.org/projects/psutil/badge/?version=latest\n    :target: https://psutil.readthedocs.io/en/latest/\n    :alt: Documentation Status\n\n.. |version| image:: https://img.shields.io/pypi/v/psutil.svg?label=pypi\n    :target: https://pypi.org/project/psutil\n    :alt: Latest version\n\n.. |py-versions| image:: https://img.shields.io/pypi/pyversions/psutil.svg\n    :alt: S",
    "url": "https://github.com/giampaolo/psutil",
    "last_updated": "2025-09-02T08:11:09+00:00"
  },
  {
    "full_name": "soodoku/selexp",
    "name": "selexp",
    "description": "Discretionary Exposure to Political Information",
    "language": "R",
    "topics": [
      "selective-exposure",
      "partisan-bias",
      "echo-chamber"
    ],
    "readme": "## Don't Expose Yourself: Discretionary Exposure to Political Information\n\nReplication materials for the chapter 'Don't Expose Yourself: Discretionary Exposure to Political Information'\n\n### Data\n\n* [ANES CDF (.7z)](data/anes_timeseries_cdf_stata12.7z)\n* [NAES 2008 Online (.7z)](data/naes08_online_all_waves_data_full.7z) and [Weights](data/naes08_online_weights.dta)\n* [YG Pilot (Only Relevant Cols)](data/yg_pilot.csv)\n* [YG Study 1 (Only Relevant Cols)](data/yg_study1.csv)\n\n### Scripts\n\n* [Figure 1: Knowledge Gap Over Time and Appendix Figure A1: Aging Curves](scripts/01_fig1_anes_knowledge_gap_over_time_plus_appenda_aging_curves.R)\n* [Figure 2: Perceived Bias of News Organizations by Party Identification](scripts/02_fig2_yg_pilot_media_bias.R)\n* [Figure 3: Frequency of Selecting News by Slant of Outlet](scripts/03_fig3_selectivity_by_hard_soft.R)\n* [Figure 4: Partisanship of Audiences of Entertainment Shows](scripts/04_fig4_partisan_entertainment.R)\n\n### Figures\n\n* [Figures](figs/)\n\n### Manuscript\n\n*[Manuscript (pdf)](http://gsood.com/research/papers/selexp.pdf)\n\n### Suggested Citation\n\nSood, Gaurav; Lelkes, Yphtach. (DATE). “Selective Exposure to Partisan Media.” In Oxford Research Encyclopedia of Politics, Ed. William Thompson. New York: Oxford University Press. Retrieved (DATE), from http://politics.oxfordre.com/view/10.1093/acrefore/9780190228637.001.0001/acrefore-9780190228637-e-39\n",
    "url": "https://github.com/soodoku/selexp",
    "last_updated": "2020-03-01T21:46:35+00:00"
  },
  {
    "full_name": "stewid/xapr",
    "name": "xapr",
    "description": "R bindings to the Xapian search engine",
    "language": "R",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/stewid/xapr.png)](https://travis-ci.org/stewid/xapr)\n\n# Introduction\n\n`xapr` is an R package that provides an interface to the\n[Xapian](http://xapian.org/) search engine from R, allowing both\nindexing and retrievel operations. A great introduction to\n[Xapian](http://xapian.org/) is the\n[Getting Started with Xapian](http://getting-started-with-xapian.readthedocs.org/en/latest/).\n\n## Indexing\n\nIndex the content of a `data.frame` to documents with the Xapian\nsearch engine A `document` is the data returned from a search.\n\nThe index plan is specified symbolically. An index plan has the form\n`data ~ terms` where `data` is the blob of data returned from a search\nand the `terms` are the basis for a search in Xapian. A first order\nterm index the text in the column as free text. A specification of the\nform `prefix:term` indicates that the text in `term` should be\nindexed with the prefix `prefix`.\n\nThe prefix is a short string at the beginning of the term to indicate\nwhich field the term indexes. Valid prefixes are: 'A' ,'D', 'E', 'G',\n'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n'X', 'Y' and 'Z'. See http://xapian.org/docs/omega/termprefixes for a\nlist of conventional prefixes.\n\nThe specification `prefix*term` is the same as `term +\nprefix:term`. The prefix `X` will create a user defined prefix by\nappending the uppercase `term` to `X`. The prefix `Q` will use data\nin the `term` column as a unique identifier for the document. `NA`\nvalues in indexed columns are skipped.\n\nNo response e.g. `~ term + prefix:term` writes the row number as\ndata to the document.\n\nThe specification `~X*.` creates prefix terms with all columns plus\nfree text.\n\nIf the response contains one or more columns, e.g. `col_1 + col_2 ~\nX*.` the response is first converted to `JSON`. A compact form to\nconvert all fields to `JSON` is to use `. ~ terms`. It is also\npossible to drop response fields e.g. `. - col_1 - col_2 ~ X*.` to\ninclude all fields i",
    "url": "https://github.com/stewid/xapr",
    "last_updated": "2023-01-27T23:21:47+00:00"
  },
  {
    "full_name": "mimno/TidyMallet",
    "name": "TidyMallet",
    "description": "A tidy-native LDA implementation in Rcpp",
    "language": "C++",
    "topics": [],
    "readme": "# TidyMallet\nA tidy-native LDA implementation in Rcpp\n\nThe fundamental data structure for Gibbs sampling in LDA is a sequence of tokens, each with a word ID and a topic variable, grouped into documents. It's essentially a tidy text format. So why not run the algorithm directly on the tidy data structure?\n\nI use Rcpp to create a function that implements one sweep of a Gibbs sampler, and that can be used by dplyr as long as tokens are grouped into documents:\n\n    tokens_df %>% mutate(word_id = word_id, topic = sampleTopics(word_id, topic, topic_model))\n\nThe key to efficient sampling is being able to get the current topic counts for the word type at the current token. The best way I could figure out to do this is to have a data frame with one column per word type and one row per topic. It's cumbersome and weird, but I'm not good enough at Rcpp to find a better way. Suggestions are welcome!\n\nRight now this is a very straightforward implementation, with none of the tricks that I used in Mallet. For small collections it's suitably fast, but there are some simple improvements that might make a difference.\n",
    "url": "https://github.com/mimno/TidyMallet",
    "last_updated": "2019-06-21T16:05:45+00:00"
  },
  {
    "full_name": "Hipo/university-domains-list",
    "name": "university-domains-list",
    "description": "University Domains and Names Data List & API",
    "language": "Python",
    "topics": [],
    "readme": "# University Domains and Names Data List & API\n\nDo you need a list of universities and their domain names? You found it!\n\nThis package includes a JSON file that contains domains, names and countries of most of the universities of the world.\n\nExample usecases: \n- You can create a validation script that checks the email domain.\n- You can automatically generate a user's country and university by looking at their emails.\n\nYou can use this data source in three ways:\n\n- Use the JSON file as your data source and do whatever you like with your favourite programming language.\n- Use free hosted-API.\n- Use the tiny Python app to serve a fast API that you can query data.\n\n\n### 1 - Using the Data Source\n\nThe whole data source is located in the `world_universities_and_domains.json` file. It is just a list of dictionaries in the following format:\n\n    [\n    \t...\n    \t{\n    \t    \"alpha_two_code\": \"TR\",\n    \t    \"country\": \"Turkey\",\n    \t    \"state-province\": null,\n    \t    \"domains\": [\n    \t        \"sabanciuniv.edu\",\n    \t        \"sabanciuniv.edu.tr\"\n    \t    ],\n    \t    \"name\": \"Sabanci University\",\n    \t    \"web_pages\": [\n    \t        \"http://www.sabanciuniv.edu/\",\n    \t        \"http://www.sabanciuniv.edu.tr/\"\n    \t    ],\n    \t},\n    \t...\n    ]\n\nIf you want a smaller final payload and only need a subset of countries, run\n\n```bash\nfilter.py $country1 [Optional: $country2]\n```\n\nfrom the root directory to return\n\n```\nfiltered_world_universities_and_domains.json\n```\n\nNOTE: Some universities use a format like `[user]@[department].[domain]`, but this list only contains the `[domain]` portion.\nFor example, an email address might be `[student]@cs.usc.edu`, and this list will contain 'usc.edu', the domain for the\nUniversity of Southern California. Take this into consideration if using this list for email address validation.\n\n### 2 - Using The Hosted API\n\nThis is the easiest method if you're making a small project or just want to discover the data without any hassle.\nIt is sponsored by [Hi",
    "url": "https://github.com/Hipo/university-domains-list",
    "last_updated": "2025-09-02T08:14:39+00:00"
  },
  {
    "full_name": "jdoughertyii/PyVCF",
    "name": "PyVCF",
    "description": "A Variant Call Format reader for Python.",
    "language": "Python",
    "topics": [],
    "readme": "**Development repository for PyVCF is at https://github.com/jamescasbon/PyVCF **\n\nA VCFv4.0 parser for Python.\n\nThe intent of this module is to mimic the ``csv`` module in the Python stdlib,\nas opposed to more flexible serialization formats like JSON or YAML.  ``vcf``\nwill attempt to parse the content of each record based on the data types\nspecified in the meta-information lines --  specifically the ##INFO and\n##FORMAT lines.  If these lines are missing or incomplete, it will check\nagainst the reserved types mentioned in the spec.  Failing that, it will just\nreturn strings.\n\nThere is currently one piece of interface: ``VCFReader``.  It takes a file-like\nobject and acts as a reader::\n\n    >>> import vcf\n    >>> vcf_reader = vcf.VCFReader(open('example.vcf', 'rb'))\n    >>> for record in vcf_reader:\n    ...     print record\n    Record(CHROM='20', POS=14370, ID='rs6054257', REF='G', ALT=['A'], QUAL=29,\n    FILTER='PASS', INFO={'H2': True, 'NS': 3, 'DB': True, 'DP': 14, 'AF': [0.5]\n    }, FORMAT='GT:GQ:DP:HQ', samples=[{'GT': '0', 'HQ': [58, 50], 'DP': 3, 'GQ'\n    : 49, 'name': 'NA00001'}, {'GT': '0', 'HQ': [65, 3], 'DP': 5, 'GQ': 3, 'nam\n    e' : 'NA00002'}, {'GT': '0', 'DP': 3, 'GQ': 41, 'name': 'NA00003'}])\n\nThis produces a great deal of information, but it is conveniently accessed.\nThe attributes of a Record are the 8 fixed fields from the VCF spec plus two\nmore.  That is:\n\n    * ``Record.CHROM``\n    * ``Record.POS``\n    * ``Record.ID``\n    * ``Record.REF``\n    * ``Record.ALT``\n    * ``Record.QUAL``\n    * ``Record.FILTER``\n    * ``Record.INFO``\n\nplus two more attributes to handle genotype information:\n\n    * ``Record.FORMAT``\n    * ``Record.samples``\n\n``samples``, not being the title of any column, is left lowercase.  The format\nof the fixed fields is from the spec.  Comma-separated lists in the VCF are\nconverted to lists.  In particular, one-entry VCF lists are converted to\none-entry Python lists (see, e.g., ``Record.ALT``).  Semicolon-delimited lists\nof key=value p",
    "url": "https://github.com/jdoughertyii/PyVCF",
    "last_updated": "2025-08-26T10:45:21+00:00"
  },
  {
    "full_name": "ramSeraph/opendata",
    "name": "opendata",
    "description": "collection of Indian open government data related scripts. ",
    "language": "Python",
    "topics": [
      "india",
      "jjm",
      "lgd",
      "maps",
      "opendata",
      "soi",
      "bbnl",
      "bharat-broadband-limited",
      "jal-jeevan-mission",
      "local-government-directory",
      "rfcoverage",
      "survey-of-india"
    ],
    "readme": "Contains scripts to deal with pulling/analyzing data from various Indian government websites.\n\n[lgd/](lgd/) - Code for pulling data from [Local Government Directory](https://lgdirectory.gov.in) - [data](http://ramseraph.github.io/opendata/lgd)\n\n[bbnl/](bbnl/) - Code for pulling data from [Bharat Broadband Network Limited](https://bbnl.nic.in/) - [data](http://ramseraph.github.io/opendata/bbnl)\n\n[maps/SOI/](maps/SOI/) - Code for pulling data from [Survey Of India](https://onlinemaps.surveyofindia.gov.in/) - [data](http://ramseraph.github.io/opendata/maps/SOI)\n\n[doc/](doc/) - Code for the [Website](http://ramseraph.github.io/opendata) for accessing pulled data\n",
    "url": "https://github.com/ramSeraph/opendata",
    "last_updated": "2025-08-26T16:45:56+00:00"
  },
  {
    "full_name": "jkrijthe/Rtsne",
    "name": "Rtsne",
    "description": "R wrapper for Van der Maaten's Barnes-Hut implementation of t-Distributed Stochastic Neighbor Embedding",
    "language": "C++",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN\nversion](http://www.r-pkg.org/badges/version/Rtsne)](https://cran.r-project.org/package=Rtsne/)\n[![R-CMD-check](https://github.com/jkrijthe/Rtsne/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/jkrijthe/Rtsne/actions/workflows/R-CMD-check.yaml)\n[![codecov.io](https://codecov.io/github/jkrijthe/Rtsne/coverage.svg?branch=master)](https://app.codecov.io/github/jkrijthe/Rtsne?branch=master)\n[![CRAN mirror\ndownloads](http://cranlogs.r-pkg.org/badges/Rtsne)](https://cran.r-project.org/package=Rtsne/)\n\n# R wrapper for Van der Maaten’s Barnes-Hut implementation of t-Distributed Stochastic Neighbor Embedding\n\n## Installation\n\nTo install from CRAN:\n\n``` r\ninstall.packages(\"Rtsne\") # Install Rtsne package from CRAN\n```\n\nTo install the latest version from the github repository, use:\n\n``` r\nif(!require(devtools)) install.packages(\"devtools\") # If not already installed\ndevtools::install_github(\"jkrijthe/Rtsne\")\n```\n\n## Usage\n\nAfter installing the package, use the following code to run a simple\nexample (to install, see below).\n\n``` r\nlibrary(Rtsne) # Load package\niris_unique <- unique(iris) # Remove duplicates\nset.seed(42) # Sets seed for reproducibility\ntsne_out <- Rtsne(as.matrix(iris_unique[,1:4])) # Run TSNE\nplot(tsne_out$Y,col=iris_unique$Species,asp=1) # Plot the result\n```\n\n![](tools/example-1.png)<!-- -->\n\n# Details\n\nThis R package offers a wrapper around the Barnes-Hut TSNE C++\nimplementation of \\[2\\] \\[3\\]. Changes were made to the original code to\nallow it to function as an R package and to add additional functionality\nand speed improvements.\n\n# References\n\n\\[1\\] L.J.P. van der Maaten and G.E. Hinton. “Visualizing\nHigh-Dimensional Data Using t-SNE.” Journal of Machine Learning Research\n9(Nov):2579-2605, 2008.\n\n\\[2\\] L.J.P van der Maaten. “Accelerating t-SNE using tree-based\nalgorithms.” Journal of Machine Learning Research 15.1:3221-3245, 2014.\n\n\\[3\\] <https://lvdmaaten.gi",
    "url": "https://github.com/jkrijthe/Rtsne",
    "last_updated": "2025-08-28T03:54:05+00:00"
  },
  {
    "full_name": "DeclareDesign/estimatr",
    "name": "estimatr",
    "description": "estimatr: Fast Estimators for Design-Based Inference",
    "language": "R",
    "topics": [],
    "readme": "estimatr: Fast Estimators for Design-Based Inference\n================\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/estimatr)](https://cran.r-project.org/package=estimatr)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/grand-total/estimatr?color=green)](https://r-pkg.org/pkg/estimatr)\n[![Build\nstatus](https://github.com/DeclareDesign/estimatr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/DeclareDesign/estimatr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/DeclareDesign/estimatr/graph/badge.svg)](https://app.codecov.io/gh/DeclareDesign/estimatr)\n[![Replications](https://softwarecite.com/badge/estimatr)](https://softwarecite.com/package/estimatr)\n\n**estimatr** is an `R` package providing a range of commonly-used linear\nestimators, designed for speed and for ease-of-use. Users can easily\nrecover robust, cluster-robust, and other design appropriate estimates.\nWe include two functions that implement means estimators,\n`difference_in_means()` and `horvitz_thompson()`, and three linear\nregression estimators, `lm_robust()`, `lm_lin()`, and `iv_robust()`. In\neach case, users can choose an estimator to reflect cluster-randomized,\nblock-randomized, and block-and-cluster-randomized designs. The [Getting\nStarted\nGuide](https://declaredesign.org/r/estimatr/articles/getting-started.html)\ndescribes each estimator provided by **estimatr** and how it can be used\nin your analysis.\n\nYou can also see the multiple ways you can [get regression tables out of\nestimatr](https://declaredesign.org/r/estimatr/articles/regression-tables.html)\nusing commonly used `R` packages such as `texreg` and `stargazer`. Fast\nestimators also enable fast simulation of research designs to learn\nabout their properties (see [DeclareDesign](https://declaredesign.org)).\n\n## Installing estimatr\n\nTo install the latest stable release of **estimatr**, please ensu",
    "url": "https://github.com/DeclareDesign/estimatr",
    "last_updated": "2025-06-27T14:44:32+00:00"
  },
  {
    "full_name": "kushalkolar/lazytensor",
    "name": "lazytensor",
    "description": "An interface for lazy evaluation of tensors in linear algebra routines",
    "language": "Python",
    "topics": [],
    "readme": "# lazytensor\nAn interface for lazy evaluation of tensors in linear algebra routines\n",
    "url": "https://github.com/kushalkolar/lazytensor",
    "last_updated": "2025-01-15T15:29:50+00:00"
  },
  {
    "full_name": "GuangchuangYu/emojifont",
    "name": "emojifont",
    "description": ":lollipop:Emoji and fontawesom in both base and ggplot2 graphics",
    "language": "R",
    "topics": [
      "emoji",
      "ggplot2",
      "r",
      "visualization",
      "font-awesome"
    ],
    "readme": "emojifont: Emoji Fonts for using in R\n---------\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/emojifont?color=green)](https://cran.r-project.org/package=emojifont)\n![](http://cranlogs.r-pkg.org/badges/grand-total/emojifont?color=green)\n![](http://cranlogs.r-pkg.org/badges/emojifont?color=green)\n![](http://cranlogs.r-pkg.org/badges/last-week/emojifont?color=green)\n[![Research software impact](http://depsy.org/api/package/cran/emojifont/badge.svg)](http://depsy.org/package/r/emojifont)\n[![gitter](https://img.shields.io/badge/GITTER-join%20chat-green.svg)](https://gitter.im/GuangchuangYu/Bioinformatics)\n\n\nAn implementation of using emoji font in both base and 'ggplot2' graphics.\n\n\n## Authors ##\n\nGuangchuang YU <https://guangchuangyu.github.io>\n\nSchool of Public Health, The University of Hong Kong\n\n## Installation ##\n\nGet the released version from CRAN:\n\n```r\ninstall.packages(\"emojifont\")\n```\n\nOr the development version from github:\n\n```r\n## install.packages(\"devtools\")\ndevtools::install_github(\"GuangchuangYu/emojifont\")\n```\n\n## Note\n\nIn addition to emoji font, I also packed other iconic fonts into this package.\n\nCurrently, this package support Emoji Font and Font Awesome.\n\n+ `EmojiOne.ttf` is downloaded from <https://github.com/eosrei/emojione-color-font>\n+ `OpenSansEmoji.ttf` is downloaded from <https://github.com/MorbZ/OpenSansEmoji>\n+ `fontawesome-webfont.ttf` is downloaded from <https://github.com/FortAwesome/Font-Awesome>.\n\nFeel free to fork this package to add your favorite iconic fonts.\n\n",
    "url": "https://github.com/GuangchuangYu/emojifont",
    "last_updated": "2025-05-01T10:33:31+00:00"
  },
  {
    "full_name": "cfpb/proxy-methodology",
    "name": "proxy-methodology",
    "description": "",
    "language": "Stata",
    "topics": [],
    "readme": "# BISG_RACE_ETHNICITY\n\nIn conducting fair lending analysis in both supervisory and enforcement\ncontexts, the Bureau’s Office of Research (OR) and Division of Supervision,\nEnforcement, and Fair Lending (SEFL) rely on a Bayesian Improved Surname\nGeocoding (BISG) proxy method, which combines geography- and surname-based\ninformation into a single proxy probability for race and ethnicity used in fair\nlending analysis conducted for non-mortgage products.\nThis document describes the steps needed to build the BISG proxies.\n\nThe methodology described here is an example of a proxy methodology that\nOR and SEFL use, although we may alter this methodology in particular analyses,\ndepending on the circumstances involved.\nIn addition, the proxy method may be revised as we become aware of enhancements\nthat would increase accuracy and performance.\nFor more details, see [“Using Publicly Available Information to Proxy for\nUnidentified Race and Ethnicity: A Methodology and Assessment”][paper].\n\nIncluded are a series of Stata scripts and subroutines that prepare the\npublicly available census geography and surname data and that construct the\nsurname-only, geography-only, and BISG proxies for race and ethnicity.\nThe scripts, subroutines, and data provided here do not contain directly\nidentifiable personal information or other confidential information,\nsuch as confidential supervisory information.\n\nPlease note that all scripts and subroutines are written for execution in\nStata 12 on a Linux platform and may need to be modified for other environments.\nUsers must define a number of parameters, including file paths and arguments for subroutines.\nThe scripts that define the subroutines also identify and describe arguments, as required.\n\nUsers must supply their own application- or individual-level data,\nand any geocoding of those data must occur prior to the execution of the\nscript sequence: **this code assumes that the input application- or\nindividual-level data are already geocoded with census",
    "url": "https://github.com/cfpb/proxy-methodology",
    "last_updated": "2025-08-05T14:51:57+00:00"
  },
  {
    "full_name": "jrnold/textools",
    "name": "textools",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "**In progress**\n\nAn R package for generating LaTeX, in the spirit of [htmltools](https://cran.r-project.org/package=htmltools)\n",
    "url": "https://github.com/jrnold/textools",
    "last_updated": "2019-07-13T21:17:43+00:00"
  },
  {
    "full_name": "ourownstory/neural_prophet",
    "name": "neural_prophet",
    "description": "NeuralProphet: A simple forecasting package",
    "language": "Python",
    "topics": [
      "forecasting",
      "time-series",
      "machine-learning",
      "fbprophet",
      "prophet",
      "forecast",
      "artificial-intelligence",
      "prediction",
      "trend",
      "seasonality",
      "autoregression",
      "pytorch",
      "timeseries",
      "forecasting-algorithm",
      "forecasting-model",
      "neuralprophet",
      "neural",
      "neural-network",
      "python",
      "deep-learning"
    ],
    "readme": "[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/ourownstory/neural_prophet?logo=github)](https://github.com/ourownstory/neural_prophet/releases)\n[![Pypi_Version](https://img.shields.io/pypi/v/neuralprophet.svg)](https://pypi.python.org/pypi/neuralprophet)\n[![Python Version](https://img.shields.io/badge/python-3.9+-blue?logo=python)](https://www.python.org/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![License](https://img.shields.io/badge/license-MIT-brightgreen)](https://opensource.org/licenses/MIT)\n[![Tests](https://github.com/ourownstory/neural_prophet/actions/workflows/tests.yml/badge.svg)](https://github.com/ourownstory/neural_prophet/actions/workflows/tests.yml)\n[![codecov](https://codecov.io/gh/ourownstory/neural_prophet/branch/master/graph/badge.svg?token=U5KXCL55DW)](https://codecov.io/gh/ourownstory/neural_prophet)\n[![Slack](https://img.shields.io/badge/slack-@neuralprophet-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40)](https://neuralprophet.slack.com/join/shared_invite/zt-sgme2rw3-3dCH3YJ_wgg01IXHoYaeCg#/shared-invite/email)\n[![Downloads](https://static.pepy.tech/personalized-badge/neuralprophet?period=total&units=international_system&left_color=black&right_color=blue&left_text=Downloads)](https://pepy.tech/project/neuralprophet)\n\n![NP-logo-wide_cut](https://user-images.githubusercontent.com/21246060/111388960-6c367e80-866d-11eb-91c1-46f2c0d21879.PNG)\n\n\nPlease note that the project is still in beta phase. Please report any issues you encounter or suggestions you have. We will do our best to address them quickly. Contributions are very welcome!\n\n# NeuralProphet: human-centered forecasting\nNeuralProphet is an easy to learn framework for interpretable time series forecasting.\nNeuralProphet is built on PyTorch and combines Neural Networks and traditional time-series algorithms, inspired by [Facebook Prophet](https://github.com/facebook/prophet) and [AR",
    "url": "https://github.com/ourownstory/neural_prophet",
    "last_updated": "2025-08-30T02:14:44+00:00"
  },
  {
    "full_name": "littleknitsstory/client-gpt",
    "name": "client-gpt",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "## client_gpt\n\nThe end user can use ClientGPT to communicate with the GT3 model from OpenAI.\n\n## install\n \npip install client_gpt\n\n## use\n\nfrom client_gpt.client_gpt import ClientGPT\n\napi_key = \"your_api_key\"\n\nmodel = \"your_model_id\"\n\nclient = ClientGPT(api_key, model)\n\nresponse_text, message_id, conversation_id = client.ask(prompt='Hello, how are you?', conversation_id=None, previous_convo_id=None)\n\nprint(response_text)\n",
    "url": "https://github.com/littleknitsstory/client-gpt",
    "last_updated": "2025-01-15T15:29:56+00:00"
  },
  {
    "full_name": "aaren/notedown",
    "name": "notedown",
    "description": "Markdown <=> IPython Notebook",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "*Python 2/3 and IPython 4 / Jupyter compatible!* <a href='https://travis-ci.org/aaren/wavelets'> <img src='https://secure.travis-ci.org/aaren/wavelets.png?branch=master'></a>\n\nConvert IPython Notebooks to markdown (and back)\n------------------------------------------------\n\n[notedown] is a simple tool to create [IPython notebooks][ipython]\nfrom markdown (and r-markdown).\n\n[ipython]: http://www.ipython.org/notebook\n[notedown]: http://github.com/aaren/notedown\n\n`notedown` separates your markdown into code and not code. Code\nblocks (fenced or indented) go into input cells, everything else\ngoes into markdown cells.\n\nUsage:\n\n    notedown input.md > output.ipynb\n\nInstallation:\n\n    pip install notedown\n\nor the latest on github:\n\n    pip install https://github.com/aaren/notedown/tarball/master\n\n\n### Conversion to markdown\n\nConvert a notebook into markdown, stripping all outputs:\n\n    notedown input.ipynb --to markdown --strip > output.md\n\nConvert a notebook into markdown, with output JSON intact:\n\n    notedown input.ipynb --to markdown > output_with_outputs.md\n\nThe outputs are placed as JSON in a code-block immediately after the\ncorresponding input code-block. `notedown` understands this\nconvention as well, so it is possible to convert this\nmarkdown-with-json back into a notebook.\n\nThis means it is possible to edit markdown, convert to notebook,\nplay around a bit and convert back to markdown.\n\nNB: currently, notebook and cell metadata is not preserved in the\nconversion.\n\nStrip the output cells from markdown:\n\n    notedown with_output_cells.md --to markdown --strip > no_output_cells.md\n\n\n### Running an IPython Notebook\n\n    notedown notebook.md --run > executed_notebook.ipynb\n\n### Editing in the browser *(new!)*\n\nYou can configure IPython / Jupyter to seamlessly use markdown as its storage\nformat. Add the following to your config file:\n\n    c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n\n\nNow you can edit your markdown files in the browser, execut",
    "url": "https://github.com/aaren/notedown",
    "last_updated": "2025-07-23T02:37:13+00:00"
  },
  {
    "full_name": "hernamesbarbara/law_and_order",
    "name": "law_and_order",
    "description": "playing with Law & Order data",
    "language": "Python",
    "topics": [],
    "readme": "Funnin w/ law and order data\n\nhttps://api.mongolab.com/api/1/databases/episodes/collections/corpora?apiKey=5067b7c4e4b0c0cda3668ebe\n",
    "url": "https://github.com/hernamesbarbara/law_and_order",
    "last_updated": "2025-02-23T12:13:05+00:00"
  },
  {
    "full_name": "jennybc/candy",
    "name": "candy",
    "description": "candy survey data",
    "language": "R",
    "topics": [],
    "readme": "## 2015 Candy Hierarchy\n\nBenjamin Cohen and David Ng have been conducting a Halloween candy survey for a \ncouple of years and writing it up:\n\n  * The Candy Hierarchy 2015: your essential guide to Hallowen treats:  \n    <http://boingboing.net/2015/10/31/the-candy-hierarchy-2015-your.html>\n  * 2015 CANDY HIERARCHY: SUPPLEMENTAL RAW DATA AND NEW ANALYSES:  \n    <http://www.scq.ubc.ca/2015-candy-hierarchy-supplemental-raw-data-and-new-analyses/>\n  * 2015 data as a Google Sheet:  \n    <https://docs.google.com/spreadsheets/d/1REZvjqv0lj3dEYb0CsGyDXkXrjhJ4izlAEImgaufjCc/pubhtml>\n  * STRATIGRAPHICAL ANALYSIS OF FRIDAY-SUNDAY IDENTIFICATION IN RELATION TO SUGAR \nCONSUMPTION PREFERENCES AND RELATED DEMOGRAPHIC VARIABLES:  \n    <http://www.scq.ubc.ca/stratigraphical-analysis-of-friday-sunday-identification-in-relation-to-sugar-consumption-preferences-and-related-demographic-variables/>\n  * The Candy Hierarchy, 2014:  \n    <http://boingboing.net/2014/10/31/the-candy-hierarchy-2014.html>\n  * CANDY HIERARCHY 2014: SUPPLEMENTARY RAW DATA:  \n    <http://www.scq.ubc.ca/candy-hierarchy-2014-supplementary-raw-data/>\n\nThis year in [STAT545](http://stat545-ubc.github.io) were going to use this as \nour final challenge in data cleaning, wrangling, and exploration. As we make \nprogress ... who knows? Maybe this will become a proper data package? Maybe \nwe'll make a gallery of visualizations? I have no idea where this is going. For \nnow, I'm just parking the raw data in [data-raw](data-raw) for the students to \nwork on in class and for homework.\n\nHere's a visualization from Cohen and Ng ... wonder what we will come up with?\n\n![](data-raw/candyhierarchy2015.jpg)",
    "url": "https://github.com/jennybc/candy",
    "last_updated": "2024-06-27T00:43:28+00:00"
  },
  {
    "full_name": "adletaw/captioner",
    "name": "captioner",
    "description": "an R package for generating figure/table numbers and captions, especially for Rmd docs",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/captioner)](http://cran.r-project.org/package=captioner)\n\nStatus\n======\n\n*captioner* is no longer under development. Feel free to fork, copy, use as desired. No attribution necessary.\n\nOverview\n========\n\n*captioner* is an R package for generating figure numbers and captions\n\nInformation about recent updates can be found in [NEWS.md](https://github.com/adletaw/captioner/blob/master/NEWS.md).\n\nInstallation\n============\n\n*captioner* is now available on [CRAN](https://cran.r-project.org/web/packages/captioner/index.html). If you would like to install the current development version:\n\n-   `install.packages(\"devtools\")`\n-   `devtools::install_github(\"adletaw/captioner\")`\n\nor if you want the vignette:\n\n-   `devtools::install_github(\"adletaw/captioner\", build_vignettes = TRUE)`\n-   `vignette(\"using_captioner\")`\n\nBasic Usage\n===========\n\nA call to the function `captioner()` returns a captioner function for each set of figures, tables, etc. that you want to create.\n\nYou can generate a full caption:\n\n``` r\nfig_nums <- captioner()\nfig_nums(\"my_first_figure\", \"My first figure's caption.\")\n#> [1] \"Figure  1: My first figure's caption.\"\n```\n\nAnd display it under your plot using inline code chunks:\n\n``` r\nplot(cars)\n```\n\n![](README-ex_1b-1.png)\n\nFigure 1: My first figure's caption.\n\nOnce you have created the caption, you can display it by using your function and specifying the name of the caption you created.\n\n``` r\nfig_nums(\"my_first_figure\")\n#> [1] \"Figure  1: My first figure's caption.\"\n```\n\nThe `display` parameter allows you to adjust how much of the caption is displayed. For example, you can also generate a figure reference:\n\n``` r\nfig_nums(\"my_first_figure\", display = \"cite\")\n#> [1] \"Figure  1\"\n```\n\nWhich can be displayed inline using inline code chunks, like so (Figure 1).\n\nEach new call to your captioner function will create a caption with an in",
    "url": "https://github.com/adletaw/captioner",
    "last_updated": "2025-04-18T07:38:27+00:00"
  },
  {
    "full_name": "DeclareDesign/fabricatr",
    "name": "fabricatr",
    "description": "fabricatr: Imagine Your Data Before You Collect It",
    "language": "R",
    "topics": [],
    "readme": "fabricatr: Imagine your data before you collect it\n================\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/fabricatr)](https://cran.r-project.org/package=fabricatr)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/grand-total/fabricatr?color=green)](https://r-pkg.org/pkg/fabricatr)\n[![Build\nstatus](https://github.com/DeclareDesign/fabricatr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/DeclareDesign/fabricatr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/DeclareDesign/fabricatr/graph/badge.svg)](https://app.codecov.io/gh/DeclareDesign/fabricatr)\n[![Replications](https://softwarecite.com/badge/fabricatr)](https://softwarecite.com/package/fabricatr)\n\nMaking decisions about research design and analysis strategies is often\ndifficult before data is collected, because it is hard to imagine the\nexact form data will take. Instead, researchers typically modify\nanalysis strategies to fit the data. **fabricatr** helps researchers\nimagine what data will look like before they collect it. Researchers can\nevaluate alternative analysis strategies, find the best one given how\nthe data will look, and precommit before looking at the realized data.\n\n### Installing fabricatr\n\nTo install the latest stable release of **fabricatr**, please ensure\nthat you are running version 3.5 or later of R and run the following\ncode:\n\n``` r\ninstall.packages(\"fabricatr\")\n```\n\n### Getting started\n\nOnce you have installed **fabricatr**, you can easily import your own\ndata or generate new data. **fabricatr** is designed to help you solve\ntwo key problems:\n\n1.  Generating variables that look like the real thing, including Likert\n    survey responses, treatment status, demographic variables, and\n    variables correlated by group.\n2.  Generating data that are structured like the real thing, including\n    panel data, multi-level (“nested”) data or cr",
    "url": "https://github.com/DeclareDesign/fabricatr",
    "last_updated": "2025-02-07T06:36:46+00:00"
  },
  {
    "full_name": "tobimensch/termsql",
    "name": "termsql",
    "description": "Convert text from a file or from stdin into SQL table and query it instantly. Uses sqlite as backend. The idea is to make SQL into a tool on the command line or in scripts.",
    "language": "Python",
    "topics": [
      "sql",
      "sqlparse",
      "terminal",
      "pipe",
      "sqlite",
      "parse",
      "convert",
      "query",
      "scripts",
      "stdin"
    ],
    "readme": "TERMSQL\n=======\n\nConvert text from a file or from stdin into SQL table and query it instantly. Uses sqlite as backend.\nThe idea is to make SQL into a tool on the command line or in scripts.\n\nInstall\n=======\n\nRequirements:\n - python\n - sqlite3\n - sqlparse module 0.1.15 and up from https://github.com/andialbrecht/sqlparse (optional)\n \nInstalling using pip:\n\n    pip3 install termsql\n\nTo install termsql open a terminal and run:\n\n    sudo python setup.py install\n\nTermsql now supports shorter and more convenient sql statements given the sqlparse module from https://pypi.python.org/pypi/sqlparse/ version 0.1.15 or higher is installed.\n\n    termsql \"select col0,col1 from tbl\"\n    #is equal to:\n    termsql select col0,col1\n    \n    termsql -m line -1 \"select USER,COUNT(*) from tbl group by USER\"\n    #is equal to:\n    termsql -m line -1 \"select USER,COUNT(*) group by USER\"\n\nTherefore installing the sqlparse module (which is tiny anyway) from https://pypi.python.org/pypi/sqlparse/ or\nhttps://github.com/andialbrecht/sqlparse is highly recommended. At least if you want to save yourself some typing.\n\nLearn more\n==========\n\nalways helpful is:\n\n    termsql --help\n  \nand also:\n\n    man termsql\n  \nOnline manual:\n\n  http://tobimensch.github.io/termsql\n\nSo what can it do?\n==================\n\n- convert text/CSV files into sqlite database/table\n- work on stdin data on-the-fly\n- it can be used as swiss army knife kind of tool for extracting information\n  from other processes that send their information to termsql via a pipe\n  on the command line or in scripts\n- termsql can also pipe into another termsql of course\n- you can quickly sort and extract data\n- creates string/integer/float column types automatically\n- gives you the syntax and power of SQL on the command line\n\nExamples\n========\n\n    export LC_ALL=en_US; top -b | head | termsql -1 -H 6 \"select [PID],[USER],[COMMAND],[%CPU] from tbl where [%CPU]>=25\"\n\n> termsql doesn't recognize numbers like \"25,3\" as numbers, but as strings. expor",
    "url": "https://github.com/tobimensch/termsql",
    "last_updated": "2025-07-03T20:57:43+00:00"
  },
  {
    "full_name": "ryanmcdermott/trump-speeches",
    "name": "trump-speeches",
    "description": ":page_facing_up: 1mb Archive of Donald Trump Speeches",
    "language": "",
    "topics": [],
    "readme": "# trump-speeches\n\n## What is it?\n`speeches.txt`: 1mb of text data taken from speeches made by Donald Trump at various points in his 2016 campaign for President of the United States.\n\n## What is this for?\nFor all of your data science, machine learning, and entertainment needs.\n\n![word_cloud](examples/word_cloud.png)\n\n## Examples\nRun the example Word Cloud generator.\n\n```bash\npip install wordcloud\ncd examples\npython trump_wordcloud.py\n```\n\n## License\nCopyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use\n",
    "url": "https://github.com/ryanmcdermott/trump-speeches",
    "last_updated": "2025-06-04T13:23:16+00:00"
  },
  {
    "full_name": "jcushman/pdfquery",
    "name": "pdfquery",
    "description": "A fast and friendly PDF scraping library.",
    "language": "Python",
    "topics": [],
    "readme": "========\nPDFQuery\n========\n------------------------------------------------------------\nConcise, friendly PDF scraping using JQuery or XPath syntax.\n------------------------------------------------------------\n\n.. image:: https://travis-ci.org/jcushman/pdfquery.png\n   :alt: Travis Build Status\n   :target: https://travis-ci.org/jcushman/pdfquery\n.. image:: https://ci.appveyor.com/api/projects/status/d9or9795d9b66ai7?svg=true\n   :alt: Appveyor Build Status\n   :target: https://ci.appveyor.com/project/jcushman/pdfquery\n\n\nPDFQuery is a light wrapper around pdfminer, lxml and pyquery. It's designed to reliably extract data from sets of\nPDFs with as little code as possible.\n\n.. contents:: **Table of Contents**\n\nInstallation\n============\n\n``easy_install pdfquery`` or ``pip install pdfquery``.\n\nQuick Start\n===========\n\nThe basic idea is to transform a PDF document into an element tree so we can find items with JQuery-like selectors\nusing pyquery. Suppose we're trying to extract a name from a set of PDFs, but all we know is that it appears\nunderneath the words \"Your first name and initial\" in each PDF::\n\n    >>> pdf = pdfquery.PDFQuery(\"tests/samples/IRS_1040A.pdf\")\n    >>> pdf.load()\n    >>> label = pdf.pq('LTTextLineHorizontal:contains(\"Your first name and initial\")')\n    >>> left_corner = float(label.attr('x0'))\n    >>> bottom_corner = float(label.attr('y0'))\n    >>> name = pdf.pq('LTTextLineHorizontal:in_bbox(\"%s, %s, %s, %s\")' % (left_corner, bottom_corner-30, left_corner+150, bottom_corner)).text()\n    >>> name\n    'John E.'\n\nNote that we don't have to know where the name is on the page, or what page it's on,\nor how the PDF has it stored internally.\n\n*Performance Note:* The initial call to pdf.load() runs very slowly, because the underlying\npdfminer library has to compare every element on the page to every other element.\nSee the Caching section to avoid this on subsequent runs.\n\nNow let's extract and format a bunch of data all at once::\n\n    >>> pdf = pdfquery.PDFQuery(",
    "url": "https://github.com/jcushman/pdfquery",
    "last_updated": "2025-08-19T09:24:20+00:00"
  },
  {
    "full_name": "EthicalML/awesome-production-machine-learning",
    "name": "awesome-production-machine-learning",
    "description": "A curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning",
    "language": "",
    "topics": [
      "machine-learning",
      "mlops",
      "interpretability",
      "explainability",
      "responsible-ai",
      "deep-learning",
      "machine-learning-operations",
      "ml-ops",
      "ml-operations",
      "privacy-preserving",
      "privacy-preserving-ml",
      "privacy-preserving-machine-learning",
      "data-mining",
      "large-scale-ml",
      "production-ml",
      "large-scale-machine-learning",
      "production-machine-learning",
      "awesome",
      "awesome-list"
    ],
    "readme": "[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n[![X](https://img.shields.io/badge/X-%23000000?logo=X&logoColor=white)](https://twitter.com/EthicalML)\n\n# Awesome Production Machine Learning\n\nThis repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning 🚀\n\nYou can keep up to date by watching this github repo to get a summary of the new production ML libraries added every month [via releases](https://github.com/EthicalML/awesome-production-machine-learning/releases) 🤩\n\nAdditionally, we provide a [search toolkit](https://huggingface.co/spaces/zhiminy/Awesome-Production-Machine-Learning-Search) that helps you quickly navigate through the toolchain.\n\n## Quick links to sections on this page\n\n| | | |\n|-|-|-|\n| [🔧 AutoML](#automl) | [🧮 Computation & Communication Optimisation](#computation-and-communication-optimisation) | [🏷️ Data Annotation & Synthesis](#data-annotation-and-synthesis) |\n| [🧵 Data Pipeline](#data-pipeline) | [📓 Data Science Notebook](#data-science-notebook) | [💾 Data Storage Optimisation](#data-storage-optimisation) |\n| [💸 Data Stream Processing](#data-stream-processing) | [💪 Deployment & Serving](#deployment-and-serving) | [📈 Evaluation & Monitoring](#evaluation-and-monitoring) |\n| [🔍 Explainability & Fairness](#explainability-and-fairness) | [🎁 Feature Store](#feature-store) | [🔴 Industry-strength Anomaly Detection](#industry-strength-anomaly-detection) |\n| [👁️ Industry-strength Computer Vision](#industry-strength-computer-vision) | [🔥 Industry-strength Information Retrieval](#industry-strength-information-retrieval) | [🔠 Industry-strength Natural Language Processing](#industry-strength-nlp) |\n| [🙌 Industry-strength Recommender System](#industry-strength-recommender-system) | [🍕 Industry-strength Reinforcement Learning](#industry-strength-reinforcement-learning) | [📊 Industry-strength Visualisation](#industry-strength-visualisation) |",
    "url": "https://github.com/EthicalML/awesome-production-machine-learning",
    "last_updated": "2025-09-02T09:20:57+00:00"
  },
  {
    "full_name": "webrecorder/pywb",
    "name": "pywb",
    "description": "Core Python Web Archiving Toolkit for replay and recording of web archives",
    "language": "JavaScript",
    "topics": [
      "python",
      "wayback",
      "pywb",
      "web-archiving",
      "web-archives"
    ],
    "readme": "Webrecorder pywb 2.9\n====================\n\n.. image:: https://raw.githubusercontent.com/webrecorder/pywb/main/pywb/static/pywb-logo.png\n\n.. image:: https://github.com/webrecorder/pywb/workflows/CI/badge.svg\n      :target: https://github.com/webrecorder/pywb/actions\n.. image:: https://codecov.io/gh/webrecorder/pywb/branch/main/graph/badge.svg\n      :target: https://codecov.io/gh/webrecorder/pywb\n\nWeb Archiving Tools for All\n---------------------------\n\n`View the full pywb documentation <https://pywb.readthedocs.org>`_\n\n**pywb** is a Python 3 web archiving toolkit for replaying web archives large and small as accurately as possible.\nThe toolkit now also includes new features for creating high-fidelity web archives.\n\nThis toolset forms the foundation of Webrecorder project, but also provides a generic web archiving toolkit\nthat is used by other web archives, including the traditional \"Wayback Machine\" functionality.\n\n\nNew Features\n^^^^^^^^^^^^\n\nThe 2.x release included a major overhaul of pywb and introduces many new features, including the following:\n\n* Dynamic multi-collection configuration system with no-restart updates.\n\n* New recording capability to create new web archives from the live web or other archives.\n\n* Componentized architecture with standalone Warcserver, Recorder and Rewriter components.\n\n* Support for Memento API aggregation and fallback chains for querying multiple remote and local archival sources.\n\n* HTTP/S Proxy Mode with customizable certificate authority for proxy mode recording and replay.\n\n* Flexible rewriting system with pluggable rewriters for different content-types.\n\n* Standalone, modular `client-side rewriting system (wombat.js) <https://github.com/webrecorder/wombat>`_ to handle most modern web sites.\n\n* Improved 'calendar' query UI with incremental loading, grouping results by year and month, and updated replay banner.\n\n* Extensible UI customizations system for modifying all aspects of the UI.\n\n* Robust access control system for blockin",
    "url": "https://github.com/webrecorder/pywb",
    "last_updated": "2025-08-28T20:39:31+00:00"
  },
  {
    "full_name": "rtrangucci/mrp_2016_election",
    "name": "mrp_2016_election",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "Overview\n--------\n\nThis repo contains most of the code used to do the analysis and generate the\ngraphs in our arXiv paper [Voting patterns in 2016: Exploration using\nmultilevel regression and poststratification (MRP) on pre-election\npolls](https://arxiv.org/abs/1802.00842).\n\nDescription\n--------------\n\nThe code used to run the multilevel regression and poststratification from\nstart to finish can be found\n[here](https://github.com/rtrangucci/mrp_2016_election/blob/master/mrp/R_code/2016_mrp.R). All the data necessary to run the code is on the repo.\n\nThe data preparation code for polling data and CPS data are in the files\n[polling_2012.R](https://github.com/rtrangucci/mrp_2016_election/blob/master/mrp/R_code/polling_2012.R) and [pums_cps_clean_2012.R](https://github.com/rtrangucci/mrp_2016_election/blob/master/mrp/R_code/pums_cps_clean_2012.R).\n\nThe graphing and mapping code can be found in the [graphics](https://github.com/rtrangucci/mrp_2016_election/tree/master/mrp/R_code/graphics) directory.\n\nAll of the MRP and data prep code requires\n[rstanarm](https://CRAN.R-project.org/package=rstanarm),\n[dplyr](https://CRAN.R-project.org/package=dplyr),\n[plyr](https://CRAN.R-project.org/package=plyr)\n\n",
    "url": "https://github.com/rtrangucci/mrp_2016_election",
    "last_updated": "2023-09-27T00:41:11+00:00"
  },
  {
    "full_name": "tcpd/Urban_Local_Body",
    "name": "Urban_Local_Body",
    "description": "Dataset consists of information on the Urban Local Body Elections ",
    "language": "",
    "topics": [],
    "readme": "# Urban-Local-Body \n\nPublic repository for public data of Municipal Elections for Urban Local Bodies\n\nthis repository contains following files:\n1. tcpd_casi_ulb_v1.1.csv : election data for Urban Local Bodies\n2. tcpd_casi_ulb_codebook_v1.1.pdf : documentation and detailed description of variables in the dataset.\n\n\nThis dataset consists of information on the Urban Local Body Elections for 8 Indian States - Bihar, Kerala, Madhya Pradesh, Maharashtra, Rajasthan, Telangana, West Bengal, and Uttar Pradesh. This dataset has been built through the collaborative efforts of the Trivedi Centre for Political Data, Ashoka University, and the Center for the Advanced Study of India, University of Pennsylvania.\n\n<b>Terms and conditions</b> <br />\nThe Urban Local Body dataset consists of election data pertaining to Urban Local Bodies from 2008 to 2021. This dataset and codebook is part of the TCPD repository which contains our work in progress. Please ensure you use the latest version of the data and documentation. For questions, please email us at tcpd-contact@ashoka.edu.in. Users are free to download, display or include the data in other products for non-commercial purposes at no cost subject to the following limitations: <br />\n\n<b>Data Citation:</b> “TCPD-CASI Urban Local Body Dataset (TCPD-CASI-ULB), 2008-2022”. Trivedi Centre for Political Data, Ashoka University and Center for the Advanced Study of India, University of Pennsylvania. <br />\n<b>Codebook Citation:</b> Neelesh Agrawal, Adam Auerbach, Mohammed Zahir Ali, Srishti Gupta, Mohit Kumar, Aditya Sarkar, Tariq Thachil, Gilles Verniers, and S. V. Sai Vikas. 2023. “TCPD-CASI Urban Local Body Dataset (TCPD-CASI-ULB), 2008-2022 Codebook 1.1\", Trivedi Centre for Political Data, Ashoka University, Center for the Advanced Study of India, University of Pennsylvania.<br />\n<b>No Endorsement:</b> The user must not claim or imply that the Trivedi Centre for Political Data endorses the user's use of the data or use of the Centre's l",
    "url": "https://github.com/tcpd/Urban_Local_Body",
    "last_updated": "2023-07-23T20:57:58+00:00"
  },
  {
    "full_name": "simzou/nielsen-dma",
    "name": "nielsen-dma",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "This is the map of Nielen [Designated Marketing Areas](http://en.wikipedia.org/wiki/Media_market) using D3.js. Hover over for data related to each area. \n\nSee map [here](http://bl.ocks.org/simzou/6459889)\n\nSee code [here](https://github.com/simzou/nielsen-dma/)\n\n### Credits\n\n* Map adapted from Mike Bostock's [map example](http://bl.ocks.org/mbostock/2206590)\n* Original topojson made by converting shapefile from [here](http://geocommons.com/overlays/306767) to topojson via [Mike Bostock's instructions](http://bost.ocks.org/mike/map/)\n     * Shapefile link is dead, an archived Geojson from Geocommons can be found [here](https://raw.githubusercontent.com/geoiq/gc_data/master/datasets/998.geojson)\n* Nielsen rank and TV data via [tvb](http://www.tvb.org/media/file/TVB_Market_Profiles_Nielsen_Household_DMA_Ranks2.pdf)\n\n### Notes\n\n* Some areas are shaped really strangely (such as a slice of Nevada being part of the Denver media market) but that is really how they are divided \n",
    "url": "https://github.com/simzou/nielsen-dma",
    "last_updated": "2025-06-26T15:28:11+00:00"
  },
  {
    "full_name": "tensorflow/skflow",
    "name": "skflow",
    "description": "Simplified interface for TensorFlow (mimicking Scikit Learn) for Deep Learning",
    "language": "Python",
    "topics": [],
    "readme": "SkFlow has been moved to Tensorflow.\n====================================\n\nSkFlow has been moved to http://github.com/tensorflow/tensorflow into contrib folder specifically located `here <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn>`__.\nThe development will continue there. Please submit any issues and pull requests to Tensorflow repository instead. \n\nThis repository will ramp down, including after next Tensorflow release we will wind down code here. \nPlease see instructions on most recent installation `here <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn>`__.\n",
    "url": "https://github.com/tensorflow/skflow",
    "last_updated": "2025-08-09T16:02:20+00:00"
  },
  {
    "full_name": "hrbrmstr/urldiversity",
    "name": "urldiversity",
    "description": "🔗 Quantify 'URL' Diversity and Apply Popular Biodiversity Indices to a 'URL' Collection",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "species-diversity",
      "url",
      "urls",
      "uri",
      "r-cyber"
    ],
    "readme": "\n# urldiversity\n\nQuantify ‘URL’ Diversity and Apply Popular Biodiversity Indices to a\n‘URL’ Collection\n\n## Description\n\nMethods are provided to compute the ‘WSDL Diversity Index’\n<http://ws-dl.blogspot.com/2018/05/2018-05-04-exploration-of-url-diversity.html>\nalong with selected biodiversity indidces to a corpus (collection) of\n‘URLs’.\n\n## NOTE\n\nAll credit goes to [Alexander Nwala](http://www.cs.odu.edu/~anwala/) for\nthe algorithm research and [original Python\nimplementation](https://github.com/anwala/url-diversity).\n\n## TODO\n\n  - \\[ \\] Handle some edge cases\n  - \\[ \\] Tests\n  - \\[ \\] Better documentation\n  - \\[ \\] Vignette with many citations from the WSDL blog post\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `uri_diversity`: Quantify URL diversity\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/urldiversity\")\n```\n\n## Usage\n\n``` r\nlibrary(urldiversity)\n\n# current verison\npackageVersion(\"urldiversity\")\n```\n\n    ## [1] '0.1.0'\n\n``` r\ncollection <- readLines(system.file(\"extdat\", \"corpus.txt\", package = \"urldiversity\"))\n\nprint(collection)\n```\n\n    ##  [1] \"http://www.niaid.nih.gov/topics/ebolaMarburg/understandingEbola/\"             \n    ##  [2] \"http://www.niaid.nih.gov/topics/ebolaMarburg/understandingEbola/\"             \n    ##  [3] \"http://www.niaid.nih.gov/topics/ebolaMarburg/understandingEbola/\"             \n    ##  [4] \"http://www.niaid.nih.gov/topics/ebolaMarburg/understandingEbola/\"             \n    ##  [5] \"http://www.niaid.nih.gov/topics/ebolaMarburg/understandingEbola/\"             \n    ##  [6] \"http://www.cdc.gov/vhf/ebola/pdf/facts-about-ebola-french.pdf\"                \n    ##  [7] \"http://www.cdc.gov/vhf/ebola/pdf/facts-about-ebola-french.pdf\"                \n    ##  [8] \"http://www.cdc.gov/vhf/ebola/outbreaks/2014-west-africa/previous-updates.html\"\n    ##  [9] \"http://www.cdc.gov/vhf/ebola/outbreaks/2014-west-africa/previous-updates.html\"\n    ## [10] \"http://www.cdc.gov/vhf/ebola/outbreaks/2014-west-africa/previ",
    "url": "https://github.com/hrbrmstr/urldiversity",
    "last_updated": "2025-03-22T11:07:22+00:00"
  },
  {
    "full_name": "amzn/metalearn-leap",
    "name": "metalearn-leap",
    "description": "Original PyTorch implementation of the Leap meta-learner (https://arxiv.org/abs/1812.01054) along with code for running the Omniglot experiment presented in the paper.",
    "language": "Python",
    "topics": [],
    "readme": "## Transferring Knowledge across Learning Processes  \n\n[[Blog post]](https://medium.com/@flnr/transferring-knowledge-across-learning-processes-f6f63e9e6f46)  [[Paper]](https://arxiv.org/abs/1812.01054)\n\nOriginal [PyTorch](https://pytorch.org/) implementation of the Leap meta-learner (https://arxiv.org/abs/1812.01054)\nalong with code for running the Omniglot experiment presented in the paper.\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n\n## Authors\n\nSebastian Flennerhag\n\n## Install\n\nThis repository was developed against PyTorch v0.4 on Ubuntu 16.04 using Python 3.6. To install\nLeap, clone the repo and install the source code:\n\n```bash\ngit clone https://github.com/amazon/pytorch-leap\ncd pytorch-leap/src/leap\npip install -e .\n```\n\nThis installs the ``leap`` package and the ``Leap`` meta-learner class. The meta-learner can be used with any\n``torch.nn.Module`` class as follows:\n\n```python\nRequire: criterion, model, tasks, opt_cls, meta_opt_cls, opt_kwargs, meta_opt_kwargs\n\nleap = Leap(model)\nmopt = meta_opt_cls(leap.parameters(), **meta_opt_kwargs)\nfor meta_steps:\n    meta_batch = tasks.sample()\n    for task in meta_batch:\n        leap.init_task()\n        leap.to(model)\n        opt = opt_cls(model.parameters(), **opt_kwargs)\n\n        for x, y in task:\n            loss = criterion(model(x), y)\n            loss.backward()\n\n            leap.update(loss, model)\n\n            opt.step()\n            opt.zero_grad()  # MUST come after leap.update\n    ###\n    leap.normalize()\n    meta_optimizer.step()\n    meta_optimizer.zero_grad()\n```\n\n## Omniglot\n\nTo run the Omniglot experiment, first prepare the dataset using the ``make_omniglot.sh`` script\nin the root directory. The ``p`` flag downloads the dataset, ``d`` installs dependencies and ``l``\ncreates log directories.\n\n```bash\nbash make_omniglot.sh -pdl\n```\n\nTo train a meta-learner, use the ``main.py`` script. To replicate experiments in the paper select a meta\nlearner and number of pretraining tasks. For inst",
    "url": "https://github.com/amzn/metalearn-leap",
    "last_updated": "2024-09-17T15:30:48+00:00"
  },
  {
    "full_name": "tensorflow/playground",
    "name": "playground",
    "description": "Play with neural networks!",
    "language": "TypeScript",
    "topics": [],
    "readme": "# Deep playground\n\nDeep playground is an interactive visualization of neural networks, written in\nTypeScript using d3.js. We use GitHub issues for tracking new requests and bugs.\nYour feedback is highly appreciated!\n\n**If you'd like to contribute, be sure to review the [contribution guidelines](CONTRIBUTING.md).**\n\n## Development\n\nTo run the visualization locally, run:\n- `npm i` to install dependencies\n- `npm run build` to compile the app and place it in the `dist/` directory\n- `npm run serve` to serve from the `dist/` directory and open a page on your browser.\n\nFor a fast edit-refresh cycle when developing run `npm run serve-watch`.\nThis will start an http server and automatically re-compile the TypeScript,\nHTML and CSS files whenever they change.\n\n## For owners\nTo push to production: `git subtree push --prefix dist origin gh-pages`.\n\nThis is not an official Google product.\n",
    "url": "https://github.com/tensorflow/playground",
    "last_updated": "2025-09-02T02:01:28+00:00"
  },
  {
    "full_name": "INFO-474/m7-d3-intro",
    "name": "m7-d3-intro",
    "description": "Module 7: Introduction to D3.js",
    "language": "HTML",
    "topics": [],
    "readme": "# Module 7: Introduction to D3.js\n\n## Overview\nFinally. Given the foundational skills of the previous modules, we're ready to start working with D3. Rather than provide you with some canned code to make some visualizations, this module challenges you to understand the inner-workings of the D3 library. It's simple to find the code for awesome examples online, but without understanding how the library actually works, you won't be able to adopt code to suit your needs, or invent novel layouts.\n\nFirst, a clarification: even though we'll use D3 to build charts, it's **not**  a _\"charting library\"_: it's a **DOM manipulation library** optimized for working with data. The \"charts\" you make are up to you - you want a bar chart? Make some rectangles. A scatter-plot? Put some circles on the DOM. D3 will provide you with a robust set of tools for translating between data properties and visual properties: the rest is up to you. The module focuses on how to leverage the D3 library to create visual elements from your data. Once you understand how to drive a graphical layout with data, picking up the rest of the library will be (comparatively) trivial.\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Contents**\n\n- [Resources](#resources)\n- [Introductory Methods](#introductory-methods)\n- [From Data to DOM](#from-data-to-dom)\n  - [Challenge](#challenge)\n  - [Solution](#solution)\n    - [Selections](#selections)\n    - [The Data Join](#the-data-join)\n  - [Implications](#implications)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Resources\nThe open source community has generated a plethora of D3 resources: here are a few that I believe help capture the core concepts that you need to understand to harness the power of the library.\n\n- [Thinking with Joins](https://bost.ocks.org/mike/join/) _(Bostock)_\n- [D3 Selections](https://bost.ocks.org/mike/sele",
    "url": "https://github.com/INFO-474/m7-d3-intro",
    "last_updated": "2017-02-24T16:50:49+00:00"
  },
  {
    "full_name": "PatrickBaus/pyAsyncFluke5440B",
    "name": "pyAsyncFluke5440B",
    "description": "A Python AsyncIO library for the Fluke 5440B calibrator",
    "language": "Python",
    "topics": [],
    "readme": "[![pylint](../../actions/workflows/pylint.yml/badge.svg)](../../actions/workflows/pylint.yml)\n[![PyPI](https://img.shields.io/pypi/v/fluke5440b_async)](https://pypi.org/project/fluke5440b_async/)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/fluke5440b_async)\n![PyPI - Status](https://img.shields.io/pypi/status/fluke5440b_async)\n[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](LICENSE)\n[![code style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n# fluke5440b-async\nPython3 asyncio Fluke 5440B driver. This library requires Python [asyncio](https://docs.python.org/3/library/asyncio.html) and asyncio library for the GPIB adapter.\n\nThe library is fully type-hinted.\n\n> :warning: The following features are not supported (yet):\n> - External calibration: I do not have the means to test this. If you want to help, open a ticket and we can get this done\n> - Setting and retrieving DUUT tolerances and errors. I believe this is best done in software on the host computer and not done internally in the calibrator. If you really need that featuer open a ticket.\n\n## Supported GPIB Hardware\n| Device                                                                              |Supported|Tested|Comments|\n|-------------------------------------------------------------------------------------|--|--|--|\n| [asyncio Prologix GPIB library](https://github.com/PatrickBaus/pyAsyncPrologixGpib) |:heavy_check_mark:|:heavy_check_mark:|  |\n| [asyncio linux-gpib wrapper](https://github.com/PatrickBaus/pyAsyncGpib)            |:heavy_check_mark:|:heavy_check_mark:|  |\n\nTested using Linux, but should work on Mac OSX, Windows or any OS with Python support.\n\n## Documentation\nThe full documentation can be found on GitHub Pages:\n[https://patrickbaus.github.io/pyAsyncFluke5440B/](https://patrickbaus.github.io/pyAsyncFluke5440B/). I use the\n[Numpydoc](https://numpydoc.readthedocs.io/en/latest/format.html) style for docume",
    "url": "https://github.com/PatrickBaus/pyAsyncFluke5440B",
    "last_updated": "2025-08-13T07:28:23+00:00"
  },
  {
    "full_name": "karpathy/char-rnn",
    "name": "char-rnn",
    "description": "Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch",
    "language": "Lua",
    "topics": [],
    "readme": "\n# char-rnn\n\nThis code implements **multi-layer Recurrent Neural Network** (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. The context of this code base is described in detail in my [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n\nIf you are new to Torch/Lua/Neural Nets, it might be helpful to know that this code is really just a slightly more fancy version of this [100-line gist](https://gist.github.com/karpathy/d4dee566867f8291f086) that I wrote in Python/numpy. The code in this repo additionally: allows for multiple layers, uses an LSTM instead of a vanilla RNN, has more supporting code for model checkpointing, and is of course much more efficient since it uses mini-batches and can run on a GPU.\n\n## Update: torch-rnn\n\n[Justin Johnson](http://cs.stanford.edu/people/jcjohns/) (@jcjohnson) recently re-implemented char-rnn from scratch with a much nicer/smaller/cleaner/faster Torch code base. It's under the name [torch-rnn](https://github.com/jcjohnson/torch-rnn). It uses Adam for optimization and hard-codes the RNN/LSTM forward/backward passes for space/time efficiency. This also avoids headaches with cloning models in this repo. In other words, torch-rnn should be the default char-rnn implemention to use now instead of the one in this code base.\n\n## Requirements\n\nThis code is written in Lua and requires [Torch](http://torch.ch/). If you're on Ubuntu, installing Torch in your home directory may look something like: \n\n```bash\n$ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash\n$ git clone https://github.com/torch/distro.git ~/torch --recursive\n$ cd ~/torch; \n$ ./install.sh      # and enter \"yes\" at the end to modify your ",
    "url": "https://github.com/karpathy/char-rnn",
    "last_updated": "2025-09-01T13:53:46+00:00"
  },
  {
    "full_name": "paulhendricks/anonymizer",
    "name": "anonymizer",
    "description": "Anonymize data containing Personally Identifiable Information (PII) in R",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\nanonymizer\n==========\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/anonymizer)](http://cran.r-project.org/package=anonymizer) [![Downloads from the RStudio CRAN mirror](http://cranlogs.r-pkg.org/badges/anonymizer)](http://cran.rstudio.com/package=anonymizer) [![Build Status](https://travis-ci.org/paulhendricks/anonymizer.png?branch=master)](https://travis-ci.org/paulhendricks/anonymizer) [![Build status](https://ci.appveyor.com/api/projects/status/qu5j8q9wvit2i3pe/branch/master?svg=true)](https://ci.appveyor.com/project/paulhendricks/anonymizer/branch/master) [![codecov.io](http://codecov.io/github/paulhendricks/anonymizer/coverage.svg?branch=master)](http://codecov.io/github/paulhendricks/anonymizer?branch=master) [![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)\n\n`anonymizer` [anonymizes](https://en.wikipedia.org/wiki/Data_anonymization) data containing [Personally Identifiable Information](https://en.wikipedia.org/wiki/Personally_identifiable_information) (PII) using a combination of [salting](https://en.wikipedia.org/wiki/Salt_%28cryptography%29) and [hashing](https://en.wikipedia.org/wiki/Hash_function). You can find quality examples of data anonymization in R [here](http://jangorecki.github.io/blog/2014-11-07/Data-Anonymization-in-R.html), [here](http://stackoverflow.com/questions/10454973/how-to-create-example-data-set-from-private-data-replacing-variable-names-and-l), and [here](http://4dpiecharts.com/2011/08/23/anonymising-data/).\n\nInstallation\n------------\n\nYou can install the latest development version from CRAN:\n\n``` r\ninstall.packages(\"anonymizer\")\n```\n\nOr from GitHub with:\n\n``` r\nif (packageVersion(\"devtools\") < 1.6) {\n  install.packages(\"devtools\")\n}\ndevtools::install_github(\"paulhendricks/anonymizer\")\n```\n\nIf you encount",
    "url": "https://github.com/paulhendricks/anonymizer",
    "last_updated": "2025-08-16T20:52:27+00:00"
  },
  {
    "full_name": "pyenv/pyenv",
    "name": "pyenv",
    "description": "Simple Python version management",
    "language": "Roff",
    "topics": [
      "python",
      "shell"
    ],
    "readme": "# Simple Python Version Management: pyenv\n\n[![Join the chat at https://gitter.im/yyuu/pyenv](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/yyuu/pyenv?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\npyenv lets you easily switch between multiple versions of Python. It's\nsimple, unobtrusive, and follows the UNIX tradition of single-purpose\ntools that do one thing well.\n\nThis project was forked from [rbenv](https://github.com/rbenv/rbenv) and\n[ruby-build](https://github.com/rbenv/ruby-build), and modified for Python.\n\n### What pyenv _does..._\n\n* Lets you **change the global Python version** on a per-user basis.\n* Provides support for **per-project Python versions**.\n* Allows you to **override the Python version** with an environment\n  variable.\n* Searches for commands from **multiple versions of Python at a time**.\n  This may be helpful to test across Python versions with [tox](https://pypi.python.org/pypi/tox).\n\n\n### In contrast with pythonbrew and pythonz, pyenv _does not..._\n\n* **Depend on Python itself.** pyenv was made from pure shell scripts.\n    There is no bootstrap problem of Python.\n* **Need to be loaded into your shell.** Instead, pyenv's shim\n    approach works by adding a directory to your `PATH`.\n* **Manage virtualenv.** Of course, you can create [virtualenv](https://pypi.python.org/pypi/virtualenv)\n    yourself, or [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv)\n    to automate the process.\n\n\n----\n\n\n## Table of Contents\n\n* **[Installation](#installation)**\n  * [Getting Pyenv](#a-getting-pyenv)\n    * [Linux/UNIX](#linuxunix)\n      * [Automatic Installer](#1-automatic-installer-recommended)\n      * [Basic GitHub Checkout](#2-basic-github-checkout)\n    * [MacOS](#macos)\n      * [Homebrew in macOS](#homebrew-in-macos)\n    * [Windows](#windows)\n  * [Set up your shell environment for Pyenv](#b-set-up-your-shell-environment-for-pyenv)\n  * [Restart your shell](#c-restart-your-shell)\n  * [Install Python buil",
    "url": "https://github.com/pyenv/pyenv",
    "last_updated": "2025-09-02T09:45:33+00:00"
  },
  {
    "full_name": "soodoku/scaling",
    "name": "scaling",
    "description": "Scaling ML Products At Startups: A Practitioner's Guide",
    "language": "TeX",
    "topics": [],
    "readme": "## Scaling ML Products At Startups: A Practitioner's Guide\n\nHow do you scale a machine learning product at a startup? In particular, how do you serve a greater volume, velocity, and variety of queries cost-effectively? We break down costs into variable costs—the cost of serving the model and performant—and fixed costs—the cost of developing and training new models. We propose a framework for conceptualizing these costs, breaking them into finer categories, and limn ways to reduce costs. Lastly, since in our experience, the most expensive fixed cost of a machine learning system is the cost of identifying the root causes of failures and driving continuous improvement, we present a way to conceptualize the issues and share our methodology for the same.\n\n## Manuscript\n\n* [ms](ms/)\n\n## Authors\n\nAtul Dhingra and Gaurav Sood\n",
    "url": "https://github.com/soodoku/scaling",
    "last_updated": "2023-04-21T07:16:11+00:00"
  },
  {
    "full_name": "HenrikBengtsson/matrixStats",
    "name": "matrixStats",
    "description": "R package: Methods that Apply to Rows and Columns of Matrices (and to Vectors) ",
    "language": "R",
    "topics": [
      "r",
      "package",
      "matrix",
      "vector",
      "performance",
      "cran"
    ],
    "readme": "\n\n<div id=\"badges\"><!-- pkgdown markup -->\n<a href=\"https://CRAN.R-project.org/web/checks/check_results_matrixStats.html\"><img border=\"0\" src=\"https://www.r-pkg.org/badges/version/matrixStats\" alt=\"CRAN check status\"/></a> <a href=\"https://github.com/HenrikBengtsson/matrixStats/actions?query=workflow%3AR-CMD-check\"><img border=\"0\" src=\"https://github.com/HenrikBengtsson/matrixStats/actions/workflows/R-CMD-check.yaml/badge.svg?branch=develop\" alt=\"R CMD check status\"/></a>    <a href=\"https://ci.appveyor.com/project/HenrikBengtsson/matrixstats\"><img border=\"0\" src=\"https://ci.appveyor.com/api/projects/status/github/HenrikBengtsson/matrixStats?svg=true\" alt=\"Build status\"/></a> <a href=\"https://app.codecov.io/gh/HenrikBengtsson/matrixStats\"><img border=\"0\" src=\"https://codecov.io/gh/HenrikBengtsson/matrixStats/branch/develop/graph/badge.svg\" alt=\"Coverage Status\"/></a> <a href=\"https://github.com/rstats-gsoc/gsoc2015/wiki/table-of-proposed-coding-projects\"><img border=\"0\" src=\"https://img.shields.io/badge/GSoC-2015-blue\" alt=\"A Google Summer of Code 2015 project\"/></a>\n<a href=\"https://github.com/rstats-gsoc/gsoc2021/wiki/table-of-proposed-coding-projects\"><img border=\"0\" src=\"https://img.shields.io/badge/GSoC-2021-blue\" alt=\"A Google Summer of Code 2021 project\"/></a>\n</div>\n\n# matrixStats: Functions that Apply to Rows and Columns of Matrices (and to Vectors) \n\n## Introduction\n\nThe matrixStats package provides highly optimized functions for\ncomputing common summaries over rows and columns of matrices,\ne.g. `rowQuantiles()`. There are also functions that operate on\nvectors, e.g. `logSumExp()`. Their implementations strive to minimize\nboth memory usage and processing time. They are often remarkably\nfaster compared to good old `apply()` solutions. The calculations are\nmostly implemented in C, which allow us to optimize beyond what is\npossible to do in plain R. The package installs out-of-the-box on all\ncommon operating systems, including Linux, macOS and Windows.\n\n## Ex",
    "url": "https://github.com/HenrikBengtsson/matrixStats",
    "last_updated": "2025-08-25T16:28:32+00:00"
  },
  {
    "full_name": "justinelliotmeyers/INDIA_PINCODES",
    "name": "INDIA_PINCODES",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "",
    "url": "https://github.com/justinelliotmeyers/INDIA_PINCODES",
    "last_updated": "2025-05-27T06:30:03+00:00"
  },
  {
    "full_name": "datacamp/funneljoin",
    "name": "funneljoin",
    "description": "Join tables based on events occurring in sequence in a funnel.",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# funneljoin <img src='man/figures/logo.png' align=\"right\" height=\"139\" />\n\n[![Travis-CI Build\nStatus](https://travis-ci.org/datacamp/funneljoin.svg?branch=master)](https://travis-ci.org/datacamp/funneljoin)\n\nThe goal of funneljoin is to make it easy to analyze behavior funnels.\nFor example, maybe you’re interested in finding the people who visit a\npage and then register. Or you want all the times people click on an\nitem and add it to their cart within 2 days. These can all be answered\nquickly with funneljoin’s `after_join()` or `funnel_start()` and\n`funnel_step()`. As funneljoin uses dplyr, it can also work with remote\ntables, **but has only been tried on postgres**.\n\nFor more examples of how to use funneljoin, check out the vignette,\nwhich shows different types of joins and the optional arguments, or this\n[blog\npost](https://hookedondata.org/introducing-the-funneljoin-package/),\nwhich showcases how to use funneljoin analyze questions and answers on\nStackOverflow.\n\n## Installation\n\nYou can install this package from GitHub with\n[remotes](https://github.com/r-lib/remotes):\n\n``` r\nlibrary(remotes)\ninstall_github(\"datacamp/funneljoin\")\n```\n\n## after\\_join()\n\n``` r\nlibrary(dplyr)\nlibrary(funneljoin)\n```\n\nWe’ll take a look at two tables that come with the package, `landed` and\n`registered`. Each has a column `user_id` and `timestamp`.\n\nLet’s say we wanted to get the first time people landed and the first\ntime afterward they registered. We would `after_inner_join()` with a\n`first-firstafter` type:\n\n``` r\nlanded %>%\n  after_inner_join(registered, \n                   by_user = \"user_id\",\n                   by_time = \"timestamp\",\n                   type = \"first-firstafter\",\n                   suffix = c(\"_landed\", \"_registered\"))\n#> # A tibble: 5 x 3\n#>   user_id timestamp_landed timestamp_registered\n#>     <dbl> <date>           <date>              \n#> 1       1 2018-07-01       2018-07-02          \n#",
    "url": "https://github.com/datacamp/funneljoin",
    "last_updated": "2025-04-29T20:57:39+00:00"
  },
  {
    "full_name": "pbstark/Padova15",
    "name": "Padova15",
    "description": "Materials for a 30-hour course in Statistics for Engineers, given at University of Padova",
    "language": "CSS",
    "topics": [],
    "readme": "# Padova 15\n## Materials for a 30-hour course in Statistics for Engineers\n## Given at University of Padova, June&ndash;July 2015\n## Philip B. Stark http://www.stat.berkeley.edu/~stark\n\n### License: CC BY-NC-ND.  Non-commercial use with attribution; no derivative works.\n\nThis is a series of Jupyter notebooks using R.\nTo view it in a browser using nbviewer online without installing software,\nnavigate to http://nbviewer.ipython.org/github/pbstark/Padova15/blob/master/index.ipynb\n",
    "url": "https://github.com/pbstark/Padova15",
    "last_updated": "2023-06-05T21:16:43+00:00"
  },
  {
    "full_name": "kolesarm/Robust-Small-Sample-Standard-Errors",
    "name": "Robust-Small-Sample-Standard-Errors",
    "description": "Degrees of freedom adjustments for robust standard errors described in Imbens and Kolesár (2016, Review of Economics and Statistics)",
    "language": "R",
    "topics": [],
    "readme": "[![R-CMD-check](https://github.com/kolesarm/Robust-Small-Sample-Standard-Errors/workflows/R-CMD-check/badge.svg)](https://github.com/kolesarm/Robust-Small-Sample-Standard-Errors/actions) [![Coverage status](https://codecov.io/gh/kolesarm/Robust-Small-Sample-Standard-Errors/branch/master/graph/badge.svg)](https://app.codecov.io/github/kolesarm/Robust-Small-Sample-Standard-Errors?branch=master) [![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/dfadjust)](https://cran.r-project.org/package=dfadjust) [![Download statistics](https://cranlogs.r-pkg.org/badges/grand-total/dfadjust)](https://cran.r-project.org/package=dfadjust)\n\n# dfadjust\n\nThis package implements the small-sample degrees of freedom adjustments for\nrobust and cluster-robust standard errors in linear regression described in\n[Imbens and Kolesár (2016)](https://doi.org/10.1162/REST_a_00552).\n\nSee vignette [dfadjust](doc/dfadjust.pdf) for description of the package\n(available through `vignette(\"dfadjust\")` once package is installed), and the\npackage [manual](doc/manual.pdf) for documentation of the package functions.\n\n\n## Example\n\nNo clustering:\n``` r\nx <- sin(1:10)\ny <- tan(1:10)\nfm <- lm(y~x)\ndfadjustSE(fm)\n```\nClustering:\n``` r\nclustervar <- as.factor(c(rep(1, 6), rep(2, 2), rep(3, 2)))\ndfadjustSE(fm, clustervar)\n```\nHere we defined the first six observations to be in cluster 1, the next two in\ncluster 2, and the last three in cluster three.\n\nThe package handles cluster fixed effects, and large clusters. Computing the\nadjustment with one million observations and 50 clusters takes about 5 seconds:\n\n``` r\nN <- 10^6\nx <- sin(1:N)\ny <- seq(N)\nclustervar <- as.factor(rep(1:50, each=N/50))\nfm <- lm(y~x+clustervar)\n## Inference on x, i.e. second coefficient\ndfadjustSE(fm, ell=2, clustervar=clustervar)\n```\n\n## Installation\n\nYou can install the released version of `dfadjust` from\n[CRAN](https://CRAN.R-project.org/package=dfadjust) with:\n\n``` r\ninstall.packages(\"dfadjust\")\n```\n\nAlternatively, you can get the ",
    "url": "https://github.com/kolesarm/Robust-Small-Sample-Standard-Errors",
    "last_updated": "2024-12-19T15:33:35+00:00"
  },
  {
    "full_name": "uber-archive/plato-research-dialogue-system",
    "name": "plato-research-dialogue-system",
    "description": "This is the Plato Research Dialogue System, a flexible platform for developing conversational AI agents.",
    "language": "Python",
    "topics": [
      "nlp",
      "conversational-ai",
      "dialogue-systems",
      "conversational-ui",
      "conversational-agent",
      "machine-learning",
      "deep-learning"
    ],
    "readme": "![PlatoRDS-Logo](plato/resources/PlatoRDSLogo.png)\n\n# Plato Research Dialogue System\n\nThis is v0.3.1\n\nThe Plato Research Dialogue System is a flexible framework that can be used to \ncreate, train, and evaluate conversational AI agents in various environments. \nIt supports interactions through speech, text, or dialogue acts and each \nconversational agent can interact with data, human users, or other \nconversational agents (in a multi-agent setting). Every component of every \nagent can be trained independently online or offline and Plato provides an \neasy way of wrapping around virtually any existing model, as long as Plato's \ninterface is adhered to. \n\n\nPublication citations:\n\nAlexandros Papangelis, Mahdi Namazifar, Chandra Khatri, Yi-Chia Wang, \nPiero Molino, and Gokhan Tur, \"Plato Dialogue System: A Flexible conversational\n AI Research Platform\", ArXiv Preprint [[paper](http://arxiv.org/abs/2001.06463)]\n\nAlexandros Papangelis, Yi-Chia Wang, Piero Molino, and Gokhan Tur, \n“Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning”, \nSIGDIAL 2019 [[paper](https://arxiv.org/abs/1907.05507)]\n\n*Plato wrote several dialogues between characters who argue on a topic by asking\nquestions. Many of these dialogues feature Socrates including Socrates' trial. \n(Socrates was acquitted in a new trial held in Athens, Greece on May 25th 2012).*\n\n#### **News**\n**v0.2**: The main update from v0.1 is that Plato RDS is now provided as a package. This\nmakes it easier to create and maintain new conversational AI applications and\nall tutorials have been updated to reflect this. Plato now also comes with an\noptional GUI. Enjoy!\n\n# ReadMe Contents\n\n* [How does the Plato Research Dialogue System work?](#how-does-the-plato-research-dialogue-system-work)\n* [Installation](#installation)\n  * [Installing Plato from source code (Recommended)](#installing-plato-from-source-code-recommended)\n  * [Common Issues During Installation](#common-issues-during-installation)\n* [Running Plat",
    "url": "https://github.com/uber-archive/plato-research-dialogue-system",
    "last_updated": "2025-08-27T11:12:57+00:00"
  },
  {
    "full_name": "allr/purdue-fastr",
    "name": "purdue-fastr",
    "description": "",
    "language": "Java",
    "topics": [],
    "readme": "# FastR\n\nFastR implements the [R Language](http://www.r-project.org/). Currently,\nFastR can run the [R implementation](http://r.cs.purdue.edu/hg/r-shootout/)\nof the [Language Shootout Benchmarks](http://shootout.alioth.debian.org/) and \nthe [Benchmark 25 suite](http://r.research.att.com/benchmarks/).\n\nThis version of FastR has been published at [VEE'15](http://dl.acm.org/citation.cfm?id=2576205): \n\"A fast abstract syntax tree interpreter for R\". The project now continues [here](https://bitbucket.org/allr/fastr).\n\n## Quick Start\n\n1. download the [latest code](https://github.com/allr/fastr/archive/master.zip): `wget https://github.com/allr/fastr/archive/master.zip`\n2. unzip it: `unzip master.zip`\n3. build: `cd fastr-master ; ant`\n4. run the console: `./r.sh`\n5. run the binarytrees benchmark for size 5: `./r.sh --args 5 -f test/r/shootout/binarytrees/binarytrees.r`\n\n## Minimal Requirements\n\nTo run the binarytrees benchmark as shown above, FastR requires Java. All\nShootout benchmarks can be run this way, but some of the mandelbrot\nonly on Unix, as they spawn the `cat` process. \n\n## Full Installation\n\nTo run the benchmarks from the Benchmark 25 suite, and for best performance\nof all benchmarks, build native glue code which links FastR to the GNU-R\nMath Library, system Math library, and openBLAS.  The build scripts are\ntested on Ubuntu 13.10. Any platform supported by GNU-R and Java could\nbe supported by FastR.\n\n1. install Oracle JDK8 (for best performance); if you must use JDK7, customize `native/netlib-java/build.sh`\n2. set `JAVA_HOME` and `PATH` accordingly\n3. follow the steps in Quick Start\n4. install Ubuntu packages `r-base`, `r-mathlib`, `libopenblas-base`\n5. build glue code for system libraries and GNU-R: `cd native ; ./build.sh`\n6. build glue code for native BLAS and LAPACK: `cd netlib-java ; ./build.sh` \n7. check the glue code can be loaded: `cd ../.. ; ./nr.sh` should give output  \n`Using LAPACK: org.netlib.lapack.NativeLAPACK`  \n`Using BLAS: org.netlib.blas.Nat",
    "url": "https://github.com/allr/purdue-fastr",
    "last_updated": "2025-08-13T11:10:34+00:00"
  },
  {
    "full_name": "notnews/news-url-classifier",
    "name": "news-url-classifier",
    "description": "Use human readable portion of the URL to classify the kind of news",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## News URL Classifier\n\nUse strings in the URL to classify the kind of news. \n\nData from: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OTJMYQ\n\nHard/soft news labels for ~ 300 articles hand-coded. \n\n### Reference\n\n1. See this paper: https://osf.io/krhmq\n\n  \"Finally, we point curious researchers to the paths of URLs, which often contain human-readable  text  similar  to  titles.   For  example,  from  the  URL https://www.theguardian.com/politics/2023/aug/01/boris-johnson-swimming-pool-newts-oxfordshire, one could use “boris johnson swimming pool newts oxfordshire” as input for an NLP classifier. Although we know of no studies doing so, this seems like an intriguing possibility.\"\n\n2. For regex versions from Bakshy et al., see https://github.com/themains/rdomains/blob/master/R/not_news.R#L9 and https://github.com/notnews/notnews/blob/master/notnews/soft_news_url_cat_us.py\n",
    "url": "https://github.com/notnews/news-url-classifier",
    "last_updated": "2023-09-19T19:27:50+00:00"
  },
  {
    "full_name": "iunary/burrowkv",
    "name": "burrowkv",
    "description": "Burrowkv is a simple key-value store implementation in Python",
    "language": "Python",
    "topics": [
      "key-value",
      "key-value-store",
      "kv",
      "python3",
      "thread-safe"
    ],
    "readme": "[![Burrowkv](https://github.com/iunary/burrowkv/actions/workflows/app.yml/badge.svg)](https://github.com/iunary/burrowkv/actions/workflows/app.yml)\n\n# Burrowkv\n\nBurrowkv is a simple key-value store implementation in Python. It provides basic functionality to store and retrieve key-value pairs, as well as additional features such as JSON serialization and deserialization.\n\n## Features\n\n- Set a value for a given key.\n- Retrieve the value associated with a key.\n- Delete a key-value pair.\n- Check if a key exists in the store.\n- Get a list of all keys.\n- Get a list of all values.\n- Get a list of all key-value pairs.\n- Serialize the key-value store to JSON.\n- Deserialize JSON into the key-value store.\n\n## Installation\n\n```\npip install burrowkv\n```\n\n## Usage\n```python\nfrom burrowkv import burrowkv\n\n# Create a new instance of burrowkv\nstore = burrowkv()\n\n# Set a value for a key\nstore.set('name', 'John')\n\n# Retrieve the value associated with a key\nname = store.get('name')  # Returns 'John'\n\n# Delete a key-value pair\nstore.delete('name')\n\n# Check if a key exists\nif store.contains('name'):\n    print('Key exists')\nelse:\n    print('Key does not exist')\n\n# Get a list of all keys\nkeys = store.keys()  # Returns a list of keys\n\n# Get a list of all values\nvalues = store.values()  # Returns a list of values\n\n# Get a list of all key-value pairs\nitems = store.items()  # Returns a list of (key, value) tuples\n\n# Serialize the key-value store to JSON\n\njson_data = store.to_json()\n\n# Deserialize JSON into the key-value store\nstore.from_json(json_data)\n```\n\n## License\nThis project is licensed under the MIT License. See the LICENSE file for more information.\n",
    "url": "https://github.com/iunary/burrowkv",
    "last_updated": "2025-02-20T13:50:22+00:00"
  },
  {
    "full_name": "python3statement/python3statement.github.io",
    "name": "python3statement.github.io",
    "description": "",
    "language": "CSS",
    "topics": [],
    "readme": "# A pledge to migrate to Python 3.\n\nThis is the main website for a pledge to stop supporting Python 2 for free in\nopen source software.\n\n## History\n\nThis page is now an archive of part of the transition from Python 2 to 3.\n\nBy around 2015, when Python 2 support was originally planned to end, many\nimportant Python libraries and tools supported Python 3. But Python 2 still had\na lot of users, and projects needed to support both major versions. The end of\nPython 2 support was postponed to 2020, and some people argued that development\nof Python 2 should resume. It seemed like a real possibility that the end date\nwould be postponed again, and we'd need to support two versions of the language\nindefinitely.\n\nThe Python 3 statement was drawn up around 2016. Projects pledged to require\nPython 3 by 2020, giving other projects confidence that they could plan a similar\ntransition, and allowing downstream users to figure out their options without a\nnasty surprise. We didn't force people to move to Python 3, but if they wanted\nto stick with Python 2, they would stop getting new versions of our projects.\nThe focus was originally on the scientific Python ecosystem, with Jupyter and\nmatplotlib among the first projects involved, but in late 2017 it was expanded\nto any Python projects.\nA rapidly growing number of projects signed up as we approached 2020.\n\nThe long-term transition we hoped for has succeeded: in 2024 it is entirely\nnormal for projects to support only Python 3, simplifying maintainers' lives\nand letting us take full advantage of newer language features.\n\nThank-you to all of the people, in projects big and small, who contributed\ntheir support to the statement!\n\n## Run locally\n\nInstall Jekyll : `gem install jekyll`, `gem install github-pages`\n\nClone this locally, `cd` in the newly created directory.\n\nRun `jekyll serve -w` in one terminal, open your browser to `localhost:4000`.\n\nModify the various files, refresh your browser and enjoy.\n\nPRs welcomed.\n\n## Add your project\n\nW",
    "url": "https://github.com/python3statement/python3statement.github.io",
    "last_updated": "2025-08-19T16:48:38+00:00"
  },
  {
    "full_name": "soodoku/partisan-gaps",
    "name": "partisan-gaps",
    "description": "How do (biased) guessing encouraging features and guessing agnostic coding techniques affect the partisan gap?  ",
    "language": "Jupyter Notebook",
    "topics": [
      "partisanship",
      "knowledge-gaps",
      "partisan-gaps"
    ],
    "readme": "## A Measurement Gap? Effect of Survey Instrument and Scoring on the Partisan Knowledge Gap\n\nThis repository contains data, code, and manuscript source for the paper \"_A Measurement Gap? Effect of Survey Instrument and Scoring on the Partisan Knowledge Gap_\". The code for this paper is written in R and Stata (Python sparingly). The manuscript in pdf is compiled using LaTeX. The manuscript and the key results can be reproduced either by running the scripts directly or using the makefiles `make` utilities. \n\n<p align=\"center\">\n  <img width=\"100%\" src=\"figs/partisan-gap-by-item-arm.png\">\n</p>\n\n### Manuscript preparation\nThe Overleaf repository for the manuscript is linked to its own [GitHub repository](https://github.com/LSYS/overleaf-partisan-gap) and included as a Git submodule in this repository. See  `./overleaf-partisan-gap/`. When cloning or pulling this repo, the submodule metadata (via `.gitsubmodules`) and the folder for the manuscript submodule are included. However, the actual contents of the manuscript are not included. To get the contents, `cd` into `overleaf-partisan-gap/` submodule folder, do a `git submodule init` and then `git submodule update`.\n\nThe `./Makefile` includes a recipe to update all [output](#output-of-scripts) of scripts into the `./overleaf-partisan-gap/` submodule folder. Type `make update` to do so. The `./overleaf-partisan-gap/` repository also includes a Makefile to compile the manuscript in LaTeX and clean up auxiliary files using `latexmk`. Type `make ms` from `./overleaf-partisan-gap/` to compile the [manuscript](https://github.com/LSYS/overleaf-partisan-gap/blob/main/ms/partisan_gap.pdf).\n\n### Output of scripts\nSee [here](tabs/) for the tables (TeX) and [here](figs/) for the figures (pdf, png).\n\n### Key data\n* [Study 1 (MTurk sample 1)](data/turk/mturk-recoded.csv)\n* [Study 2 (YouGov)](data/survey_exp/selex.csv)\n* [Study 3 (Texas Lyceum)](data/tx_lyceum/Texas%20Lyceum%202012%20Data.dta)\n* [Study 4 (MTurk sample 2)](data/mturk_hk/m",
    "url": "https://github.com/soodoku/partisan-gaps",
    "last_updated": "2025-04-27T05:04:47+00:00"
  },
  {
    "full_name": "hrbrmstr/indialights",
    "name": "indialights",
    "description": "💡 Tools to Work with the ‘India Lights’ ‘API’",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "dmsp",
      "indialights",
      "sodd"
    ],
    "readme": "\n# indialights\n\nTools to Work with the ‘India Lights’ ‘API’\n\n## Description\n\nThe ‘India Lights’ ‘API’ shows light output at night for 20 years, from\n1993 to 2013, for 600,000 villages across India. The ‘Defense\nMeteorological Satellite Program’ (‘DMSP’), run by the ‘U.S.’\n‘Department of Defense’, has taken pictures of the Earth every night\nfor 20 years. Researchers at the ‘University of Michigan’ used the\n‘DMSP’ images to extract the data and provide it in tabular form.\nThe ‘India Lights’ ‘API’ provides the data at convenient endpoints that\nallows you to look at specific time intervals and administration levels.\nTools are provided to query these ‘API’ endpoints.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `ial_district`: Time series for a single district\n  - `ial_districts`: Time series for all districts in a state\n  - `ial_regions`: List of all the regions (districts) with display name\n    and id\n  - `ial_state`: Time series for a single state\n  - `ial_states`: Time series for all states in the nation\n  - `ial_villages`: Time series for comma separated list of villages\n\n## Installation\n\n``` r\ndevtools::install_git(\"https://gitlab.com/hrbrmstr/indialights.git\")\n```\n\n## Usage\n\n``` r\nlibrary(indialights)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"indialights\")\n```\n\n    ## [1] '0.1.0'\n\n### The examples from <http://api.nightlights.io/>\n\nTime series for a single district:\n\n``` r\nial_district(\"gujarat-anand\", \"1993.3\", \"1993.4\")\n```\n\n    ## # A tibble: 2 x 10\n    ##   key            year month satellite count vis_median quintile1 quintile2 quintile3 quintile4\n    ## * <chr>         <int> <int> <chr>     <int> <chr>      <chr>     <chr>     <chr>     <chr>    \n    ## 1 gujarat-anand  1993     3 F10        6619 0.6481     -2.1790   -0.2632   1.7368    4.5750   \n    ## 2 gujarat-anand  1993     4 F10        6497 1.0545     -4.7625   -0.2222   2.2115    4.9867\n\nTime series for all districts in a state:\n\n``` r\nial_state(\n  state_id = \"g",
    "url": "https://github.com/hrbrmstr/indialights",
    "last_updated": "2025-06-28T12:34:57+00:00"
  },
  {
    "full_name": "itfoundry/hind",
    "name": "hind",
    "description": "Hind, a Devanagari + Latin family for Google Fonts.",
    "language": "Python",
    "topics": [],
    "readme": "# Hind\n\nHind is an open source typeface supporting the Devanagari and Latin scripts. Developed explicitly for use in User Interface design, the Hind font family includes five styles. Hind’s letterforms have a humanist-style construction, which is paired with seemingly monolinear strokes. Most of these strokes have flat endings: they either terminate with a horizontal or a vertical shear, rather than on a diagonal. This helps create clear-cut counter forms between the characters. In addition to this, Hind’s letterforms feature open apertures. The entire typeface family feels very legible when used to set text.\n\nThe Devanagari and Latin script components are scaled in relation to each other so that the Devanagari headline falls just below the Latin capital-height. In other words, the Devanagari base characters are 94% as tall as the Latin uppercase. Text set in the Devanagari script sits nicely alongside the Latin lowercase, too. Hind’s Devanagari vowel marks take forms that tends toward the traditional end of the design spectrum, while the knotted terminals, etc. inside of the base characters feature a treatment that appears more contemporary.\n\nEach font in the Hind family has 1146 glyphs, which include hundreds of unique Devanagari conjuncts. These ensure full support for the major languages written with the Devanagari script. The Latin component’s character set is a basic western one, which enables typesetting in English and the other Western European languages. Hind is a solid choice for UI design, and a wise selection for electronic display embedding.\n\nManushi Parikh designed Hind for the Indian Type Foundry, who first published the fonts in 2014.\n\n## Code base\n\nThis working directory is forked from the common code base, [ITF Base Devanagari (for Google Fonts)](https://github.com/itfoundry/base-devanagari-gf).\n\n## Dependencies\n\n- [Adobe Font Development Kit for OpenType](http://www.adobe.com/devnet/opentype/afdko.html) (AFDKO), version 2.5 build 63209 (Sep 18 201",
    "url": "https://github.com/itfoundry/hind",
    "last_updated": "2025-08-22T16:15:49+00:00"
  },
  {
    "full_name": "jeremybmerrill/beauvoir",
    "name": "beauvoir",
    "description": "Guess a person's gender by their first name. Caveats apply.",
    "language": "Ruby",
    "topics": [],
    "readme": "Beauvoir\n========\n\nBeauvoir is a gem for guessing a person's gender by their first name. Caveats apply (see below)\n\nBeauvoir uses more precise data sourced from [Open Gender Tracker](http://opengendertracking.org/)'s [Global Name Data](https://github.com/OpenGenderTracking/globalnamedata). Beauvoir lets you set avg and lower bounds and choose countries from which to draw data (so far US, UK only, more to come soon).\n\nCaution\n-------\nThis is pre-alpha software. The API will change, I guarantee it.\n\nCaveats\n-------\n\nIt's important to note that many people identify as neither a man nor a woman. It's important, too, to note that many people who do identify as male or female have names for which most other people with that name identify as a different gender. All of these people deserve not to be misgendered.\n\nNevertheless, automatically classifying people by apparent gender can be a very useful tool to perform censuses of communities or publications to detect and quantify perhaps-invisible bias. VIDA is a pioneer in performing theses censuses, but their \"Count\" is limited by a manual methodology that depends hundreds of person-hours of labor. There is a place for more automated counts and Beauvoir can help, but if you plan to publish a count like this, you should be careful. Beauvoir's confidence thresholds are set very high by default on purpose, you shouldn't lower them unless you take other steps to make sure that you're very unlikely to misgender someone; you should also be prepared to be responsive and respectful if you do. You should include your methodology, prominently. You might also consider emphasizing aggregate numbers over your mapping of individual people's names to genders.\n\nUsage\n-----\n\nBasic case:\n````\nrequire 'beauvoir'\nb = Beauvoir::Categorizer.new\n\nb.guess(\"John\")\n=> :male\nb.guess(\"Mary\")\n=> :female\nb.guess(\"Sam\")\n=> :unknown\n\nb.estimated_female_value(\"Sam\")\n=> 0.0103360994972961\nb.estimated_male_value(\"Sam\")\n=> 0.9896639005027039\n````\n\nSet other opt",
    "url": "https://github.com/jeremybmerrill/beauvoir",
    "last_updated": "2025-08-06T20:21:41+00:00"
  },
  {
    "full_name": "geomorphR/geomorph",
    "name": "geomorph",
    "description": "Geomorph is a software package for performing all stages of geometric morphometric shape analysis of landmark points and curves in 2-and-3-dimensions as well as 3D surfaces in the R statistical computing environment. This repository is dedicated to providing stable and beta versions between CRAN uploads",
    "language": "R",
    "topics": [],
    "readme": "# geomorph\r\nGeomorph is a software package for performing all stages of geometric morphometric shape analysis of 2- and 3-dimensional landmark points, as well as semilandmarks on curves and surfaces, in the R statistical computing environment. This repository is dedicated to providing beta versions between CRAN uploads.\r\n\r\n### To install the current geomorph R-package from CRAN:\r\n\r\n<i> Within R:</i>\r\n\r\n<code> install.packages(\"geomorph\") </code>\r\n\r\nFor Mac users:  please also install XQuartz from <https://www.xquartz.org/>. This allows the library(rgl) to function.\r\n\r\n### To install the current version of geomorph R-package from Github using devtools:\r\n\r\n<i> Within R:</i>\r\n\r\n<code> install.packages(\"devtools\")</code>\r\n\r\n<code> devtools::install_github(\"geomorphR/geomorph\", ref = \"Stable\", build_vignettes = TRUE)</code>\r\n\r\nThis installs a stable release of the current version of geomorph on CRAN, allowing us to quickly fix errors that slip thorough the cracks and are uploaded with the CRAN version.\r\n\r\n",
    "url": "https://github.com/geomorphR/geomorph",
    "last_updated": "2025-07-30T16:16:14+00:00"
  },
  {
    "full_name": "edwindj/chunked",
    "name": "chunked",
    "description": "Chunkwise Text-file Processing for 'dplyr'",
    "language": "R",
    "topics": [
      "dplyr",
      "chunk",
      "database",
      "r"
    ],
    "readme": "\n# chunked\n\n[![version](https://cran.r-project.org/package=chunked)](https://cran.r-project.org/package=chunked)\n[![Downloads](https://cranlogs.r-pkg.org/badges/chunked)](https://cran.r-project.org/package=chunked)\n[![R-CMD-check](https://github.com/edwindj/chunked/workflows/R-CMD-check/badge.svg)](https://github.com/edwindj/chunked/actions)\n[![Coverage\nStatus](https://coveralls.io/repos/edwindj/chunked/badge.svg?branch=master&service=github)](https://coveralls.io/github/edwindj/chunked?branch=master)\nR is a great tool, but processing data in large text files is\ncumbersome. `chunked` helps you to process large text files with *dplyr*\nwhile loading only a part of the data in memory. It builds on the\nexcellent R package [*LaF*](https://github.com/djvanderlaan/LaF).\n\nProcessing commands are written in dplyr syntax, and `chunked` (using\n`LaF`) will take care that chunk by chunk is processed, taking far less\nmemory than otherwise. `chunked` is useful for **select**-ing columns,\n**mutate**-ing columns and **filter**-ing rows. It is less helpful in\n**group**-ing and **summarize**-ation of large text files. It can be\nused in data pre-processing.\n\n## Install\n\n‘chunked’ can be installed with\n\n``` r\ninstall.packages('chunked')\n```\n\nbeta version with:\n\n``` r\ninstall.packages('chunked', repos=c('https://cran.rstudio.com', 'https://edwindj.github.io/drat'))\n```\n\nand the development version with:\n\n``` r\ndevtools::install_github('edwindj/chunked')\n```\n\nEnjoy! Feedback is welcome…\n\n# Usage\n\n## Text file -> process -> text file\n\nMost common case is processing a large text file, select or add columns,\nfilter it and write the result back to a text file\n\n``` r\n  read_chunkwise(\"./large_file_in.csv\", chunk_size=5000) %>% \n  select(col1, col2, col5) %>%\n  filter(col1 > 10) %>% \n  mutate(col6 = col1 + col2) %>% \n  write_chunkwise(\"./large_file_out.csv\")\n```\n\n`chunked` will write process the above statement in chunks of 5000\nrecords. This is different from for example `read.csv` which reads",
    "url": "https://github.com/edwindj/chunked",
    "last_updated": "2025-08-23T07:48:09+00:00"
  },
  {
    "full_name": "lmullen/slavery-map",
    "name": "slavery-map",
    "description": "The Spread of U.S. Slavery, 1790-1860",
    "language": "CSS",
    "topics": [],
    "readme": "A [map of the spread of slavery][] in the United States, 1790 to 1860,\nbased on U.S. Census data from [NHGIS][].\n\nMap by [Lincoln Mullen][]. Code licensed [MIT][]. Built with [D3.js][].\n\n## Citation\n\nIf you use this map in your research, I would appreciate a citation.\nHere is the suggested form:\n\n> Lincoln Mullen, \"The Spread of U.S. Slavery, 1790--1860,\" interactive\n> map, <http://lincolnmullen.com/projects/slavery/>, doi:\n> 10.5281/zenodo.9825.\n\n![][]\n\nYou should also cite the NHGIS:\n\n> Minnesota Population Center, *National Historical Geographic\n> Information System: Version 2.0* (Minneapolis, MN: University of\n> Minnesota, 2011), <http://www.nhgis.org>.\n\n  [map of the spread of slavery]: http://lincolnmullen.com/projects/slavery/\n  [NHGIS]: https://www.nhgis.org/\n  [Lincoln Mullen]: http://lincolnmullen.com\n  [MIT]: http://lmullen.mit-license.org/\n  [D3.js]: http://d3js.org\n  []: https://zenodo.org/badge/3774/lmullen/slavery-map.png\n",
    "url": "https://github.com/lmullen/slavery-map",
    "last_updated": "2024-02-02T12:21:46+00:00"
  },
  {
    "full_name": "kevinushey/sourcetools",
    "name": "sourcetools",
    "description": "Tools for reading, tokenizing, and parsing R code.",
    "language": "C++",
    "topics": [],
    "readme": "\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/kevinushey/sourcetools/workflows/R-CMD-check/badge.svg)](https://github.com/kevinushey/sourcetools/actions)\n<!-- badges: end -->\n\n# sourcetools\n\nTools for reading, tokenizing, and (eventually) parsing `R` code.\n\n## Getting Started\n\nYou can install `sourcetools` from CRAN with:\n\n\n```r\ninstall.packages(\"sourcetools\")\n```\n\nOr, you can install the development version from GitHub with:\n\n\n```r\ndevtools::install_github(\"kevinushey/sourcetools\")\n```\n\n## Reading\n\n`sourcetools` comes with a couple fast functions for reading\nfiles into `R`.\n\nUse `read()` and `read_lines()` to quickly read a file into\n`R` as character vectors. `read_lines()` handles both\nWindows style `\\r\\n` line endings, as well as Unix-style\n`\\n` endings. Performance is on par with the readers\nprovided by the\n[readr](https://cran.r-project.org/package=readr) package.\n\n\n```r\ntext <- replicate(10000, {\n  paste(sample(letters, 200, TRUE), collapse = \"\")\n})\nfile <- tempfile()\ncat(text, file = file, sep = \"\\n\")\nmb <- microbenchmark::microbenchmark(times = 10,\n  base::readLines(file),\n  readr::read_lines(file),\n  sourcetools::read_lines(file)\n)\nsm <- summary(mb)\nprint(sm[c(\"expr\", \"mean\", \"median\")], digits = 3)\n```\n\n```\n##                            expr  mean median\n## 1         base::readLines(file) 17.29  16.22\n## 2       readr::read_lines(file) 30.70   8.11\n## 3 sourcetools::read_lines(file)  6.67   6.43\n```\n\n```r\nunlink(file)\n```\n\n## Tokenization\n\n`sourcetools` provides the `tokenize_string()` and\n`tokenize_file()` functions for generating a tokenized\nrepresentation of R code. These produce 'raw' tokenized\nrepresentations of the code, with each token's value as a\nstring, and a recorded row, column, and type:\n\n\n```r\ntokenize_string(\"if (x < 10) 20\")\n```\n\n```\n##    value row column       type\n## 1     if   1      1    keyword\n## 2          1      3 whitespace\n## 3      (   1      4    bracket\n## 4      x   1      5     symbol\n## 5          1      6 whi",
    "url": "https://github.com/kevinushey/sourcetools",
    "last_updated": "2025-05-06T23:04:23+00:00"
  },
  {
    "full_name": "andrewallenbruce/provider",
    "name": "provider",
    "description": "Public Healthcare Provider APIs :stethoscope:",
    "language": "R",
    "topics": [
      "cms",
      "healthcare",
      "healthcare-data",
      "npi",
      "nppes",
      "rstats",
      "medicare",
      "physician",
      "network-analysis",
      "api"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# provider <img src=\"man/figures/logo.svg\" align=\"right\" height=\"200\" />\n\n> Providing easy access to [healthcare\n> provider](https://en.wikipedia.org/wiki/Health_care_provider) data\n> through publicly available APIs.\n\n<!-- badges: start -->\n\n![GitHub R package\nversion](https://img.shields.io/github/r-package/v/andrewallenbruce/provider?style=flat-square&logo=R&label=Package&color=%23192a38)\n[![R-CMD-check](https://github.com/andrewallenbruce/provider/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/andrewallenbruce/provider/actions/workflows/R-CMD-check.yaml)\n[![License:\nMIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://choosealicense.com/licenses/mit/)\n[![code\nsize](https://img.shields.io/github/languages/code-size/andrewallenbruce/provider.svg)](https://github.com/andrewallenbruce/provider)\n[![last\ncommit](https://img.shields.io/github/last-commit/andrewallenbruce/provider.svg)](https://github.com/andrewallenbruce/provider/commits/main)\n[![Codecov test\ncoverage](https://codecov.io/gh/andrewallenbruce/provider/branch/main/graph/badge.svg)](https://app.codecov.io/gh/andrewallenbruce/provider?branch=main)\n[![CodeFactor](https://www.codefactor.io/repository/github/andrewallenbruce/provider/badge)](https://www.codefactor.io/repository/github/andrewallenbruce/provider)\n<!-- badges: end -->\n\n## :package: Installation\n\nYou can install **`provider`** from [GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"andrewallenbruce/provider\")\n```\n\n## :beginner: Usage\n\n``` r\nlibrary(provider)\nlibrary(dplyr)\nlibrary(tidyr)\n```\n\n### `affiliations()`\n\n``` r\naffiliations(facility_ccn = 370781)\n```\n\n    #> # A tibble: 16 × 7\n    #>    npi        pac        first     middle last        facility_type facility_ccn\n    #>    <chr>      <chr>      <chr>     <chr>  <chr>       <fct>         <chr>       \n    #>  1 1053547950 7618131160 AARON     S      SIZELOVE    ",
    "url": "https://github.com/andrewallenbruce/provider",
    "last_updated": "2025-08-23T15:56:32+00:00"
  },
  {
    "full_name": "propublica/propertyassessments",
    "name": "propertyassessments",
    "description": "Analysis behind the \"How the Cook County Assessor Failed Taxpayers\"",
    "language": "R",
    "topics": [],
    "readme": "# How We Analyzed Commercial and Industrial Property Assessments in Chicago and Cook County\n\nThis code was used for the analysis in the story,[\"How the Cook County Assessor Failed Taxpayers.\"](https://features.propublica.org/the-tax-divide/cook-county-commercial-and-industrial-property-tax-assessments/)\n\nMethodology for this project can be found [here.](https://projects.propublica.org/graphics/the-tax-divide-analysis)\n\nData can be found in the ProPublica Data store [here.](https://www.propublica.org/datastore/dataset/cook-county-commercial-property-tax-assessments)\n\nThe scripts used for this project are all R scripts that can be run in R Studio. \n\n## To examine the fairness and accuracy of commercial and industrial property assessments under Cook County Assessor Joseph Berrios, ProPublica Illinois and the Chicago Tribune conducted three separate analyses:\n\n- An examination of unchanged assessor’s initial valuations over multiple reassessment periods. Methodology for this analysis is [here.](https://projects.propublica.org/graphics/the-tax-divide-analysis#first-pass-assessment-analysis) Analysis code can be found in [`first-pass.R`](https://github.com/propublica/propertyasessments/blob/master/first-pass.R) \n\n- A sales ratio study comparing the assessor’s valuations to actual sales prices. Methodology for this analysis is [here.](https://projects.propublica.org/graphics/the-tax-divide-analysis#sales-ratio-studies-2011-2015) Analysis code can be found in [`sales-ratio-study.R`](https://github.com/propublica/propertyasessments/blob/master/sales-ratio-study.R)\n\n- A look at appeals of commercial and industrial assessments in Cook County. Methodology for this analysis is [here.](https://projects.propublica.org/graphics/the-tax-divide-analysis#appeals-analysis) Analysis code can be found in [`appeals-analysis.R`](https://github.com/propublica/propertyasessments/blob/master/appeals-analysis.R)\n\n",
    "url": "https://github.com/propublica/propertyassessments",
    "last_updated": "2024-10-18T19:48:21+00:00"
  },
  {
    "full_name": "resume/resume.github.com",
    "name": "resume.github.com",
    "description": "Resumes generated using the GitHub informations",
    "language": "JavaScript",
    "topics": [],
    "readme": "# [GitHub Résumé](https://resume.github.io/)\n\n**A service that creates a résumé based on your GitHub repos/activity.**\n\nGitHub Résumé is **opt-in**. To make your resume visible, just **star** [this project](https://github.com/resume/resume.github.com). To view your résumé, go to `https://resume.github.io/?yourusername` or follow the instructions on the [home page](https://resume.github.io/).\n\nGreat for all the tech-savy bosses who want to have a **quick view** of person's git/github activity, _before the interview_.\n\n### Development\n\nTo run the app in development mode:\n\n    $ rackup config.ru\n\n(You must have Ruby and the rack gem installed.)\n",
    "url": "https://github.com/resume/resume.github.com",
    "last_updated": "2025-09-02T02:38:33+00:00"
  },
  {
    "full_name": "timelyportfolio/sunburstR",
    "name": "sunburstR",
    "description": "R htmlwidget for interactive sunburst plots",
    "language": "JavaScript",
    "topics": [
      "rstats",
      "htmlwidgets",
      "d3",
      "visualization",
      "package",
      "interactive",
      "javascript",
      "r",
      "chart"
    ],
    "readme": "<!-- badges: start -->\n[![R-CMD-check](https://github.com/timelyportfolio/sunburstR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/timelyportfolio/sunburstR/actions/workflows/R-CMD-check.yaml)\n[![CRAN status](https://www.r-pkg.org/badges/version/sunburstR)](https://CRAN.R-project.org/package=sunburstR)\n<!-- badges: end -->\n\n### Help Me/Pay Me to Use, Improve, and Extend\n\n`sunburstR` has already seen extensive use in many projects across multiple domains.  If you have any interest in collaborating with me on this project or applying `sunburstR`, please let me know (see [Time Isn't Money](https://buildingwidgets.com/2016-12-09_post_break/)).\n\n\n# sunburstR\n\nEasily make these interactive `d3.js` sequence sunburst charts in `R` originally modeled on this [example](https://gist.github.com/kerryrodden/7090426) from Kerry Rodden.  In addition, `sunburstr` provides another beautiful and functional version using [`d2b`](https://github.com/d2bjs/d2b) from Kevin Warne.\n\n### Examples\n\nSee the [examples](https://github.com/timelyportfolio/sunburstR/tree/master/inst/examples).\n\n### Improve/Iterate\n\nThese are still far from feature-complete.  I would love your input, feedback, and comments.\n\n### Code of Conduct\n\nPlease note that this project is released with a [Contributor Code of Conduct](https://github.com/timelyportfolio/sunburstR/blob/master/CONDUCT.md). By participating in this project you agree to abide by its terms.\n",
    "url": "https://github.com/timelyportfolio/sunburstR",
    "last_updated": "2025-08-20T19:37:04+00:00"
  },
  {
    "full_name": "juliasilge/tidy-text-mining",
    "name": "tidy-text-mining",
    "description": "Manuscript of the book \"Tidy Text Mining with R\" by Julia Silge and David Robinson",
    "language": "TeX",
    "topics": [
      "book",
      "text-mining",
      "tidyverse",
      "bookdown",
      "r"
    ],
    "readme": "This is the repo for the book [**Text Mining with R: A Tidy Approach**](http://tidytextmining.com/), by Julia Silge and David Robinson.\n\n[![Build Status](https://github.com/juliasilge/tidy-text-mining/workflows/bookdown/badge.svg)](https://github.com/juliasilge/tidy-text-mining/actions)\n[![Netlify Status](https://api.netlify.com/api/v1/badges/2e9741eb-97a9-4d49-949e-1d668735d836/deploy-status)](https://app.netlify.com/sites/tidy-text-mining/deploys)\n\nPlease note that this work is written under a [Contributor Code of Conduct](CONDUCT.md) and released under a [CC-BY-NC-SA license](https://creativecommons.org/licenses/by-nc-sa/3.0/us/). By participating in this project (for example, by submitting a [pull request](https://github.com/juliasilge/tidy-text-mining/issues) with suggestions or edits) you agree to abide by its terms.\n",
    "url": "https://github.com/juliasilge/tidy-text-mining",
    "last_updated": "2025-08-30T13:59:39+00:00"
  },
  {
    "full_name": "hansthompson/rusps",
    "name": "rusps",
    "description": "R package for using USPS' free address validation api",
    "language": "R",
    "topics": [],
    "readme": "# rusps -   Validate And Standardize US Addresses \nHave you ever needed to do inner joins of addresses from different data sets? \nHook into the USPS api using this package!  \n### \n```\ndevtools::install_github(\"hansthompson/rusps\")\nlibrary(rusps)\nlibrary(XML)\nusername <- 'XXXYYYYYZZZZ' # get this quickly and freely by signing up at https://registration.shippingapis.com/ (not commercial).\nstreet1 <- '333 W raspberry road'\nstreet2 <- '333  raspberry rd'\ncity   <- 'anchorage'\nstate  <- 'ak'\n\nvalidate_address_usps(username, street, city, state)\n\n#  Address.Address2       Address.City      Address.State       Address.Zip5       Address.Zip4  Address..attrs.ID \n# \"333 RASPBERRY RD\"        \"ANCHORAGE\"               \"AK\"            \"99518\"             \"1565\"                \"0\" \n\n# Check if two diffent Addresses resolve to the same address\nall(validate_address_usps(username, street, city, state)  == validate_address_usps(username, street, city, state))\n# TRUE\n```\n\nMore Documentation Here:\nhttps://www.usps.com/business/web-tools-apis/address-information-api.htm#_Toc410982986\n",
    "url": "https://github.com/hansthompson/rusps",
    "last_updated": "2023-05-05T04:18:13+00:00"
  },
  {
    "full_name": "imsnif/bandwhich",
    "name": "bandwhich",
    "description": "Terminal bandwidth utilization tool",
    "language": "Rust",
    "topics": [
      "cli",
      "networking",
      "bandwidth",
      "dashboard"
    ],
    "readme": "# bandwhich\n\n![demo](res/demo.gif)\n\nThis is a CLI utility for displaying current network utilization by process, connection and remote IP/hostname\n\n## Table of contents\n\n- [bandwhich](#bandwhich)\n  - [Table of contents](#table-of-contents)\n  - [Project status](#project-status)\n  - [How does it work?](#how-does-it-work)\n  - [Installation](#installation)\n    - [Downstream packaging status](#downstream-packaging-status)\n    - [Download a prebuilt binary](#download-a-prebuilt-binary)\n  - [Building from source](#building-from-source)\n    - [Cross-compiling](#cross-compiling)\n      - [Android](#android)\n  - [Post install (Linux)](#post-install-linux)\n    - [1. `setcap`](#1-setcap)\n      - [Capabilities explained](#capabilities-explained)\n    - [2. `sudo` (or alternative)](#2-sudo-or-alternative)\n  - [Post install (Windows)](#post-install-windows)\n  - [Usage](#usage)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n## Project status\n\nThis project is in passive maintenance. Critical issues will be addressed, but\nno new features are being worked on. However, this is due to a lack of funding\nand/or manpower more than anything else, so pull requests are more than welcome.\nIn addition, if you are able and willing to contribute to this project long-term,\nwe would like to invite you to apply for co-maintainership.\n\nFor more details, see [The Future of Bandwhich #275](https://github.com/imsnif/bandwhich/issues/275).\n\n## How does it work?\n\n`bandwhich` sniffs a given network interface and records IP packet size, cross referencing it with the `/proc` filesystem on linux, `lsof` on macOS, or using WinApi on windows. It is responsive to the terminal window size, displaying less info if there is no room for it. It will also attempt to resolve ips to their host name in the background using reverse DNS on a best effort basis.\n\n## Installation\n\n### Downstream packaging status\n\nFor detailed instructions for each platform, see [INSTALL.md](INSTALL.md).\n\n<a href=\"https://repology.or",
    "url": "https://github.com/imsnif/bandwhich",
    "last_updated": "2025-09-02T00:53:38+00:00"
  },
  {
    "full_name": "shubhvjain/ant",
    "name": "ant",
    "description": "Assist-ant",
    "language": "Python",
    "topics": [],
    "readme": "",
    "url": "https://github.com/shubhvjain/ant",
    "last_updated": "2025-01-15T15:29:28+00:00"
  },
  {
    "full_name": "rocker-org/rocker",
    "name": "rocker",
    "description": "R configurations for Docker",
    "language": "Shell",
    "topics": [
      "docker",
      "r"
    ],
    "readme": "[![Build Status](https://travis-ci.org/rocker-org/rocker.svg?branch=master)](https://travis-ci.org/rocker-org/rocker)\n\n# rocker #\n\n## Overview ##\n\nThis repository contains Dockerfiles for different Docker containers of interest to R users. \n\n## Getting Started ##\n\nTo get started right away, ensure you have [Docker installed](https://docs.docker.com/installation/) and start a container with `docker run --rm -ti rocker/r-base` (see [here](https://docs.docker.com/engine/reference/run/) for the `docker run` command options). In this case we are starting the `r-base` container (the base package to build from) in an interactive mode, see below for details of the other containers currently available. To get started on the `rstudio` container or its derivative containers (eg. `hadleyverse` and `ropensci`) you need to open a port, see the [instructions in the wiki](https://github.com/rocker-org/rocker/wiki/Using-the-RStudio-image). The [wiki](https://github.com/rocker-org/rocker/wiki) also contains further instructions and information on the project, including how to extend these images and contribute to development.\n\n## Status ##\n\nThis is work in progress; please read our [instructions to contributors](https://github.com/rocker-org/rocker/wiki/How-to-contribute) and talk to @eddelbuettel and @cboettig about how to get involved.\n\n## Base Docker Containers ##\n\n\nimage            | description                               | size   | metrics | build status \n---------------- | ----------------------------------------- | ------ | ------- | --------------\n[r-base](https://hub.docker.com/r/_/r-base)            |  Current R via apt-get with `debian:testing` & `unstable` repos  | [![](https://images.microbadger.com/badges/image/library/r-base.svg)](https://microbadger.com/images/library/r-base) | [![](https://img.shields.io/docker/pulls/library/r-base.svg)](https://hub.docker.com/r/library/r-base) |  [![](https://img.shields.io/docker/automated/rocker/r-base.svg)](https://hub.docker.",
    "url": "https://github.com/rocker-org/rocker",
    "last_updated": "2025-08-29T12:53:58+00:00"
  },
  {
    "full_name": "NVIDIA/caffe",
    "name": "caffe",
    "description": "Caffe: a fast open framework for deep learning.",
    "language": "C++",
    "topics": [],
    "readme": "# NVIDIA has discontinued support and maintenance for this repository. Everything is provided as-is with no further updates being accepted. Thanks for all the contributions and engagement!\n\n\n\n# Caffe\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by the Berkeley Vision and Learning Center ([BVLC](http://bvlc.eecs.berkeley.edu))\nand community contributors.\n\n# NVCaffe\n\nNVIDIA Caffe ([NVIDIA Corporation &copy;2017](http://nvidia.com)) is an NVIDIA-maintained fork\nof BVLC Caffe tuned for NVIDIA GPUs, particularly in multi-GPU configurations.\nHere are the major features:\n* **16 bit (half) floating point train and inference support**.\n* **Mixed-precision support**. It allows to store and/or compute data in either \n64, 32 or 16 bit formats. Precision can be defined for every layer (forward and \nbackward passes might be different too), or it can be set for the whole Net.\n* **Layer-wise Adaptive Rate Control (LARC) and adaptive global gradient scaler** for better\n accuracy, especially in 16-bit training.\n* **Integration with  [cuDNN](https://developer.nvidia.com/cudnn) v8**.\n* **Automatic selection of the best cuDNN convolution algorithm**.\n* **Integration with v2.2 (or higher) of [NCCL library](https://github.com/NVIDIA/nccl)**\n for improved multi-GPU scaling.\n* **Optimized GPU memory management** for data and parameters storage, I/O buffers \nand workspace for convolutional layers.\n* **Parallel data parser, transformer and image reader** for improved I/O performance.\n* **Parallel back propagation and gradient reduction** on multi-GPU systems.\n* **Fast solvers implementation with fused CUDA kernels for weights and history update**.\n* **Multi-GPU test phase** for even memory load across multiple GPUs.\n* **Backward compatibility with BVLC Caffe and NVCaffe 0.15 and higher**.\n* **Extended set of optimized models** (including 16 bit floating point examples).\n* _Experimental feature (no official support)_ **Multi-node trai",
    "url": "https://github.com/NVIDIA/caffe",
    "last_updated": "2025-08-24T19:00:17+00:00"
  },
  {
    "full_name": "christophergandrud/dpmr",
    "name": "dpmr",
    "description": "Data Package Manager for R",
    "language": "R",
    "topics": [],
    "readme": "![](https://github.com/christophergandrud/dpmr/blob/master/img/logo.png?raw=true)\n\nData Package Manager for R\n====\n\nVersion: 0.1.9 [![CRAN Version](http://www.r-pkg.org/badges/version/dpmr)](http://cran.r-project.org/package=dpmr)\n[![Build Status](https://travis-ci.org/christophergandrud/dpmr.svg?branch=master)](https://travis-ci.org/christophergandrud/dpmr)\n![CRAN Monthly Downloads](http://cranlogs.r-pkg.org/badges/last-month/dpmr)\n![CRAN Total Downloads](http://cranlogs.r-pkg.org/badges/grand-total/dpmr)\n\n## Description\n\nThe R package for creating and installing data packages that follow the\n[Open Knowledge Foundation](https://okfn.org/)'s\n[Data Package Protocol](http://dataprotocols.org/data-packages/).\n\n**dpmr** has three core functions:\n\n- `datapackage_init`: initialises a new data package from an R data frame and\n(optionally) a meta data list.\n\n- `datapackage_install`: installs a data package either stored locally or\nremotely, e.g. on GitHub.\n\n- `datapackage_info:` reads a data package's metadata (stored in its\n*[datapackage.json](http://dataprotocols.org/data-packages/)*\nfile) into the R Console and (optionally) as a list.\n\n## Examples\n\n### Create Data Packages\n\nTo initiate a barebones data package in the current working directory called\n`My_Data_Package` use:\n\n```r\n# Create fake data\nA <- B <- C <- sample(1:20, size = 20, replace = TRUE)\nID <- sort(rep('a', 20))\nData <- data.frame(ID, A, B, C)\n\ndatapackage_init(df = Data, package_name = 'My_Data_Package')\n```\n\nThis will create a data package with barebones metadata in a\n*[datapackage.json](http://dataprotocols.org/data-packages/)*\nfile. You can then edit this by hand.\n\nAlternatively, you can also create a list with the metadata in R and have this\nincluded with the data package:\n\n```r\nmeta_list <- list(name = 'My_Data_Package',\n                    title = 'A fake data package',\n                    last_updated = Sys.Date(),\n                    version = '0.1',\n                    license = data.frame(type = '",
    "url": "https://github.com/christophergandrud/dpmr",
    "last_updated": "2025-04-20T15:34:04+00:00"
  },
  {
    "full_name": "lithammer/shortuuid",
    "name": "shortuuid",
    "description": "A generator library for concise, unambiguous and URL-safe UUIDs",
    "language": "Go",
    "topics": [
      "shortuuid",
      "uuid",
      "go",
      "golang"
    ],
    "readme": "# shortuuid\n\n[![Build Status](https://github.com/lithammer/shortuuid/workflows/CI/badge.svg)](https://github.com/lithammer/shortuuid/actions)\n[![Godoc](https://img.shields.io/badge/godoc-reference-blue.svg?style=flat)](https://pkg.go.dev/github.com/lithammer/shortuuid/v4)\n\nA Go library that generates concise, unambiguous, URL-safe UUIDs. Based on and\ncompatible with the Python library\n[`shortuuid`](https://github.com/skorokithakis/shortuuid).\n\nOften, one needs to use non-sequential IDs in places where users will see them,\nbut the IDs must be as concise and easy to use as possible. shortuuid solves\nthis problem by generating UUIDs using\n[google/uuid](https://github.com/google/uuid) and then translating them to\nbase57 using lowercase and uppercase letters and digits, and removing\nsimilar-looking characters such as l, 1, I, O and 0.\n\n## Usage\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/lithammer/shortuuid/v4\"\n)\n\nfunc main() {\n\tu := shortuuid.New()\n\tfmt.Println(u) // KwSysDpxcBU9FNhGkn2dCf\n}\n```\n\nTo use UUID v5 (instead of the default v4), use `NewWithNamespace(name string)`\ninstead of `New()`.\n\n```go\nshortuuid.NewWithNamespace(\"http://example.com\")\n```\n\nIt's possible to use a custom alphabet as well (at least 2\ncharacters long).  \nIt will automatically sort and remove duplicates from your alphabet to ensure consistency\n\n```go\nalphabet := \"23456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxy=\"\nshortuuid.NewWithAlphabet(alphabet) // iZsai==fWebXd5rLRWFB=u\n```\n\nBring your own encoder! For example, base58 is popular among bitcoin.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/btcsuite/btcutil/base58\"\n\t\"github.com/google/uuid\"\n\t\"github.com/lithammer/shortuuid/v4\"\n)\n\ntype base58Encoder struct{}\n\nfunc (enc base58Encoder) Encode(u uuid.UUID) string {\n\treturn base58.Encode(u[:])\n}\n\nfunc (enc base58Encoder) Decode(s string) (uuid.UUID, error) {\n\treturn uuid.FromBytes(base58.Decode(s))\n}\n\nfunc main() {\n\tenc := base58Encoder{}\n\tfmt.Println(shortuuid.NewWithEncoder(",
    "url": "https://github.com/lithammer/shortuuid",
    "last_updated": "2025-09-02T05:02:23+00:00"
  },
  {
    "full_name": "AykutSarac/jsoncrack.com",
    "name": "jsoncrack.com",
    "description": "✨ Innovative and open-source visualization application that transforms various data formats, such as JSON, YAML, XML, CSV and more, into interactive graphs.",
    "language": "TypeScript",
    "topics": [
      "json",
      "tool",
      "react",
      "visualization",
      "graph",
      "nextjs",
      "diagrams",
      "csv",
      "yaml"
    ],
    "readme": "<!-- PROJECT LOGO -->\n<p align=\"center\">\n  <a href=\"https://github.com/AykutSarac/jsoncrack.com\">\n   <img src=\"./public/assets/192.png\" height=\"50\" alt=\"Logo\">\n  </a>\n\n  <h1 align=\"center\">JSON Crack</h1>\n\n  <p align=\"center\">\n    The open-source JSON Editor.\n    <br />\n    <a href=\"https://jsoncrack.com\"><strong>Learn more »</strong></a>\n    <br />\n    <br />\n    <a href=\"https://todiagram.com\">ToDiagram</a>\n    ·\n    <a href=\"https://discord.gg/yVyTtCRueq\">Discord</a>\n    ·\n    <a href=\"https://jsoncrack.com\">Website</a>\n    ·\n    <a href=\"https://github.com/AykutSarac/jsoncrack.com/issues\">Issues</a>\n    ·\n    <a href=\"https://marketplace.visualstudio.com/items?itemName=AykutSarac.jsoncrack-vscode\">VS Code</a>\n  </p>\n</p>\n\n<!-- ABOUT THE PROJECT -->\n\n## About the Project\n\n<img width=\"100%\" alt=\"booking-screen\" src=\"./public/assets/editor.webp\">\n\n## Visualize JSON into interactive graphs\n\nJSON Crack is a tool for visualizing JSON data in a structured, interactive graphs, making it easier to explore, format, and validate JSON. It offers features like converting JSON to other formats (CSV, YAML), generating JSON Schema, executing queries, and exporting visualizations as images. Designed for both readability and usability.\n\n* **Visualizer**: Instantly convert JSON, YAML, CSV, XML, and TOML into interactive graphs or trees in dark or light mode.\n* **Convert**: Seamlessly transform data formats, like JSON to CSV or XML to JSON, for easy sharing.\n* **Format & Validate**: Beautify and validate JSON, YAML, and CSV for clear and accurate data.\n* **Code Generation**: Generate TypeScript interfaces, Golang structs, and JSON Schema.\n* **JSON Schema**: Create JSON Schema, mock data, and validate various data formats.\n* **Advanced Tools**: Decode JWT, randomize data, and run jq or JSON path queries.\n* **Export Image**: Download your visualization as PNG, JPEG, or SVG.\n* **Privacy**: All data processing is local; nothing is stored on our servers.\n\n## Recognition\n\n<a href=\"https:",
    "url": "https://github.com/AykutSarac/jsoncrack.com",
    "last_updated": "2025-09-02T03:43:49+00:00"
  },
  {
    "full_name": "gojiplus/datasky",
    "name": "datasky",
    "description": "Post a random repo. from Dataverse",
    "language": "Python",
    "topics": [
      "bluesky",
      "dataverse",
      "github"
    ],
    "readme": "# 📊 Dataverse to Bluesky GitHub Action\n\nThis GitHub Action:\n1. Pulls datasets from your Harvard Dataverse collection\n2. Commits the dataset list as `datasets.json` to the repository\n3. Posts a random dataset to your Bluesky feed with title + link\n\nPerfect for sharing research assets and building visibility.\n\n---\n\n## 🔧 Setup\n\n### 1. Required Secrets\nSet the following secrets in your GitHub repository:\n\n| Secret Name       | Description                                      |\n|------------------|--------------------------------------------------|\n| `DATAVERSE_TOKEN`| Your Harvard Dataverse API token                 |\n| `BSKY_HANDLE`     | Your Bluesky handle (e.g. soodoku.bsky.social)  |\n| `BSKY_PASSWORD`   | Your Bluesky password or app password           |\n\n---\n\n### 2. Sample Workflow (`.github/workflows/monthly.yml`)\n\n```yaml\nname: Monthly Dataverse + Bsky\n\non:\n  schedule:\n    - cron: \"0 6 1 * *\"  # Run on the 1st of every month\n  workflow_dispatch:\n\njobs:\n  update-and-post:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: pip install requests atproto\n\n      - name: List datasets\n        env:\n          DATAVERSE_TOKEN: ${{ secrets.DATAVERSE_TOKEN }}\n        run: |\n          python scripts/list_datasets.py \\\n            -b https://dataverse.harvard.edu \\\n            -t $DATAVERSE_TOKEN \\\n            --alias soodoku \\\n            -o datasets.json\n\n      - name: Commit datasets.json\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"github-actions[bot]@users.noreply.github.com\"\n          git add datasets.json\n          git commit -m \"Update datasets.json [skip ci]\" || echo \"No changes\"\n          git push\n\n      - name: Post to Bluesky\n        env:\n          BSKY_HANDLE: ${{ secrets.BSKY_HANDLE }}\n          BS",
    "url": "https://github.com/gojiplus/datasky",
    "last_updated": "2025-04-29T19:12:53+00:00"
  },
  {
    "full_name": "hrbrmstr/hrbrthemes",
    "name": "hrbrthemes",
    "description": ":lock_with_ink_pen: Opinionated, typographic-centric ggplot2 themes and theme components",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "ggplot2",
      "datavisualization",
      "visualization",
      "data-visualization",
      "ggplot2-themes",
      "ggplot-extension",
      "ggplot2-scales"
    ],
    "readme": "\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-8%25-lightgrey.svg)\n\n[![cran\nchecks](https://cranchecks.info/badges/worst/hrbrthemes.png)](https://cranchecks.info/pkgs/hrbrthemes)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/hrbrthemes.png)](https://www.r-pkg.org/pkg/hrbrthemes)\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-4.0.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n![downloads](https://cranlogs.r-pkg.org/badges/grand-total/hrbrthemes.png)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2545422.svg)](https://doi.org/10.5281/zenodo.2545422)\n\n## hrbrthemes\n\nAdditional Themes and Theme Components for ‘ggplot2’\n\n------------------------------------------------------------------------\n\nThis is a very focused package that provides typography-centric themes\nand theme components for ggplot2.\n\nThe core theme: `theme_ipsum` (“ipsum” is Latin for “precise”) uses\nArial Narrow which should be installed on practically any modern system,\nso it’s “free”-ish. This font is condensed, has solid default kerning\npairs and geometric numbers. That’s what I consider the “font trifecta”\nmust-have for charts. An additional quality for fonts for charts is that\nthey have a diversity of weights. Arial Narrow (the one on most systems,\nanyway) does not have said diversity but this quality is not (IMO) a\n“must have”.\n\nThe following functions are implemented/objects are exported:\n\nThemes:\n\n- `theme_ipsum`: Arial Narrow\n- `theme_ipsum_gs`: Goldman Sans Condensed\n- `theme_ipsum_es`: Econ Sans Condensed\n- `theme_ipsum_rc`: Roboto Condensed\n- `theme_ipsum_ps`: IBM Plex Sans font\n- `theme_ipsum_pub`: Public Sans\n- `theme_ipsum_t",
    "url": "https://github.com/hrbrmstr/hrbrthemes",
    "last_updated": "2025-08-27T19:10:53+00:00"
  },
  {
    "full_name": "alexhanna/nyu-shortcourse",
    "name": "nyu-shortcourse",
    "description": "NYU Shortcourse -- \"Data Science and Social Science\" materials",
    "language": "HTML",
    "topics": [],
    "readme": "\n# New York University Shortcourse: Data Science and Social Science\n\n## Co-sponsored by \n* [Institute for Education Sciences-funded Predoctoral Interdisciplinary Research Training (IES-PIRT) program](http://steinhardt.nyu.edu/ihdsc/iespirt)\n* [Center for the Promotion of Research Involving Innovative Statistical Methodology (PRIISM)](http://steinhardt.nyu.edu/priism/)\n\n## January 20-22, 2016\n \n## Instructors\n\n* [Pablo Barber&aacute;](http://pablobarbera.com/)\n\n## Teaching Assistants\n\n* Denis Stukal\n* Kevin Munger\n* Peter Crosta\n* Varun D N\n\n## Description\n\nThis is a three-day short course covering key topics at the intersection of Data Science and Social Science. Each day will be structured at a 6-hour project-based course focused on instruction and implementation of data science methodology with real social science data. This course will cover an introduction to the R programming and statistical language, modeling and visualization, automated textual analysis, social network analysis, and web scraping & APIs. \n\n## Setup and Preparation\n\nYou will need to bring a laptop to all sessions of the workshop. You will need [R](https://cran.r-project.org/) or [RStudio](https://www.rstudio.com/) installed.\n\n### Instructions for using course materials on GitHub ###\n\nYou have three options for downloading the course material found on this page:  \n\n1.  You can download the materials by clicking on each link.  \n\n2.  You can \"clone\" repository, using the buttons found to the right side of your browser window as you view this repository.  This is the button labelled \"Clone in Desktop\".  If you do not have a git client installed on your system, you will need to [get one here](https://git-scm.com/download/gui) and also to make sure that [git is installed](https://git-scm.com/downloads).  This is preferred, since you can refresh your clone as new content gets pushed to the course repository.  (And new material will get actively pushed to the course repository at least once per day as ",
    "url": "https://github.com/alexhanna/nyu-shortcourse",
    "last_updated": "2025-01-10T00:04:41+00:00"
  },
  {
    "full_name": "gaborcsardi/maxygen",
    "name": "maxygen",
    "description": ":exclamation: OUTDATED Markdown + Roxygen = Maxygen",
    "language": "R",
    "topics": [],
    "readme": "\n\n\n# maxygen\n\n> Markdown + Roxygen = Maxygen\n\n[![Linux Build Status](https://travis-ci.org/gaborcsardi/maxygen.svg?branch=master)](https://travis-ci.org/gaborcsardi/maxygen)\n[![Windows Build status](https://ci.appveyor.com/api/projects/status/github/gaborcsardi/maxygen?svg=true)](https://ci.appveyor.com/project/gaborcsardi/maxygen)\n[![](http://www.r-pkg.org/badges/version/maxygen)](http://www.r-pkg.org/pkg/maxygen)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/maxygen)](http://www.r-pkg.org/pkg/maxygen)\n\n\nWrite your Roxygen documentation in CommonMark Markdown. See\nhttp://spec.commonmark.org/ for the complete specification.\n\n## Installation\n\n\n```r\ndevtools::install_github(\"gaborcsardi/maxygen\")\n```\n\n## Usage\n\n```r\nlibrary(maxygen)\nmacument(pkg_dir)\n```\n\nYou can use the `macument()` command instead of `devtools::document()` to\ncreate the Rd documentation from Markdown-Roxygen. `macument()` converts\nthe Markdown formatting within the Roxygen comments to proper R manual\nformat, and then calls `devtools::document()` itself.\n\nSee a [complete example](inst/example/R/example.R) and the\n[generated `Rd` files](inst/example/man).\n\n## Markup\n\n### Inline code\n\nUse backticks instead of `\\code{}`:\n\n```r\n#' @param ns Optionally, a named vector giving prefix-url pairs, as\n#'   produced by `xml_ns`. If provided, all names will be explicitly\n#'   qualified with the ns prefix, i.e. if the element `bar` is define ...\n```\n\n### Code blocks\n\nPut your code between three backticks:\n\n```r\n#' ```\n#' pkg <- make_packages(\n#'   foo1 = { f <- function() print(\"hello!\") ; d <- 1:10 },\n#'   foo2 = { f <- function() print(\"hello again!\") ; d <- 11:20 }\n#' )\n#' foo1::f()\n#' foo2::f()\n#' foo1::d\n#' foo2::d\n#' dispose_packages(pkg)\n#' ```\n```\n\n### Lists\n\nRegular Markdown lists are recognized and converted to\n`\\enumerate{}` or `\\itemize{}` lists:\n\n```r\n#' There are two ways to use this function:\n#' 1. If its first argument is not named, then it returns a function\n#'    that can be ",
    "url": "https://github.com/gaborcsardi/maxygen",
    "last_updated": "2024-11-04T13:37:44+00:00"
  },
  {
    "full_name": "fragilefamilieschallenge/slides",
    "name": "slides",
    "description": "slides related to the Fragile Families Challenge",
    "language": "TeX",
    "topics": [],
    "readme": "# slides\nslides related to the Fragile Families Challenge\n",
    "url": "https://github.com/fragilefamilieschallenge/slides",
    "last_updated": "2021-02-27T22:20:11+00:00"
  },
  {
    "full_name": "berianjames/ICCRankings",
    "name": "ICCRankings",
    "description": "ICC Test Cricket Rankings module in Python",
    "language": "Python",
    "topics": [],
    "readme": "**********************************\n ICC Test Cricket Ranking Package\n**********************************\n\n:Date: April 7, 2012\n:Version: 0.1\n:Authors: Berian James\n:Web site: https://github.com/berianjames/ICCRankings\n:Copyright: This document has been placed in the public domain.\n:License: This code is released under the MIT license.\n:Requires: BeautifulSoup4, urllib2, datetime, dateutil\n\n====================\nICC Rankings package\n====================\n\nICCRankings.py is a module to obtain and update the `ICC Test Cricket rankings`__. To get started, run ``python test_script.py`` from the command line and explore the results.\n\n.. __: http://icc-cricket.yahoo.net/match_zone/team_ranking.php\n\nThe ICC Test Cricket rankings are computed with statistical methodology that has not been publically verified by the ICC. `This algorithm`__ has been followed in the construction of this module, and appears to function correctly. Nevertheless, slight differences may be noticed with respect to the `online predictor`__, as this implementation does not remove the impact of series that are more than three years old, or downweight series that are more than 18 months old, as the official algorithm supposedly requires. \n\n.. __: http://en.wikipedia.org/wiki/ICC_Test_Championship#Test_championship_calculations\n\n.. __: http://icc-cricket.yahoo.net/match_zone/test_predictor.php\n\nThe purpose of this package is to study the ranking methodology currently used by the ICC and to consider whether an alternative system---in particular, the Elo rating system---will perform better.\n\nModule details\n==============\n\nThe module provides three classes: Team, Series, and TeamDict.\n\n* A Team object contains a unicode team name, rating points, matches played, rating in rounded and floating-point form, and a datetime.date representing the timestamp for these data.\n\n* A Series object takes two Team objects, a number of matches, wins for each team and a datestamp. Draws are inferred from these data. Note that ma",
    "url": "https://github.com/berianjames/ICCRankings",
    "last_updated": "2023-06-14T10:17:18+00:00"
  },
  {
    "full_name": "datanews/tables",
    "name": "tables",
    "description": "Tables is a simple command-line tool and powerful library for importing data like a CSV or JSON file into relational tables",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Tables\n\nTables is a simple command-line tool and powerful library for importing data like a CSV or JSON file into relational database tables.  The goal is to make data import easy, configurable, and stable for large datasets into a relational database for better analysis.\n\n[![Build Status](https://travis-ci.org/datanews/tables.svg?branch=master)](https://travis-ci.org/datanews/tables) [![Dependencies](https://david-dm.org/datanews/tables.svg)](https://david-dm.org/datanews/tables)\n[![License](https://img.shields.io/npm/l/tables.svg)]()\n\n## Install\n\n* To include as a library: `npm install tables`\n* To use as a command-line tool: `npm install -g tables`\n\n## Features\n\n* Automatic data type guessing.\n* Automatic indexes guessing.\n* Efficient memory utilizing Node streams.\n* Supports CSV-ish, JSON, NDJSON, and HTML table data sources (utilizes [tito](https://github.com/shawnbot/tito)).\n* Default use of SQLite so no need to setup a database server.\n* Resumable (currently only works for CSV in input mode).\n* Sane defaults.\n* Supports MySQL, Postgres, and SQLite.\n    * *Note*: Postgres has some limitations, specific with updating data.\n* Verbose, structured output.\n\n## Command line use\n\nThe command line version can handle most of the simple options that the library uses.  A simple example that will read in a CSV and create an SQLite database at `examples/nyc-water-quality-complaints.sql` with a `nyc_water_quality_complaints` table with the converted data.\n\n```bash\ntables -i examples/nyc-water-quality-complaints.csv\n```\n\n### Options\n\nUse the `--help` option to get a full, up-to-date look at what the options are, but here are most of them for reference:\n\n* `--help`: Shows help message.\n* `--version`: Outputs version.\n* `--input`: The input file to use.  Tables also can use a piped in source as well, if this is not provided.\n* `--db`: The database URI to push data into.  By default, Tables will use the input option to create a SQLite database with the same path with a `.sql`",
    "url": "https://github.com/datanews/tables",
    "last_updated": "2024-08-14T13:07:56+00:00"
  },
  {
    "full_name": "ryankirkman/pyglicko2",
    "name": "pyglicko2",
    "description": "Github mirror of my original https://code.google.com/archive/p/pyglicko2/. For details about the Glicko-2 rating system see: http://www.glicko.net/glicko/glicko2.html and http://www.glicko.net/glicko/glicko2.pdf",
    "language": "Python",
    "topics": [
      "python",
      "glicko",
      "rating",
      "glicko2",
      "glicko-2"
    ],
    "readme": "",
    "url": "https://github.com/ryankirkman/pyglicko2",
    "last_updated": "2025-04-13T18:09:13+00:00"
  },
  {
    "full_name": "benfred/implicit",
    "name": "implicit",
    "description": "Fast Python Collaborative Filtering for Implicit Feedback Datasets",
    "language": "Python",
    "topics": [
      "collaborative-filtering",
      "machine-learning",
      "matrix-factorization",
      "recommender-system",
      "recommendation-system",
      "recommendation"
    ],
    "readme": "Implicit\n=======\n\n[![Build\nStatus](https://github.com/benfred/implicit/workflows/Build/badge.svg)](https://github.com/benfred/implicit/actions?query=workflow%3ABuild+branch%3Amain)\n[![Documentation](https://img.shields.io/badge/documentation-blue.svg)](https://benfred.github.io/implicit/)\n\n\nFast Python Collaborative Filtering for Implicit Datasets.\n\nThis project provides fast Python implementations of several different popular recommendation algorithms for\nimplicit feedback datasets:\n\n * Alternating Least Squares as described in the papers [Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf) and [Applications of the Conjugate Gradient Method for Implicit\nFeedback Collaborative Filtering](https://pdfs.semanticscholar.org/bfdf/7af6cf7fd7bb5e6b6db5bbd91be11597eaf0.pdf).\n\n * [Bayesian Personalized Ranking](https://arxiv.org/pdf/1205.2618.pdf).\n\n * [Logistic Matrix Factorization](https://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf)\n\n * Item-Item Nearest Neighbour models using Cosine, TFIDF or BM25 as a distance metric.\n\nAll models have multi-threaded training routines, using Cython and OpenMP to fit the models in\nparallel among all available CPU cores.  In addition, the ALS and BPR models both have custom CUDA\nkernels - enabling fitting on compatible GPU's. Approximate nearest neighbours libraries such as [Annoy](https://github.com/spotify/annoy), [NMSLIB](https://github.com/searchivarius/nmslib)\nand [Faiss](https://github.com/facebookresearch/faiss) can also be used by Implicit to [speed up\nmaking recommendations](https://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/).\n\n#### Installation\n\nImplicit can be installed from pypi with:\n\n```\npip install implicit\n```\n\nInstalling with pip will use prebuilt binary wheels on x86_64 Linux, Windows\nand OSX. These wheels include GPU support on Linux.\n\nImplicit can also be installed with conda:\n\n```\n# CPU only package\nconda install -c conda-forge impl",
    "url": "https://github.com/benfred/implicit",
    "last_updated": "2025-08-28T11:06:52+00:00"
  },
  {
    "full_name": "kmunger/Replication-Materials-for-Tweetment-Effects-on-the-Tweeted",
    "name": "Replication-Materials-for-Tweetment-Effects-on-the-Tweeted",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# Replication-Materials-for-Tweetment-Effects-on-the-Tweeted\n\nThis repository contains replication materials for the Political Behavior article (Munger, 2016). <i>Tweetment Effects on the Tweeted: Experimentally Reducing Online Harassment.</i>\n\nAll of the data provided has been calculated from word counts collected from subjects' Twitter accounts. In order to protect subjects' privacy, the raw text is not avaliable; even a single tweet can be enough to uniquely identify a user.\n\nTo replicate the figures in both the body of the text and the appendix, first run read_in_data_fit_models.R. Choose to read in the data from the body of the text (main assumption) or appendix (conservative assumption). Then run plot_results.R. Choose to plot the pooled models or the models with subgroup analysis based on the subjects' anonymity.\n",
    "url": "https://github.com/kmunger/Replication-Materials-for-Tweetment-Effects-on-the-Tweeted",
    "last_updated": "2022-03-02T17:00:01+00:00"
  },
  {
    "full_name": "dmlc/xgboost",
    "name": "xgboost",
    "description": "Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library,  for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Dask, Flink and DataFlow",
    "language": "C++",
    "topics": [
      "gbdt",
      "gbrt",
      "gbm",
      "distributed-systems",
      "xgboost",
      "machine-learning"
    ],
    "readme": "<img src=\"https://xgboost.ai/images/logo/xgboost-logo-trimmed.png\" width=200/> eXtreme Gradient Boosting\n===========\n\n[![Build Status](https://badge.buildkite.com/aca47f40a32735c00a8550540c5eeff6a4c1d246a580cae9b0.svg?branch=master)](https://buildkite.com/xgboost/xgboost-ci)\n[![XGBoost-CI](https://github.com/dmlc/xgboost/workflows/XGBoost%20CI/badge.svg?branch=master)](https://github.com/dmlc/xgboost/actions)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org)\n[![GitHub license](https://dmlc.github.io/img/apache2.svg)](./LICENSE)\n[![CRAN Status Badge](https://www.r-pkg.org/badges/version/xgboost)](https://cran.r-project.org/web/packages/xgboost)\n[![PyPI version](https://badge.fury.io/py/xgboost.svg)](https://pypi.python.org/pypi/xgboost/)\n[![Conda version](https://img.shields.io/conda/vn/conda-forge/py-xgboost.svg)](https://anaconda.org/conda-forge/py-xgboost)\n[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)\n[![Twitter](https://img.shields.io/badge/@XGBoostProject--_.svg?style=social&logo=twitter)](https://twitter.com/XGBoostProject)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost/badge)](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-training/xgboost/notebooks/how_to_use_comet_with_xgboost_tutorial.ipynb)\n\n[Community](https://xgboost.ai/community) |\n[Documentation](https://xgboost.readthedocs.org) |\n[Resources](demo/README.md) |\n[Contributors](CONTRIBUTORS.md) |\n[Release Notes](https://xgboost.readthedocs.io/en/latest/changes/index.html)\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***.\nIt implements machine learning algorithms under the [Gra",
    "url": "https://github.com/dmlc/xgboost",
    "last_updated": "2025-09-02T08:09:36+00:00"
  },
  {
    "full_name": "wenliangdai/Multimodal-End2end-Sparse",
    "name": "Multimodal-End2end-Sparse",
    "description": "The code repository for NAACL 2021 paper \"Multimodal End-to-End Sparse Model for Emotion Recognition\".",
    "language": "Python",
    "topics": [],
    "readme": "# Multimodal End-to-End Sparse Model for Emotion Recognition\n\n[![](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/) [![CC BY 4.0][cc-by-shield]][cc-by]\n\n\n<img align=\"right\" src=\"img/HKUST.jpg\" width=\"15%\"/>\n\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg\n\n[[Paper]](https://www.aclweb.org/anthology/2021.naacl-main.417/) accepted at the NAACL 2021:\n\n**Multimodal End-to-End Sparse Model for Emotion Recognition**, by **[Wenliang Dai *](https://wenliangdai.github.io/)**, Samuel Cahyawijaya *, [Zihan Liu](https://zliucr.github.io/), [Pascale Fung](https://pascale.home.ece.ust.hk/).\n\n## Paper Abstract\n\n> Existing works on multimodal affective computing tasks, such as emotion recognition, generally adopt a two-phase pipeline, first extracting feature representations for each single modality with hand-crafted algorithms and then performing end-to-end learning with the extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extraction algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-the-art models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain performance with around half the computation in the feature extraction part.\n\nIf you work is inspired by our paper or code, please cite it, th",
    "url": "https://github.com/wenliangdai/Multimodal-End2end-Sparse",
    "last_updated": "2025-08-04T12:17:31+00:00"
  },
  {
    "full_name": "datacamp/testwhat",
    "name": "testwhat",
    "description": "Write Submission Correctness Tests for R exercises",
    "language": "R",
    "topics": [
      "fs",
      "le"
    ],
    "readme": "> :warning: **This repo had outdated tokens in its travisci config**\n> To make new releases for this project it needs to be moved to circleci\n\n# testwhat\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fdatacamp%2Ftestwhat.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2Fdatacamp%2Ftestwhat?ref=badge_shield)\n\nVerify R code submissions and auto-generate meaningful feedback messages.\nOriginally developed for R exercises on DataCamp for so-called Submission Correctness Tests, but can also be used independently.\n\n- If you are new to teaching on DataCamp, check out https://instructor-support.datacamp.com.\n- If you want to learn what SCTs are and how they work, visit [this article](https://instructor-support.datacamp.com/courses/course-development/submission-correctness-tests) specifically.\n- For a complete overview of all functionality inside `testwhat` and articles about what to use when, consult https://datacamp.github.io/testwhat.\n\nFor details, questions and suggestions, [contact us](mailto:content-engineering@datacamp.com).\n\n\n## Installation\n\n```R\nlibrary(\"remotes\")\ninstall_github(\"datacamp/testwhat\")\n```\n\n## Demo\n\nExperimenting locally:\n\n```R\nlibrary(testwhat)\nsetup_state(sol_code = \"x <- 5\",\n            stu_code = \"x <- 4\")\n\nex() %>% check_object(\"x\")\n# No error: x is defined in both student and solution code\n\nex() %>% check_object(\"x\") %>% check_equal()\n# Error: The contents of the variable `x` aren't correct.\n\n# Debugging state\ns <- ex() %>% check_object()\ns                     # only prints out state class\nstr(s)                # full overview of state\ns$get(\"student_code\") # access student code in state\n```\n\nTo include an SCT in a DataCamp course, visit https://instructor-support.datacamp.com.\n\n## Tests\n\n`testwhat` currently depends on the proprietary `RBackend` and `RCompletion` packages to run tests. Tests run automatically on every branch that is updated through travis.\n\n```R\ndevtools::test()\n```\n\n## Documentation\n\nWhe",
    "url": "https://github.com/datacamp/testwhat",
    "last_updated": "2025-08-30T04:04:40+00:00"
  },
  {
    "full_name": "yinanxu0/uphill",
    "name": "uphill",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# uphill\n[![Python-Version](https://img.shields.io/badge/Python-3.7%7C3.8-brightgreen)](https://github.com/yinanxu0/uphill)\n\nEasy to process and store data.\n\n## installation\n### install from pip\n```\npip3 install uphill\n```\n### install \n```\ngit clone https://github.com/yinanxu0/uphill\ncd uphill\npip3 install .\n```\n\n## Document\nWe afford python package and bin mode. For more details, please check `uphill -h`. \n```\nusage: uphill [-h] [-v] {download,prepare,peek} ...\n\nUphill Line Interface\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         show UpHill version\n\nsubcommands:\n  use \"uphill [sub-command] --help\" to get detailed information about each sub-command\n\n  {download,prepare,peek}\n    download            ⏬ download a dataset automatically\n    prepare             👋 prepare a dataset automatically\n    peek                👀 peek a dataset quickly\n\nuphill v0.1.3, a toolkit based on pytorch. Visit https://github.com/yinanxu0/uphill for tutorials and documents.\n```\nFor convenience, you can use `uh` instead of `uphill`, like `uh -h`.\n\n### Download dataset\nFor example, download Aishell dataset\n```\nuh download -d aishell -o ${download_dir}\n```\n\nMore details of parameters in help mode.\n```\nuh download -h\n```\n\n### Prepare dataset\n```\nuh prepare -d aishell -c ${download_dir}/aishell -o ${data_dir} -j 8 -z\n```\n\nMore details of parameters in help mode.\n```\nuh prepare -h\n```\n\n### Peek dataset\n```\nuh peek -i ${data_dir}/supervisions_train.jsonl -n 2\n```\n",
    "url": "https://github.com/yinanxu0/uphill",
    "last_updated": "2025-01-15T15:29:10+00:00"
  },
  {
    "full_name": "JelteF/PyLaTeX",
    "name": "PyLaTeX",
    "description": "A Python library for creating LaTeX files",
    "language": "Python",
    "topics": [],
    "readme": "PyLaTeX |Actions| |License| |PyPi| |Latest Docs|\n==============================================================\n\nPyLaTeX is a Python library for creating and compiling LaTeX files or\nsnippets. The goal of this library is being an easy, but extensible\ninterface between Python and LaTeX.\n\nInstallation\n------------\nSimply install using ``pip``::\n\n    pip install pylatex\n\nAnd then install a relevant LaTeX processor and other dependencies. Examples:\n\nUbuntu\n~~~~~~~\n    sudo apt-get install texlive-pictures texlive-science \\\n    texlive-latex-extra latexmk\n\nDocumentation\n-------------\n\n- For more details on how to use the library take a look at `the documentation\n  <https://jeltef.github.io/PyLaTeX/current/>`__.\n\nContributing\n------------\n\nRead the `How to\ncontribute <https://jeltef.github.io/PyLaTeX/latest/contributing.html>`__\npage for tips and rules when you want to contribute.\n\nExamples\n--------\n\nThe documentation contains a lot of examples that show the\nfunctionality. To give an impression of what can be generated see this\npicture:\n\n.. figure:: https://raw.github.com/JelteF/PyLaTeX/master/docs/source/_static/screenshot.png\n   :alt: Generated PDF by PyLaTeX\n\nCopyright and License\n---------------------\n\nCopyright 2014 Jelte Fennema, under `the MIT\nlicense <https://github.com/JelteF/PyLaTeX/blob/master/LICENSE>`__\n\n.. |Actions| image:: https://github.com/JelteF/PyLaTeX/actions/workflows/ci.yml/badge.svg\n   :target: https://github.com/JelteF/PyLaTeX/actions/workflows/ci.yml\n   \n.. |License| image:: https://img.shields.io/github/license/jeltef/pylatex.svg   \n   :target: https://github.com/JelteF/PyLaTeX/blob/master/LICENSE\n\n.. |PyPi| image:: https://img.shields.io/pypi/v/pylatex.svg\n   :target: https://pypi.python.org/pypi/PyLaTeX\n   \n.. |Latest Docs| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat\n   :target: https://jeltef.github.io/PyLaTeX/latest/\n",
    "url": "https://github.com/JelteF/PyLaTeX",
    "last_updated": "2025-08-31T14:12:10+00:00"
  },
  {
    "full_name": "notnews/nytimes-corpus-extractor",
    "name": "nytimes-corpus-extractor",
    "description": "Extract all the fields from the NY Times Corpus to a csv",
    "language": "Python",
    "topics": [
      "ny-times-corpus"
    ],
    "readme": "### Extract All the Fields from the New York Times Corpus to a CSV\n\nThe New York Times Corpus is a collection of 1.8 million articles published between 1987 and 2007 along with a fair bit of meta data. For more details about The NY Times Corpus, see [https://catalog.ldc.upenn.edu/LDC2008T19](https://catalog.ldc.upenn.edu/LDC2008T19).\n\nOnce you have the NY Times Corpus, unzip it to a folder. And then run the script. Script produces a csv and text files containing story text.\n\n### Requirements\n\nPython 2.x\n\n#### Installation\n\nTo install the dependency [lxml 3.1.1](https://pypi.python.org/pypi/lxml/3.1.1):\n\n```\npip install -r requirements.txt\n```\n\n#### Usage\n\n```\npython nytextract.py [options] <xml directory>\n```\n\n**Options:**\n\n```\n\n  -h, --help            show this help message and exit\n  -a, --append          Append if existing (default: False)\n  -o OUTFILE, --out=OUTFILE\n                        CSV output file (default: outfile.csv)\n  -d OUTDIR, --dir=OUTDIR\n                        Text output directory (default: text)\n```\n\n**Example**\nTo process all XML files in the folder 2000 (carrying files from year 2000):  \n\n```\npython nytextract.py -o 2000.csv -d text 2000\n```\n\nThe script will generate a CSV \"2000.csv\". Story text files will be stored in a folder \"text.\" This folder will have the exact same structure as the folder '2000.'\n\n#### License\nScripts are released under the [MIT License](https://opensource.org/licenses/MIT).\n",
    "url": "https://github.com/notnews/nytimes-corpus-extractor",
    "last_updated": "2025-05-23T03:05:25+00:00"
  },
  {
    "full_name": "allenai/deep_qa",
    "name": "deep_qa",
    "description": "A deep NLP library, based on Keras / tf, focused on question answering (but useful for other NLP too)",
    "language": "Python",
    "topics": [
      "deep-learning",
      "question-answering",
      "nlp"
    ],
    "readme": "[![Build Status](https://api.travis-ci.org/allenai/deep_qa.svg?branch=master)](https://travis-ci.org/allenai/deep_qa)\n[![Documentation Status](https://readthedocs.org/projects/deep-qa/badge/?version=latest)](http://deep-qa.readthedocs.io/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/allenai/deep_qa/branch/master/graph/badge.svg)](https://codecov.io/gh/allenai/deep_qa)\n\n# DEPRECATED\n\nDeepQA is built on top of Keras.  We've decided that [pytorch](http://pytorch.org) is a\nbetter platform for NLP research.  We re-wrote DeepQA into a pytorch library called\n[AllenNLP](https://github.com/allenai/allennlp).  There will be no more development\nof DeepQA.  But, we're pretty excited about AllenNLP - if you're doing deep learning\nfor natural language processing, you should [check it out](http://allennlp.org)!\n\n# DeepQA\n\nDeepQA is a library for doing high-level NLP tasks with deep learning, particularly focused on\nvarious kinds of question answering.  DeepQA is built on top of [Keras](https://keras.io) and\n[TensorFlow](https://www.tensorflow.org/), and can be thought of as an interface to these\nsystems that makes NLP easier.\n\nSpecifically, this library provides the following benefits over plain Keras / TensorFlow:\n\n- It is easy to get NLP right in DeepQA.\n    - In Keras, there are a lot of issues around padding sequences and masking\n      that are not handled well in the main Keras code, and we have well-tested\n      code that does the right thing for, e.g., computing attentions over\n      padded sequences, padding all training instances to the same lengths\n      (possibly dynamically by batch, to minimize computation wasted on padding\n      tokens), or distributing text encoders across several sentences or words.\n    - DeepQA provides a nice, consistent API around building NLP models.  This\n      API has functionality around processing data instances, embedding words\n      and/or characters, easily getting various kinds of sentence encoders, and\n      so on.  It mak",
    "url": "https://github.com/allenai/deep_qa",
    "last_updated": "2025-01-26T03:50:55+00:00"
  },
  {
    "full_name": "dessant/buster",
    "name": "buster",
    "description": "Captcha solver extension for humans, available for Chrome, Edge and Firefox",
    "language": "JavaScript",
    "topics": [
      "browser-extension",
      "chrome-extension",
      "firefox-extension",
      "captcha",
      "recaptcha",
      "captcha-solving",
      "captcha-solver"
    ],
    "readme": "<p align=\"center\"><img width=\"128\" height=\"128\" src=\"https://i.imgur.com/uVpmR8l.png\"></p>\n<h1 align=\"center\">Buster: Captcha Solver for Humans</h1>\n\n<p align=\"center\">\n  </br></br>\n  <a href=\"https://chrome.google.com/webstore/detail/buster-captcha-solver-for/mpbjkejclgfgadiemmefgebjfooflfhl\">\n    <picture>\n      <source srcset=\"https://i.imgur.com/XBIE9pk.png\" media=\"(prefers-color-scheme: dark)\">\n      <img height=\"58\" src=\"https://i.imgur.com/oGxig2F.png\" alt=\"Chrome Web Store\"></picture></a>\n  <a href=\"https://addons.mozilla.org/firefox/addon/buster-captcha-solver/\">\n    <picture>\n      <source srcset=\"https://i.imgur.com/ZluoP7T.png\" media=\"(prefers-color-scheme: dark)\">\n      <img height=\"58\" src=\"https://i.imgur.com/4PobQqE.png\" alt=\"Firefox add-ons\"></picture></a>\n  <a href=\"https://microsoftedge.microsoft.com/addons/detail/buster-captcha-solver-fo/admkpobhocmdideidcndkfaeffadipkc\">\n    <picture>\n      <source srcset=\"https://i.imgur.com/Jog9cQP.png\" media=\"(prefers-color-scheme: dark)\">\n      <img height=\"58\" src=\"https://i.imgur.com/aiprUt8.png\" alt=\"Microsoft Store\"></picture></a>\n  <a href=\"https://addons.opera.com/extensions/details/buster-captcha-solver-for-humans/\">\n    <picture>\n      <source srcset=\"https://i.imgur.com/ziehy0f.png\" media=\"(prefers-color-scheme: dark)\">\n      <img height=\"58\" src=\"https://i.imgur.com/ytVATu0.png\" alt=\"Opera add-ons\"></picture></a>\n  </br></br>\n</p>\n\n## Supporting the Project\n\nThe continued development of Buster is made possible\nthanks to the support of awesome backers. If you'd like to join them,\nplease consider contributing with\n[Patreon](https://armin.dev/go/patreon?pr=buster&src=repo),\n[PayPal](https://armin.dev/go/paypal?pr=buster&src=repo) or\n[Bitcoin](https://armin.dev/go/bitcoin?pr=buster&src=repo).\n\n## Description\n\nBuster is a browser extension which helps you to solve difficult CAPTCHAs\nby completing reCAPTCHA audio challenges using speech recognition.\nChallenges are solved by clicking on the extension butt",
    "url": "https://github.com/dessant/buster",
    "last_updated": "2025-09-02T08:07:58+00:00"
  },
  {
    "full_name": "bbolker/cricketr",
    "name": "cricketr",
    "description": "R code for evaluating stopping rules for cricket games",
    "language": "R",
    "topics": [],
    "readme": "# cricketr\n\nWhen limited-overs cricket games are stopped prematurely for weather-related or other reasons, \n*stopping rules* such as the [Duckworth-Lewis method](http://en.wikipedia.org/wiki/Duckworth%E2%80%93Lewis_method)\nare used to determine which team won, i.e. how many runs the team that batted second should have scored\n(its *target*), considering the number of overs played and wickets lost, in order to be considered the winner.\n\nWhile the Duckworth-Lewis method is \"an attempt to set a statistically fair target for the second team's innings\",\nits actual statistical properties have rarely been evaluated objectively and independently. This repository includes/will include:\n\n* code for scraping relevant match data from ESPN's web site \n(as far as we can tell, such scraping is within the terms of service provided the\nresults are not redistributed)\n* code for tabulating resource tables (% of resources remaining as a function of\nnumber of overs played and wickets lost)\n* different methods of estimation of resource surfaces (essentially stoppage-rule tables)\nfrom raw resource tables\n* tools for evaluating methods (RMSE, kappa, bias, etc.) and applying cross-validation\n",
    "url": "https://github.com/bbolker/cricketr",
    "last_updated": "2018-08-21T12:25:34+00:00"
  },
  {
    "full_name": "scrapy/scrapy",
    "name": "scrapy",
    "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.",
    "language": "Python",
    "topics": [
      "python",
      "scraping",
      "crawling",
      "framework",
      "crawler",
      "hacktoberfest",
      "web-scraping",
      "web-scraping-python"
    ],
    "readme": "|logo|\n\n.. |logo| image:: https://raw.githubusercontent.com/scrapy/scrapy/master/docs/_static/logo.svg\n   :target: https://scrapy.org\n   :alt: Scrapy\n   :width: 480px\n\n|version| |python_version| |ubuntu| |macos| |windows| |coverage| |conda| |deepwiki|\n\n.. |version| image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. |python_version| image:: https://img.shields.io/pypi/pyversions/Scrapy.svg\n   :target: https://pypi.org/pypi/Scrapy\n   :alt: Supported Python Versions\n\n.. |ubuntu| image:: https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu\n   :alt: Ubuntu\n\n.. |macos| image:: https://github.com/scrapy/scrapy/workflows/macOS/badge.svg\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AmacOS\n   :alt: macOS\n\n.. |windows| image:: https://github.com/scrapy/scrapy/workflows/Windows/badge.svg\n   :target: https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows\n   :alt: Windows\n\n.. |coverage| image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: https://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n.. |conda| image:: https://anaconda.org/conda-forge/scrapy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/scrapy\n   :alt: Conda Version\n\n.. |deepwiki| image:: https://deepwiki.com/badge.svg\n   :target: https://deepwiki.com/scrapy/scrapy\n   :alt: Ask DeepWiki\n\nScrapy_ is a web scraping framework to extract structured data from websites.\nIt is cross-platform, and requires Python 3.9+. It is maintained by Zyte_\n(formerly Scrapinghub) and `many other contributors`_.\n\n.. _many other contributors: https://github.com/scrapy/scrapy/graphs/contributors\n.. _Scrapy: https://scrapy.org/\n.. _Zyte: https://www.zyte.com/\n\nInstall with:\n\n.. code:: bash\n\n    pip install scrapy\n\nAnd follow the documentation_ to learn how to use it.\n\n.. _documentation: http",
    "url": "https://github.com/scrapy/scrapy",
    "last_updated": "2025-09-02T08:10:30+00:00"
  },
  {
    "full_name": "FakeNewsChallenge/fnc-1",
    "name": "fnc-1",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# Stance Detection dataset for FNC-1\n\nFor details of the task, see [FakeNewsChallenge.org](http://fakenewschallenge.org)\n\n\nThe data provided is `(headline, body, stance)` instances, where `stance` is one of `{unrelated, discuss, agree, disagree}`. The dataset is provided as two CSVs:\n\n\n### `train_bodies.csv`\n\nThis file contains the body text of articles (the `articleBody` column) with corresponding IDs (`Body ID`)\n\n### `train_stances.csv`\n\nThis file contains the labeled stances (the `Stance` column) for pairs of article headlines (`Headline`) and article bodies (`Body ID`, referring to entries in `train_bodies.csv`).\n\n### Distribution of the data\n\nThe distribution of `Stance` classes in `train_stances.csv` is as follows:\n\n|   rows |   unrelated |   discuss |     agree |   disagree |\n|-------:|------------:|----------:|----------:|-----------:|\n|  49972 |    0.73131  |  0.17828  | 0.0736012 |  0.0168094 |\n\nCredits:\n\n- Edward Misback\n- Craig Pfeifer\n",
    "url": "https://github.com/FakeNewsChallenge/fnc-1",
    "last_updated": "2025-07-05T02:34:34+00:00"
  },
  {
    "full_name": "Pakillo/R-GIS-tutorial",
    "name": "R-GIS-tutorial",
    "description": "Spatial data in R: using R as a GIS",
    "language": "R",
    "topics": [
      "gis",
      "r",
      "maps",
      "spatial-data"
    ],
    "readme": "\nSpatial data in R: Using R as a GIS     \n========================================================\n\n\nA tutorial to perform basic operations with spatial data in R, such as importing and exporting data (both vectorial and raster), plotting, analysing and making maps.\n\n\n[Francisco Rodriguez-Sanchez](http://sites.google.com/site/rodriguezsanchezf) \n\n\nv 2.2  \n\n27-01-2015 \n\nLicence: [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/)\n\n\nCheck out latest version at [http://pakillo.github.io/R-GIS-tutorial](http://pakillo.github.io/R-GIS-tutorial)\n",
    "url": "https://github.com/Pakillo/R-GIS-tutorial",
    "last_updated": "2025-05-14T00:21:08+00:00"
  },
  {
    "full_name": "crsh/papaja",
    "name": "papaja",
    "description": "papaja (Preparing APA Journal Articles) is an R package that provides document formats to produce complete APA manuscripts from RMarkdown-files (PDF and Word documents) and helper functions that facilitate reporting statistics, tables, and plots.",
    "language": "HTML",
    "topics": [
      "rmarkdown",
      "apa",
      "journal",
      "apa-guidelines",
      "psychology",
      "r",
      "manuscript",
      "reproducible-paper",
      "reproducible-research",
      "r-package"
    ],
    "readme": "<img src='tools/images/papaja_hex.png' align='right' height='150' />papaja:\nPrepare APA Journal Articles<br />with R Markdown\n================\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN/METACRAN](https://img.shields.io/cran/v/papaja?label=CRAN&logo=r)](https://cran.r-project.org/package=papaja)\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n![GitHub last commit\n(main)](https://img.shields.io/github/last-commit/crsh/papaja/main?label=Last%20commit&logo=github)\n[![R-CMD-check](https://github.com/crsh/papaja/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/crsh/papaja/actions/workflows/R-CMD-check.yaml)\n[![codecov](https://codecov.io/gh/crsh/papaja/branch/master/graph/badge.svg)](https://app.codecov.io/gh/crsh/papaja)\n[![GitHub bug\nissues](https://img.shields.io/github/issues/crsh/papaja/bug?label=Bugs&logo=github)](https://github.com/crsh/papaja/issues?q=is%3Aopen+is%3Aissue+label%3Abug)\n[![StackOverflow\nquestions](https://img.shields.io/stackexchange/stackoverflow/t/papaja?label=Questions&logo=stackoverflow)](https://stackoverflow.com/questions/tagged/papaja)\n\n**Sections** [Example](#example) \\| [Installation](#installation) \\|\n[Usage](#usage) \\| [Getting help](#getting-help) \\|\n[Citation](#citation) \\| [papaja in the wild](#papaja-in-the-wild) \\|\n[Computational reproducibility](#computational-reproducibility) \\|\n[Contribute](#contribute) \\| [Related R packages](#related-r-packages)\n\\| [Package dependencies](#package-dependencies)\n\n**papaja** is an\n[award-winning](https://improvingpsych.org/mission/awards/) R package\nthat facilitates creating computationally reproducible, submission-ready\nmanuscripts which conform to the American Psychological Association\n(APA) manuscript guidelines (6th Edition). **papaja** provides\n\n- an [R Markdown](https://rmarkdown.rstudio.co",
    "url": "https://github.com/crsh/papaja",
    "last_updated": "2025-08-15T08:22:56+00:00"
  },
  {
    "full_name": "wrathematics/Rdym",
    "name": "Rdym",
    "description": "\"Did you mean?\" for R.",
    "language": "R",
    "topics": [],
    "readme": "# Rdym\n\n* **Version:** 0.4.0\n* **Status:** [![Build Status](https://travis-ci.org/wrathematics/Rdym.png)](https://travis-ci.org/wrathematics/Rdym) \n* **License:** [![License](http://img.shields.io/badge/license-BSD%202--Clause-orange.svg?style=flat)](http://opensource.org/licenses/BSD-2-Clause)\n* **Authors:** Drew Schmidt and Homer White\n\n\nMost search engines have a \"did you mean?\" feature, where suggestions are given in the presence of likely typos.  And while search engines use sophisticated NLP methods on their vast amounts of user-generated data to create accurate suggestions, it turns out that you can get by with some ancient spellchecker techniques.\n\n\n\n## Installation\n\n<!-- To install the R package, run:\n\n```r\ninstall.package(\"Rdym\")\n``` -->\n\nThe development version is maintained on GitHub, and can easily be installed by any of the packages that offer installations from GitHub:\n\n```r\n### Pick your preference\ndevtools::install_github(\"wrathematics/Rdym\")\nghit::install_github(\"wrathematics/Rdym\")\nremotes::install_github(\"wrathematics/Rdym\")\n```\n\n\n\n## Example\n\nUsage of the package is completely passive, beyond loading it with the usual `library(Rdym)` call.  Say for example you run:\n\n```r\nshapro.test(x=rnorm(20))\n# Error: object 'shapro.test' not found\n```\n\nNote the missing \"i\" in what should be `shapiro.test()`.  With Rdym loaded, you can get a \"did you mean?\" suggestion along with the error:\n\n```r\nlibrary(Rdym)\n\nshapro.test(x=rnorm(20))\n# Error: could not find function \"shapro.test\"\n# \n# Did you mean:  shapiro.test()  ?\n# shapiro.test(x=rnorm(20))\n```\n\nIf the spellchecker guessed correctly, then you should be able to just copy/paste the suggestion after the \"Did you mean\" line into R.\n\nSuggestions are given as errors are discovered by the R interpreter.  For example:\n\n```r\nlibrary(Rdym)\n\nshapro.test(rmorm(20))\n# Error: could not find function \"shapro.test\"\n# \n# Did you mean:  shapiro.test  ?\n# shapiro.test(rmorm(20))\n\nshapiro.test(rmorm(20))\n# Error in stopifno",
    "url": "https://github.com/wrathematics/Rdym",
    "last_updated": "2024-08-31T17:16:39+00:00"
  },
  {
    "full_name": "bqplot/bqplot",
    "name": "bqplot",
    "description": "Plotting library for IPython/Jupyter notebooks",
    "language": "TypeScript",
    "topics": [
      "visualizations",
      "jupyter",
      "ipython"
    ],
    "readme": "# bqplot\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/bqplot/design/master/bqplot-logo-dark-nobackground.svg\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/bqplot/design/master/bqplot-logo-nobackground.svg\">\n  <img alt=\"bqplot.\" src=\"https://raw.githubusercontent.com/bqplot/design/master/bqplot-logo-dark-nobackground.svg\">\n</picture>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bqplot/bqplot/stable?filepath=examples/Index.ipynb)\n[![Chat](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jupyter-widgets/Lobby)\n\n2-D plotting library for Project Jupyter\n\n## Introduction\n\n`bqplot` is a 2-D visualization system for Jupyter, based on the constructs of\nthe *Grammar of Graphics*.\n\n[![Wealth of Nations](./wealth-of-nations.gif)](https://github.com/bqplot/bqplot/blob/master/examples/Applications/Wealth%20Of%20Nations/Bubble%20Chart.ipynb)\n\nIn bqplot, every component of a plot is an interactive widget. This allows the\nuser to integrate visualizations with other Jupyter interactive widgets to\ncreate integrated GUIs with a few lines of Python code.\n\n## Documentation\n\nYou can follow the documentation on https://bqplot.github.io/bqplot\n\n## Trying it online\n\nTo try out bqplot interactively in your web browser, just click on the binder\nlink:\n\n[![Binder](docs/source/binder-logo.svg)](https://mybinder.org/v2/gh/bqplot/bqplot/stable?filepath=examples/Index.ipynb)\n\n### Dependencies\n\nThis package depends on the following packages:\n\n- `ipywidgets` (version >=7.0.0, <8.0)\n- `traitlets` (version >=4.3.0, <5.0)\n- `traittypes` (Version >=0.2.1, <0.3)\n- `numpy`\n- `pandas`\n\n### Installation\n\nUsing pip:\n\n```\n$ pip install bqplot\n```\n\nUsing conda\n\n```\n$ conda install -c conda-forge bqplot\n```\n\nIf you are using JupyterLab <=2:\n\n```\n$ jupyter labextension install @jupyter-widgets/jupyterlab-manager bqplot\n```\n\n##### Development installation\n\nFor a developmen",
    "url": "https://github.com/bqplot/bqplot",
    "last_updated": "2025-08-26T02:22:46+00:00"
  },
  {
    "full_name": "hhatto/autopep8",
    "name": "autopep8",
    "description": "A tool that automatically formats Python code to conform to the PEP 8 style guide.",
    "language": "Python",
    "topics": [
      "python",
      "formatter",
      "pep8",
      "codeformatter"
    ],
    "readme": "========\nautopep8\n========\n\n.. image:: https://img.shields.io/pypi/v/autopep8.svg\n    :target: https://pypi.org/project/autopep8/\n    :alt: PyPI Version\n\n.. image:: https://github.com/hhatto/autopep8/workflows/Python%20package/badge.svg\n    :target: https://github.com/hhatto/autopep8/actions\n    :alt: Build status\n\n.. image:: https://codecov.io/gh/hhatto/autopep8/branch/main/graph/badge.svg\n    :target: https://codecov.io/gh/hhatto/autopep8\n    :alt: Code Coverage\n\nautopep8 automatically formats Python code to conform to the `PEP 8`_ style\nguide. It uses the pycodestyle_ utility to determine what parts of the code\nneeds to be formatted. autopep8 is capable of fixing most of the formatting\nissues_ that can be reported by pycodestyle.\n\n.. _PEP 8: https://www.python.org/dev/peps/pep-0008/\n.. _issues: https://pycodestyle.readthedocs.org/en/latest/intro.html#error-codes\n\n.. contents::\n\n\nInstallation\n============\n\nFrom pip::\n\n    $ pip install --upgrade autopep8\n\nConsider using the ``--user`` option_.\n\n.. _option: https://pip.pypa.io/en/latest/user_guide/#user-installs\n\n\nRequirements\n============\n\nautopep8 requires pycodestyle_.\n\n.. _pycodestyle: https://github.com/PyCQA/pycodestyle\n\n\nUsage\n=====\n\nTo modify a file in place (with aggressive level 2)::\n\n    $ autopep8 --in-place --aggressive --aggressive <filename>\n\nBefore running autopep8.\n\n.. code-block:: python\n\n    import math, sys;\n\n    def example1():\n        ####This is a long comment. This should be wrapped to fit within 72 characters.\n        some_tuple=(   1,2, 3,'a'  );\n        some_variable={'long':'Long code lines should be wrapped within 79 characters.',\n        'other':[math.pi, 100,200,300,9876543210,'This is a long string that goes on'],\n        'more':{'inner':'This whole logical line should be wrapped.',some_tuple:[1,\n        20,300,40000,500000000,60000000000000000]}}\n        return (some_tuple, some_variable)\n    def example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));\n    c",
    "url": "https://github.com/hhatto/autopep8",
    "last_updated": "2025-08-29T20:11:16+00:00"
  },
  {
    "full_name": "f0uriest/interpax",
    "name": "interpax",
    "description": "Interpolation and function approximation with JAX",
    "language": "Python",
    "topics": [],
    "readme": "\n########\ninterpax\n########\n|License| |DOI| |Issues| |Pypi|\n\n|Docs| |UnitTests| |Codecov|\n\ninterpax is a library for interpolation and function approximation using JAX.\n\nIncludes methods for nearest neighbor, linear, and several cubic interpolation schemes\nin 1d, 2d, and 3d, as well as Fourier interpolation for periodic functions in\n1d and 2d.\n\nComing soon:\n- Spline interpolation for rectilinear grids in N-dimensions\n- RBF interpolation for unstructured data in N-dimensions\n- Smoothing splines for noisy data\n\n\nInstallation\n============\n\ninterpax is installable with `pip`:\n\n.. code-block:: console\n\n    pip install interpax\n\n\n\nUsage\n=====\n\n.. code-block:: python\n\n    import jax.numpy as jnp\n    import numpy as np\n    from interpax import interp1d\n\n    xp = jnp.linspace(0, 2 * np.pi, 100)\n    xq = jnp.linspace(0, 2 * np.pi, 10000)\n    f = lambda x: jnp.sin(x)\n    fp = f(xp)\n\n    fq = interp1d(xq, xp, fp, method=\"cubic\")\n    np.testing.assert_allclose(fq, f(xq), rtol=1e-6, atol=1e-5)\n\n\nFor full details of various options see the `API documentation <https://interpax.readthedocs.io/en/latest/api.html>`__\n\n\n.. |License| image:: https://img.shields.io/github/license/f0uriest/interpax?color=blue&logo=open-source-initiative&logoColor=white\n    :target: https://github.com/f0uriest/interpax/blob/master/LICENSE\n    :alt: License\n\n.. |DOI| image:: https://zenodo.org/badge/706703896.svg\n    :target: https://zenodo.org/doi/10.5281/zenodo.10028967\n    :alt: DOI\n\n.. |Docs| image:: https://img.shields.io/readthedocs/interpax?logo=Read-the-Docs\n    :target: https://interpax.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation\n\n.. |UnitTests| image:: https://github.com/f0uriest/interpax/actions/workflows/unittest.yml/badge.svg\n    :target: https://github.com/f0uriest/interpax/actions/workflows/unittest.yml\n    :alt: UnitTests\n\n.. |Codecov| image:: https://codecov.io/github/f0uriest/interpax/graph/badge.svg?token=MB11I7WE3I\n    :target: https://codecov.io/github/f0uriest/interp",
    "url": "https://github.com/f0uriest/interpax",
    "last_updated": "2025-08-28T09:33:46+00:00"
  },
  {
    "full_name": "HistoryAtState/travels",
    "name": "travels",
    "description": "Source data for Foreign Travels of the President and Secretary of State",
    "language": "XQuery",
    "topics": [],
    "readme": "# Travels\n\n[![exist-db CI](https://github.com/HistoryAtState/travels/actions/workflows/build.yml/badge.svg)](https://github.com/HistoryAtState/travels/actions/workflows/build.yml)\n\nSource data for [Foreign Travels of the President and Secretary of State](http://history.state.gov/departmenthistory/travels)\n\n## Build\n\n1. Single `xar` file: The `collection.xconf` will only contain the index, not any triggers!\n\n    ```shell\n    ant\n    ```\n\n    1. Since Releases have been automated when building locally you might want to supply your own version number (e.g. `X.X.X`) like this:\n\n    ```shell\n    ant -Dapp.version=X.X.X\n    ```\n\n## Release\n\nReleases for this data package are automated. Any commit to the `master` branch will trigger the release automation.\n\nAll commit message must conform to [Conventional Commit Messages](https://www.conventionalcommits.org/en/v1.0.0/) to determine semantic versioning of releases, please adhere to these conventions, like so:\n\n| Commit message  | Release type |\n|-----------------|--------------|\n| `fix(pencil): stop graphite breaking when too much pressure applied` | Patch Release |\n| `feat(pencil): add 'graphiteWidth' option` | ~~Minor~~ Feature Release |\n| `perf(pencil): remove graphiteWidth option`<br/><br/>`BREAKING CHANGE: The graphiteWidth option has been removed.`<br/>`The default graphite width of 10mm is always used for performance reasons.` | ~~Major~~ Breaking Release |\n\nWhen opening PRs commit messages are checked using commitlint.\n",
    "url": "https://github.com/HistoryAtState/travels",
    "last_updated": "2025-08-25T09:03:08+00:00"
  },
  {
    "full_name": "mstem/archive.org-getter",
    "name": "archive.org-getter",
    "description": "Ruby script to download bulk results from Archive.org's TV News database of closed captions",
    "language": "Python",
    "topics": [],
    "readme": "archive.org-getter\n==================\n\nRuby script to download bulk results from Archive.org's TV News database of closed captions\n\n*Authors*\nRahul Bhargava and Matt Stempeck\n_MIT Center for Civic Media_\n\nRequirements\n------------\n\nYou need ruby and the internets.\n\nRunning the Script\n------------------\n\n1. Open the script in a text editor and edit line 11 of the code to change 'Your Query' to your preferred search term(s), and save it\n2. Go to the command line (Terminal on a Mac, DOS or Cygwin in Windows)\n3. Navigate to the folder that contains the script\n4. Type in `ruby archive.org-getter.rb` and hit enter\n\nResults\n-------\nYour results will show up in the same directory as the script itself. The results returned will be in JSON, the open data format. You can adjust how many results to return at once (by changing the ROWS variable in the script), but *go easy on Archive.org’s servers*: You’ll get your results faster (nearly instantly) in smaller batches of 200 or so.\n\nUsing the Data\n--------------\n\nOnce you have your data, you can combine, clean, and parse it with [Google Refine](http://code.google.com/p/google-refine/). I found [ProPublica’s guide to cleaning messy data really helpful](http://www.propublica.org/nerds/item/using-google-refine-for-data-cleaning). You may also want to de-duplicate, because Archive.org records TV news broadcasts on the both the east and west coasts.\n\nWhat you can do with it\n-----------------------\n\n*Analyze a story:* You could search for a specific story, like the recent controversial Steubenville rape case, and quickly get a sense of which news companies are covering the case and which words they use to talk about it. You can also share links to specific clips with your friends and colleagues.\n\nYou could also investigate our professional media’s treatment of a broader topic. You could trace the spread of the phrase “Obamacare” or watch the many breathless news segments covering “technology.”\n\n*Visualize TV news data*: You’ll also hav",
    "url": "https://github.com/mstem/archive.org-getter",
    "last_updated": "2020-08-27T04:52:29+00:00"
  },
  {
    "full_name": "quanteda/quanteda",
    "name": "quanteda",
    "description": "An R package for the Quantitative Analysis of Textual Data",
    "language": "R",
    "topics": [
      "r",
      "text-analytics",
      "natural-language-processing",
      "corpus",
      "quanteda"
    ],
    "readme": "\n[![quanteda: quantitative analysis of textual\ndata](https://cdn.rawgit.com/quanteda/quanteda/master/images/quanteda_logo.svg)](http://quanteda.io)\n\n<!-- badges: start -->\n\n[![CRAN\nVersion](https://www.r-pkg.org/badges/version/quanteda)](https://CRAN.R-project.org/package=quanteda)\n[![](https://img.shields.io/badge/devel%20version-4.3.1-royalblue.svg)](https://github.com/quanteda/quanteda)\n[![Downloads](https://cranlogs.r-pkg.org/badges/quanteda)](https://CRAN.R-project.org/package=quanteda)\n[![Total\nDownloads](https://cranlogs.r-pkg.org/badges/grand-total/quanteda?color=orange)](https://CRAN.R-project.org/package=quanteda)\n[![R-CMD-check](https://github.com/quanteda/quanteda/actions/workflows/check-standard.yaml/badge.svg)](https://github.com/quanteda/quanteda/actions/workflows/check-standard.yaml)\n[![codecov](https://codecov.io/gh/quanteda/quanteda/branch/master/graph/badge.svg)](https://app.codecov.io/gh/quanteda/quanteda)\n[![DOI](https://zenodo.org/badge/5424649.svg)](https://zenodo.org/badge/latestdoi/5424649)\n[![DOI](http://joss.theoj.org/papers/10.21105/joss.00774/status.svg)](https://doi.org/10.21105/joss.00774)\n<!-- badges: end -->\n\n## About\n\n**quanteda** is an R package for managing and analyzing text, created\nand maintained by [Kenneth Benoit](https://kenbenoit.net) and [Kohei\nWatanabe](https://blog.koheiw.net/). Its creation was funded by the\nEuropean Research Council grant ERC-2011-StG 283794-QUANTESS and its\ncontinued development is supported by the [Quanteda Initiative\nCIC](https://quanteda.org).\n\nFor more details, see <https://quanteda.io>.\n\n## **quanteda** version 4\n\nThe **quanteda** 4.0 is a major release that improves functionality and\nperformance and further improves function consistency by removing\npreviously deprecated functions. It also includes significant new\ntokeniser rules that make the default tokeniser smarter than ever, with\nnew Unicode and ICU-compliant rules enabling it to work more\nconsistently with even more languages.\n\nWe describe ",
    "url": "https://github.com/quanteda/quanteda",
    "last_updated": "2025-08-05T16:26:50+00:00"
  },
  {
    "full_name": "mfbx9da4/git-sub-dir",
    "name": "git-sub-dir",
    "description": "Downloads sub-directory of a github repository.",
    "language": "Python",
    "topics": [],
    "readme": "Downloads git sub dir\n\n## Usage\n    python get_git_sub_dir.py user/repo <options>\n    python get_git_sub_dir.py user/private_repo --private <options>\n\n## Options Flags:\n- `--private`: the repo is private (default is `False`, username and password will be requested)\n- `-r`: recursive download (default is `True`)\n- `-p`: filepath\n- `-b`: branch\n\n## Example\n\nLet's download the docs from twitter bootstrap https://github.com/twbs/bootstrap/tree/master/docs\n\n    python get_git_sub_dir.py twbs/bootstrap -p docs\n\nIf we don't want it to be recursive\n\n    python get_git_sub_dir.py twbs/bootstrap -p docs -r False\n\nIf we want a specific file\n\n    python get_git_sub_dir.py twbs/bootstrap -p docs/examples/blog/index.html\n\nIf we want to download from a specific branch (say `fix-15534`)\n\n    python get_git_sub_dir.py twbs/bootstrap -p docs/examples/blog/index.html -b fix-15534\n",
    "url": "https://github.com/mfbx9da4/git-sub-dir",
    "last_updated": "2024-12-16T08:13:05+00:00"
  },
  {
    "full_name": "hadley/secure",
    "name": "secure",
    "description": "Secure private R data in public packages",
    "language": "R",
    "topics": [],
    "readme": "# secure\n\n[![Build Status](https://travis-ci.org/hadley/secure.png?branch=master)](https://travis-ci.org/hadley/secure)\n\nThe secure package provides a secure vault within a publicly available code repository. It allows you to store private information in a public repository so that only select people can read it. This is particularly useful for testing because you can now store private credentials in your public repo, without them being readable by the world.\n\nSecure is built on top of asymmetric (public/private key) encryption. Secure generates a random master key and uses that to encrypt (with AES256) each file in `vault/`. The master key is not stored unencrypted anywhere; instead, an encrypted copy is stored for each user, using their own public key. Each user can than decrypt the encrypted master key using their private key, then use that to decrypt each file.\n\n## Installation\n\nSecure is currently only available on github. Install it with:\n\n```R\n# install.packages(\"devtools\")\ndevtools::install_github(\"s-u/PKI\") # needed for bug fixes not currently on CRAN\ndevtools::install_github(\"hadley/secure\")\n```\n\n## First steps\n\nTo get started:\n\n* Create a `vault` directory.\n\n* Add yourself as as user with `secure::add_user(\"your name\", local_key())`. \n  This will add your name and public key to `vault/users.json`.\n  (You can add other people from their `github_key()`s).\n\n* Securely store data: \n  `secure::encrypt(\"google\", key = \"abcdasdf\", secret = \"asdfsad\")`.\n  This creates `secure/google.rds.enc`, an encrypted rds file.\n\n* Retrieve encrypted data: `secure::decrypt(\"google\")`. This decrypts\n  the encrypted file using your private key.\n\n## In a package\n\n* Create `inst/vault` and add `secure` to the `Suggests` field in the \n  `DESCRIPTION` (or run `secure::use_secure()`).\n\n* If you use travis, add the public key for your travis repo:\n  `secure::add_user(\"travis\", travis_key(\"user/repo\"))`.\n\n* When developing locally, you can use all functions as is. They look for\n  a vau",
    "url": "https://github.com/hadley/secure",
    "last_updated": "2025-04-17T14:34:59+00:00"
  },
  {
    "full_name": "BIDS/sparkR-demo",
    "name": "sparkR-demo",
    "description": "Demo and examples for using spark with R",
    "language": "",
    "topics": [],
    "readme": "# Notes for the sparkR demo  \n_January 28, 11 - 12 pm_\n\n__Installation__\n\nIf you're not keen on building from source, download the [SparkR binaries](http://www.cs.berkeley.edu/~shivaram/sparkr-bin/). There is also a [training dataset](http://d12yw77jruda6f.cloudfront.net/training-downloads.zip). \n\n\n__Exercises__\n\n1. [http://ampcamp.berkeley.edu/5/exercises/sparkr.html](http://ampcamp.berkeley.edu/5/exercises/sparkr.html)\n",
    "url": "https://github.com/BIDS/sparkR-demo",
    "last_updated": "2016-05-31T09:55:51+00:00"
  },
  {
    "full_name": "markvanderloo/hashr",
    "name": "hashr",
    "description": "Quicly compute hash values for R objects",
    "language": "C",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/markvanderloo/hashr.svg)](https://travis-ci.org/markvanderloo/hashr) \n[![Coverage Status](https://coveralls.io/repos/markvanderloo/hashr/badge.svg?branch=master)](https://coveralls.io/r/markvanderloo/hashr?branch=master)\n[![CRAN version](http://www.r-pkg.org/badges/version/hashr)](http://www.r-pkg.org/pkg/hashr)\n[![status](https://tinyverse.netlify.app/badge/hashr)](https://CRAN.R-project.org/package=<package>)\n[![RStudio CRAN Downloads](http://cranlogs.r-pkg.org/badges/hashr)](http://www.r-pkg.org/pkg/hashr)\n\n# hashr\nQuicly compute hash values for R objects\n\n# Installation\nFrom R:\n```\ninstall.packages(\"hashr\")\n```\nThe latest beta-versions are available through my [drat](http://www.r-pkg.org/pkg/drat) repository. \n```\n# if you don't have drat:\ninstall.packages('drat')\n\n# to install from my drat repo:\ndrat::addRepo(\"markvanderloo\")\ninstall.packages(\"hashr\")\n```\nIf you are in for some excitement you can build the development version by cloning this github page.\nIn a `bash` command shell do:\n```\ngit clone https://github.com/markvanderloo/hashr.git\ncd hashr\n./build.bash\nR CMD INSTALL output/hashr*.tar.gz\n```\n\n\n",
    "url": "https://github.com/markvanderloo/hashr",
    "last_updated": "2024-06-14T09:13:54+00:00"
  },
  {
    "full_name": "kbroman/Talk_Graphs",
    "name": "Talk_Graphs",
    "description": "Talk: How to display data badly",
    "language": "R",
    "topics": [],
    "readme": "## Talk on &ldquo;Creating effective figures and tables&rdquo;\n\nThese are slides for a talk that I give as often as possible, because\nit's fun.\n\nA recent PDF is\n[here](https://www.biostat.wisc.edu/~kbroman/presentations/graphs2018.pdf).\n\nA video is available [on youtube](https://youtu.be/Ssso_5X1UPs).\n\nA whole bunch of older versions of the slides are at\n[my UW&ndash;Madison webpage](https://kbroman.org/talks.html)\nincluding the\n[Director's cut](https://www.biostat.wisc.edu/~kbroman/presentations/graphs_mac.ppt)\nof\n[the censored version](https://kbroman.wordpress.com/2012/11/21/the-hopkins-sph-logo-part-2/).\n\nThe [R](https://github.com/kbroman/Talk_Graphs/tree/master/R)\ndirectory contains the R code to make the (generally nicer) figures,\nplus Excel files in which the uglier figures were made.\n\nThe [Figs](https://github.com/kbroman/Talk_Graphs/tree/master/Figs)\ndirectory contains only the Excel-based figures; the R-based figures\nare not included in the repository.\n\nThere are also slides of the\n&ldquo;[Top Ten Worst Graphs](https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs)&rdquo;\n([pdf here](https://www.biostat.wisc.edu/~kbroman/presentations/topten.pdf)).\nThe needed figures are not in this repository, but are available\n[here](https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/TopTenWorstGraphs.zip).\n\n---\n\nTo the extent possible under law,\n[Karl Broman](https://github.com/kbroman)\nhas waived all copyright and related or neighboring rights to\n&ldquo;[Creating effective figures and tables](https://github.com/kbroman/Talk_Graphs)&rdquo;.\nThis work is published from the United States.\n<br/>\n[![CC0](https://i.creativecommons.org/p/zero/1.0/88x31.png)](https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "url": "https://github.com/kbroman/Talk_Graphs",
    "last_updated": "2025-06-09T00:34:51+00:00"
  },
  {
    "full_name": "sholiday/papers",
    "name": "papers",
    "description": "Academic papers and articles that I have read or plan to read",
    "language": "",
    "topics": [],
    "readme": "papers\n======\n\nAcademic papers and articles that I have read or plan to read\n",
    "url": "https://github.com/sholiday/papers",
    "last_updated": "2024-10-21T10:17:42+00:00"
  },
  {
    "full_name": "gpuRcore/gpuRcuda",
    "name": "gpuRcuda",
    "description": "CUDA GPU functions for R Objects",
    "language": "R",
    "topics": [],
    "readme": "# gpuRcuda: The Simple CUDA GPU Interface for R\n[![Travis-CI Build Status](https://travis-ci.org/gpuRcore/gpuRcuda.png?branch=master)](https://travis-ci.org/gpuRcore/gpuRcuda)\n\nTest coverage: [![Coverage Status](https://coveralls.io/repos/github/gpuRcore/gpuRcuda/badge.svg?branch=master)](https://coveralls.io/github/gpuRcore/gpuRcuda?branch=master)\n\nWelcome to gpuRcuda!  This package is designed to be an extension upon the\nmore general [gpuR](https://github.com/cdeterman/gpuR) package.  Essentially,\nthis package creates a new series of classes that mirror those from\ngpuR classes.  The key aspect of this\npackage is to allow the user to use a CUDA backend where the NVIDIA specific\nlanguage will improve overall performance.\n\nThe syntax is designed to be identical to [gpuR](https://github.com/cdeterman/gpuR)\n\n```r\nORDER <- 1024\nA <- matrix(rnorm(ORDER^2), nrow=ORDER)\nB <- matrix(rnorm(ORDER^2), nrow=ORDER)\ngpuA <- cudaMatrix(A, type=\"double\")\ngpuB <- cudaMatrix(B, type=\"double\")\n\nC <- A %*% B\ngpuC <- gpuA %*% gpuB\n\nall(C == gpuC)\n[1] TRUE\n```\n\n### Dependencies\n1. opencl-headers (shared library)\n2. NVIDIA Drivers & SDK\n\n### NVIDIA Driver and CUDA/OpenCL\n#### Up-to-date Card\nIf you are fortunate enough to have a very recent card that you can\nuse the most recent drivers.  THis install is much more simple\n```\n# Install Boost & OpenCL headers\nsudo apt-get install opencl-headers\n\n# Install NVIDIA Drivers and CUDA\nsudo add-apt-repository -y ppa:xorg-edgers/ppa\nsudo apt-get update\nsudo apt-get install nvidia-346 nvidia-settings\nsudo apt-get install cuda\n```\n\n#### Older Card\nIf you have an older card that doesn't support the newest drivers:\n\n1. Purge any existing nvidia and cuda implementations \n(`sudo apt-get purge cuda* nvidia-*`)\n2. Download appropriate CUDA toolkit for the specific card.  You can figure \nthis out by first checking which NVIDIA driver is compatible with your card\nby searching for it in [NVIDIA's Driver Downloads](http://www.nvidia.com/Download/index.aspx?lang",
    "url": "https://github.com/gpuRcore/gpuRcuda",
    "last_updated": "2025-02-27T23:31:04+00:00"
  },
  {
    "full_name": "YuanheZ/LoRA-One",
    "name": "LoRA-One",
    "description": "LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large  Language Models, Provably and Efficiently (ICML2025 Oral)",
    "language": "Python",
    "topics": [
      "chain-of-thought",
      "commonsense-reasoning",
      "cot",
      "instruction-tuning",
      "large-language-models",
      "large-vision-language-models",
      "lora",
      "parameter-efficient-fine-tuning",
      "reasoning",
      "icml-2025"
    ],
    "readme": "The Official PyTorch implementation of [**LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently**](https://arxiv.org/abs/2502.01235).\n\n### Content Overview\n\nThis paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA) ([Hu et al., 2022](https://arxiv.org/abs/2106.09685)) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately—applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation.\n<h1 align=\"center\"> \n    <img src=\"./img/math_long_run-1.png\" width=\"600\">\n</h1>\n\n---\n### Algorithmic Overview\n\nFor each weight matrix, we first compute the full-batch gradient $\\nabla_{W} L$ under full fine-tuning and perform SVD on $-\\nabla_{W} L$ to get $U$, $\\Sigma$, $V$, then we initialize LoRA via\n```math\n\\mathbf{A}_{0}=\\frac{1}{\\sqrt{\\gamma}} U_{[:,:r]} Diag(S[:r])\\,,\\quad \\mathbf{B}_{0}=\\frac{1}{\\sqrt{\\gamma}} Diag(S[:r]) V_{[:,:r]}^\\top\\,,\\quad W_{adapted} = W_{pre}+\\frac{\\alpha}{\\sqrt{r}}\\mathbf{A}_{0} \\mathbf{B}_{0}\\,,\n```\nwhich is equivalent to perform one best r-rank full-batch gradient descent under full fine-tuning with learning rate $\\fr",
    "url": "https://github.com/YuanheZ/LoRA-One",
    "last_updated": "2025-08-21T12:22:27+00:00"
  },
  {
    "full_name": "EliotAndres/pretrained.ml",
    "name": "pretrained.ml",
    "description": "[DEPRECATED] Compilation of pre-trained deep learning models with demos and code.",
    "language": "Python",
    "topics": [],
    "readme": "# [DEPRECATED] pretrained.ml\n\n[DEPRECATED] Sortable and searchable compilation of pre-trained deep learning models. With demos and code.\n\n**DEPRECATED**: You can find an alternative on [modeldepot.io (with live demos)](https://modeldepot.io) or on [modelzoo.co](http://modelzoo.co)\n\n## A word of warning\nThis is running on a server without GPU, hence it seems slow.\n\nAlso, the code may look a bit like monkey-patching for the following reasons:\n - Models are cloned as submodules: therefore we have to mess around with the python path :-(\n - There is a queuing systems for the jobs (allows user to see their job's position in the queue)\n\n## About\nHaving spent too much time installing deep learning models just to evaluate their performance, I created this repo for several reasons:\n - Access a free demo of deep learning models\n - Gather available deep learning models\n - Get a docker container running the model for a quick install\n\n## Installation\nRequirements: Docker, docker-compose and enough space free for the model weights.\n\n    git clone https://github.com/EliotAndres/pretrained.ml --recursive\n    cd containers\n    docker-compose build\n    docker-compose up -d\n\n## Useful commands\n    docker ps #list images\n    docker attach [container_id] #attach a shell to specific image\n\n## Contributing\nMany models are missing. Any help is welcome ! You have two options to contribute.\n\nEasy way: Add a model to the list without a demo:\n - Fork the repo\n - Edit the **docs/models.yaml** file (you can even edit it with Github's editor)\n - Make a pull request\n\nOther way: add a model with a demo:\n - Fork the repo\n - Add the model inside one of the docker containers\n - Create a route in the serve.py file\n - Add a demo calling the route\n - Make a pull request\n\n\n## Todos\n- [ ] Use nvidia-docker ?\n- [ ] Add flag to compile Tensorflow\n- [ ] Consider splitting each model in a different container ?\n- [ ] Linter\n- [ ] Add analytics\n",
    "url": "https://github.com/EliotAndres/pretrained.ml",
    "last_updated": "2025-08-09T16:01:02+00:00"
  },
  {
    "full_name": "gostevehoward/confseq",
    "name": "confseq",
    "description": "Confidence sequences and uniform boundaries",
    "language": "C++",
    "topics": [],
    "readme": "# Confidence sequences and uniform boundaries\n\nThis library supports calculation of uniform boundaries, confidence sequences,\nand always-valid p-values. These constructs are useful in sequential A/B\ntesting, best-arm identification, and other sequential statistical\nprocedures. The library is written in C++ and Python with a full Python interface and \npartial R interface. The main references are\n\n- Howard, S. R., Ramdas, A., McAuliffe, J., and Sekhon, J. (2021), [Time-uniform,\nnonparametric, nonasymptotic confidence \nsequences](https://arxiv.org/abs/1810.08240), The Annals of Statistics, 49(2), \n1055-1080.\n\n- Howard, S. R. and Ramdas, A. (2021), [Sequential estimation of quantiles with\napplications to A/B-testing and best-arm\nidentification](https://arxiv.org/abs/1906.09712), Bernoulli, to appear.\n\n- Waudby-Smith, I. and Ramdas, A. (2021), [Estimating means of bounded random\nvariables by betting](https://arxiv.org/pdf/2010.09686.pdf), preprint,\narXiv:2010.09686.\n\n- Waudby-Smith, I. and Ramdas, A. (2020), [Confidence sequences for sampling\nwithout replacement](https://arxiv.org/pdf/2006.04347.pdf), NeurIPS, 33.\n\nThis library is in early-stage development and should not be considered\nstable. Automated tests run on Python 3.7, 3.8, 3.9 and 3.10 on the latest \nUbuntu and macOS.\n\nThe C++ implementation requires a compiler with C++14 to build\nthe package, as well as the Boost C++ headers.\n\nIn the Python package, functions are split across modules by topic, as detailed\nbelow. In the R package, all functions mentioned below are exported in a single\nnamespace.\n\n## Installing the python package\n\nRun `pip install confseq` at the command line.\n\n## Installing the R package\n\nRun the following in the R console:\n\n```R\ninstall.packages('devtools')\ndevtools::install_github('gostevehoward/confseq/r_package')\n```\n\n## Demos\n\n### Estimating average treatment effect in a randomized trial\n\n`demo/ate_demo.py` illustrates how to compute a confidence sequence for average\ntreatment effect in a ",
    "url": "https://github.com/gostevehoward/confseq",
    "last_updated": "2025-08-03T12:07:37+00:00"
  },
  {
    "full_name": "scipion-em/scipion-em-isonet",
    "name": "scipion-em-isonet",
    "description": "This plugin is a Scipion wrapper for Isonet, a cryo-electron tomography software for filling the missing wedge information. The official webpage of the software is https://isonetcryoet.com/ and the source code can be found at https://github.com/IsoNet-cryoET/IsoNet",
    "language": "Python",
    "topics": [
      "tomo"
    ],
    "readme": "================\nIsoNet plugin\n================\n\nThis plugin allows to use IsoNet programs within the Scipion framework\n\n`IsoNet <https://github.com/IsoNet-cryoET/IsoNet/>`_ stand for ISOtropic reconstruction of Electron Tomography.\nIt train deep neural networks to reconstruct meaningful contents in the missing wedge for electron tomography\nincrease signal-to-noise ratio using the information learned from the original tomogram. The software requires\ntomograms as input. Observing at about 30A resolutions, the IsoNet generated tomograms are largely isotropic.\n\n\nYou will need to use `3.0.0 <https://scipion-em.github.io/docs/release-3.0.0/docs/scipion-modes/how-to-install.html>`_ version of Scipion to run these protocols.\n\n\nProtocols\n==========\n\n* **Isotropic Reconstruction**: Isotropic Reconstruction of Electron Tomograms with Deep Learning\n\n**Latest plugin version**\n==========================\n\n**v3.0.2**\n-----------\n* **fixed**     :  Validated gcc version\n* **fixed**     :  Fixed an error related with cuda lib\n\n\n**v3.0.1**\n-----------\n* **new**     :  Isotropic Reconstruction\n\n**v3.0.0**\n-----------\n* **new**     :  First plugin version\n\n\n**Installing the plugin**\n=========================\n\nIn order to install the plugin follow these instructions:\n\n1. **Install the plugin**\n\n.. code-block::\n\n     scipion installp -p scipion-em-isonet\n\nor through the **plugin manager** by launching Scipion and following **Configuration** >> **Plugins**\n\n**To install in development mode**\n\n- Clone or download the plugin repository\n\n.. code-block::\n\n          git clone https://github.com/scipion-em/scipion-em-isonet.git\n\n- Install the plugin in developer mode.\n\n.. code-block::\n\n  scipion installp -p local/path/to/scipion-em-isonet --devel\n\n**Configuration variables**\n=========================\n\nThere are some *optional* variables related to the IsoNet plugin installation. For example, if you have installed IsoNet outside of Scipion, you may define ``ISONET_HOME`` in your ``scipion.conf``",
    "url": "https://github.com/scipion-em/scipion-em-isonet",
    "last_updated": "2025-03-03T14:27:20+00:00"
  },
  {
    "full_name": "guidance-ai/guidance",
    "name": "guidance",
    "description": "A guidance language for controlling large language models.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "<div align=\"right\"></div>\n<div align=\"center\"><picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/figures/guidance_logo_blue_dark.svg\">\n  <img alt=\"guidance\" src=\"docs/figures/guidance_logo_blue.svg\" width=300\">\n</picture></div>\n<br/>\n\n**Guidance is an efficient programming paradigm for steering language models.** With Guidance, you can control how output is structured and get high-quality output for your use case—*while reducing latency and cost vs. conventional prompting or fine-tuning.* It allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditionals, loops, tool use) and generation seamlessly.\n\n   * [Install](#install)\n   * [Features](#features)\n\n\n## Install\nGuidance is available through PyPI and supports a variety of backends (Transformers, llama.cpp, OpenAI, etc.).\nIf you already have the backend required for your model, you can simply run\n```bash\npip install guidance\n```\n\n## Features\n\n### A Pythonic interface for language models\n\nWhen using Guidance, you can work with large language models using common Python idioms:\n\n```python\nfrom guidance import system, user, assistant, gen\nfrom guidance.models import Transformers\n\n# Could also do LlamaCpp or many other models\nphi_lm = Transformers(\"microsoft/Phi-4-mini-instruct\")\n\n# Model objects are immutable, so this is a copy\nlm = phi_lm\n\nwith system():\n    lm += \"You are a helpful assistant\"\n\nwith user():\n    lm += \"Hello. What is your name?\"\n\nwith assistant():\n    lm += gen(max_tokens=20)\n\nprint(lm)\n```\nIf run at the command line, this will produce output like:\n```\n<|system|>You are a helpful assistant<|end|><|user|>Hello. What is your name?<|end|><|assistant|>I am Phi, an AI developed by Microsoft. How can I help you today?\n```\nHowever, if running in a Jupyter notebook, then Guidance provides a widget for a richer user experience:\n\n<img src=\"docs/figures/widget_basic_example_20250703.png\" alt=\"Guidance widget showing HTML generation\" />\n\nWith Guidanc",
    "url": "https://github.com/guidance-ai/guidance",
    "last_updated": "2025-09-02T09:37:03+00:00"
  },
  {
    "full_name": "voideditor/void",
    "name": "void",
    "description": "",
    "language": "TypeScript",
    "topics": [
      "cursor",
      "editor",
      "chatgpt",
      "claude",
      "copilot",
      "developer-tools",
      "llm",
      "open-source",
      "openai",
      "visual-studio-code",
      "vscode",
      "vscode-extension"
    ],
    "readme": "# Welcome to Void.\n\n<div align=\"center\">\n\t<img\n\t\tsrc=\"./src/vs/workbench/browser/parts/editor/media/slice_of_void.png\"\n\t \talt=\"Void Welcome\"\n\t\twidth=\"300\"\n\t \theight=\"300\"\n\t/>\n</div>\n\nVoid is the open-source Cursor alternative.\n\nUse AI agents on your codebase, checkpoint and visualize changes, and bring any model or host locally. Void sends messages directly to providers without retaining your data.\n\nThis repo contains the full sourcecode for Void. If you're new, welcome!\n\n- 🧭 [Website](https://voideditor.com)\n\n- 👋 [Discord](https://discord.gg/RSNjgaugJs)\n\n- 🚙 [Project Board](https://github.com/orgs/voideditor/projects/2)\n\n\n## Contributing\n\n1. To get started working on Void, check out our Project Board! You can also see [HOW_TO_CONTRIBUTE](https://github.com/voideditor/void/blob/main/HOW_TO_CONTRIBUTE.md).\n\n2. Feel free to attend a casual weekly meeting in our Discord channel!\n\n\n## Reference\n\nVoid is a fork of the [vscode](https://github.com/microsoft/vscode) repository. For a guide to the codebase, see [VOID_CODEBASE_GUIDE](https://github.com/voideditor/void/blob/main/VOID_CODEBASE_GUIDE.md).\n\n## Note\nWork is temporarily paused on the Void IDE (this repo) while we experiment with a few novel AI coding ideas for Void. Stay alerted with new releases in our Discord channel.\n\n## Support\nYou can always reach us in our Discord server or contact us via email: hello@voideditor.com.\n",
    "url": "https://github.com/voideditor/void",
    "last_updated": "2025-09-02T08:19:44+00:00"
  },
  {
    "full_name": "ianozsvald/featherweight_web_api",
    "name": "featherweight_web_api",
    "description": "Featherweight web API provider for serving R&D methods as web functions",
    "language": "Python",
    "topics": [],
    "readme": "# Featherweight function-to-Internet-callable-function server\nExpose Python functions (or class methods) as a web-enabled function for others to call\n\n\n> \"I used your featherweight_api in order to deploy a phishing classifier as a prototype REST service. By using your API I definitely reduced the time to market a lot.\" - _Alejandro Correa Bahnsen, Lead Data Scientist at Easy Solutions, Colombia (Feb 2016)_\n\nGoals:\n* Data scientist focused tool to publish simple APIs\n* It is a \"featherweight\" server which turns your R&D code into a web-enabled function\n* Solve the \"but how can we quickly plumb our new-data-sci-code into the demo environment so it shows value to the bosses?\" problem without writing a \"proper server\" (especially if you don't know how to write a Proper Server)\n* Publishes a function using Flask with just 3 lines and little web knowledge\n* Supports `scikit-learn` and `numpy` objects (without making you think about correct `JSON` encoding) \n* Useful error messages are provided at run-time to help diagnose issues\n* Text arguments from an HTTP call are automatically converted to `float` arguments by default\n\nIt does *not solve* these problems:\n* It is not scalable (it isn't designed for production use)\n* It has no security\n* It does not replace Flask, Django or any other Proper Web Framework\n\nWritten for:\n* Python 3.4+ \n* Flask 0.10+\n\n##Example (`example_tiny_function.py`):\n\nLet's make a Python function that calculates something with a couple of input arguments (`myfn`) and then expose it as a web-callable function (`http://localhost:5000/myfn?<args>`). Once you've called it the result is passed back as a JSON block:\n\n```\nimport featherweight_api\n\ndef myfn(x, c):\n    \"\"\"Example function\"\"\"\n    return = x*x + c\n\nfeatherweight_api.register(myfn) \nfeatherweight_api.run()  # run the server on localhost:5000\n```\n\nIf you put the following into the URL bar in your browser it'll make a GET request and you'll get a successful result:\n\n```\nhttp://localhost:5000/myfn?",
    "url": "https://github.com/ianozsvald/featherweight_web_api",
    "last_updated": "2024-05-22T11:40:14+00:00"
  },
  {
    "full_name": "gitronald/domains",
    "name": "domains",
    "description": "Repository of data on web domains.",
    "language": "Python",
    "topics": [],
    "readme": "# domains\n\nA repository for aggregating __web domain metrics__, like partisanship or veracity classification, from peer reviewed publications. All data gathering and aggregating can be replicated by running `bash replicate.sh`. If you're looking for the final product see: `data/domains.tsv`\n\n\nNews is classifications are available in the `news_is_news` column, and are defined using:  \n1. 488 domains identified as ‘hard news’ by Bakshy et al. (2015)\n2. 1,250 domains manually identified as news by Grinberg et al. (2019), and \n3. 6,288 domains aggregated from local news listings by Yin (2018)\n\n\nCurrently includes data from:\n\nGrinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B., & Lazer, D. (2019). Fake news on Twitter during the 2016 US presidential election. Science, 363(6425), 374-378.  [Download data](https://github.com/LazerLab/twitter-fake-news-replication/tree/master/domains/domain_coding/data)  \n\nRobertson, R. E., Jiang, S., Joseph, K., Friedland, L., Lazer, D., & Wilson, C. (2018). Auditing Partisan Audience Bias within Google Search. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 148.  [Download data](http://personalization.ccs.neu.edu/static/archive/bias_scores.tar.gz)  \n\nLeon Yin. (2018). yinleon/LocalNewsDataset: Initial release (V1.0). Zenodo. https://doi.org/10.5281/zenodo.1345145  \n\nRobertson et al. (2018) includes data from:  \n- AllSides. 2018. Media Bias Ratings. AllSides. (2018). [Download Data](https://www.allsides.com/media-bias/media-bias-ratings)  \n- Amy Mitchell, Jeffrey Gottfried, Jocelyn Kiley, and Katerina Eva Matsa. 2014. Political Polarization & Media Habits. Pew Research Center’s Journalism Project. (Oct. 2014). [Download data](https://assets.pewresearch.org/wp-content/uploads/sites/13/2014/10/Political-Polarization-and-Media-Habits-FINAL-REPORT-7-27-15.pdf)  \n- Ceren Budak, Sharad Goel, and Justin M Rao. 2016. Fair and balanced? Quantifying media bias through crowdsourced content analysis. Public Opinion Quarterly 80,",
    "url": "https://github.com/gitronald/domains",
    "last_updated": "2025-07-27T04:01:09+00:00"
  },
  {
    "full_name": "agermanidis/pigeon",
    "name": "pigeon",
    "description": "🐦 Quickly annotate data from the comfort of your Jupyter notebook",
    "language": "Python",
    "topics": [],
    "readme": "🐦 pigeon - Quickly annotate data on Jupyter\n========================\n\nPigeon is a simple widget that lets you quickly annotate a dataset of\nunlabeled examples from the comfort of your Jupyter notebook.\n\nPigeon currently supports annotation for classification tasks (set of\nlabels), regression tasks (int/float range), or captioning tasks\n(variable-length text). Anything that can be displayed on Jupyter\n(text, images, audio, graphs, etc.) can be displayed by pigeon\nby providing the appropriate :code:`display_fn` argument.\n\nInstallation\n-----\n\n.. code-block:: bash\n\n    pip install pigeon-jupyter\n\nExamples\n-----\n\n- Text classification\n\nCode: \n\n.. code-block:: python\n\n    from pigeon import annotate\n    annotations = annotate(\n      ['I love this movie', 'I was really disappointed by the book'],\n      options=['positive', 'negative']\n    )\n\n\nPreview:\n\n.. image:: http://i.imgur.com/00ry4Li.gif\n\n- Image classification\n\nCode: \n\n.. code-block:: python\n\n    from pigeon import annotate\n    from IPython.display import display, Image\n\n    annotations = annotate(\n      ['assets/img_example1.jpg', 'assets/img_example2.jpg'],\n      options=['cat', 'dog', 'horse'],\n      display_fn=lambda filename: display(Image(filename))\n    )\n\nPreview:\n\n.. image:: http://i.imgur.com/PiE3eDt.gif\n",
    "url": "https://github.com/agermanidis/pigeon",
    "last_updated": "2025-08-30T14:27:08+00:00"
  },
  {
    "full_name": "SciutoAlex/Historical-Travel-of-US-Secretary-State",
    "name": "Historical-Travel-of-US-Secretary-State",
    "description": "Dataset and generative scripts for 3,200+ US Secretary of State visits (1905-present)",
    "language": "JavaScript",
    "topics": [],
    "readme": "US Secretary Of State Travel Data\n==================================\n\nThe US Secretary of State's website publishes an [overview](https://history.state.gov/departmenthistory/travels/secretary) of the official international travel of the Secretary of State. This data goes back continuously to 1905. Unfortunately the data is only available as a series of webpages and individual rows may contain multiple cities.\n\nThis repository contains a single CSV file of this data. It also includes the Node.js scripts used to process the data. Hopefully this data is useful visualization proejcts, statistical analyses, and other uses for geographic, time-based, historical data.\n\nData Preview\n----------\n![alt tag](preview-img/overview.png)\n\nThe Dataset\n-----------\n[/data/destinations.csv](data/destinations.csv)\n- `original_country`: text string identifying country scraped from Secretary Of State website\n- `original_city`: text string identifying city scraped from Secretary Of State website.\n- `original_date`: text string identifying date interval scraped from Secretary Of State website.\n- `description`: text string describing the Secretary's travel arrangement.\n- `country_modified_for_geo`: text string based on original but modified for geocoding.\n- `city_modified_for_geo`: text string based on original but modified for geocoding.\n- `date`: unused should remove\n- `sec_id`: text string identifying the Secretary of State\n- `sec_name`: text string of Secretary's name\n- `id`: chronological integer of destinations\n- `glat`: latitude of location from Google\n- `glon`: longitude of location from Google\n- `gcity`: text string of what Google identifies as the city of the destination\n- `gcountry`: text string of what Google identifies as the country of the destination\n- `isGeocoded`: binary note that the destination was geocoded\n- `split_added`: binary note that the entry was interpolated from a single Secretary of State entry. \n- `original_line`: binary note that the entry generated multiple d",
    "url": "https://github.com/SciutoAlex/Historical-Travel-of-US-Secretary-State",
    "last_updated": "2023-10-10T16:55:48+00:00"
  },
  {
    "full_name": "leeper/csvy",
    "name": "csvy",
    "description": "Import and Export CSV Data With a YAML Metadata Header",
    "language": "R",
    "topics": [
      "csv",
      "csvy",
      "yaml",
      "r",
      "cran",
      "data"
    ],
    "readme": "# Import and Export CSV Data With a YAML Metadata Header\n\nCSVY is a file format that combines the simplicity of CSV (comma-separated values) with the metadata of other plain text and binary formats (JSON, XML, Stata, etc.). The [CSVY file specification](http://csvy.org/) is simple: place a YAML header on top of a regular CSV. The yaml header is formatted according to the [Table Schema](https://frictionlessdata.io/specs/table-schema/) of a [Tabular Data Package](https://frictionlessdata.io/specs/tabular-data-package/).\n\nA CSVY file looks like this:\n\n```\n#---\n#profile: tabular-data-resource\n#name: my-dataset\n#path: https://raw.githubusercontent.com/csvy/csvy.github.io/master/examples/example.csvy\n#title: Example file of csvy \n#description: Show a csvy sample file.\n#format: csvy\n#mediatype: text/vnd.yaml\n#encoding: utf-8\n#schema:\n#  fields:\n#  - name: var1\n#    type: string\n#  - name: var2\n#    type: integer\n#  - name: var3\n#    type: number\n#dialect:\n#  csvddfVersion: 1.0\n#  delimiter: \",\"\n#  doubleQuote: false\n#  lineTerminator: \"\\r\\n\"\n#  quoteChar: \"\\\"\"\n#  skipInitialSpace: true\n#  header: true\n#sources:\n#- title: The csvy specifications\n#  path: http://csvy.org/\n#  email: ''\n#licenses:\n#- name: CC-BY-4.0\n#  title: Creative Commons Attribution 4.0\n#  path: https://creativecommons.org/licenses/by/4.0/\n#---\nvar1,var2,var3\nA,1,2.0\nB,3,4.3\n```\n\nWhich we can read into R like this:\n\n\n\n```r\nlibrary(\"csvy\")\nstr(read_csvy(system.file(\"examples\", \"example1.csvy\", package = \"csvy\")))\n```\n\n```\n## 'data.frame':\t2 obs. of  3 variables:\n##  $ var1: chr  \"A\" \"B\"\n##  $ var2: int  1 3\n##  $ var3: num  2 4.3\n##  - attr(*, \"profile\")= chr \"tabular-data-resource\"\n##  - attr(*, \"title\")= chr \"Example file of csvy\"\n##  - attr(*, \"description\")= chr \"Show a csvy sample file.\"\n##  - attr(*, \"name\")= chr \"my-dataset\"\n##  - attr(*, \"format\")= chr \"csvy\"\n##  - attr(*, \"sources\")=List of 1\n##   ..$ :List of 3\n##   .. ..$ name : chr \"CC-BY-4.0\"\n##   .. ..$ title: chr \"Creative Commons Attributio",
    "url": "https://github.com/leeper/csvy",
    "last_updated": "2025-03-22T08:14:15+00:00"
  },
  {
    "full_name": "propublica/stateface",
    "name": "stateface",
    "description": "A typeface of U.S. state shapes to use in web apps.",
    "language": "HTML",
    "topics": [],
    "readme": "# StateFace\n\nA font you can use in your web apps when you want tiny state shapes as a design element. [Documentation](http://propublica.github.io/stateface/)\n",
    "url": "https://github.com/propublica/stateface",
    "last_updated": "2025-05-12T01:42:42+00:00"
  },
  {
    "full_name": "romantseg/rbooks",
    "name": "rbooks",
    "description": "A curated list of #rstats books",
    "language": "R",
    "topics": [],
    "readme": "R Books [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n===\nA curated list of books about different aspects and applications of [R](https://www.r-project.org/) programming language and an attempt to modernize [R Books]() in a more user friendly way. Inspired by [GoBooks]().\n\n* [R Books](#r-books)\n\t* [Beginner Books](#beginner-books)\n\t* [Advanced Books](#advanced-books)\n\t* [Data Science](#data-science)\n\t* [Finance](#finance)\n\t* [Machine Learning](#machine-learning)\n\t* [R Development](#r-development)\n\t* [Reports](#reports)\n\t* [Visualization](#visualization)\n\nBeginner Books\n---\n\n#### Learning R [[Amazon]](www.amazon.com/Learning-R-Richard-Cotton/dp/1449357105/) \n\n<img src=\"http://akamaicovers.oreilly.com/images/0636920028352/cat.gif\" width=\"200px\"/>\n\nLearn how to perform data analysis with the R language and software environment, even if you have little or no programming experience. With the tutorials in this hands-on guide, you’ll learn how to use the essential R tools you need to know to analyze data, including data types and programming concepts.\n\n#### Hands-On Programming with R [[Amazon]](http://www.amazon.com/Hands-On-Programming-Write-Functions-Simulations/dp/1449359019)\n\n<img src=\"http://ecx.images-amazon.com/images/I/51Hbowka72L._SX379_BO1,204,203,200_.jpg\" width=\"200px\"/>\n\nLearn how to program by diving into the R language, and then use your newfound skills to solve practical data science problems. With this book, you' ll learn how to load data, assemble and disassemble data objects, navigate R's environment system, write your own functions, and use all of R's programming tools.\n\n#### The R Book [[Amazon]](http://www.amazon.com/The-Book-Michael-J-Crawley/dp/0470973927)\n\n<img src=\"http://ecx.images-amazon.com/images/I/41NNNTouQbL._SX371_BO1,204,203,200_.jpg\" width=\"200px\"/>\n\nVery massive, hugely successful and popular text presenting an extensive and com",
    "url": "https://github.com/romantseg/rbooks",
    "last_updated": "2025-08-14T14:25:58+00:00"
  },
  {
    "full_name": "rajashekar/llm-litellm",
    "name": "llm-litellm",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# llm-litellm\n\n[![PyPI](https://img.shields.io/pypi/v/llm-litellm.svg)](https://pypi.org/project/llm-litellm/)\n[![Changelog](https://img.shields.io/github/v/release/rajashekar/llm-litellm?include_prereleases&label=changelog)](https://github.com/rajashekar/llm-litellm/releases)\n[![Tests](https://github.com/rajashekar/llm-litellm/workflows/Test/badge.svg)](https://github.com/rajashekar/llm-litellm/actions?query=workflow%3ATest)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/rajashekar/llm-litellm/blob/main/LICENSE)\n\n[LLM](https://llm.datasette.io/) plugin for models hosted by [LiteLLM](https://github.com/BerriAI/litellm) proxy server.\n\nLiteLLM is a self-hosted proxy server that provides a unified interface to 100+ LLMs including OpenAI, Anthropic, Cohere, Replicate, PaLM, and more.\n\n## Installation\n\nFirst, [install the LLM command-line utility](https://llm.datasette.io/en/stable/setup.html).\n\nNow install this plugin in the same environment as LLM:\n```bash\nllm install llm-litellm\n```\n\n### Updating to New Versions\n\nTo update the plugin to the latest version:\n```bash\nllm install --upgrade llm-litellm\n```\n\nOr using pip directly:\n```bash\npip install --upgrade llm-litellm\n```\n\n### Installing from Source\n\nFor development or to install from source:\n```bash\npip install -e .\n```\n\n## Configuration\n\n### 1. Set up LiteLLM Server\n\nFirst, you need to have a LiteLLM server running. You can set it up by:\n\n```bash\npip install litellm[proxy]\nlitellm --model gpt-3.5-turbo\n# This starts the server on http://localhost:4000\n```\n\nOr use Docker:\n```bash\ndocker run -p 4000:4000 -e OPENAI_API_KEY=your-key ghcr.io/berriai/litellm:main-latest --model gpt-3.5-turbo\n```\n\n### 2. Set Environment Variable\n\nSet the `LITELLM_URL` environment variable to point to your LiteLLM server:\n\n```bash\nexport LITELLM_URL=http://localhost:4000\n```\n\n### 3. Set API Key (Optional)\n\nIf your LiteLLM server requires authentication, set the API key:\n\n```bash\nllm keys set litell",
    "url": "https://github.com/rajashekar/llm-litellm",
    "last_updated": "2025-07-16T18:58:52+00:00"
  },
  {
    "full_name": "ledell/subsemble",
    "name": "subsemble",
    "description": "subsemble R package for ensemble learning on subsets of data",
    "language": "R",
    "topics": [
      "ensemble",
      "ensemble-learning",
      "cross-validation",
      "machine-learning",
      "machine-learning-algorithms",
      "r",
      "big-data"
    ],
    "readme": "# subsemble\n\nThe **subsemble** package is an R implementation of the Subsemble algorithm. Subsemble is a general subset ensemble prediction method, which can be used for small, moderate, or large datasets. Subsemble partitions the full dataset into subsets of observations, fits a specified underlying algorithm on each subset, and uses a unique form of k-fold cross-validation to output a prediction function that combines the subset-specific fits. An oracle result provides a theoretical performance guarantee for Subsemble.\n\n[Stephanie Sapp](https://www.linkedin.com/in/sappstephanie), [Mark J. van der Laan](http://www.stat.berkeley.edu/~laan/index.html) & [John Canny](https://en.wikipedia.org/wiki/John_Canny) (2014) Subsemble: An ensemble method for combining subset-specific algorithm fits. *Journal of Applied Statistics*, 41(6):1247-1259.\n\n- Article: [https://www.tandfonline.com/doi/abs/10.1080/02664763.2013.864263](https://www.tandfonline.com/doi/abs/10.1080/02664763.2013.864263)\n- Preprint: [https://biostats.bepress.com/ucbbiostat/paper313/](https://biostats.bepress.com/ucbbiostat/paper313)\n\nImplementation details of the **subsemble** R package available here: \n\nLeDell, E. (2015) Scalable Ensemble Learning and Computationally Efficient Variance Estimation (Doctoral Dissertation).  University of California, Berkeley, USA. [https://github.com/ledell/phd-thesis/blob/main/ledell-phd-thesis.pdf](https://github.com/ledell/phd-thesis/blob/main/ledell-phd-thesis.pdf)\n\n## Install subsemble\n\nYou can install:\n\n-   the latest released version from CRAN with\n\n    ```r\n    install.packages(\"subsemble\")\n    ```\n\n-   the latest development version from GitHub with\n\n    ```r\n    devtools::install_github(\"ledell/subsemble\")\n    ```\n\n## Using subsemble\n\nHere are some examples of how to use the **subsemble** package to do various types of learning tasks.  These examples are also part of the `subsemble` function documentation in the R package.\n\nLoad some example binary outcome data to u",
    "url": "https://github.com/ledell/subsemble",
    "last_updated": "2024-02-25T23:15:18+00:00"
  },
  {
    "full_name": "TaddyLab/textir",
    "name": "textir",
    "description": "Inverse regression analysis of text",
    "language": "R",
    "topics": [],
    "readme": "textir v2.0\n======\n\nThis is the textir package for R, implementing the MNIR routines of \"<a href=\"http://amstat.tandfonline.com/doi/full/10.1080/01621459.2012.734168\">multinomial inverse regression for text analysis</a>\".  It also provides a minimalist partial least squares algorithm.    \n\nThe cran page is at https://CRAN.R-project.org/package=textir.\n\nVersions 2+ make use of the <a href=\"https://CRAN.R-project.org/package=distrom\">distrom</a> package, for DMR as in \"<a href=\"http://arxiv.org/abs/1311.6139\">distributed multinomial regression</a>\". These algorithms differ from those in the original MNIR in significant ways: penalties are chosen from full regularization paths (instead of being fixed), via the <a href=\"http://arxiv.org/abs/1308.5623\">gamma lasso</a> algorithm as implemented in <a href=\"https://CRAN.R-project.org/package=gamlr\">gamlr</a> (instead of exact log penalties), in parallel for independent Poisson log regressions (instead of jointly for a full multinomial likelihood).  It also uses replaces the slam library for sparse simple triplet matrices with the more common Matrix library.\n\nI have kept the mnlm function in textir for backwards compatability, but for simplicity recommend  that you use distrom's dmr instead.  The argument list is exactly the same (mnlm just calls dmr).\n\nThe last pre-2.0 version of textir, matching the implementation in the original MNIR paper, is textir_1.8-8.  This is available in archives on the cran page.\n\n",
    "url": "https://github.com/TaddyLab/textir",
    "last_updated": "2024-10-08T04:27:08+00:00"
  },
  {
    "full_name": "facebookresearch/fastText",
    "name": "fastText",
    "description": "Library for fast text representation and classification.",
    "language": "HTML",
    "topics": [],
    "readme": "# fastText\n[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.\n\n[![CircleCI](https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg)](https://circleci.com/gh/facebookresearch/fastText/tree/master)\n\n## Table of contents\n\n* [Resources](#resources)\n   * [Models](#models)\n   * [Supplementary data](#supplementary-data)\n   * [FAQ](#faq)\n   * [Cheatsheet](#cheatsheet)\n* [Requirements](#requirements)\n* [Building fastText](#building-fasttext)\n   * [Getting the source code](#getting-the-source-code)\n   * [Building fastText using make (preferred)](#building-fasttext-using-make-preferred)\n   * [Building fastText using cmake](#building-fasttext-using-cmake)\n   * [Building fastText for Python](#building-fasttext-for-python)\n* [Example use cases](#example-use-cases)\n   * [Word representation learning](#word-representation-learning)\n   * [Obtaining word vectors for out-of-vocabulary words](#obtaining-word-vectors-for-out-of-vocabulary-words)\n   * [Text classification](#text-classification)\n* [Full documentation](#full-documentation)\n* [References](#references)\n   * [Enriching Word Vectors with Subword Information](#enriching-word-vectors-with-subword-information)\n   * [Bag of Tricks for Efficient Text Classification](#bag-of-tricks-for-efficient-text-classification)\n   * [FastText.zip: Compressing text classification models](#fasttextzip-compressing-text-classification-models)\n* [Join the fastText community](#join-the-fasttext-community)\n* [License](#license)\n\n## Resources\n\n### Models\n- Recent state-of-the-art [English word vectors](https://fasttext.cc/docs/en/english-vectors.html).\n- Word vectors for [157 languages trained on Wikipedia and Crawl](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md).\n- Models for [language identification](https://fasttext.cc/docs/en/language-identification.html#content) and [various supervised tasks](https://fasttext.cc/docs/en/supe",
    "url": "https://github.com/facebookresearch/fastText",
    "last_updated": "2025-09-02T09:48:02+00:00"
  },
  {
    "full_name": "QwenLM/Qwen-Image",
    "name": "Qwen-Image",
    "description": "Qwen-Image is a powerful image generation foundation model capable of complex text rendering and precise image editing.",
    "language": "Python",
    "topics": [],
    "readme": "<p align=\"center\">\r\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png\" width=\"400\"/>\r\n<p> \r\n<p align=\"center\">&nbsp&nbsp💜 <a href=\"https://chat.qwen.ai/\">Qwen Chat</a>&nbsp&nbsp |\r\n           &nbsp&nbsp🤗 <a href=\"https://huggingface.co/Qwen/Qwen-Image\">HuggingFace(T2I)</a>&nbsp&nbsp |\r\n           &nbsp&nbsp🤗 <a href=\"https://huggingface.co/Qwen/Qwen-Image-Edit\">HuggingFace(Edit)</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image\">ModelScope-T2I</a>&nbsp&nbsp | &nbsp&nbsp🤖 <a href=\"https://modelscope.cn/models/Qwen/Qwen-Image-Edit\">ModelScope-Edit</a>&nbsp&nbsp| &nbsp&nbsp 📑 <a href=\"https://arxiv.org/abs/2508.02324\">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen-image/\">Blog(T2I)</a> &nbsp&nbsp | &nbsp&nbsp 📑 <a href=\"https://qwenlm.github.io/blog/qwen-image-edit/\">Blog(Edit)</a> &nbsp&nbsp \r\n<br>\r\n🖥️ <a href=\"https://huggingface.co/spaces/Qwen/Qwen-Image\">T2I Demo</a>&nbsp&nbsp | 🖥️ <a href=\"https://huggingface.co/spaces/Qwen/Qwen-Image-Edit\">Edit Demo</a>&nbsp&nbsp | &nbsp&nbsp💬 <a href=\"https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png\">WeChat (微信)</a>&nbsp&nbsp | &nbsp&nbsp🫨 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp\r\n</p>\r\n\r\n<p align=\"center\">\r\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg\" width=\"1024\"/>\r\n<p>\r\n\r\n## Introduction\r\nWe are thrilled to release **Qwen-Image**, a 20B MMDiT image foundation model that achieves significant advances in **complex text rendering** and **precise image editing**. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.\r\n\r\n\r\n![](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center)\r\n\r\n## News\r\n- 2025.08.19: We have observed performance misalignments of Qwen-Image-Edit. To ensure optimal results, please update to the latest d",
    "url": "https://github.com/QwenLM/Qwen-Image",
    "last_updated": "2025-09-02T09:53:12+00:00"
  },
  {
    "full_name": "PRBonn/lidar-visualizer",
    "name": "lidar-visualizer",
    "description": "A LiDAR visualization tool for all your datasets",
    "language": "Python",
    "topics": [],
    "readme": "# LiDAR Visualizer 🚀\n\nA flexible, easy-to-use, LiDAR (or any point cloud) visualizer for Linux, Windows, and macOS.\n\n![out](https://user-images.githubusercontent.com/21349875/234777083-eeb4ec57-cb50-4c69-babd-4cc8e63cff86.png)\n\nIf you also need to obtain poses from your dataset, consider checking out [KISS-ICP](https://github.com/PRBonn/kiss-icp).\n\n## Install (\\*)\n\n```sh\npip install lidar-visualizer\n```\n\n(\\*) This package relies on the power of [Open3D](https://www.open3d.org) but does not list it as a dependency. If you haven't installed `open3d` then `pip install open3d` or check [the official instructions](https://www.open3d.org/docs/release/getting_started.html)\n\n## Optional dependencies\n\nDepending on the [dataloaders](./src/lidar_visualizer/datasets/) you plan to use you might need to install optional dependencies. The tool will prompt which tools is the one you are requesting and is not accessible, but if you want to go for brute force and install all of it just run:\n\n```sh\npip install lidar-visualizer[all]\n```\n\n## Usage\n\n```sh\nlidar_visualizer --help\n```\n\n## Citation\n\nIf you use this visualizer for any academic work, please cite our original [paper](https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/vizzo2023ral.pdf).\n\n```bibtex\n@article{vizzo2023ral,\n  author    = {Vizzo, Ignacio and Guadagnino, Tiziano and Mersch, Benedikt and Wiesmann, Louis and Behley, Jens and Stachniss, Cyrill},\n  title     = {{KISS-ICP: In Defense of Point-to-Point ICP -- Simple, Accurate, and Robust Registration If Done the Right Way}},\n  journal   = {IEEE Robotics and Automation Letters (RA-L)},\n  pages     = {1029--1036},\n  doi       = {10.1109/LRA.2023.3236571},\n  volume    = {8},\n  number    = {2},\n  year      = {2023},\n  codeurl   = {https://github.com/PRBonn/kiss-icp},\n}\n```\n",
    "url": "https://github.com/PRBonn/lidar-visualizer",
    "last_updated": "2025-08-20T20:39:23+00:00"
  },
  {
    "full_name": "sudhof/politeness",
    "name": "politeness",
    "description": "Sample implementation of a politeness model, trained on the Stanford Politeness Corpus",
    "language": "OpenEdge ABL",
    "topics": [],
    "readme": "Stanford Politeness API\n=======================\n\n#### A new home: ConvoKit (July 2019)\nThe functionality of this API is now integrated with [ConvoKit](https://convokit.cornell.edu/). This new implementation more directly exposes politeness strategies, which can used in downstream tasks (see example scripts in ConvoKit).  As such, we are discontinuing support for this API.\n\n#### Version 2.00 (released March 2017)\n \n Python3 version is available here: https://github.com/sudhof/politeness/tree/python3 (refactored from Version 1.01 through the kindness of Benjamin Meyers). \n \n Note: This python3 version was not yet tested by us, nor compared against the results from our paper (listed below).  The code used in the paper is still here in the master branch of this repository (keep on reading).\n \n \n#### Version 1.01 (released October 2014)\n\nPython implementation of a politeness classifier for requests, based on the work described in:\n\n\tA computational approach to politeness with application to social factors.  \t\n\tCristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, Christopher Potts.  \n\tProceedings of ACL, 2013.\n\n\nWe release this code hoping that others will use and improve on our work.\n\nNOTE: If you use this API in your work please send an email to cristian@cs.cornell.edu so we can add you to our list of users.  Thanks!\n\n\n**Further resources:**\n\n    Info about our work: http://cs.cornell.edu/~cristian/Politeness.html\n\n    A web interface to the politeness model: http://politeness.cornell.edu/\n\n    The Stanford Politeness Corpus: http://cs.cornell.edu/~cristian/Politeness_files/Stanford_politeness_corpus.zip\n\n\n**Using this API you can:**\n\n- classify requests using politeness.model.score  (using the provided pre-trained model)\n\n- train new models on new data using politeness.scripts.train_model\n\n- experiment with new politeness features in politeness.features.vectorizer and politeness.features.politeness_strategies\n\n\n**Input:** Requests must be pre-p",
    "url": "https://github.com/sudhof/politeness",
    "last_updated": "2025-03-24T15:40:09+00:00"
  },
  {
    "full_name": "jeroen/mongolite",
    "name": "mongolite",
    "description": "Fast and Simple MongoDB Client for R",
    "language": "C",
    "topics": [],
    "readme": "# mongolite\n\n##### *Fast and Simple MongoDB Client for R*\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/mongolite)](http://cran.r-project.org/package=mongolite)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/mongolite)](http://cran.r-project.org/web/packages/mongolite/index.html)\n[![Research software impact](http://depsy.org/api/package/cran/mongolite/badge.svg)](http://depsy.org/package/r/mongolite)\n\n> High-level, high-performance MongoDB client based on libmongoc and\n  jsonlite. Includes support for aggregation, indexing, map-reduce, streaming,\n  SSL encryption and SASL authentication. The vignette gives a brief overview\n  of the available methods in the package.\n\n## Documentation\n\nAbout the R package:\n\n - Book: [Mongolite User Manual](https://jeroen.github.io/mongolite/)\n - Presentation: [UseR 2015 slides](http://jeroen.github.io/mongo-slides/)\n\n## Hello World\n\n\nExample using a public test server\n\n```r\ncon <- mongo(\"mtcars\", url =\n  \"mongodb+srv://readwrite:test@cluster0-84vdt.mongodb.net/test\")\n\n# Wipe collection\nif(con$count() > 0) \n  con$drop()\n  \n# Insert some data\ncon$insert(mtcars)\nstopifnot(con$count() == nrow(mtcars))\n\n# Query data\nmydata <- con$find()\nstopifnot(all.equal(mydata, mtcars))\ncon$drop()\n\n# Automatically disconnect when connection is removed\nrm(con)\ngc()\n```\n\nInsert/retrieve data from your local mongodb server:\n\n```r\n# Init connection to local mongod\nlibrary(mongolite)\nm <- mongo(collection = \"diamonds\")\n\n# Insert test data\ndata(diamonds, package=\"ggplot2\")\nm$insert(diamonds)\n\n# Check records\nm$count()\nnrow(diamonds)\n\n# Perform a query and retrieve data\nout <- m$find('{\"cut\" : \"Premium\", \"price\" : { \"$lt\" : 1000 } }')\n\n# Compare\nnrow(out)\nnrow(subset(diamonds, cut == \"Premium\" & price < 1000))\n```\n\nMore advanced features include map reduce:\n\n```r\n# Cross-table\ntbl <- m$mapreduce(\n  map = \"function(){emit({cut:this.cut, color:this.color}, 1)}\",\n  reduce = \"function(id, counts){return Array.sum(counts)}\"\n)\n# Same",
    "url": "https://github.com/jeroen/mongolite",
    "last_updated": "2025-08-07T21:35:32+00:00"
  },
  {
    "full_name": "hrbrmstr/pluralize",
    "name": "pluralize",
    "description": "An R package to \"Pluralize and Singularize Any Word\"",
    "language": "JavaScript",
    "topics": [
      "r",
      "singularize",
      "plural",
      "rstats"
    ],
    "readme": "\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-100%25-lightgrey.svg)\n[![Linux build\nStatus](https://travis-ci.org/hrbrmstr/pluralize.svg?branch=master)](https://travis-ci.org/hrbrmstr/pluralize)\n[![Coverage\nStatus](https://codecov.io/gh/hrbrmstr/pluralize/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/pluralize)\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-3.6.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# pluralize\n\nPluralize and ‘Singularize’ Any (English) Word\n\n## Description\n\nTools are provided to create plural, singular and regular forms of\nEnglish words along with tools to augment the built-in rules to fit\nspecializied needs. Core functionality is based on a JavaScript library,\n<https://github.com/blakeembrey/pluralize>.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `add_irregular_rule`: Add a custom rule for making “deregularizing”\n    a word\n  - `add_plural_rule`: Add a custom rule for making a word plural\n  - `add_singular_rule`: Add a custom rule for making a word singular\n  - `add_uncountable_rule`: Make a word “uncountable”\n  - `is_plural`: Test plural state of a word\n  - `pluralize`: Pluralize a word\n  - `singularize`: Singularize a word\n\n## Installation\n\n``` r\nremotes::install_gitlab(\"hrbrmstr/pluralize\")\n# or\nremotes::install_github(\"hrbrmstr/pluralize\")\n```\n\nNOTE: To use the ‘remotes’ install options you will need to have the\n[{remotes} package](https://github.com/r-lib/remotes) installed.\n\n## Usage\n\n``` r\nlibrary(pluralize)\n\n# current version\npackageVersion(\"pluralize\")\n## [1] '0.2.0'\n```\n\n``` r\npluralize('test')\n## [1] \"tests\"\n\nsingularize('test')",
    "url": "https://github.com/hrbrmstr/pluralize",
    "last_updated": "2025-03-22T11:19:51+00:00"
  },
  {
    "full_name": "PiRogueToolSuite/scarlet-shark-python",
    "name": "scarlet-shark-python",
    "description": "Scarlet Shark REST API Python client",
    "language": "Python",
    "topics": [],
    "readme": "# scarlet-shark-python\nScarlet Shark REST API Python client.\n\n## Installation\n```\npip install scarlet-shark-client\n```\n\n## Usage example\n\n```python\nfrom scarlet_shark_client.client import ClientFactory\n\nclient = ClientFactory.get_client('<your API key>', api_version='v0.4', print_json=True)\nresult = client.search_domain(domain='scarletshark.com')\n```\nprints\n```json\n{\n  \"age\": 4043,\n  \"associated_urls\": [],\n  \"classification\": \"security\",\n  \"disclosure_email\": \"unknown\",\n  \"domain\": \"scarletshark.com\",\n  \"domain_description\": \"\",\n  \"ip\": \"40.[redacted]\",\n  \"reference_url\": \"\",\n  \"registered\": \"2012-04-02\",\n  \"service\": {\n    \"service_email\": \"\",\n    \"service_id\": 0,\n    \"service_name\": \"unknown\",\n    \"service_trust\": \"unknown\",\n    \"service_type\": \"unknown\",\n    \"service_url\": \"\"\n  },\n  \"tags\": [\n    \"security\"\n  ],\n  \"threat_actor_aliases\": [],\n  \"threat_actor_id\": 0,\n  \"threat_classification\": \"safe\",\n  \"threat_explanation\": \"The domain is considered to be safe and it hosts no known malicious content.\",\n  \"tracking_domain\": false\n}\n```\n\n## Supported actions\n### API version 0.4\n* search_dns\n* search_domain\n* search_email\n* search_hash\n* search_ip\n* search_network\n* search_threat_actors\n* search_threat_tools\n* search_url",
    "url": "https://github.com/PiRogueToolSuite/scarlet-shark-python",
    "last_updated": "2025-01-15T15:28:34+00:00"
  },
  {
    "full_name": "lensacom/sparkit-learn",
    "name": "sparkit-learn",
    "description": "PySpark + Scikit-learn = Sparkit-learn",
    "language": "Python",
    "topics": [
      "scikit-learn",
      "apache-spark",
      "machine-learning",
      "distributed-computing",
      "python"
    ],
    "readme": "Sparkit-learn\n=============\n\n|Build Status| |PyPi| |Gitter| |Gitential|\n\n**PySpark + Scikit-learn = Sparkit-learn**\n\nGitHub: https://github.com/lensacom/sparkit-learn\n\nAbout\n=====\n\nSparkit-learn aims to provide scikit-learn functionality and API on\nPySpark. The main goal of the library is to create an API that stays\nclose to sklearn's.\n\nThe driving principle was to *\"Think locally, execute distributively.\"*\nTo accomodate this concept, the basic data block is always an array or a\n(sparse) matrix and the operations are executed on block level.\n\n\nRequirements\n============\n\n-  **Python 2.7.x or 3.4.x**\n-  **Spark[>=1.3.0]**\n-  NumPy[>=1.9.0]\n-  SciPy[>=0.14.0]\n-  Scikit-learn[>=0.16]\n\n\n\nRun IPython from notebooks directory\n====================================\n\n.. code:: bash\n\n    PYTHONPATH=${PYTHONPATH}:.. IPYTHON_OPTS=\"notebook\" ${SPARK_HOME}/bin/pyspark --master local\\[4\\] --driver-memory 2G\n\n\nRun tests with\n==============\n\n.. code:: bash\n\n    ./runtests.sh\n\n\nQuick start\n===========\n\nSparkit-learn introduces three important distributed data format:\n\n-  **ArrayRDD:**\n\n   A *numpy.array* like distributed array\n\n   .. code:: python\n\n       from splearn.rdd import ArrayRDD\n\n       data = range(20)\n       # PySpark RDD with 2 partitions\n       rdd = sc.parallelize(data, 2) # each partition with 10 elements\n       # ArrayRDD\n       # each partition will contain blocks with 5 elements\n       X = ArrayRDD(rdd, bsize=5) # 4 blocks, 2 in each partition\n\n   Basic operations:\n\n   .. code:: python\n\n       len(X) # 20 - number of elements in the whole dataset\n       X.blocks # 4 - number of blocks\n       X.shape # (20,) - the shape of the whole dataset\n\n       X # returns an ArrayRDD\n       # <class 'splearn.rdd.ArrayRDD'> from PythonRDD...\n\n       X.dtype # returns the type of the blocks\n       # numpy.ndarray\n\n       X.collect() # get the dataset\n       # [array([0, 1, 2, 3, 4]),\n       #  array([5, 6, 7, 8, 9]),\n       #  array([10, 11, 12, 13, 14]),\n       #  array([15, 16, 17",
    "url": "https://github.com/lensacom/sparkit-learn",
    "last_updated": "2025-08-23T01:02:03+00:00"
  },
  {
    "full_name": "jsfenfen/whatwordwhere",
    "name": "whatwordwhere",
    "description": "Tooling to extract data from scanned paper forms OCR-ed by Tesseract using the HOCR standard.",
    "language": "HTML",
    "topics": [],
    "readme": "What Word Where?\n=============\n\nThe routines in this repo treat a page of scanned text as a geography, where a bounding box around each word is analogous to the boundaries of a geographic region. That's not an analogy--it actually relies on code developed for geographic information systems (GIS) applications. By entering a sample of a larger corpus of documents into a spatially-enabled database, users can create (and test) reasonable page identifiers (to choose pages of interest) and extraction templates (to locate specific sections of text to extract). Once the general extraction parameters are set, users can read through the full document set, applying the page identifiers and, when applicable, the extraction templates, without having to load any of the data into a database.\n\nThis library assumes data has already been extracted by optical character recognition (OCR) according to the [hOCR format](http://en.wikipedia.org/wiki/HOCR) using [tesseract](http://code.google.com/p/tesseract-ocr/).  HOCR outputs bounding boxes for words (and a whole hierarchy of sections of text; see more [here](https://docs.google.com/a/sunlightfoundation.com/document/d/1QQnIQtvdAC_8n92-LhwPcjtAUFwBlzE8EWnKAxlgVf0/preview).) \n\nThis repo includes utilities for parsing pages from hOCR docs and uploading them in bulk to postgis/postgres. Also subroutines to return pages as an array of words with [GEOS](http://trac.osgeo.org/geos/)-style polygons (we're using [django's bindings to GEOS](https://docs.djangoproject.com/en/dev/ref/contrib/gis/geos/), so the shapes are actually GEOSGeometry objects).\n\nThe original use case for this library is a massive corpus of tax documents, which are released as image files and OCR-ed using [tesseract](http://code.google.com/p/tesseract-ocr/). Each tax form consists of many different \"schedules\" which contain various information. Regularly extracting a particular value requires first identifying all the pages it might appear on, writing a page identifier that ",
    "url": "https://github.com/jsfenfen/whatwordwhere",
    "last_updated": "2023-05-17T21:41:44+00:00"
  },
  {
    "full_name": "brodieG/diffobj",
    "name": "diffobj",
    "description": "Compare R Objects with a Diff",
    "language": "R",
    "topics": [
      "r",
      "diff"
    ],
    "readme": "# diffobj - Diffs for R Objects\n\n[![R build\nstatus](https://github.com/brodieG/diffobj/workflows/R-CMD-check/badge.svg)](https://github.com/brodieG/diffobj/actions)\n[![](https://codecov.io/github/brodieG/diffobj/coverage.svg?branch=rc)](https://app.codecov.io/gh/brodieG/diffobj?branch=rc)\n\nGenerate a colorized diff of two R objects for an intuitive visualization of their differences.\n\n> See the [introductory vignette for details][1].\n\n## Output\n\nIf your terminal supports formatting through ANSI escape sequences, `diffobj` will output colored diffs to the terminal.  Otherwise, output will be colored with HTML/CSS and sent to the IDE viewport or to your browser.  `diffobj` comes with several built-in color schemes that can be further customized.  Some examples:\n\n![Output Examples](https://raw.githubusercontent.com/brodieG/diffobj/master/cliandrstudio.png)\n\n## Installation\n\nThis package is available on [CRAN](https://cran.r-project.org/package=diffobj).\n\n```\ninstall.packages(\"diffobj\")\nbrowseVignettes(\"diffobj\")\n```\n\n## Related Software\n\n* [tools::Rdiff][2].\n* [Daff](https://cran.r-project.org/package=daff) diff, patch and merge for\n  data.frames.\n* [GNU diff](https://www.gnu.org/software/diffutils/).\n* [waldo](https://cran.r-project.org/package=waldo), which internally uses\n  `diffobj` for diffs but takes a more hands-on approach to detailing object\n  differences.\n\n## Acknowledgements\n\n* R Core for developing and maintaining such a wonderful language.\n* CRAN maintainers, for patiently shepherding packages onto CRAN and maintaining\n  the repository, and Uwe Ligges in particular for maintaining\n  [Winbuilder](https://win-builder.r-project.org/).\n* The users who have reported bugs and possible fixes, and/or made feature\n  requests (see NEWS.md).\n* [Gábor Csárdi](https://github.com/gaborcsardi) for\n  [crayon](https://github.com/r-lib/crayon).\n* [Jim Hester](https://github.com/jimhester) for\n  [covr](https://cran.r-project.org/package=covr), and with Rstudio for\n  [r-lib/a",
    "url": "https://github.com/brodieG/diffobj",
    "last_updated": "2025-04-21T18:50:30+00:00"
  },
  {
    "full_name": "feathericons/feather",
    "name": "feather",
    "description": "Simply beautiful open-source icons",
    "language": "JavaScript",
    "topics": [
      "icons",
      "svg",
      "javascript"
    ],
    "readme": "# Feather\n\n[![Coverage](https://img.shields.io/codecov/c/github/feathericons/feather/master.svg?style=flat-square)](https://codecov.io/gh/feathericons/feather)\n[![npm downloads](https://img.shields.io/npm/dm/feather-icons.svg?style=flat-square)](https://www.npmjs.com/package/feather-icons)\n[![npm version](https://img.shields.io/npm/v/feather-icons.svg?style=flat-square)](https://www.npmjs.com/package/feather-icons)\n[![CDNJS version](https://img.shields.io/cdnjs/v/feather-icons.svg?style=flat-square)](https://cdnjs.com/libraries/feather-icons)\n\n## What is Feather?\n\nFeather is a collection of simply beautiful open-source icons. Each icon is designed on a 24x24 grid with an emphasis on simplicity, consistency, and flexibility.\n\nhttps://feathericons.com\n\n```shell\nnpm install feather-icons\n```\n\n## Table of contents\n\n- [Quick start](#quick-start)\n- [Usage](#usage)\n  - [Client-side JavaScript](#client-side-javascript)\n  - [Node](#node)\n  - [SVG sprite](#svg-sprite)\n  - [Figma](#figma)\n- [API reference](#api-reference)\n  - [`feather.icons`](#feathericons)\n  - [`feather.icons[name].toSvg()`](#feathericonsnametosvgattrs)\n  - [`feather.replace()`](#featherreplaceattrs)\n  - [`feather.toSvg()` (DEPRECATED) ](#feathertosvgname-attrs-deprecated)\n- [Contributing](#contributing)\n- [Related projects](#related-projects)\n- [License](#license)\n\n## Quick start\n\nStart with this [CodePen Template](https://codepen.io/pen?template=WOJZdM) to begin prototyping with Feather in the browser.\n\nOr copy and paste the following code snippet into a blank `html` file.\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n  <title></title>\n  <script src=\"https://unpkg.com/feather-icons\"></script>\n  <body>\n    <!-- example icon -->\n    <i data-feather=\"circle\"></i>\n\n    <script>\n      feather.replace();\n    </script>\n  </body>\n</html>\n```\n\n## Usage\n\nAt its core, Feather is a collection of [SVG](https://svgontheweb.com/#svg) files. This means that you can use Feather icons in all the same ways you can use SVGs (e.g.",
    "url": "https://github.com/feathericons/feather",
    "last_updated": "2025-09-01T20:25:33+00:00"
  },
  {
    "full_name": "impira/docquery",
    "name": "docquery",
    "description": "An easy way to extract information from documents",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n\nNOTE: DocQuery is not actively maintained anymore. We still welcome contributions and discussions among the community!\n\n# DocQuery: Document Query Engine Powered by Large Language Models\n\n[![Demo](https://img.shields.io/badge/Demo-Gradio-brightgreen)](https://huggingface.co/spaces/impira/docquery)\n[![Demo](https://img.shields.io/badge/Demo-Colab-orange)](https://github.com/impira/docquery/blob/main/docquery_example.ipynb)\n[![PyPI](https://img.shields.io/pypi/v/docquery?color=green&label=pip%20install%20docquery)](https://pypi.org/project/docquery/)\n[![Discord](https://img.shields.io/discord/1015684761471160402?label=Chat)](https://discord.gg/HucNfTtx7V)\n[![Downloads](https://static.pepy.tech/personalized-badge/docquery?period=total&units=international_system&left_color=grey&right_color=green&left_text=Downloads)](https://pepy.tech/project/docquery)\n\n</div>\n\nDocQuery is a library and command-line tool that makes it easy to analyze semi-structured and unstructured documents (PDFs, scanned\nimages, etc.) using large language models (LLMs). You simply point DocQuery at one or more documents and specify a\nquestion you want to ask. DocQuery is created by the team at [Impira](https://impira.com?utm_source=github&utm_medium=referral&utm_campaign=docquery).\n\n## Quickstart (CLI)\n\nTo install `docquery`, you can simply run `pip install docquery`. This will install the command line tool as well as the library.\nIf you want to run OCR on images, then you must also install the [tesseract](https://github.com/tesseract-ocr/tesseract) library:\n\n- Mac OS X (using [Homebrew](https://brew.sh/)):\n\n  ```sh\n  brew install tesseract\n  ```\n\n- Ubuntu:\n\n  ```sh\n  apt install tesseract-ocr\n  ```\n\n`docquery` scan allows you to ask one or more questions to a single document or directory of files. For example, you can\nfind the invoice number <https://templates.invoicehome.com/invoice-template-us-neat-750px.png> with:\n\n```bash\ndocquery scan \"What is the invoice number?\" https://",
    "url": "https://github.com/impira/docquery",
    "last_updated": "2025-09-01T01:57:18+00:00"
  },
  {
    "full_name": "jaredks/tweetokenize",
    "name": "tweetokenize",
    "description": "Tokenization and pre-processing for Twitter data used to train classifiers.",
    "language": "Python",
    "topics": [],
    "readme": "tweetokenize\n============\n\nRegular expression based tokenizer for Twitter. Focused on tokenization\nand pre-processing to train classifiers for sentiment, emotion, or mood.\n\nIntended as glue between Python wrappers for Twitter API and machine\nlearning algorithms of the Natural Language Toolkit (NLTK), but probably\napplicable to tokenizing any short messages of the social networking\nvariety.\n\n```python\nfrom tweetokenize import Tokenizer\ngettokens = Tokenizer()\ngettokens.tokenize('hey playa!:):3.....@SHAQ can you still dunk?#old🍕🍔😵LOL')\n[u'hey', u'playa', u'!', u':)', u':3', u'...', u'USERNAME', u'can', u'you', u'still', u'dunk', u'?', u'#old', u'🍕', u'🍔', u'😵', u'LOL']\n```\n\nFeatures\n--------\n\n* Can easily replace tweet features like usernames, urls, phone numbers, times, \netc. with tokens in order to reduce feature set complexity and improve \nperformance of classifiers\n* Allows user-defined sets of emoticons to be used in tokenization\n* Correctly separates emoji, written consecutively, into individual tokens\n\nInstallation\n------------\n\n    python setup.py install\n\nAfter installation, you can make sure everything is working by running the following inside the project root folder,\n\n    python tests\n\nDocumentation\n-------------\n\nhttp://htmlpreview.github.io/?https://raw.github.com/jaredks/tweetokenize/master/documentation/tweetokenize.Tokenizer-class.html\n\nLicense\n-------\n\n\"Modified BSD License\". See LICENSE for details. Copyright Jared Suttles, 2013.\n",
    "url": "https://github.com/jaredks/tweetokenize",
    "last_updated": "2025-03-10T14:10:22+00:00"
  },
  {
    "full_name": "lh3/bwa",
    "name": "bwa",
    "description": "Burrow-Wheeler Aligner for short-read alignment (see minimap2 for long-read alignment)",
    "language": "C",
    "topics": [
      "bioinformatics",
      "sequence-alignment",
      "genomics",
      "fm-index"
    ],
    "readme": "[![Build Status](https://github.com/lh3/bwa/actions/workflows/ci.yaml/badge.svg)](https://github.com/lh3/bwa/actions)\n[![SourceForge Downloads](https://img.shields.io/sourceforge/dt/bio-bwa.svg?label=SF%20downloads)](https://sourceforge.net/projects/bio-bwa/files/?source=navbar)\n[![GitHub Downloads](https://img.shields.io/github/downloads/lh3/bwa/total.svg?style=flat&label=GitHub%20downloads)](https://github.com/lh3/bwa/releases)\n[![BioConda Install](https://img.shields.io/conda/dn/bioconda/bwa.svg?style=flag&label=BioConda%20install)](https://anaconda.org/bioconda/bwa)\n\n**Note: [minimap2][minimap2] has replaced BWA-MEM for __PacBio and Nanopore__ read\nalignment.** It retains all major BWA-MEM features, but is ~50 times as fast,\nmore versatile, more accurate and produces better base-level alignment.\n[BWA-MEM2][bwa-mem2] is 50-100% faster than BWA-MEM and outputs identical alignments.\n\n[minimap2]: https://github.com/lh3/minimap2\n[bwa-mem2]: https://github.com/bwa-mem2/bwa-mem2\n\n## Getting started\n\n\tgit clone https://github.com/lh3/bwa.git\n\tcd bwa; make\n\t./bwa index ref.fa\n\t./bwa mem ref.fa read-se.fq.gz | gzip -3 > aln-se.sam.gz\n\t./bwa mem ref.fa read1.fq read2.fq | gzip -3 > aln-pe.sam.gz\n\n## Introduction\n\nBWA is a software package for mapping DNA sequences against a large reference\ngenome, such as the human genome. It consists of three algorithms:\nBWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina\nsequence reads up to 100bp, while the rest two for longer sequences ranged from\n70bp to a few megabases. BWA-MEM and BWA-SW share similar features such as the\nsupport of long reads and chimeric alignment, but BWA-MEM, which is the latest,\nis generally recommended as it is faster and more accurate. BWA-MEM also has\nbetter performance than BWA-backtrack for 70-100bp Illumina reads.\n\nFor all the algorithms, BWA first needs to construct the FM-index for the\nreference genome (the **index** command). Alignment algorithms are invoked with\ndifferent s",
    "url": "https://github.com/lh3/bwa",
    "last_updated": "2025-09-01T02:50:06+00:00"
  },
  {
    "full_name": "cjgb/rPython-win",
    "name": "rPython-win",
    "description": "rPython for Windows",
    "language": "R",
    "topics": [],
    "readme": "# rPython-win\n\nThis is rPython for Windows. If you are on Linux/Mac, you can [install it from CRAN](http://cran.r-project.org/web/packages/rPython/index.html).\n\n## Installation\n\n* Install [Rtools](http://cran.r-project.org/bin/windows/Rtools/)\n* Install [devtools](http://cran.r-project.org/web/packages/devtools/index.html) by running \n```\ninstall.packages(\"devtools\")\nlibrary(devtools)\n```\n* Download the package as a zip file (see \"Download Zip\" button to the right of this page).\n* Unzip the package and rename the directory as `rPython` (not `rPython-win`).\n* If needed, edit the `configure.win` file inside the directory. By default, it contains the lines\n\n```\necho 'PKG_LIBS=-LC:/python27/libs -lpython27'  > src/makevars.win\necho 'PKG_CFLAGS=-I\"C:/Python27/include\"'     >> src/makevars.win\n``` \n\nand you should edit them to point to the path where you installed Python. In the default configuration it is assumed that the Python version is 2.7 and that it is installed in `C:/Python27`. Change it according to your Python installation.\n* Install the package running `install(\"path/to/rPython\")`. For instance, I run\n```\ninstall(\"C:/Users/carlos/Downloads/rPython\")\n```\n\n## Issues\n\nrPython on Windows is mostly experimental and has only been tested on one Windows box. Please, contact the maintainer (me) if you find any issues on your platform.\n",
    "url": "https://github.com/cjgb/rPython-win",
    "last_updated": "2025-04-30T21:37:56+00:00"
  },
  {
    "full_name": "hyperopt/hyperopt",
    "name": "hyperopt",
    "description": "Distributed Asynchronous Hyperparameter Optimization in Python",
    "language": "Python",
    "topics": [
      "hacktoberfest"
    ],
    "readme": "\n# Hyperopt: Distributed Hyperparameter Optimization\n\n<p align=\"center\">\n<img src=\"https://i.postimg.cc/TPmffWrp/hyperopt-new.png\" />\n</p>\n\n[![build](https://github.com/hyperopt/hyperopt/actions/workflows/build.yml/badge.svg)](https://github.com/hyperopt/hyperopt/actions/workflows/build.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/hyperopt/hyperopt/master.svg)](https://results.pre-commit.ci/latest/github/hyperopt/hyperopt/master)\n[![PyPI version](https://badge.fury.io/py/hyperopt.svg)](https://badge.fury.io/py/hyperopt)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/hyperopt/badges/version.svg)](https://anaconda.org/conda-forge/hyperopt)\n\n[Hyperopt](https://github.com/hyperopt/hyperopt) is a Python library for serial and parallel optimization over awkward\nsearch spaces, which may include real-valued, discrete, and conditional\ndimensions.\n\n## Getting started\n\nInstall hyperopt from PyPI\n\n```bash\npip install hyperopt\n```\n\nto run your first example\n\n```python\n# define an objective function\ndef objective(args):\n    case, val = args\n    if case == 'case 1':\n        return val\n    else:\n        return val ** 2\n\n# define a search space\nfrom hyperopt import hp\nspace = hp.choice('a',\n    [\n        ('case 1', 1 + hp.lognormal('c1', 0, 1)),\n        ('case 2', hp.uniform('c2', -10, 10))\n    ])\n\n# minimize the objective over the space\nfrom hyperopt import fmin, tpe, space_eval\nbest = fmin(objective, space, algo=tpe.suggest, max_evals=100)\n\nprint(best)\n# -> {'a': 1, 'c2': 0.01420615366247227}\nprint(space_eval(space, best))\n# -> ('case 2', 0.01420615366247227}\n```\n\n## Contributing\n\nIf you're a developer and wish to contribute, please follow these steps.\n\n### Setup (based on [this](https://scikit-learn.org/stable/developers/contributing.html#contributing-code))\n\n1. Create an account on GitHub if you do not already have one.\n\n2. Fork the project repository: click on the ‘Fork’ button near the top of the page. This creates a copy of the code und",
    "url": "https://github.com/hyperopt/hyperopt",
    "last_updated": "2025-09-01T16:21:01+00:00"
  },
  {
    "full_name": "google-research/google-research",
    "name": "google-research",
    "description": "Google Research",
    "language": "Jupyter Notebook",
    "topics": [
      "machine-learning",
      "ai",
      "research"
    ],
    "readme": "# Google Research\n\nThis repository contains code released by\n[Google Research](https://research.google).\n\nAll datasets in this repository are released under the CC BY 4.0 International\nlicense, which can be found here:\nhttps://creativecommons.org/licenses/by/4.0/legalcode.  All source files in this\nrepository are released under the Apache 2.0 license, the text of which can be\nfound in the LICENSE file.\n\n---\n\nBecause the repo is large, we recommend you download only the subdirectory of\ninterest:\n\n* Use GitHub editor to open the project. To open the editor change the url from\ngithub.com to github.dev in the address bar.\n* In the left navigation panel, right-click on the folder of interest and select\ndownload.\n\nIf you'd like to submit a pull request, you'll need to clone the repository;\nwe recommend making a shallow clone (without history).\n\n```\ngit clone git@github.com:google-research/google-research.git --depth=1\n```\n\n---\n\n*Disclaimer: This is not an official Google product.*\n\nUpdated in 2023.",
    "url": "https://github.com/google-research/google-research",
    "last_updated": "2025-09-02T09:30:45+00:00"
  },
  {
    "full_name": "gojiplus/advertiser",
    "name": "advertiser",
    "description": "Posts to bsky a one-liner about one of your repos. with some min. number of stars (5) using OpenAI",
    "language": "Python",
    "topics": [
      "bsky-bot",
      "github"
    ],
    "readme": "<img src=\"advertiser_logo.png\" alt=\"Repo Logo\" width=\"40\" height=\"40\" />\n\n# Advertiser: Post a GenAI-Summarized GitHub Repo to Bluesky\n\nThis GitHub Action picks a GitHub repository (with at least five stars) from a curated list, summarizes it using OpenAI, and posts the summary to your Bluesky feed.\n\nPerfect for showcasing open-source gems to your followers — automatically or on demand.\n\n---\n\n## ✨ What It Does\n\n- 🔀 Randomly selects a repository from a curated list\n- 🧠 Uses GenAI (OpenAI) to summarize it into a crisp 1-liner\n- 🔗 Posts the summary, repo name, and link to your [Bluesky](https://bsky.app/) feed\n\n---\n\n## 🚀 Usage\n\n### `.github/workflows/post.yml`\n\n```yaml\nname: Post to Bluesky\n\non:\n  workflow_dispatch:  # Manual trigger\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: appeler/post-to-bsky@v1\n        with:\n          bsky_handle: ${{ secrets.BSKY_HANDLE }}\n          bsky_password: ${{ secrets.BSKY_PASSWORD }}\n          openai_api_key: ${{ secrets.OPENAI_API_KEY }}\n",
    "url": "https://github.com/gojiplus/advertiser",
    "last_updated": "2025-04-30T17:05:52+00:00"
  },
  {
    "full_name": "saurfang/SparkRext",
    "name": "SparkRext",
    "description": "SparkR extension for dplyr",
    "language": "R",
    "topics": [],
    "readme": "# SparkRext - SparkR extension for dplyr\n\n\n\n\n\n\n\nThis is a fork of the excellent package [SparkRext](https://github.com/hoxo-m/SparkRext), by [@hoxo-m](https://github.com/hoxo-m), which enables users to use [dplyr](https://github.com/saurfang/dplyr) NSE style calls for all data wrangling functions. However it's still impossible to use these functions for distributed Spark DataFrame and local R DataFrame at the same time. This fork enables such use case as shown below.\n\nThe motivation is that while SparkR provides a powerful interface to transform distributed DataFrame and practice machine learning algorithms, R still excels in small data world such as [data visualization](#interoperability-between-sparkr-and-dplyr), small data aggregation and etc. \n\n\n## Overview\n\n[Apache Spark](https://spark.apache.org/) is one of the hottest products in data science.  \nSpark 1.4.0 has formally adopted **SparkR** package which enables to handle Spark DataFrames on R.(See [this article](http://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html))\n\nSparkR is very useful and powerful.  \nOne of the reasons is that SparkR DataFrames present an API similar to **dplyr**.  \n\nFor example:\n\n\n```r\ndf <- createDataFrame(sqlContext, iris)\ndf %>%\n  select(\"Sepal_Length\", \"Species\") %>%\n  filter(df$Sepal_Length >= 5.5) %>%\n  group_by(df$Species) %>%\n  summarize(count = n(df$Sepal_Length), mean = mean(df$Sepal_Length)) %>%\n  collect  \n```\n\nThis is very cool. But I have a little discontent.\n\nOne of the reasons that dplyr is so much popular is the functions adopts NSE(non-standard evaluation).\n\n\n```r\nlibrary(dplyr)\niris %>%\n  select(Sepal.Length, Species) %>%\n  filter(Sepal.Length >= 5.5) %>%\n  group_by(Species) %>%\n  summarize(count = n(), mean = mean(Sepal.Length))\n```\n\n```\n## Source: local data frame [3 x 3]\n## \n##      Species count     mean\n##       (fctr) (int)    (dbl)\n## 1     setosa     5 5.640000\n## 2 versicolor    44 6.050000\n## 3  virginica    49 6.622449\n```\n\nIt's very smart.",
    "url": "https://github.com/saurfang/SparkRext",
    "last_updated": "2019-03-22T08:15:51+00:00"
  },
  {
    "full_name": "FakeNewsChallenge/fakenewschallenge.github.io",
    "name": "fakenewschallenge.github.io",
    "description": "Github Page for the Fake News Challenge",
    "language": "JavaScript",
    "topics": [],
    "readme": "[Fake News Challange](http://fakenewschallenge.github.io)\n",
    "url": "https://github.com/FakeNewsChallenge/fakenewschallenge.github.io",
    "last_updated": "2021-03-24T07:58:52+00:00"
  },
  {
    "full_name": "alistaire47/passport",
    "name": "passport",
    "description": "Travel smoothly between country name and code formats",
    "language": "R",
    "topics": [
      "r",
      "country-names",
      "country-codes",
      "country-data",
      "package"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# passport\n\n[![Travis-CI Build\nStatus](https://travis-ci.org/alistaire47/passport.svg?branch=master)](https://travis-ci.org/alistaire47/passport)\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/alistaire47/passport?branch=master&svg=true)](https://ci.appveyor.com/project/alistaire47/passport)\n[![Coverage\nStatus](https://codecov.io/gh/alistaire47/passport/branch/master/graph/badge.svg)](https://codecov.io/gh/alistaire47/passport)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/passport)](https://cran.r-project.org/package=passport)\n\n`passport` smooths the process of working with country names and codes\nvia powerful parsing, standardization, and conversion utilities arranged\nin a simple, consistent API. Country name formats include multiple\nsources including the Unicode CLDR common-sense standardizations in\nhundreds of languages.\n\n## Installation\n\nInstall from CRAN with\n\n``` r\ninstall.packages(\"passport\")\n```\n\nor the development version from GitHub with\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"alistaire47/passport\")\n```\n\n------------------------------------------------------------------------\n\n## Travel smoothly between country name and code formats\n\nWorking with country data can be frustrating. Even with well-curated\ndata like [`gapminder`](https://github.com/jennybc/gapminder), there are\nsome oddities:\n\n``` r\nlibrary(passport)\nlibrary(gapminder)\nlibrary(dplyr)    # Works equally well in any grammar.\nlibrary(tidyr)\nset.seed(47)\n\ngrep(\"Korea\", unique(gapminder$country), value = TRUE)\n#> [1] \"Korea, Dem. Rep.\" \"Korea, Rep.\"\ngrep(\"Yemen\", unique(gapminder$country), value = TRUE)\n#> [1] \"Yemen, Rep.\"\n```\n\n`passport` offers a framework for working with country names and codes\nwithout manually editing data or scraping codes from Wikipedia.\n\n### I. Standardize\n\nIf data has non-standardized names, standardize them to an ISO 3166-1\ncode or ot",
    "url": "https://github.com/alistaire47/passport",
    "last_updated": "2025-03-22T08:14:26+00:00"
  },
  {
    "full_name": "jflam/chat-gpt-jupyter-extension",
    "name": "chat-gpt-jupyter-extension",
    "description": "A browser extension that lets you chat with ChatGPT from any local Jupyter notebook.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# ChatGPT for Jupyter\n\n[![Binder](http://mybinder.org/badge_logo.svg)](http://mybinder.org/v2/gh/jflam/chat-gpt-jupyter-extension/HEAD?filepath=index.ipynb)\n\nThis is a browser extension that brings ChatGPT **into** your Jupyter\nnotebooks. The way I always describe Jupyter to my friends is that *it's a\ntool that handles the mundane task of writing things down for you*. When I saw\nChatGPT, I thought that there was no better home for ChatGPT than inside my\nJupyter notebooks.\n\nThis extension will reuse an existing login session that you must first\nestablish with [OpenAI's ChatGPT service](https://chat.openai.com/). If you\naren't signed in already, or your session timed out, it will prompt you to\nsign in. \n\nIt only works with the classic Jupyter notebook user interface. Help is\nwelcome to make it work with JupyterLab. It works with both local Jupyter\nnotebooks that you access via http://localhost or notebooks hosted on the\npublic MyBinder service at https://mybinder.org. \n\nIf you want it to work with Jupyter notebooks from a different URL, you'll\nneed to edit the `manifest.json` file. It works by injecting script from the\nextension into the Jupyter notebook web page and asking the extension to use\nthe bearer token obtained by logging into the Open AI service to communicate\nwith the service; the browser extension effectively acts as a privileged\nproxy. \n\nThis is the only reliable way that I've found to do this as OpenAI hasn't\nreleased an official API for the ChatGPT service. \n\n# Sample session\n\nHere's a simple example writing a ChatGPT prompt within a Jupyter notebook.\nYou create a markdown cell and enter your prompt. To distinguish a ChatGPT\nprompt cell from a regular markdown cell, you must have `##### chat` as the\nfirst line in your markdown.\n\n![Screenshot of ChatGPT Jupyter](./docs/screenshot0.png)\n\nOrdinarily in Jupyter, you press SHIFT+ENTER to render the markdown cell. The\nChatGPT Jupyter extension overloads this keystroke to send your prompt to\nChatGPT.\n\n![Screen",
    "url": "https://github.com/jflam/chat-gpt-jupyter-extension",
    "last_updated": "2025-04-12T00:25:24+00:00"
  },
  {
    "full_name": "soodoku/kirkuk",
    "name": "kirkuk",
    "description": "Data and scripts behind the paper \"What Future For Kirkuk?\"",
    "language": "R",
    "topics": [
      "deliberation",
      "survey",
      "iraq",
      "kirkuk"
    ],
    "readme": "## What Future For Kirkuk?\n\nThe repository includes all the data and scripts behind the paper, [What Future For Kirkuk?](http://gsood.com/research/papers/kirkuk.pdf) The paper is based on a deliberative intervention that we conducted in Kirkuk in early 2017.\n\nIn releasing the data, we haven't gone with the conventional, minimum dataset needed to narrowly replicate the results. We are releasing all the underlying data, including the briefing materials, questionnaires, etc. \n\n### Data\n\n1. [Briefing Materials in Arabic, English, and Kurdish](data/briefing_materials/)\n2. [Pre- and Post- Questionnaires in Arabic, English, and Kurdish](data/briefing_materials/)\n3. [Final Dataset we got from people on the ground (xlsx)](data/Raw-Data-Final.xlsx)\n4. [Wave 1 Data](data/t1.csv)\n5. [Wave 2 Data](data/t2_t3.csv)\n6. [Crosswalk between Var Name, Semantic Label, and Ordering Within Table (to group results)](data/qno_lab_cats.csv)\n\n### Scripts\n\n1. [Data inspection](scripts/00_data_inspection.R)\n2. [Recode Data](scripts/01_recode.R)\n3. [Socio-demographic Composition of the Sample (Table 1)](scripts/02_table_1.R)\n4. [Knowledge Gain (Table 2)](scripts/03_table_2.R)\n5. [Attitude Change (Tables 3 and 4)](scripts/04_table_3_4_C1.R)\n6. [Attitude Change by Ethnicity (Table 4)](scripts/05_table_5.R)\n\n### Tables\n\nSee the outputs of the scripts [here](tabs/).\n\n### Suggested Citation\n\nO'Flynn, Ian, Gaurav Sood, Jalal Mistaffa, and Nahwi Saeed. 20XX. Democratization. XX--XX.\n",
    "url": "https://github.com/soodoku/kirkuk",
    "last_updated": "2019-05-24T16:21:12+00:00"
  },
  {
    "full_name": "calebmadrigal/trackerjacker",
    "name": "trackerjacker",
    "description": "Like nmap for mapping wifi networks you're not connected to, plus device tracking",
    "language": "Python",
    "topics": [
      "hacking",
      "wireless",
      "python",
      "scapy",
      "network",
      "packets"
    ],
    "readme": "# trackerjacker\n\nLike nmap for mapping wifi networks you're not connected to. Maps and tracks wifi networks and devices through raw 802.11 monitoring.  \n\nPyPI page: https://pypi.python.org/pypi/trackerjacker\n\n#### Install\n\n    pip3 install trackerjacker\n\n*Supported platforms*: Linux (tested on Ubuntu, Kali, and RPi) and macOS (pre-alpha)\n\n![visual description](https://i.imgur.com/I5NH5KM.jpg)\n\ntrackerjacker can help with the following:\n\n* I want to know all the nearby wifi networks **and know all the devices connected to each network.**\n* I want to know who's hogging all the bandwidth.\n* I want to run a command when this MAC address sends more than 100000 bytes in a 30 second window (maybe to determine when an IP camera is uploading a video, which is indicative that it just saw motion).\n* I want to deauth anyone who uses more than 100000 bytes in a 10 second window.\n* I want to deauth every Dropcam in the area so my Airbnb hosts don't spy on me.\n* I want to be alerted when any MAC address is seen at a power level greater than -40dBm that I've never seen before.\n* I want to see when this particular person is nearby (based on the MAC of their mobile phone) and run a command to alert me.\n* I want to write my own plugin to run some script to do something fun every time a new Apple device shows up nearby.\n\n## Usage\n\nFind detailed usage like this:\n\n\ttrackerjacker -h\n\nThere are 2 major usage modes for `trackerjacker`: **map** mode and **track** mode:\n\n### Map mode example\n\nMap command:\n\n\ttrackerjacker -i wlan1337 --map\n\nBy default, this outputs the `wifi_map.yaml` YAML file, which is a map of all the nearby WiFi networks and all of their users. Here's an example `wifi_map.yaml` file:\n\t\n\tTEST_SSID:\n\t  00:10:18:6b:7a:ea:\n\t    bssid: 00:10:18:6b:7a:ea\n\t    bytes: 5430\n\t    channels:\n\t    - 11\n\t    devices:\n\t      3c:07:71:15:f1:48:\n\t        bytes: 798\n\t        signal: 1\n\t        vendor: Sony Corporation\n\t      78:31:c1:7f:25:43:\n\t        bytes: 4632\n\t        signal: -52\n\t    ",
    "url": "https://github.com/calebmadrigal/trackerjacker",
    "last_updated": "2025-08-31T00:16:01+00:00"
  },
  {
    "full_name": "simonmunzert/rscraping-jsm-2016",
    "name": "rscraping-jsm-2016",
    "description": "Repository for one-day course \"A Primer to Web Scraping with R\"",
    "language": "HTML",
    "topics": [],
    "readme": "# A primer to Web Scraping with R\n\n## General information\n\n**Summary**\n\nThe web is full of data that are of great interest to scientists and businesses alike. Firms, public institutions, and private users provide every imaginable type of information, and new channels of communication generate vast amounts of data on human behavior. But how to efficiently collect data from the Internet; retrieve information from social networks, search engines, and dynamic web pages; tap web services; and, finally, process the collected data with statistical software? We will learn about the basics of web data collection practice with R. The sessions are hands-on; we will practice every step of the process with R using various examples. We will learn how to scrape content from static and dynamic web pages, connect to APIs from popular web services such as Twitter to read out and process user data, and set up automatically working scraper programs. \n\n**Event**\n\nJoint Statistical Meetings 2016, Continuing Education Course, Chicago\n\n**Venue**\n\nMcCormick Place Convention Center, West Building, W470a\n\n**Instructor** \n\nSimon Munzert ([website](https://simonmunzert.github.io), [Twitter](https://twitter.com/simonsaysnothin))\n\n**Requirements**\n\nThis course assumes prior experience using R. Please bring a laptop with the latest version of R and Rstudio installed (see more below for the technical setup). \n\n**Time schedule**\n\n|  | Time | Topic |\n|--------|-------------------------|---------------------------------------------------------|\n| Slot 1 | 8.30 a.m. - 10.15 a.m. | Introduction, setup, and overview |\n| Slot 2 | 10.30 a.m. - 12.30 a.m. | Scraping static webpages with rvest |\n| Slot 3 | 2.00 p.m. - 3.15 p.m. | Scraping dynamic webpages with RSelenium; good practice |\n| Slot 4 | 3.30 p.m. - 5.00 p.m. | Tapping APIs |\n\n\n## Accompanying book\nTogether with Christian Rubba, Peter Meissner, and Dominic Nyhuis, I've written a book on [Automated Data Collection with R](http://r-datacollection.com",
    "url": "https://github.com/simonmunzert/rscraping-jsm-2016",
    "last_updated": "2025-06-11T02:15:33+00:00"
  },
  {
    "full_name": "ropensci/gutenbergr",
    "name": "gutenbergr",
    "description": "Search and download public domain texts from Project Gutenberg",
    "language": "R",
    "topics": [
      "r",
      "r-package",
      "rstats",
      "peer-reviewed"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# gutenbergr\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/gutenbergr)](https://CRAN.R-project.org/package=gutenbergr)\n[![rOpenSci\npeer-review](https://badges.ropensci.org/41_status.svg)](https://github.com/ropensci/software-review/issues/41)\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![R-CMD-check](https://github.com/ropensci/gutenbergr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ropensci/gutenbergr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/ropensci/gutenbergr/graph/badge.svg)](https://app.codecov.io/gh/ropensci/gutenbergr)\n<!-- badges: end -->\n\nDownload and process public domain works from the [Project\nGutenberg](https://www.gutenberg.org/) collection. Includes\n\n- A function `gutenberg_download()` that downloads one or more works\n  from Project Gutenberg by ID: e.g., `gutenberg_download(84)` downloads\n  the text of Frankenstein.\n- Metadata for all Project Gutenberg works as R datasets, so that they\n  can be searched and filtered:\n  - `gutenberg_metadata` contains information about each work, pairing\n    Gutenberg ID with title, author, language, etc\n  - `gutenberg_authors` contains information about each author, such as\n    aliases and birth/death year\n  - `gutenberg_subjects` contains pairings of works with Library of\n    Congress subjects and topics\n\n## Installation\n\n<div class=\".pkgdown-release\">\n\nInstall the released version of gutenbergr from\n[CRAN](https://cran.r-project.org/):\n\n``` r\ninstall.packages(\"gutenbergr\")\n```\n\n</div>\n\n<div class=\".pkgdown-devel\">\n\nInstall the development version of gutenbergr from\n[GitHub](https://github.com/):\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"ropensci/gutenbergr\")\n```\n\n</div>\n\n## Examples\n\nThe `gutenb",
    "url": "https://github.com/ropensci/gutenbergr",
    "last_updated": "2025-06-04T12:27:55+00:00"
  },
  {
    "full_name": "koaning/drawdata",
    "name": "drawdata",
    "description": "Draw datasets from within Python notebooks.",
    "language": "JavaScript",
    "topics": [
      "drawdata",
      "data",
      "marimo",
      "anywidget"
    ],
    "readme": "<img src=\"imgs/logo.png\" width=125 height=125 align=\"right\" style=\"z-index: 9999;\">\n\n### drawdata \n\n> \"Just draw some data and get on with your day.\"\n\nThis small Python library contains Jupyter widgets that allow you to draw a dataset in a Jupyter\nnotebook. This should be very useful when teaching machine learning algorithms.\n\n![CleanShot 2025-05-08 at 17 27 36](https://github.com/user-attachments/assets/ae87e26e-a720-494f-9fd2-ec0374a9f8f3)\n\nThe project uses [anywidget](https://anywidget.dev/) under the hood so our tools should work in Jupyter, VSCode and Colab. That also means that you get a proper widget that can interact with [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/) natively. [Here](https://www.youtube.com/watch?v=STPv0jSAQEk) is an example where updating a drawing triggers a new scikit-learn model to train ([code](https://github.com/probabl-ai/youtube-appendix/blob/main/04-drawing-data/notebook.ipynb)).\n\nYou can really get creative with this in a notebook, so feel free to give it a spin!\n\n#### Installation \n\nInstallation occurs via pip. \n\n```\npython -m pip install drawdata\n```\n\nTo read the data, `polars` is useful, but this library also suppots `pandas`:\n\n```\npython -m pip install pandas polars\n```\n\n#### Usage: `ScatterWidget`\n\nYou can load the scatter widget to start drawing immediately. \n\n```python\nfrom drawdata import ScatterWidget\n\nwidget = ScatterWidget()\nwidget\n```\n\nIf you want to use the dataset that you've just drawn you can do so via: \n\n```python\n# Get the drawn data as a list of dictionaries\nwidget.data\n\n# Get the drawn data as a dataframe\nwidget.data_as_pandas\nwidget.data_as_polars\n```\n\nIf you're eager to do scikit-learn stuff with your drawn data you may appreciate this property instead:\n\n```\nX, y = widget.data_as_X_y\n```\n\nThe assumption for this property is that if you've used multiple colors that you're interested in doing classification and if you've only drawn one color you're interested in regression. In the case of regression",
    "url": "https://github.com/koaning/drawdata",
    "last_updated": "2025-08-31T08:26:26+00:00"
  },
  {
    "full_name": "rien/reStream",
    "name": "reStream",
    "description": "Stream your reMarkable screen over SSH.",
    "language": "Rust",
    "topics": [
      "remarkable-tablet",
      "ffmpeg",
      "framebuffer",
      "video4linux-loopback"
    ],
    "readme": "# reStream\n\nreMarkable screen sharing over SSH.\n\n[![rm1](https://img.shields.io/badge/rM1-supported-green)](https://remarkable.com/store/remarkable)\n[![rm2](https://img.shields.io/badge/rM2-supported-green)](https://remarkable.com/store/remarkable-2)\n\n![A demo of reStream](extra/demo.gif)\n\n## Installation\n\n### Requirements\n\nOn your **host** machine\n\n- Any POSIX-shell (e.g. bash)\n- ffmpeg (with ffplay)\n- ssh\n- lz4\n\n#### Unix\n\n1. Install `lz4` on your host with your usual package manager.   \nOn Ubuntu, `apt install liblz4-tool` will do the trick.\n2. [Set up an SSH key and add it to the ssh-agent](https://help.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent), then add your key to the reMarkable with `ssh-copy-id root@10.11.99.1`.  \n> **Note:** the reMarkable 2 doesn't support `ed25519` keys. If it's your case, try generating an `ecdsa` or `rsa` key. Try out `ssh root@10.11.99.1`, it should **not** prompt for root@10.11.99.1's password.\n\n#### Windows\n\n1. Install [ffmpeg for windows](https://ffmpeg.org/download.html#build-windows).\n2. Download [lz4 for windows](https://github.com/lz4/lz4/releases) and extract the `zip` to a folder where you'll remember it (e.g. `C:\\Users\\{username}\\lz4`).\n3. Add the `ffmpeg` **and** `lz4` directories to the windows `Path` environment. [Here is a quick guide how.](https://www.architectryan.com/2018/03/17/add-to-the-path-on-windows-10/)\n    - Control Panel > Edit the system environment variables > Environment Variables\n    - Find the `Path` variable under System variables, click edit.\n    - Add two _New_ entries: one to the **bin** directory in the `ffmpeg` directory, and one to the `lz4` directory you created.\n    - Click OK\n4. (Re)start `bash` so the new `Path` is used.\n5. Generate a new ssh-key using `ssh-keygen`.\n6. Send the public key to the reMarkable (connect trough USB cable) using `ssh-copy-id -i ~/.ssh/id_rsa root@10.11.99.1`\n7. Try out `ssh root@10.11.99.1`, it should **not**",
    "url": "https://github.com/rien/reStream",
    "last_updated": "2025-08-26T14:00:13+00:00"
  },
  {
    "full_name": "Planetary-Computers/autotab-starter",
    "name": "autotab-starter",
    "description": "Build browser agents for real world tasks",
    "language": "Python",
    "topics": [
      "agent",
      "ai",
      "automation",
      "browser"
    ],
    "readme": "## Update (Dec 12, 2023)\n\nThanks for trying out autotab! Over the last few weeks we’ve learned from so many of you and are excited to start rolling out a smoother and more user-friendly autotab V1 (no dependencies!).\n\nAs we transition, this repo will no longer be supported. The good news is that any Python code you’ve generated with autotab will of course work as usual.\n\nJoin our [Discord channel](https://discord.gg/seGGxSUgzM) to follow along and for a chance to be one of the first to try out new updates!\n\n# autotab\n\nWelcome to autotab! autotab makes it easy to create auditable browser automations using AI. Go from a point & click demonstration in the browser to live code for those actions in seconds.\n\n> Note: This project is alpha release and actively being developed. Expect breaking changes and exciting new features regularly!\n\n## Quickstart\n\nIt usually takes 5-10 minutes to get everything set up (including gathering passwords and installing dependencies). You must have the Chrome browser installed, and we recommend setting up a Python virtual environment:\n\n```bash\ngit clone https://github.com/Planetary-Computers/autotab-starter.git\ncd autotab-starter\n# Recommended: Setup a Python virtual environment\nmake install\nbrew install --cask chromedriver\n```\n\n### Configuration\n\nConfigure your credentials: Create a `.autotab.yaml` file following the example in `.example.autotab.yaml`. (~3 minutes)\n\n### Run\n\nRun `autotab record` to open a new browser window where you can start recording your actions.\n\n> Note: When you run `autotab record`, an automation will first try to log you in to autotab using the `autotab_api_key` from your `.autotab.yaml` file. You need to be logged in to autotab to use the extension (and our Open AI API key). You log in to `autotab record` using your autotab API key which you can get for free at [autotab.com/dashboard](https://autotab.com/dashboard).\n\n## Usage\n\n### Recording an automation\n\nTo record a new automation, run `autotab record`. You can op",
    "url": "https://github.com/Planetary-Computers/autotab-starter",
    "last_updated": "2025-08-25T18:55:08+00:00"
  },
  {
    "full_name": "notnews/politifact",
    "name": "politifact",
    "description": "Partisan breakdown of politifact",
    "language": "R",
    "topics": [
      "politifact",
      "politifact-rating"
    ],
    "readme": "## Some Facts about Politifact\n\nI assessed Politifact on:  \n\n1. **Imbalance in scrutiny**: Do they vet statements by Democrats or Democratic-leaning organizations more than statements Republicans or Republican-leaning organizations?  \n\n2. **Batting average by party**: Roughly n_correct/n_checked, but instantiated here as mean Politifact rating.     \t\t  \n\nTo answer the questions, I scraped the data from [PolitiFact](http://politifact.com) and independently coded and appended data on party of the person or organization covered. (See also: the scripts used for [scraping](scripts/scrape_politifact.R) and [analyzing](scripts/analyze_politifact.R) the data, the [scraped data](data/politifacts.csv) and [data linking people and organizations to party](data/pol_names.csv).)\n\nUntil now, Politifact has checked veracity 3,859 statements by 703 politicians and organizations. Of these, I was able to establish the partisanship of 554 people and organizations. I restrict the analysis to 3,396 statements by organizations and people whose partisanship I could establish and who lean either towards the Republican or Democratic party. I code the Politifact 6-point True to Pants on Fire scale (true, mostly-true, half-true, barely-true, false, pants-fire) linearly so that it lies between 0 (pants-fire) and 1 (true).\n\nOf the 3,396 statements, about 44% (n = 1506) of the statements checked by PolitiFact are by Democrats or Democratic-leaning organizations. Rest of the roughly 56% (n = 1890) are by Republicans or Republican-leaning organizations. The average PolitiFact rating of statements by Democrats or Democratic-leaning organizations (batting average) is .63; it is .49 for statements by Republicans or Republican-leaning organizations.\n\nTo check whether the results are driven by some people receiving a lot of scrutiny, I tallied the total number of statements investigated for each person. Unsurprisingly, there is a large skew, with a few prominent politicians receiving a bulk of the atten",
    "url": "https://github.com/notnews/politifact",
    "last_updated": "2020-08-13T05:32:32+00:00"
  },
  {
    "full_name": "bayandin/awesome-awesomeness",
    "name": "awesome-awesomeness",
    "description": "A curated list of awesome awesomeness",
    "language": "Ruby",
    "topics": [],
    "readme": "# Awesome Awesomeness\n\nA curated list of amazingly awesome awesomeness.\n- Programming Languages Package Manager\n    - [Package-Manager](https://github.com/damon-kwok/awesome-package-manager)\n\n- Programming Languages\n\t- [Ada(Spark)](https://github.com/ohenley/awesome-ada)\t\n\t- [Ansible](https://github.com/jdauphant/awesome-ansible)\n\t- [AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey)\n\t- [AutoIt](https://github.com/J2TeaM/awesome-AutoIt)\n\t- [C](https://notabug.org/koz.ross/awesome-c)\n\t- [C/C++](https://github.com/fffaraz/awesome-cpp)\n\t- [CMake](https://github.com/onqtam/awesome-cmake)\n\t- Clojure\n\t\t- [by @mbuczko](https://github.com/mbuczko/awesome-clojure)\n\t\t- [by @razum2um](https://github.com/razum2um/awesome-clojure)\n\t- [ColdFusion](https://github.com/seancoyne/awesome-coldfusion)\n\t- Common Lisp\n\t\t- [Common Lisp Libraries](https://github.com/CodyReichert/awesome-cl)\n\t\t- [Learning Common Lisp](https://github.com/GustavBertram/awesome-common-lisp-learning-list)\n\t- [Coronavirus](https://github.com/soroushchehresa/awesome-coronavirus)\n\t- [Crystal](https://github.com/veelenga/awesome-crystal)\n\t- [D](https://github.com/zhaopuming/awesome-d)\n\t- [Delphi](https://github.com/Fr0sT-Brutal/awesome-delphi)\n\t- [Elixir](https://github.com/h4cc/awesome-elixir)\n\t- [Elm](https://github.com/isRuslan/awesome-elm)\n\t- Erlang\n\t\t- [by @0xAX](https://github.com/0xAX/erlang-bookmarks)\n\t\t- [by @drobakowski](https://github.com/drobakowski/awesome-erlang)\n\t\t- [by @unbalancedparentheses](https://github.com/unbalancedparentheses/spawnedshelter)\n\t- [F#](https://github.com/fsprojects/awesome-fsharp)\n\t- [Fortran](https://github.com/rabbiabram/awesome-fortran)\n\t- [Go](https://github.com/avelino/awesome-go)\n\t- [Go Patterns](https://github.com/tmrts/go-patterns)\n\t- [Groovy](https://github.com/kdabir/awesome-groovy)\n\t- [Haskell](https://github.com/krispo/awesome-haskell)\n\t- [Idris](https://github.com/joaomilho/awesome-idris)\n\t- [Java](https://github.com/akullpp/awesome-java)\n\t- [JavaScript](h",
    "url": "https://github.com/bayandin/awesome-awesomeness",
    "last_updated": "2025-09-02T03:42:25+00:00"
  },
  {
    "full_name": "mlc-ai/mlc-llm",
    "name": "mlc-llm",
    "description": "Universal LLM Deployment Engine with ML Compilation",
    "language": "Python",
    "topics": [
      "llm",
      "machine-learning-compilation",
      "language-model",
      "tvm"
    ],
    "readme": "<div align=\"center\">\n\n# MLC LLM\n\n[![Installation](https://img.shields.io/badge/docs-latest-green)](https://llm.mlc.ai/docs/)\n[![License](https://img.shields.io/badge/license-apache_2-blue)](https://github.com/mlc-ai/mlc-llm/blob/main/LICENSE)\n[![Join Discoard](https://img.shields.io/badge/Join-Discord-7289DA?logo=discord&logoColor=white)](https://discord.gg/9Xpy2HGBuD)\n[![Related Repository: WebLLM](https://img.shields.io/badge/Related_Repo-WebLLM-fafbfc?logo=github)](https://github.com/mlc-ai/web-llm/)\n\n**Universal LLM Deployment Engine with ML Compilation**\n\n[Get Started](https://llm.mlc.ai/docs/get_started/quick_start) | [Documentation](https://llm.mlc.ai/docs) | [Blog](https://blog.mlc.ai/)\n\n</div>\n\n## About\n\nMLC LLM is a machine learning compiler and high-performance deployment engine for large language models.  The mission of this project is to enable everyone to develop, optimize, and deploy AI models natively on everyone's platforms. \n\n<div align=\"center\">\n<table style=\"width:100%\">\n  <thead>\n    <tr>\n      <th style=\"width:15%\"> </th>\n      <th style=\"width:20%\">AMD GPU</th>\n      <th style=\"width:20%\">NVIDIA GPU</th>\n      <th style=\"width:20%\">Apple GPU</th>\n      <th style=\"width:24%\">Intel GPU</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Linux / Win</td>\n      <td>✅ Vulkan, ROCm</td>\n      <td>✅ Vulkan, CUDA</td>\n      <td>N/A</td>\n      <td>✅ Vulkan</td>\n    </tr>\n    <tr>\n      <td>macOS</td>\n      <td>✅ Metal (dGPU)</td>\n      <td>N/A</td>\n      <td>✅ Metal</td>\n      <td>✅ Metal (iGPU)</td>\n    </tr>\n    <tr>\n      <td>Web Browser</td>\n      <td colspan=4>✅ WebGPU and WASM </td>\n    </tr>\n    <tr>\n      <td>iOS / iPadOS</td>\n      <td colspan=4>✅ Metal on Apple A-series GPU</td>\n    </tr>\n    <tr>\n      <td>Android</td>\n      <td colspan=2>✅ OpenCL on Adreno GPU</td>\n      <td colspan=2>✅ OpenCL on Mali GPU</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nMLC LLM compiles and runs code on MLCEngine -- a unified high-performance LLM inference eng",
    "url": "https://github.com/mlc-ai/mlc-llm",
    "last_updated": "2025-09-02T08:39:15+00:00"
  },
  {
    "full_name": "x0rz/EQGRP",
    "name": "EQGRP",
    "description": "Decrypted content of eqgrp-auction-file.tar.xz",
    "language": "Perl",
    "topics": [
      "exploits",
      "shadowbrokers",
      "equationgroup",
      "hacking",
      "nsa",
      "tao"
    ],
    "readme": "# Browsable content of eqgrp-auction-file.tar.xz\n- Original file: https://mega.nz/#!zEAU1AQL!oWJ63n-D6lCuCQ4AY0Cv_405hX8kn7MEsa1iLH5UjKU\n- Passphrase: `CrDj\"(;Va.*NdlnzB9M?@K2)#>deB7mN` (as disclosed by the ShadowBrokers, [source](https://medium.com/@shadowbrokerss/dont-forget-your-base-867d304a94b1))\n- This summary is provided by the community: complaints/credits to `jvoisin` @ `dustri.org` and [@x0rz](https://www.twitter.com/x0rz)\n\n⚠️ Some binaries may be picked up by your antivirus\n\nNested Tar archives have been uncompressed in the [archive_files](/archive_files) folder.\n\n# Content\n# Unknown\n- **JACKLADDER** \n- **DAMPCROWD**\n- **ELDESTMYDLE**\n- **SUAVEEYEFUL**\n- **WATCHER**\n- **YELLOWSPIRIT**\n\n# Misc\n- **DITTLELIGHT (HIDELIGHT)** unhide **NOPEN** window to run unix oracle db scripts\n- **DUL** shellcode packer\n- **egg_timer** execution delayer (equivalent to `at`)\n- **ewok** [snmpwalk](http://www.net-snmp.org/docs/man/snmpwalk.html)-like?\n- **gr** Web crontab manager? wtf. NSA are webscale dude\n- **jackladderhelper** simple port binder\n- **magicjack** [DES](https://en.wikipedia.org/wiki/Data_Encryption_Standard) implementation in Perl \n- **PORKSERVER** inetd-based server for the **PORK** implant\n- **ri** equivalent to `rpcinfo`\n- **uX_local** Micro X server, likely for remote management\n- **ITIME** Change Date/Time of a last change on a file of an unix filesystem\n\n# Remote Code Execution \n## Solaris\n- **CATFLAP** Solaris 7/8/9 (SPARC and Intel) RCE (for a [__LOT__]( https://twitter.com/hackerfantastic/status/850799265723056128 ) of versions)\n- **EASYSTREET**/**CMSEX** and **cmsd** Solaris `rpc.cmsd` remote root\n- **EBBISLAND**/**ELVISCICADA**/**snmpXdmid** and **frown**: `CVE-2001-0236`, Solaris 2.6-2.9 - snmpXdmid Buffer Overflow\n- **sneer**: *mibissa* (Sun snmpd) RCE, with *DWARF* symbols :D\n- **dtspcdx_sparc** dtspcd RCE for SunOS 5. -5.8. what a useless exploit\n- **TOOLTALK** DEC, IRIX, or Sol2.6 or earlier Tooltalk buffer overflow RCE\n- **VIOLENTSPIRIT** RCE ",
    "url": "https://github.com/x0rz/EQGRP",
    "last_updated": "2025-09-01T18:22:29+00:00"
  },
  {
    "full_name": "Bellspringsteen/other.nyc",
    "name": "other.nyc",
    "description": "The other half of nyc",
    "language": "JavaScript",
    "topics": [],
    "readme": "# other.nyc\nThe other half of nyc\n",
    "url": "https://github.com/Bellspringsteen/other.nyc",
    "last_updated": "2023-08-26T00:07:39+00:00"
  },
  {
    "full_name": "KaHIP/KaHIP",
    "name": "KaHIP",
    "description": "KaHIP -- Karlsruhe HIGH Quality Partitioning.",
    "language": "C++",
    "topics": [
      "graph",
      "partitioning",
      "graph-partitioning",
      "evolutionary-algorithm",
      "load-balancer",
      "sharding",
      "edge-partitioning-algorithms",
      "algorithms",
      "partitioning-algorithms",
      "cpp",
      "algorithm-engineering",
      "process-mapping",
      "python"
    ],
    "readme": "KaHIP v3.19  [![Codacy Badge](https://app.codacy.com/project/badge/Grade/9d0d08ba6b2d42699ab74fe5f9697bb9)](https://www.codacy.com/gh/KaHIP/KaHIP/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=KaHIP/KaHIP&amp;utm_campaign=Badge_Grade)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n=====\n\nThe graph partitioning framework KaHIP -- Karlsruhe High Quality Partitioning.\n\nThe graph partitioning problem asks for a division of a graph's node set into k equally sized blocks such that the number of edges that run between the blocks is minimized. KaHIP is a family of graph partitioning programs. It includes KaFFPa (Karlsruhe Fast Flow Partitioner), which is a multilevel graph partitioning algorithm, in its variants Strong, Eco and Fast, KaFFPaE (KaFFPaEvolutionary), which is a parallel evolutionary algorithm that uses KaFFPa to provide combine and mutation operations, as well as KaBaPE which extends the evolutionary algorithm. Moreover, specialized techniques are included to partition road networks (Buffoon), to output a vertex separator from a given partition as well as techniques geared towards the efficient partitioning of social networks. Here is an overview of our framework:\n\n<p align=\"center\">\n<img src=\"./img/MGPall_en_new.png\"\n  alt=\"framework overview\"\n  width=\"601\" height=\"558\">\n</p>\n\n\n## NEW in v3.14:\n**Support for Python**: KaHIP can now also be used in Python. See below how to do that.\n\n*Hierarchical Partitionings*: KaHIP can compute hierarchial partitionings. All you have to do is specify the hierarchy and KaHIP is ready to go and does the multisection as you specified. \n\n*Node Ordering Algorithms*: Many applications rely on time-intensive matrix operations, such as factorization, which can be sped up significantly for large sparse matrices by interpreting the matrix as a sparse graph and computing a node ordering that minimizes the so-called fill-in. Here, we added new algorithm",
    "url": "https://github.com/KaHIP/KaHIP",
    "last_updated": "2025-08-25T01:30:42+00:00"
  },
  {
    "full_name": "yonicd/covrpage",
    "name": "covrpage",
    "description": "Create a summary readme for the testthat subdirectory to communicate with potential users",
    "language": "R",
    "topics": [
      "r"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![Travis-CI Build\nStatus](https://travis-ci.org/yonicd/covrpage.svg?branch=master)](https://travis-ci.org/yonicd/covrpage)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/yonicd/covrpage/master.svg)](https://codecov.io/github/yonicd/covrpage?branch=master)\n[![Covrpage\nSummary](https://img.shields.io/badge/covrpage-Last_Build_2022_12_23-brightgreen.svg)](http://tinyurl.com/yayrd3et)\n\n# covrpage\n\nTrust is earned not ~~inherited~~ importedFrom. Now that you’ve built a\ncool package, you want potential users to trust it so that they might\nadopt it. How to build trust in your piece of software? Unit testing is\npart of the components building trustworthiness of your package. Imagine\nyou’re at the point where you’ve tested most lines of your code with\nthorough assertions, including checks of edge cases. Proof of that hard\nwork will be a high test coverage, that potential users of your package\nmight notice thanks to a bright green coverage badge. But how would they\nknow your tests are thorough? That’s what `covrpage` helps you with, by\ncreating a [summary report](tests/README.md) of your tests that goes\nbeyond the coverage percentage. This way, potential users can see at a\nglance how good the unit testing of your package is.\n\n## Is `covrpage` report only for users?\n\nNo, it can also inform your work on your package, by helping you track\nprogress of the unit tests you’re working on, and it can show to\npotential *contributors* where help is needed.\n\n## Package Installation\n\n    # install.packages('remotes')\n\n    remotes::install_github('yonicd/covrpage')\n\n## Usage\n\n    # assuming you are in your package root directory\n\n    covrpage::covrpage()\n\n    # create the covrpage and send directly to remote repository\n\n    covrpage::covrpage(auto_push = TRUE)\n\n    # Copy output as a vignette to use in pkgdown\n\n    covrpage::covrpage(vignette = TRUE)\n\n    # assuming you are not in your package directory\n",
    "url": "https://github.com/yonicd/covrpage",
    "last_updated": "2025-03-22T08:13:35+00:00"
  },
  {
    "full_name": "r-spark/sparkwarc",
    "name": "sparkwarc",
    "description": "Load WARC files into Apache Spark with sparklyr",
    "language": "WebAssembly",
    "topics": [],
    "readme": "sparkwarc - WARC files in sparklyr\n================\n\nInstall\n=======\n\nInstall using with:\n\n``` r\ndevtools::install_github(\"javierluraschi/sparkwarc\")\n```\n\nIntro\n=====\n\nThe following example loads a very small subset of a WARC file from [Common Crawl](http://commoncrawl.org), a nonprofit 501 organization that crawls the web and freely provides its archives and datasets to the public.\n\n``` r\nlibrary(sparkwarc)\nlibrary(sparklyr)\nlibrary(DBI)\nlibrary(dplyr)\n```\n\n``` r\nsc <- spark_connect(master = \"local\")\n```\n\n    ## * Using Spark: 2.1.0\n\n``` r\nspark_read_warc(\n  sc,\n  \"warc\",\n  system.file(\"samples/sample.warc.gz\", package = \"sparkwarc\"),\n  repartition = 8)\n```\n\n``` sql\nSELECT count(value)\nFROM WARC\nWHERE length(regexp_extract(value, '<html', 0)) > 0\n```\n\n| count(value) |\n|:-------------|\n| 6            |\n\n``` r\ncc_regex <- function(ops) {\n  ops %>%\n    filter(regval != \"\") %>%\n    group_by(regval) %>%\n    summarize(count = n()) %>%\n    arrange(desc(count)) %>%\n    head(100)\n}\n\ncc_stats <- function(regex) {\n  tbl(sc, \"warc\") %>%\n    transmute(regval = regexp_extract(value, regex, 1)) %>%\n    cc_regex()\n}\n```\n\n``` r\ncc_stats(\"http-equiv=\\\"Content-Language\\\" content=\\\"(.*)\\\"\")\n```\n\n    ## # Source:     lazy query [?? x 2]\n    ## # Database:   spark_connection\n    ## # Ordered by: desc(count)\n    ##   regval count\n    ##    <chr> <dbl>\n    ## 1  ru-RU     5\n\n``` r\ncc_stats(\"<script .*src=\\\".*/(.+)\\\".*\")\n```\n\n    ## # Source:     lazy query [?? x 2]\n    ## # Database:   spark_connection\n    ## # Ordered by: desc(count)\n    ##                            regval count\n    ##                             <chr> <dbl>\n    ## 1                           08.js     5\n    ## 2                           ga.js     5\n    ## 3 jquery.formtips.1.2.2.packed.js     5\n    ## 4   jquery-ui-1.7.2.custom.min.js     5\n    ## 5             jquery-1.4.2.min.js     5\n    ## 6                        start.js     5\n    ## 7           jquery.equalHeight.js     5\n    ## 8                      lytebox.j",
    "url": "https://github.com/r-spark/sparkwarc",
    "last_updated": "2025-03-22T11:15:31+00:00"
  },
  {
    "full_name": "HistoryAtState/pocom",
    "name": "pocom",
    "description": "Source data for Principal Officers & Chiefs of Mission",
    "language": "XQuery",
    "topics": [],
    "readme": "# Principal Officers &amp; Chiefs of Mission\n\n[![exist-db CI](https://github.com/HistoryAtState/pocom/actions/workflows/build.yml/badge.svg)](https://github.com/HistoryAtState/pocom/actions/workflows/build.yml)\n\nSource data for [Principal Officers &amp; Chiefs of Mission](http://history.state.gov/departmenthistory/people/principals-chiefs).\n\n## Build\n\n1. Single `xar` file: Files `articles.xconf` and `issues.xconf` will only contain the index, no triggers!\n\n    ```shell\n    ant\n    ```\n\n    1. Since Releases have been automated when building locally you might want to supply your own version number (e.g. `X.X.X`) like this:\n\n    ```shell\n    ant -Dapp.version=X.X.X\n    ```\n\n## Release\n\nReleases for this data package are automated. Any commit to the `master` branch will trigger the release automation.\n\nAll commit message must conform to [Conventional Commit Messages](https://www.conventionalcommits.org/en/v1.0.0/) to determine semantic versioning of releases, please adhere to these conventions, like so:\n\n| Commit message  | Release type |\n|-----------------|--------------|\n| `fix(pencil): stop graphite breaking when too much pressure applied` | Patch Release |\n| `feat(pencil): add 'graphiteWidth' option` | ~~Minor~~ Feature Release |\n| `perf(pencil): remove graphiteWidth option`<br/><br/>`BREAKING CHANGE: The graphiteWidth option has been removed.`<br/>`The default graphite width of 10mm is always used for performance reasons.` | ~~Major~~ Breaking Release |\n\nWhen opening PRs commit messages are checked using commitlint.",
    "url": "https://github.com/HistoryAtState/pocom",
    "last_updated": "2025-08-25T11:05:47+00:00"
  },
  {
    "full_name": "timelyportfolio/navr",
    "name": "navr",
    "description": "R htmlwidget for responsive-nav.js",
    "language": "CSS",
    "topics": [],
    "readme": "# navr | Responsive Toolbar `htmlwidget`\nR [`htmlwidget`](http://htmlwidgets.org) seed/experiment for [responsive-nav.js](https://github.com/viljamis/responsive-nav.js).  Feel free to play, but please **note this is experimental and will change rapidly**.\n\nNot on CRAN, so use `devtools::install_github` to install.\n\n```r\ndevtools::install_github('timelyportfolio/navr')\n```\n\nFor examples, see the blog post [Week 10 | Responsive Toolbars](http://www.buildingwidgets.com/blog/2015/3/11/week-10-responsive-toolbars).\n",
    "url": "https://github.com/timelyportfolio/navr",
    "last_updated": "2023-03-18T06:08:23+00:00"
  },
  {
    "full_name": "hsnamkoong/marginal-dro",
    "name": "marginal-dro",
    "description": "Official code release for the paper \"Distributionally Robust Losses for Latent Covariate Mixtures\"",
    "language": "Python",
    "topics": [],
    "readme": "# Code release for \"Distributionally Robust Losses for Latent Covariate Mixtures\"\n\nThis repository contains the loss function and support code for the paper \"Distributionally Robust Losses for Latent Covariate Mixtures\"\n\nThe release consists of two files that each contain the distributionally robust dual and wrapper code to support bisection search.\n\n- The file `dual_lip_risk_bound` contains pytorch modules for the dual, covariate shift DRO losses. These can be used as loss function wrappers after fixing the Lipschitz smoothness L/epsilon\n- The file `utils` contains other utilities such as \n- `environment.yml` contains a copy of the working env for this project. It may also include unecessary packages.\n\nFor any questions or issues, please contact Tatsunori Hashimoto (thashim@stanford.edu)\n          \n## Usage\nThe easiest way to use this code is to use  `LipLoss` in `dual_lip_risk_bound`. This implements the smoothness-constrained DRO. `radius` is the smoothness constraint (L/epsilon), `x_in` is the input features used to define smoothness constraints, `b_init` is the initial dual variable value (can be set to zero). \n\nGiven an instance of LipLoss, one can then compute the DRO loss by passing per-example `losses` into the `forward` method, with a value for the dual variable `eta`. There are also wrappers for optimizing `eta` against a particular DRO uncertainty set (`rho`) but you can also treat `eta` directly as a hyperparameter if there is no particular setting of `rho` that is of interest. \n          \n# License \nCopyright (C) 2022 Tatsunori Hashimoto\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ",
    "url": "https://github.com/hsnamkoong/marginal-dro",
    "last_updated": "2023-09-09T12:15:11+00:00"
  },
  {
    "full_name": "yuqian5/PasswordCollection",
    "name": "PasswordCollection",
    "description": "A large list of leaked password that I've collected",
    "language": "",
    "topics": [
      "aircrack-ng",
      "password-cracker",
      "brute-force",
      "hashcat",
      "password",
      "passwords"
    ],
    "readme": "# Password Dictionary \n#### ~5GB  COMPRESSED\nA large list of leaked passwords that I've collected and consolidated. All stored in google drive.\n\n### Notice\nThis project is not long active and the collection will not receive future updates.\n\n### Download\n\n[https://drive.google.com/open?id=14xB93b5YveOzCY7EuDrcL5ElpN-HCNse]\n\n![alt text](https://github.com/yuqian5/PasswordDictionary/blob/master/README-pic.jpg)\n\n# Database Included\n\n### Common Credentials\n* 10-million-password-list-top-100.txt.bz2*\n* 10-million-password-list-top-1000.txt.bz2*\n* 10-million-password-list-top-10000.txt.bz2*\n* 10-million-password-list-top-100000.txt.bz2*\n* 10-million-password-list-top-1000000.txt.bz2*\n* 10-million-password-list-top-500.txt.bz2*\n* 10k-most-common.txt.bz2*\n* 500-worst-passwords.txt.bz2*\n* SplashData-2014.txt.bz2*\n* SplashData-2015-1.txt.bz2*\n* SplashData-2015-2.txt.bz2*\n* best1050.txt.bz2*\n* best110.txt.bz2*\n* best15.txt.bz2*\n* common-passwords-win.txt.bz2*\n* four-digit-pin-codes-sorted-by-frequency-withcount.csv.bz2*\n* medical-devices.txt.bz2*\n* temp\n* top-20-common-SSH-passwords.txt.bz2*\n* top-passwords-shortlist.txt.bz2*\n* worst-passwords-2017-top100-slashdata.txt.bz2*\n\n### Leaked Database\n* 000webhost.txt.bz2*\n* 500-worst-passwords.txt.bz2*\n* Ashley-Madison.txt.bz2*\n* Lizard-Squad.txt.bz2*\n* adobe100.txt.bz2*\n* alleged-gmail-passwords.txt.bz2*\n* alypaa.txt.bz2*\n* bible.txt.bz2*\n* cain.txt.bz2*\n* carders.cc.txt.bz2*\n* clarkson-university-82.txt.bz2*\n* conficker.txt.bz2*\n* crackstation-human-only.txt.gz*\n* crackstation.txt.gz*\n* elitehacker.txt.bz2*\n* facebook-pastebay.txt.bz2*\n* facebook-phished.txt.bz2*\n* faithwriters.txt.bz2*\n* hak5.txt.bz2*\n* honeynet2.txt.bz2*\n* hotmail.txt.bz2*\n* izmy.txt.bz2*\n* john.txt.bz2*\n* md5decryptor.uk.txt.bz2*\n* muslimMatch.txt.bz2*\n* myspace.txt.bz2*\n* passwords-youporn2012-raw.txt.bz2*\n* passwords-youporn2012.txt.bz2*\n* phpbb.txt.bz2*\n* porn-unknown.txt.bz2*\n* rockyou.txt.bz2*\n* singles.org.txt.bz2*\n* tuscl.txt.bz2*\n* twitter-banned.txt.bz2",
    "url": "https://github.com/yuqian5/PasswordCollection",
    "last_updated": "2025-08-24T21:07:18+00:00"
  },
  {
    "full_name": "dirmeier/datastructures",
    "name": "datastructures",
    "description": " :rocket: Implementation of core data structures for R",
    "language": "R",
    "topics": [
      "r",
      "datastructures",
      "rcpp",
      "algorithms"
    ],
    "readme": "# datastructures <img src=\"https://cdn.rawgit.com/dirmeier/datastructures/87d7cd08/inst/heap/heap.png\" align=\"right\" width=\"160px\"/>\n\n[![Project Status](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![Project Life](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)\n[![ci](https://github.com/dirmeier/datastructures/workflows/ci/badge.svg)](https://github.com/dirmeier/datastructures/actions?query=workflow%3Aci)\n[![codecov](https://codecov.io/gh/dirmeier/datastructures/branch/master/graph/badge.svg)](https://codecov.io/gh/dirmeier/datastructures)\n[![CRAN](http://www.r-pkg.org/badges/version/datastructures?color=brightgreen)](https://cran.r-project.org/package=datastructures)\n\nImplementation of core data structures for R.\n\n## Introduction\n\nImplementation of advanced data structures such as hashmaps, heaps, or queues in `R`.\nAdvanced data structures are essential in many computer science and statistics\nproblems, for example graph algorithms or string analysis. The package uses\n`Boost` and `STL` data types and extends these to `R` with `Rcpp` modules.\n\nSo far `datastructures` has implementations for:\n\n* Fibonacci and binomial heaps,\n* queues and stacks,\n* hashmaps, multimaps and bimaps.\n\nAs an introductory example, imagine that you want to compute shortest paths on a\ngraph and decide to use a Fibonacci heap for keeping the distances. A Fibonacci heap is an efficient tree-like data structure\nthat satisfies the *min-heap property*. We can use it to quickly get the node with the shortest distance in *O(log n)* time like this:\n\n```R\n  fh <- fibonacci_heap(\"numeric\")\n  node.labels    <- paste0(\"n\", 10:1)\n  node.distances <- seq(1, 0, length.out=length(node.labels))\n  fh <- insert(fh, node.distances, node.labels)\n\n  peek(fh)\n  $`0`\n  [1] \"n1\"\n```\n\n`datastructures` also allows storing non-orimitive objects, like `data.frames`, `matrices` or `environments`.\nFor instance, we could use",
    "url": "https://github.com/dirmeier/datastructures",
    "last_updated": "2025-06-18T12:32:26+00:00"
  },
  {
    "full_name": "notnews/hard_news",
    "name": "hard_news",
    "description": "The Softening of Network Television News",
    "language": "R",
    "topics": [
      "news",
      "tv-news",
      "soft-news",
      "network-television-news"
    ],
    "readme": "## Hard News: The Softening of Network Television News\n\nNetwork television news is among the most frequently consumed news in the country. But there is little data on what is covered on network news. Or how the quality of network news has fared over the years. We exploit the Vanderbilt Television News Archive, the largest publicly available database of TV news, to learn about two important aspects of the production of news: geographic focus and political content. Using data from a random sample of over 5,000 broadcast segments spanning 1968--2019, we find that the percentage of network television news devoted to topics unrelated to politics steadily increased from less than 5% in 1968 to over 10% in the last 15 years. The pattern of change in geographic focus is more complex, but there is a clear rise in the percentage of local news over the last two decades. The percentage of local news increased from about 5% in 2000 to over 25% in 2019.\n\n<p align=\"center\">\n<img src=\"figs/fig_prob_news_all.png\" width=\"750\">\n</p>\n\n### Data\n\n* [Vanderbilt Broadcast TV News Archives](https://github.com/notnews/vandy_tv_news_abstracts)\n* [Coding instrument](data/coding_instrument.docx)\n* [Sample](data/sample_questions.csv)\n* [Screenshots](data/screenshots/)\n* [Gold standard coding](data/sample_questions_gold.csv)\n* [Final data](data/final_data.csv)\n\n### Scripts\n\n* [Clean scraped data](scripts/01_clean_vandy.R)\n\n* [Create Sample](scripts/02_sample_f8.R)\n    - Stratified random sample by year\n    - Weighted random sample by duration of the news segment\n\n* [Recode Data](scripts/03_recode.R)\n\n* [Assess Reliability and Quality of Data](scripts/04_quality_checks.R)\n\n* [Summary statistics on full data set](scripts/05_data_description.R)\n\n* [Coverage of soft news, by channel, by year, by weekday/weekend](scripts/06_soft.R)\n\n* [Coverage of local, national, and foreign, by channel, by year, month, ](scripts/07_geo.R)\n\n* [Coverage of topics in agg., by channel, year, etc.](scripts/08_topics.R)\n\n",
    "url": "https://github.com/notnews/hard_news",
    "last_updated": "2022-02-14T04:34:24+00:00"
  },
  {
    "full_name": "DmitryUlyanov/deep-image-prior",
    "name": "deep-image-prior",
    "description": "Image restoration with neural networks but without learning.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "**Warning!** The optimization may not converge on some GPUs. We've personally experienced issues on Tesla V100 and P40 GPUs. When running the code, make sure you get similar results to the paper first. Easiest to check using text inpainting notebook.  Try to set double precision mode or turn off cudnn. \n\n# Deep image prior\n\nIn this repository we provide *Jupyter Notebooks* to reproduce each figure from the paper:\n\n> **Deep Image Prior**\n\n> CVPR 2018\n\n> Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky\n\n\n[[paper]](https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf) [[supmat]](https://box.skoltech.ru/index.php/s/ib52BOoV58ztuPM) [[project page]](https://dmitryulyanov.github.io/deep_image_prior)\n\n![](data/teaser_compiled.jpg)\n\nHere we provide hyperparameters and architectures, that were used to generate the figures. Most of them are far from optimal. Do not hesitate to change them and see the effect.\n\nWe will expand this README with a list of hyperparameters and options shortly.\n\n# Install\n\nHere is the list of libraries you need to install to execute the code:\n- python = 3.6\n- [pytorch](http://pytorch.org/) = 0.4\n- numpy\n- scipy\n- matplotlib\n- scikit-image\n- jupyter\n\nAll of them can be installed via `conda` (`anaconda`), e.g.\n```\nconda install jupyter\n```\n\n\nor create an conda env with all dependencies via environment file\n\n```\nconda env create -f environment.yml\n```\n\n## Docker image\n\nAlternatively, you can use a Docker image that exposes a Jupyter Notebook with all required dependencies. To build this image ensure you have both [docker](https://www.docker.com/) and  [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) installed, then run\n\n```\nnvidia-docker build -t deep-image-prior .\n```\n\nAfter the build you can start the container as\n\n```\nnvidia-docker run --rm -it --ipc=host -p 8888:8888 deep-image-prior\n```\n\nyou will be provided an URL through which you can connect to the Jupyter notebook.\n\n## Google Colab\n\nTo run it using Google Col",
    "url": "https://github.com/DmitryUlyanov/deep-image-prior",
    "last_updated": "2025-09-02T07:36:58+00:00"
  },
  {
    "full_name": "jeroenjanssens/tikz2pdf",
    "name": "tikz2pdf",
    "description": "Compile TikZ figures to PDF",
    "language": "Python",
    "topics": [],
    "readme": "tikz2pdf\n========\n\nTikZ is for print what [D3](http://d3js.org/) is for web. If you are unfamiliar with this nifty LaTeX package, then [this gallery of examples](http://www.texample.net/tikz/examples/all/) will convince you to use TikZ for your next publication.\n\n**tikz2pdf** is a command-line tool to aid you in creating beautiful figures.\nIt's written in Python and (only) requires a [working TeX system](http://en.wikibooks.org/wiki/LaTeX/Installation). \n\nBasic Usage\n-----------\n\nLet's say we have a file named `hello.tikz`, which contains the following:\n\n```latex\n\\begin{tikzpicture}\n\t\\node[circle, draw=blue, dashed, thick, rotate=30, font=\\Large] {Hello World!};\n\\end{tikzpicture}\n```\n\nRun `tikz2pdf hello.tikz` from the command-line in order to compile this into a PDF file named `hello.pdf`.\n\nYou can convert this PDF to PNG with [ImageMagick](http://www.imagemagick.org/script/index.php):\n\n```bash\nconvert -flatten -density 96 -quality 1000 hello.pdf hello.png\n```\n\nto get the following image:\n\n![Hello World!](examples/hello.png)\n\nTemplates\n---------\n\nTemplates are useful for when you have many figures that need the same styling (i.e., LaTeX preamble). The following is the minimal template that is used when no template is specified:\n\n```latex\n\\documentclass{article}\n\\usepackage{tikz}\n\\pagestyle{empty}\n\\usepackage[active,tightpage]{preview}\n\\PreviewEnvironment[]{tikzpicture}\n\\PreviewEnvironment[]{tabular}\n\\begin{document}\n%tikz2pdf-tikz\n\\end{document}\n```\n\nThe string `%tikz2pdf-tikz` is replaced with the actual TikZ code. If the TikZ file contains \"\\documentclass\" it is treated as a self-contained document and no template is used. So, the file `examples/swan-wave-model.tex`, which illustrates the [SWAN wave model](http://www.texample.net/tikz/examples/swan-wave-model/), can be directly converted to a PDF file:\n\n```bash\ntikz2pdf swan-wave-model.tex\nconvert -flatten -density 96 -quality 1000 swan-wave-model.pdf swan-wave-model.png\n```\n\n![SWAN Model](examples/swan-wave-mode",
    "url": "https://github.com/jeroenjanssens/tikz2pdf",
    "last_updated": "2025-04-19T16:52:24+00:00"
  },
  {
    "full_name": "lawrencehook/remove-youtube-suggestions",
    "name": "remove-youtube-suggestions",
    "description": "A browser extension that removes YouTube suggestions, comments, shorts, and more",
    "language": "JavaScript",
    "topics": [
      "youtube",
      "firefox",
      "extension",
      "firefox-extension",
      "chrome-extension",
      "firefox-addon",
      "productivity",
      "productivity-booster",
      "youtube-extension"
    ],
    "readme": "# RYS — Remove YouTube Suggestions\n#### A Browser Extension\n\n---\n\n### What it does\nThis extension aims to make YouTube less engaging and more configurable. It provides options to hide recommended videos and to customize the user interface.\n\n---\n\n### Feedback and Support\nLeave a review!\n- [Firefox](https://addons.mozilla.org/en-US/firefox/addon/remove-youtube-s-suggestions)\n- [Chrome](https://chrome.google.com/webstore/detail/remove-youtube-suggestion/cdhdichomdnlaadbndgmagohccgpejae)\n- [Google Form](https://docs.google.com/forms/d/1AzQQxTWgG6M5N87jinvXKQkGS6Mehzg19XV4mjteTK0/edit)\n\nCompletely free. Donations welcome — [Paypal](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=FF9K9YD6K6SWG&currency_code=USD&source=url)  \n\n---\n\n### Why I made it\nMotivated by an attempt to stymie the YouTube rabbit hole.\n\nThe YouTube recommendation algorithm optimizes for the most _engaging_ videos, regardless of whether or not you are interested. Persistent exposure to these suggestions can result in a waste of your time. Download this add-on and remove unwanted suggestions as you please.\n\nAvailable for download at the links below:\n- [Firefox](https://addons.mozilla.org/en-US/firefox/addon/remove-youtube-s-suggestions)\n- [Chrome](https://chrome.google.com/webstore/detail/remove-youtube-suggestion/cdhdichomdnlaadbndgmagohccgpejae)\n\n---\n\n### Development\nThis project is 100% open source. Created and maintained by me, [Lawrence Hook](https://lawrencehook.com).  \n\nHave a feature request or found a bug? Feel free to create a Github issue, submit a PR, or contact me at lawrencehook@gmail.com.\n\nThe following commands will set up a Firefox dev environment.\n\n```bash\ngit clone https://github.com/lawrencehook/remove-youtube-suggestions.git\ncd remove-youtube-suggestions/src\nnpm install --global web-ext\nweb-ext run\n```\n",
    "url": "https://github.com/lawrencehook/remove-youtube-suggestions",
    "last_updated": "2025-08-29T12:15:38+00:00"
  },
  {
    "full_name": "rmcelreath/rethinking",
    "name": "rethinking",
    "description": "Statistical Rethinking course and book package",
    "language": "R",
    "topics": [],
    "readme": "rethinking\n==========\n\nThis R package accompanies a course and book on Bayesian data analysis: McElreath 2020. Statistical Rethinking, 2nd edition, CRC Press. If you are using it with the first edition of the book, please see the notes at the bottom of this file.\n\nIt contains tools for conducting both quick quadratic approximation of the posterior distribution as well as Hamiltonian Monte Carlo (through RStan or cmdstanr - mc-stan.org). Many packages do this. The signature difference of this package is that it forces the user to specify the model as a list of explicit distributional assumptions. This is more tedious than typical formula-based tools, but it is also much more flexible and powerful and---most important---useful for teaching and learning. When students have to write out every detail of the model, they actually learn the model.\n\nFor example, a simple Gaussian model could be specified with this list of formulas:\n\n```\nf <- alist(\n    y ~ dnorm( mu , sigma ),\n    mu ~ dnorm( 0 , 10 ),\n    sigma ~ dexp( 1 )\n)\n```\n\nThe first formula in the list is the probability of the outcome (likelihood); the second is the prior for ``mu``; the third is the prior for ``sigma``.\n\n# Installation\n\nThere are three steps. (1) Install the C++ toolchain, (2) install ``cmdstanr``, (3) install ``rethinking``. Details follow.\n\nFirst, install the C++ toolchain. Go to ``https://mc-stan.org/docs/cmdstan-guide/cmdstan-installation.html#cpp-toolchain`` and follow the instructions for your platform.\n\nSecond, install the ``cmdstanr`` package. Visit ``https://mc-stan.org/cmdstanr/``. The first time you install cmdstanr, you will also need compile the libraries with ``cmdstanr::install_cmdstan()``. All this of this bother is worth it. You just have to do it once. If you don't want to use MCMC, you don't have to complete this step.\n\nThird, you can install ``rethinking`` from within R using:\n```\ninstall.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\",\"shape\"))\ndevtools::install_github(\"",
    "url": "https://github.com/rmcelreath/rethinking",
    "last_updated": "2025-09-01T19:26:11+00:00"
  },
  {
    "full_name": "soodoku/data-science",
    "name": "data-science",
    "description": "Lecture Slides for Introduction to Data Science",
    "language": "TeX",
    "topics": [
      "data-science",
      "statistical-learning"
    ],
    "readme": "Data Science: Some Basics\n==========================\n\n 1. Introduction to Data Science ([presentation](ds1/ds1_present_web.pdf), [tex](ds1/ds1_web.tex))\n    * What can Big Data do for you? \n    * What is Big Data? \n    * Implications for Statistics and Computation \n    * What is Data Science? \n    * Prerequisites\n \n 2. Get your own (Big) Data ([presentation](ds2/ds2_present_web.pdf), [tex](ds2/ds2_web.tex))\n    * Scrape web pages and pdfs. ([Scripts](https://github.com/soodoku/python-workshop)) \n    * Image to Text ([Python Script using Tesseract](https://github.com/soodoku/image-to-text))\n    * Image to Text in R using the [Abbyy FineReader Cloud OCR](https://github.com/soodoku/abbyyR)\n    * Image to Text in R using the [Captricity API](https://github.com/soodoku/captr)\n    * Web Scraping/API Applications:\n      - [Get Data on Journalists](https://github.com/soodoku/get-journalist-data)\n      - [Get Weather Data](https://github.com/soodoku/get-weather-data)\n      - [Get Cricket Data](https://github.com/soodoku/get-cricket-data)\n      - [Get Congressional Speech Data](https://gist.github.com/soodoku/85d79275c5880f67b4cf)\n      - [Track FB Likes, Twitter Followers, Youtube Views](https://github.com/soodoku/likes-followers-views)\n      - [Track Civil Rights Coverage in NY Times using NYT API](https://github.com/soodoku/nyt-civil-rights)\n    * [Get Social Networking Data](https://github.com/pablobarbera/social-media-workshop)\n    * Regular Expressions\n    * Pre-process text data\n    * [Assignment](ds2/scraping_assignment_web.txt)\n   \n 3. Databases and SQL ([presentation](ds3/ds3_present_web.pdf), [tex](ds3/ds3_web.tex))\n    * What are databases? \n    * Relational Model\n    * Relational Algebra\n    * Basic SQL\n    * Views\n \n 4a. [Introduction to Introduction to Statistical Learning](https://github.com/soodoku/ds)\n \n 4b. Introduction to Statistical Learning ([presentation](ds4/ds4_present_web.pdf), [tex](ds4/ds4_web.tex))\n    * How to learn from data? \n    * Nearest Neig",
    "url": "https://github.com/soodoku/data-science",
    "last_updated": "2025-07-22T08:25:13+00:00"
  },
  {
    "full_name": "quanteda/readtext",
    "name": "readtext",
    "description": "an R package for reading text files",
    "language": "R",
    "topics": [
      "quanteda",
      "r",
      "text",
      "encoding"
    ],
    "readme": "\n# readtext: Import and handling for plain and formatted text files\n\n<!-- badges: start -->\n\n[![CRAN\nVersion](https://www.r-pkg.org/badges/version/readtext)](https://CRAN.R-project.org/package=readtext)\n[![](https://img.shields.io/badge/devel%20version-0.92.1-royalblue.svg)](https://github.com/quanteda/readtext)\n[![Downloads](https://cranlogs.r-pkg.org/badges/readtext)](https://CRAN.R-project.org/package=readtext)\n[![Total\nDownloads](https://cranlogs.r-pkg.org/badges/grand-total/readtext?color=orange)](https://CRAN.R-project.org/package=readtext)\n[![R-CMD-check](https://github.com/quanteda/readtext/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/quanteda/readtext/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/quanteda/readtext/branch/master/graph/badge.svg)](https://app.codecov.io/gh/quanteda/readtext?branch=master)\n<!-- badges: end -->\n\nAn R package for reading text files in all their various formats, by Ken\nBenoit, Adam Obeng, Paul Nulty, Aki Matsuo, Kohei Watanabe, and Stefan\nMüller.\n\n## Introduction\n\n**readtext** is a one-function package that does exactly what it says on\nthe tin: It reads files containing text, along with any associated\ndocument-level metadata, which we call “docvars”, for document\nvariables. Plain text files do not have docvars, but other forms such as\n.csv, .tab, .xml, and .json files usually do.\n\n**readtext** accepts filemasks, so that you can specify a pattern to\nload multiple texts, and these texts can even be of multiple types.\n**readtext** is smart enough to process them correctly, returning a\ndata.frame with a primary field “text” containing a character vector of\nthe texts, and additional columns of the data.frame as found in the\ndocument variables from the source files.\n\nAs encoding can also be a challenging issue for those reading in texts,\nwe include functions for diagnosing encodings on a file-by-file basis,\nand allow you to specify vectorized input encodings to read in file\ntyp",
    "url": "https://github.com/quanteda/readtext",
    "last_updated": "2025-07-27T09:25:42+00:00"
  },
  {
    "full_name": "ramnathv/summarytrees",
    "name": "summarytrees",
    "description": "R package to compute and visualize summary trees",
    "language": "C",
    "topics": [],
    "readme": "## R package `summarytrees`\n\nR package for computation and visualization of summary trees\n\n#### Introduction\n\n`summarytrees` is an R package to help you summarize and visualize a potentially large data set that can be represented by a rooted, node-weighted tree.\n\nThe input data looks something like the table below, containing the node ID, the ID of the node's parent, the node's (non-negative) weight, and its label. The example table pictured below contains a snapshot of the [DMOZ topic hierarchy](http://www.dmoz.org/) circa 2015, with over 635,000 nodes (unique topics) in the tree, where each node weight has been set to the number of URLs belonging to the given topic in the hierarchy. There are roughly 3.7 million total URLs (this is the sum of the weights). (The level of each node is also shown in the table, but is not required as input to compute summary trees.)\n\n|   node| parent| weight|label                                 | level|\n|------:|------:|------:|:-------------------------------------|-----:|\n|      1|      0|      0|Top                                   |     1|\n|      2|      1|      0|Arts                                  |     2|\n|      3|      1|      0|Business                              |     2|\n|      4|      1|      0|Computers                             |     2|\n|      5|      1|      0|Games                                 |     2|\n|      6|      1|      0|Home                                  |     2|\n|      7|      1|      0|Recreation                            |     2|\n|    ...|    ...|   ...|...                                    |   ...|\n| 635853| 635689|      3|Instytut_Mikromechaniki_i_Fotoniki    |    15|\n| 635854| 635062|      1|Maronite                              |    15|\n| 635855| 635074|      3|Wirtschaftsprüfung_und_Steuerberatung |    15|\n\nThe `summarytrees` package implements a dynamic programming algorithm that aggregates the nodes of the input tree in an optimal way, maximizing the entropy of the distribution of the no",
    "url": "https://github.com/ramnathv/summarytrees",
    "last_updated": "2015-08-13T15:57:28+00:00"
  },
  {
    "full_name": "awesomedata/awesome-public-datasets",
    "name": "awesome-public-datasets",
    "description": "A topic-centric list of HQ open datasets.",
    "language": "",
    "topics": [
      "opendata",
      "aaron-swartz",
      "awesome-public-datasets",
      "datasets"
    ],
    "readme": "Awesome Public Datasets\n=======================\n\n.. image:: https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\n   :alt: Awesome\n   :target: https://github.com/sindresorhus/awesome\n\nThis is a list of `topic-centric public data sources <https://github.com/awesomedata/awesome-public-datasets>`_\nin high quality. They are collected and tidied from blogs, answers, and user responses.\nMost of the data sets listed below are free, however, some are not.\nThis project was incubated at `OMNILab <https://github.com/OMNILab>`_, Shanghai Jiao Tong University during Xiaming Chen's Ph.D. studies.\nOMNILab is now part of the `BaiYuLan Open AI community <https://github.com/Bai-Yu-Lan>`_.\nOther amazingly awesome lists can be found in `sindresorhus's awesome <https://github.com/sindresorhus/awesome>`_ list.\n\n**NOTICE**: This repo is automatically generated by `apd-core <https://github.com/awesomedata/apd-core/tree/master/core>`_.\nPlease **DO NOT** modify this file directly. We have provided a new way to `contribute to \nthis repo <https://github.com/awesomedata/apd-core/blob/master/CONTRIBUTING.md>`_. \n`Join <https://join.slack.com/t/awesomedataworld/shared_invite/zt-dllew5xy-PJYi~mWUdY3hupohbmVZsA>`_ \nthe `slack community <https://awesomedataworld.slack.com>`_ for an instant touch of HQ data updates.\n\n.. |OK_ICON| image:: https://raw.githubusercontent.com/awesomedata/apd-core/master/deploy/ok-24.png\n.. |FIXME_ICON| image:: https://raw.githubusercontent.com/awesomedata/apd-core/master/deploy/fixme-24.png\n\n* |OK_ICON| I am well.\n* |FIXME_ICON| Please fix me.\n\n.. contents:: **Table of Contents**\n\n    \nAgriculture\n-----------\n        \n* |OK_ICON| `The global dataset of historical yields for major crops 1981–2016 - The Global Dataset of [...] <https://doi.pangaea.de/10.1594/PANGAEA.909132>`_ [`Meta <https://github.com/awesomedata/apd-core/tree/master/core//Agriculture/Global-dataset-of-historical-yields-for-major-crops.yml>`_]\n        \n* |OK_I",
    "url": "https://github.com/awesomedata/awesome-public-datasets",
    "last_updated": "2025-09-02T09:29:39+00:00"
  },
  {
    "full_name": "SKbarbon/flet_multi_page",
    "name": "flet_multi_page",
    "description": "until now, flet does not support multi pages. With this you can start new pages on the same script.",
    "language": "Python",
    "topics": [],
    "readme": "# flet_multi_page\nuntil now, flet does not support multi pages.\nWith this tool, you can start new pages on the same script without the need of creating new `app` class or new cmd process etc...\n\n\n## install\n\n```\npip install flet-multi-page --upgrade\n```\n\n## little peek\n\n![Screen Recording 2023-04-27 at 3 22 44 PM](https://user-images.githubusercontent.com/86029286/234861120-f2dcc22d-169c-4161-8e40-a20455250f99.GIF)\n\n## usage\nIts very simple, you just need to import the package and import the class `subPage`.\nThis is an example code:\n\n```python\nfrom flet_multi_page import subPage\nimport flet\n\ndef main (page:flet.Page):\n    def start_new_page (e):\n        p = subPage(controls=[flet.Text(\"Hello from the new page!!\")], page_props={\"bgcolor\":\"blue\"})\n        p.start()\n    \n    page.add(flet.ElevatedButton(\"start new page\", on_click=start_new_page))\n    page.update()\n\n\nif __name__ == \"__main__\": #? This is so important, there will be errors without it.\n    flet.app(target=main)\n```\n\nOr if you want a second `target` function for the page you can just add `target` argument like this:\n\n```python\nfrom flet_multi_page import subPage\nimport flet\nimport random\n\ndef second_target (page:flet.Page): #? This is the target function of the second page.\n    colors = [\"blue\", \"pink\", \"black\", \"red\", \"green\"]\n    page.bgcolor = random.choice(colors)\n    page.add(flet.Text(\"Hello new page!\", color=\"white\"))\n    page.update()\n\ndef main (page:flet.Page):\n    def start_new_page (e):\n        p = subPage(target=second_target) #! This is the \"subPage\" class.\n        p.start() #! This will run and start the second page.\n    \n    page.add(flet.ElevatedButton(\"start new page\", on_click=start_new_page))\n    page.update()\n\n\nif __name__ == \"__main__\": #? This is so important, there will be errors without it.\n    flet.app(target=main)\n```\n\n## `subPage` properties\n- `controls` argument (optional): You can add the controls you need directly to the new page.\n- `page_props` argument (optional): You can ad",
    "url": "https://github.com/SKbarbon/flet_multi_page",
    "last_updated": "2025-06-11T11:21:07+00:00"
  },
  {
    "full_name": "fawda123/NeuralNetTools",
    "name": "NeuralNetTools",
    "description": "R package of generic neural network tools",
    "language": "R",
    "topics": [],
    "readme": "# README\n\n[![R-CMD-check](https://github.com/fawda123/NeuralNetTools/workflows/R-CMD-check/badge.svg)](https://github.com/fawda123/NeuralNetTools/actions)\n[![pkgdown](https://github.com/fawda123/NeuralNetTools/workflows/pkgdown/badge.svg)](https://github.com/fawda123/NeuralNetTools/actions)\n[![CRAN status](https://www.r-pkg.org/badges/version/NeuralNetTools)](https://CRAN.R-project.org/package=NeuralNetTools)\n[![](http://cranlogs.r-pkg.org/badges/grand-total/NeuralNetTools)](https://cran.rstudio.com/web/packages/NeuralNetTools/index.html)\n\nThis is the development repository for the NeuralNetTools package.  Functions within this package can be used for the interpretation of neural network models created in R, including functions to plot a neural network interpretation diagram, evaluation of variable importance, and a sensitivity analysis of input variables. \n\nThe current release can be installed from CRAN:\n\n```r\n# install NeuralNetTools\ninstall.packages('NeuralNetTools')\n```\n\nThe development version of this package can be installed from [r-universe](https://fawda123.r-universe.dev):\n\n```r\n# enable repos\noptions(repos = c(\n    tbeptech = 'https://fawda123.r-universe.dev',\n    CRAN = 'https://cloud.r-project.org'))\n\n# install NeuralNetTools\ninstall.packages('NeuralNetTools')\n```\n\nSee the [website](http://fawda123.github.io/NeuralNetTools) for additional information.\n\n### Citation\n\nPlease cite this package as follows:\n\nBeck MW (2018). \"NeuralNetTools: Visualization and Analysis Tools for Neural Networks.\" _Journal of Statistical Software_, *85*(11), pp. 1-20. doi: 10.18637/jss.v085.i11 (URL: http://doi.org/10.18637/jss.v085.i11).\n\n# Issues and suggestions\n\nPlease report any issues and suggestions on the [issues link](https://github.com/fawda123/NeuralNetTools/issues) for the repository.\n\nA guide to posting issues can be found [here](.github/ISSUE_TEMPLATE.md).\n\n# Contributing\n\nPlease view our [contributing](.github/CONTRIBUTING.md) guidelines for any changes or pull req",
    "url": "https://github.com/fawda123/NeuralNetTools",
    "last_updated": "2024-11-06T09:05:07+00:00"
  },
  {
    "full_name": "justingrimmer/ModelInference",
    "name": "ModelInference",
    "description": "Slides and homework for model based inference",
    "language": "TeX",
    "topics": [],
    "readme": "# ModelInference\n",
    "url": "https://github.com/justingrimmer/ModelInference",
    "last_updated": "2020-04-12T03:48:13+00:00"
  },
  {
    "full_name": "kjordahl/SciPy-Tutorial-2015",
    "name": "SciPy-Tutorial-2015",
    "description": " Geospatial data tutorial",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "Geospatial Data with Open Source Tools in Python\n================================================\n\nThis tutorial will focus on open source libraries that provide a high-level, pythonic interface to geographic data and computations. Students will learn to read standard GIS file formats, perform spatial calculations, and plot results.\n\nTutorial materials\n------------------\n\n- [Presentation slides](http://kjordahl.github.io/SciPy-Tutorial-2015)\n- [Examples](examples)\n- [Exercises](exercises)\n\nInstallation\n------------\n\nThe following packages will be required for this tutorial:\n\n* [pyproj](https://pypi.python.org/pypi/pyproj)\n* [Fiona](https://pypi.python.org/pypi/Fiona)\n* [Shapely](https://pypi.python.org/pypi/Shapely)\n* [rasterio](https://pypi.python.org/pypi/rasterio)\n* [geopandas](https://github.com/geopandas/geopandas)\n\nOptional packages that may be used in demonstrations, but not required for exercises, include:\n\n* [basemap](https://pypi.python.org/pypi/basemap/1.0.2)\n* [Cartopy](http://scitools.org.uk/cartopy)\n* [geojsonio.py](https://github.com/jwass/geojsonio.py)\n\nThese packages have a number of prerequisites, including NumPy, pandas, matplotlib, and GDAL. I recommend starting with a standard scientific python distribution such as [Canopy](https://store.enthought.com) or [Anaconda](https://store.continuum.io/cshop/anaconda).\n\nAll packages are supported for Python 2.7 and recent versions of Python 3 (3.4 is recommended). Any of the 3 major platforms (Windows, OS X, and Linux) should work. Particularly on Windows, using precompiled packages when available will usually be much easier and less error-prone.\n\n### Installing in Canopy ###\n\n`Fiona`, `Shapely`, and `pyproj` (as well as optional packages `GDAL`, `basemap` and `cartopy`) can be installed with the Canopy package manager, or from the command line using `enpkg`. Then follow the instructions for installing `rasterio` and `geopandas` with `pip` below.\n\n### Installing with conda ###\n\nMany of the packages are av",
    "url": "https://github.com/kjordahl/SciPy-Tutorial-2015",
    "last_updated": "2025-02-14T05:21:19+00:00"
  },
  {
    "full_name": "jonathan-g/decktape",
    "name": "decktape",
    "description": "PDF exporter for HTML presentation frameworks",
    "language": "JavaScript",
    "topics": [],
    "readme": "# DeckTape\r\n\r\nDeckTape is a high-quality PDF exporter for HTML5 presentation frameworks. It supports all the features that you would expect from a PDF exporter like font embedding, selectable text, hyperlinks, SVG graphics objects, file compression.\r\n\r\nDeckTape is built on top of [PhantomJS](http://phantomjs.org) which relies on [Qt WebKit](https://wiki.qt.io/Qt_WebKit) for laying out and rendering Web pages and provides a headless WebKit scriptable with a JavaScript API.\r\n\r\nDeckTape currently supports the [CSSS](http://leaverou.github.io/csss/), [deck.js](http://imakewebthings.com/deck.js/), [DZSlides](http://paulrouget.com/dzslides/), [flowtime.js](http://flowtime-js.marcolago.com), [HTML Slidy](http://www.w3.org/Talks/Tools/), [impress.js](http://impress.github.io/impress.js), [remark.js](http://remarkjs.com), [reveal.js](http://lab.hakim.se/reveal-js) and [Shower](http://shwr.me/) presentation frameworks out-of-the-box. Besides, DeckTape provides a [generic command](#generic) that emulates the end-user interaction and that can be used to convert presentations from virtually any kind of frameworks. That is particularly useful to support HTML presentation frameworks that don't expose any API nor accessible state, like [Bespoke.js](https://github.com/markdalgleish/bespoke.js) for example.\r\n\r\nDeckTape's plugin-based architecture exposes an extension API so that it is possible to add support for other frameworks or tailored existing plugins to your specific needs.\r\n\r\nDeckTape can optionally be used to capture screenshots of your slide decks in various resolutions, similarly to [pageres](https://github.com/sindresorhus/pageres). That can be useful to make sure your presentations are responsive or to create handouts for them.\r\n\r\nYou can browse some slide deck [examples](#examples) below that have been exported with DeckTape.\r\n\r\n## Install\r\n\r\n1. Shallow clone DeckTape Git repository:\r\n\r\n        git clone --depth 1 https://github.com/astefanutti/decktape.git\r\n\r\n2. Change",
    "url": "https://github.com/jonathan-g/decktape",
    "last_updated": "2015-09-11T21:27:21+00:00"
  },
  {
    "full_name": "eldraco/domain_analyzer",
    "name": "domain_analyzer",
    "description": "Analyze the security of any domain by finding all the information possible. Made in python.",
    "language": "Python",
    "topics": [],
    "readme": "# Domain Analyzer v0.8.3\n\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/eldraco/domain_analyzer)\n![example workflow](https://github.com/eldraco/domain_analyzer/actions/workflows/docker-image.yml/badge.svg)\n![example workflow](https://github.com/eldraco/domain_analyzer/actions/workflows/codeql-analysis.yml/badge.svg)\n![Docker Pulls](https://img.shields.io/docker/pulls/verovaleros/domain_analyzer?color=green)\n[![Branch Maintenance](https://img.shields.io/badge/Branch%20Maintained%3F-sometimes-orange.svg)](https://bitbucket.org/lbesson/ansi-colors)\n\n## What\nDomain analyzer is a security analysis tool that automatically discovers and reports information about a given domain. Its main purpose is to analyze domains in an unattended way.\nIt has many crazy features, such as getting more domains from DNS zones, automatic nmap, webcrawler, and world domination mode. Check the features.\n\nIf you want nmap to scan more ports and run scripts and to run the crawler on those websites, you need to be root.\n\n## Example default options\n\n![domainanalyzer-gif-demo](https://user-images.githubusercontent.com/2458879/152254361-a923d460-660a-4695-9453-9e7d8142b109.gif)\n*See in asciinema at https://asciinema.org/a/466274*\n\n## How\nDomain analyzer takes a domain name and finds information about it, such as DNS servers, mail servers, IP addresses, mails on Google, SPF information, etc. After all the information is stored and organized it scans the ports of every IP found using nmap and perform several other security checks. After the ports are found, it uses the tool crawler.py from @verovaleros, to spider the complete web page of all the web ports found. This tool has the option to download files and find open folders.\n\nThe main features are:\n\n- It creates a directory with all the information, including nmap output files.\n- It uses colors to remark important information on the console. \n- It detects some security problems like host name problems, unusual port numbers",
    "url": "https://github.com/eldraco/domain_analyzer",
    "last_updated": "2025-08-12T12:30:32+00:00"
  },
  {
    "full_name": "mimno/info3300-spr2018",
    "name": "info3300-spr2018",
    "description": "Course materials for Data-Driven Web Applications",
    "language": "HTML",
    "topics": [],
    "readme": "## Data-Driven Web Applications\n\nNotes and pre-class work for INFO/CS 3300 and INFO 5100\n\nAccess the website for this repo at [this URL](https://mimno.github.io/info3300-spr2018/).\n",
    "url": "https://github.com/mimno/info3300-spr2018",
    "last_updated": "2025-03-28T01:42:53+00:00"
  },
  {
    "full_name": "dirkschumacher/r-orms",
    "name": "r-orms",
    "description": "R for Operations Research",
    "language": "HTML",
    "topics": [
      "operations-research",
      "r"
    ],
    "readme": "# r-orms.org\n\nSourcecode for [r-orms.org](https://www.r-orms.org). \n\nr-orms provides free ressources around R and [Operations Research](https://en.wikipedia.org/wiki/Operations_research). It is work in progress and a hobby project.\n\n## License\n\nCode: MIT\n\nText: Creative Commons Attribution-NonCommercial 4.0 International\n\nCopyright (c) 2018 Dirk Schumacher\n\n## Hugo\n\nThe site is generated with `blogdown` and `hugo`. It is based on the [Hugo theme](https://github.com/matcornic/hugo-theme-learn) by Grav, Mathieu Cornic and Valere Jeantet. The original theme is distributed under the MIT license.\n",
    "url": "https://github.com/dirkschumacher/r-orms",
    "last_updated": "2024-04-25T22:39:13+00:00"
  },
  {
    "full_name": "bluesky-api/python-client",
    "name": "python-client",
    "description": "Python client for blueskyapi.io",
    "language": "Python",
    "topics": [],
    "readme": "# python-client\n\n[![Coverage Status](https://coveralls.io/repos/github/bluesky-api/python-client/badge.svg?branch=main)](https://coveralls.io/github/bluesky-api/python-client?branch=main)\n[![Stable Version](https://img.shields.io/pypi/v/blueskyapi?label=latest)](https://pypi.org/project/blueskyapi/)\n\nPython client for [blueskyapi.io](https://blueskyapi.io).\n\n## Installation\n\n```bash\npip install blueskyapi\n```\n\nThe package is also available on conda-forge:\n\n```bash\nconda config --add channels conda-forge\nconda install blueskyapi\n```\n\n## Documentation\n\nYou can find the full documentation [here](https://blueskyapi.readthedocs.io/en/stable/).\n\nFor available variables see the [blueskyapi.io data documentation](https://blueskyapi.io/docs/data).\n",
    "url": "https://github.com/bluesky-api/python-client",
    "last_updated": "2025-04-24T22:06:18+00:00"
  },
  {
    "full_name": "Netflix/SimianArmy",
    "name": "SimianArmy",
    "description": "Tools for keeping your cloud operating in top form. Chaos Monkey is a resiliency tool that helps applications tolerate random instance failures.",
    "language": "Java",
    "topics": [],
    "readme": "[![NetflixOSS Lifecycle](https://img.shields.io/osslifecycle/Netflix/SimianArmy.svg)](OSSMETADATA)\n[![Build Status](https://travis-ci.org/Netflix/SimianArmy.svg?branch=master)](https://travis-ci.org/Netflix/SimianArmy)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n## PROJECT STATUS: RETIRED\n\n**The Simian Army project is no longer actively maintained**. Some of the Simian\nArmy functionality has been moved to other Netflix projects:\n\n* A [newer version of Chaos Monkey](https://github.com/netflix/chaosmonkey) is available as a standalone service.\n* [Swabbie] is a new standalone service that will replace the functionality provided by Janitor Monkey.\n* Conformity Monkey functionality will be rolled into other [Spinnaker] backend services.\n\n\n[Swabbie]: https://github.com/spinnaker/swabbie\n[Spinnaker]: https://www.spinnaker.io/\n\n### DESCRIPTION\n\nThe Simian Army is a suite of tools for keeping your cloud operating in top\nform.  Chaos Monkey, the first member, is a resiliency tool that helps ensure\nthat your applications can tolerate random instance failures\n\n\n### DETAILS\n\nPlease see the [wiki](https://github.com/Netflix/SimianArmy/wiki).\n\n### SUPPORT\n\n[Simian Army Google group](http://groups.google.com/group/simianarmy-users)\n\nBecause the project is no longer maintained, there is a good chance that nobody will be able to answer a support question.\n\n### LICENSE\n\nCopyright 2012-2016 Netflix, Inc.\n\nLicensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in\ncompliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is\ndistributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied. See the License for the specific language governing permissions and limitations under the\nLicense",
    "url": "https://github.com/Netflix/SimianArmy",
    "last_updated": "2025-08-31T10:35:26+00:00"
  },
  {
    "full_name": "scottpham/twitterBot",
    "name": "twitterBot",
    "description": "Twitter Bots!",
    "language": "JavaScript",
    "topics": [],
    "readme": "BotMaker\n==========\n\nIn March, a medium-sized earthquake hit the Los Angeles area.  It's not particularly uncommon, but there had been an earthquake-drought of sorts, so it warranted a write-up. The very first publication to break that story was the Los Angeles Times, [thanks to a news-writing bot written bot](http://www.slate.com/blogs/future_tense/2014/03/17/quakebot_los_angeles_times_robot_journalist_writes_article_on_la_earthquake.html) written by Ken Schwencke, a journalist and programmer for the paper.\n\nWe're guessing that in the near future, a lot more stories will be broken by bots. It's likely that the writers of those bots won't be journalists and they won't work for traditional news publications.  But it doesn't have to be that way.\n\nAnxiety\n========\nThere's a lot of anxiety about bots in journalism. Journalists worry that bots might take their jobs one day. They think bots are antithetical to responsible journalism. But we think bots should be about fun and exploration. Maybe someday the best bots will free the rest of us to focus on the stories that matter — the stories that use our full human faculties and require insight and judgment. \n\nBots in Seconds\n===============\nThe goal of BotMaker is to make writing and deploying Twitter bots dead simple. Its target audience is journalists who might be interested in automation, but don't have the time or skills to build one from scratch.\n\nIf you can use a Google spreadsheet, you can use BotMaker and roll your own Twitter bot.\n\nUse Cases\n============\nThe public service use cases are easy to imagine: bots that tweet out crimes, environmental data, earthquakes.  Some of these exist right now.  But the very best use cases are the ones we or you haven't even thought of yet! If making a bot is easy and quick, then ideas can come to you on the fly. Do you have unused data sets waiting for a story? Maybe a bot can get that out.  Are you scraping a site to gather statistics? Make use of those individual data points and",
    "url": "https://github.com/scottpham/twitterBot",
    "last_updated": "2017-07-21T07:22:34+00:00"
  },
  {
    "full_name": "pablobarbera/big-data-upf",
    "name": "big-data-upf",
    "description": "RECSM-UPF Summer School: Social Media and Big Data Research",
    "language": "HTML",
    "topics": [
      "social-media",
      "big-data",
      "twitter",
      "facebook",
      "text-analysis",
      "social-network-analysis",
      "rstudio",
      "scraping-websites"
    ],
    "readme": "# Summer School: Social Media and Big Data Research\n\n## Sponsored by \n* [Barcelona Summer School in Survey Methodology](https://eventum.upf.edu/event_detail/7273/detail/barcelona-summer-school-in-survey-methodology-2017.html)\n\n## June 26 - June 29, 2017\n \n## Instructor\n\n* [Pablo Barber&aacute;](http://pablobarbera.com/)\n\n(with some content based on materials prepared by [Dan Cervone](http://dcervone.com/), [Alex Hanna](http://alex-hanna.com), [Ken Benoit](http://www.kenbenoit.net/), [Paul Nulty](https://github.com/pnulty), [Kevin Munger](https://github.com/kmunger), and [Justin Grimmer](http://www.justingrimmer.org/).)\n\n## Description\n\nCitizens across the globe spend an increasing proportion of their daily lives online. Their activities leave behind granular, time-stamped footprints of human behavior and personal interactions that represent a new and exciting source of data to study standing questions about political and social behavior. At the same time, the volume and heterogeneity of web data present unprecedented methodological challenges. The goal of this course is to introduce participants to new computational methods and tools required to explore and analyze Big Data from online sources using the R programming language. We will focus in particular on data collected from social networking sites, such as Facebook and Twitter, whose use is becoming widespread in the social sciences.\n\n## Setup and Preparation\n\nThere are two ways you can follow the course and run the code contained in this GitHub repository. The recommended method is to connect to the provided RStudio server where all the R packages have already been installed, and all the R code is available. To access the server, visit bigdata.pablobarbera.com and log in with the information provided during class.\n\nAlternatively, you can run the code in your own laptop. You will need [R](https://cran.r-project.org/) and [RStudio](https://www.rstudio.com/) installed. Given the number of attendants, I will not be ",
    "url": "https://github.com/pablobarbera/big-data-upf",
    "last_updated": "2022-12-20T16:49:14+00:00"
  },
  {
    "full_name": "yixuan/tinydnn",
    "name": "tinydnn",
    "description": "Tiny yet Powerful Deep Neural Networks",
    "language": "C++",
    "topics": [
      "neural-network"
    ],
    "readme": "# tinydnn\n\n## Introduction\n\n**tinydnn** is an (experimental) R wrapper of the\n[tiny-dnn](https://github.com/tiny-dnn/tiny-dnn) library for implementing\nDeep Neural Networks (DNN). The largest advantage of `tiny-dnn` over other deep\nlearning frameworks is its minimal dependency on external software and the\nease of installation. As a result, the R package **tinydnn** is also very\nconvenient to install as long as you have a C++ 11 compiler, and it runs on\nall major platforms including Linux, Mac, Windows etc.\n\n**tinydnn** may be a good option for building DNN models if:\n\n- You use R! (You may want to consider [MXNet](https://github.com/dmlc/mxnet) first)\n- You have a CPU-only environment with limited resources\n- You want to quickly try DNN models without spending too much time on\ninstallation and configuration\n- You need different packages to compare the results\n- You want to learn the internals of DNN (The included `tiny-dnn` library\nprovides an excellent coding example of DNN)\n\n## Development Status\n\n**tinydnn** is still in the experiment stage. Functions and interface may change,\nand more features will be added per request. Feedbacks and contributions are\nhighly welcome.\n\n## Examples\n\nThis package has not been fully documented. The examples below are mostly\nself-explanatory.\n\n### Regression\n\nWe use the [wine quality data](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\non UCI machine learning repository to demonstrate a regression example, in which\nwe use several attributes of the wine to predict its quality.\n\n```r\n## Wine quality data set\n## https://archive.ics.uci.edu/ml/datasets/Wine+Quality\ndat_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\ndat = read.csv(url(dat_url), sep = \";\")\nn = nrow(dat)\nx = scale(as.matrix(dat[, -ncol(dat)]))\ny = dat[, ncol(dat)]\n\n## Splitting training and testing data\nset.seed(123)\nind = sample(1:n, floor(0.8 * n))\ntrain_x = x[ind, ]\ntrain_y = y[ind]\ntest_x = x[-ind, ]\ntest_y = y[-",
    "url": "https://github.com/yixuan/tinydnn",
    "last_updated": "2025-03-22T11:18:33+00:00"
  },
  {
    "full_name": "rstudio/gt",
    "name": "gt",
    "description": "Easily generate information-rich, publication-quality tables from R",
    "language": "R",
    "topics": [
      "r",
      "summary-tables",
      "easy-to-use",
      "docx",
      "html",
      "latex",
      "rtf"
    ],
    "readme": "<div align=\"center\">\n\n<a href=\"https://gt.rstudio.com/\"><img src=\"man/figures/logo.svg\" width=\"350px\"/></a>\n\n<!-- badges: start -->\n[![CRAN status](https://www.r-pkg.org/badges/version/gt)](https://CRAN.R-project.org/package=gt)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![R-CMD-check](https://github.com/rstudio/gt/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/gt/actions/workflows/R-CMD-check.yaml)\n[![Codecov test coverage](https://codecov.io/gh/rstudio/gt/graph/badge.svg)](https://app.codecov.io/gh/rstudio/gt)\n\n[![Best Practices](https://bestpractices.coreinfrastructure.org/projects/5593/badge)](https://bestpractices.coreinfrastructure.org/projects/5593)\n[![The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Monthly Downloads](https://cranlogs.r-pkg.org/badges/gt)](https://CRAN.R-project.org/package=gt)\n[![Total Downloads](https://cranlogs.r-pkg.org/badges/grand-total/gt)](https://CRAN.R-project.org/package=gt)\n\n[![Follow on Bluesky](https://img.shields.io/badge/Bluesky-0285FF?logo=bluesky&logoColor=0285FF&label=Follow%20on&color=0285FF)](https://bsky.app/profile/gt-package.bsky.social)\n[![Posit Cloud](https://img.shields.io/badge/Posit%20Cloud-gt%20Test%20Drive-blue?style=social&logo=rstudio&logoColor=75AADB)](https://rstudio.cloud/project/779965)\n\n[![Discord](https://img.shields.io/discord/1086103944280952992?color=%237289da&label=Discord)](https://discord.com/invite/Ux7nrcXHVV)\n\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://www.contributor-covenant.org/version/2/1/code_of_conduct.html)\n<!-- badges: end -->\n<hr style=\"color:transparent\" />\n<br />\n</div>\n\nWith the **gt** package, anyone can make wonderful-looking tables using\nthe **R** programming language. The **gt** philosophy: we can",
    "url": "https://github.com/rstudio/gt",
    "last_updated": "2025-08-27T12:16:07+00:00"
  },
  {
    "full_name": "sckott/textminer",
    "name": "textminer",
    "description": "text mine via Crossref's TDM",
    "language": "Ruby",
    "topics": [
      "crossref",
      "text-mining",
      "literature"
    ],
    "readme": "textminer\n=========\n\n[![gem version](https://img.shields.io/gem/v/textminer.svg)](https://rubygems.org/gems/textminer)\n[![Build Status](https://travis-ci.org/sckott/textminer.svg?branch=master)](https://travis-ci.org/sckott/textminer)\n[![codecov.io](http://codecov.io/github/sckott/textminer/coverage.svg?branch=master)](http://codecov.io/github/sckott/textminer?branch=master)\n\n__`textminer` helps you text mine through Crossref's TDM (Text & Data Mining) services:__\n\n## Changes\n\nFor changes see the [CHANGELOG][changelog]\n\n## gem API\n\n* `Textiner.search` - search by DOI, query string, filters, etc. to get Crossref metadata, which you can use downstream to get full text links. This method essentially wraps `Serrano.works()`, but only a subset of params - this interface may change depending on feedback.\n* `Textiner.fetch` - Fetch full text given a url, supports Crossref's Text and Data Mining service\n* `Textiner.extract` - Extract text from a pdf\n\n## Install\n\n### Release version\n\n```\ngem install textminer\n```\n\n### Development version\n\n```\ngit clone git@github.com:sckott/textminer.git\ncd textminer\nrake install\n```\n\n## Examples\n\n### Within Ruby\n\n#### Search\n\nSearch by DOI\n\n```ruby\nrequire 'textminer'\n# link to full text available\nTextminer.search(doi: '10.7554/elife.06430')\n# no link to full text available\nTextminer.search(doi: \"10.1371/journal.pone.0000308\")\n```\n\nMany DOIs at once\n\n```ruby\nrequire 'serrano'\ndois = Serrano.random_dois(sample: 6)\nTextminer.search(doi: dois)\n```\n\nSearch with filters\n\n```ruby\nTextminer.search(filter: {has_full_text: true})\n```\n\n#### Get full text links\n\nThe object returned form `Textminer.search` is a class, which has methods for pulling out all links, xml only, pdf only, or plain text only\n\n```ruby\nx = Textminer.search(filter: {has_full_text: true})\nx.links_xml\nx.links_pdf\nx.links_plain\n```\n\n#### Fetch full text\n\n`Textminer.fetch()` gets full text based on URL input. We determine how to pull down and parse the content based on content type.\n",
    "url": "https://github.com/sckott/textminer",
    "last_updated": "2017-08-17T16:46:35+00:00"
  },
  {
    "full_name": "hughjonesd/huxtable",
    "name": "huxtable",
    "description": "An R package to create styled tables in multiple output formats, with a friendly, modern interface.",
    "language": "R",
    "topics": [
      "r",
      "html",
      "latex",
      "tables",
      "reproducible-research",
      "huxtable",
      "cran",
      "microsoft-word",
      "powerpoint"
    ],
    "readme": "\n<style>\n.huxtable {\n  border-collapse: collapse;\n  border: 0px;\n  margin-bottom: 2em;\n  margin-top: 2em;\n}\n.huxtable-cell {\n  vertical-align: top;\n  text-align: left;\n  white-space: normal;\n  border-style: solid;\n  border-width: 0pt;\n  padding: 6pt;\n  font-weight: normal;\n}\n.huxtable-header {\n  font-weight: bold;\n}\n</style>\n\n<table class=\"huxtable\" data-quarto-disable-processing=\"true\"  style=\"margin-left: auto; margin-right: auto;\">\n\n<col>\n\n<col>\n\n<col>\n\n<col>\n\n<col>\n\n<col>\n\n<tbody>\n\n<tr>\n\n<td class=\"huxtable-cell\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 1.2pt 1.2pt 1.2pt 1.2pt; border-top-color: rgb(0, 0, 0);  border-right-color: rgb(0, 0, 0);  border-bottom-color: rgb(0, 0, 0);  border-left-color: rgb(0, 0, 0); padding: 0pt 0pt 0pt 0pt;    font-family: DejaVu Sans;\">\n\n</td>\n\n<td class=\"huxtable-cell\" rowspan=\"2\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 1.2pt 1.2pt 1.2pt 1.2pt; border-top-color: rgb(0, 0, 0);  border-right-color: rgb(0, 0, 0);  border-bottom-color: rgb(0, 0, 0);  border-left-color: rgb(0, 0, 0); padding: 0pt 0pt 0pt 0pt;    font-family: DejaVu Sans;\">\n\n</td>\n\n<td class=\"huxtable-cell\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 1.2pt 1.2pt 1.2pt 1.2pt; border-top-color: rgb(0, 0, 0);  border-right-color: rgb(0, 0, 0);  border-bottom-color: rgb(0, 0, 0);  border-left-color: rgb(0, 0, 0); padding: 0pt 0pt 0pt 0pt;  font-weight: bold;  font-family: DejaVu Sans;\">\n\nh\n</td>\n\n<td class=\"huxtable-cell\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 1.2pt 1.2pt 1.2pt 1.2pt; border-top-color: rgb(0, 0, 0);  border-right-color: rgb(0, 0, 0);  border-bottom-color: rgb(0, 0, 0);  border-left-color: rgb(0, 0, 0); padding: 0pt 0pt 0pt 0pt;    font-family: DejaVu Sans;\">\n\nu\n</td>\n\n<td class=\"huxtable-cell\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 1.2pt 1.2pt 1.2pt 1.2pt; b",
    "url": "https://github.com/hughjonesd/huxtable",
    "last_updated": "2025-08-23T07:39:27+00:00"
  },
  {
    "full_name": "soodoku/on-writing",
    "name": "on-writing",
    "description": "Writing Tips, Tricks, and Tools",
    "language": "HTML",
    "topics": [
      "advice",
      "writers",
      "authors",
      "writing",
      "presenting",
      "presentation"
    ],
    "readme": "## Writing Well\n\nWriting is not transcribing. It is creating new ideas and finding new ways of expressing old ideas. Thus, writing encompasses both thinking about what to write and how to write it.\n\nIt also holds then that if we don't write, we don't create. If we don't write, some connections will not be made, and some innovations in phrasing will remain unthought. The act of writing, much like the act of talking, causes us to create.\n\nSomewhat frighteningly, how we create remains beyond us. Take a second to think if you are conscious of where the words come from. You are in for an unsettling discovery.\n\nWe may not know where the words come from, but we do know how to turn the tap on. Reading, thinking, and talking about something generally make writing about that thing easier. So, read deeply, think hard, and talk extensively about what you want to write about.\n\nEven when you do all this, the first draft is liable to be rife with errors. For it seems, often enough, we can't fully appreciate the stupidity of an argument until we have put it down on paper. And even then, recognizing errors in arguments and style generally requires a 'fresh pair of eyes,' ours or someone else's. So, take time between revisions. And ask others for help.\n\nLastly, a large chunk of a writer's output is never read. But it doesn't mean the work is wasted. Think of it as practice (like musicians) or do it because it is pleasant.\n\n### Resources\n\n#### Succinct Advice on How to Write Better\n\n* [Robert's Rules: Suggestions for Writing](lit/Roberts-Rules-January-2013.pdf) by Robert Luskin.</li>\n\n* [QJPS Style Guidelines](lit/QJPS-Style-Guidelines.pdf)</a></li>\n\n* [John Cochrane's Writing Tips for Ph.D. Students](lit/phd_paper_writing.pdf)\n\n* [Stephen Pinker's 13 Rules](https://twitter.com/sapinker/status/1084490338629242880)\n    \n    * Reverse-engineer what you read. If it feels like good writing, what makes it good? If it’s awful, why?\n    \n    * Prose is a window onto the world. Let your reade",
    "url": "https://github.com/soodoku/on-writing",
    "last_updated": "2025-05-30T16:32:09+00:00"
  },
  {
    "full_name": "WizardMac/librdata",
    "name": "librdata",
    "description": "Read and write R data frames from C",
    "language": "C",
    "topics": [
      "r",
      "rdata",
      "data-frames"
    ],
    "readme": "# librdata - Read and write R data frames from C\n[![Build Status](https://github.com/WizardMac/librdata/workflows/build/badge.svg)](https://github.com/WizardMac/librdata/actions)\n[![Build status](https://ci.appveyor.com/api/projects/status/xrao0cdroh5xn950?svg=true)](https://ci.appveyor.com/project/evanmiller/librdata)\n\nOriginally part of [ReadStat](https://github.com/WizardMac/ReadStat), librdata\nis a small C library for reading and writing R data frames.\n\nFeatures:\n\n* Read both RData and RDS formats\n* Read compressed files (requires bzip2, zlib, and lzma)\n* Write factors, timestamps, logical vectors, and more\n\n## Installation\n\n```\n./autogen.sh\n./configure\nmake\nmake install\n```\n\nIf you're on Mac and see errors about `AM_ICONV` when you run `./autogen.sh`,\nyou'll need to install [gettext](https://www.gnu.org/software/gettext/).\n\n## Language bindings\n\n* Python: [pyreadr](https://github.com/ofajardo/pyreadr)\n\n## Read API\n\nExample usage:\n\n```c\n#include \"rdata.h\"\n\nstatic int handle_table(const char *name, void *ctx) {\n    printf(\"Read table: %s\\n\", name);\n\n    return 0; /* non-zero to abort processing */\n}\n\n// Called once for all columns with the following caveats:\n// * `name` is NULL for some columns (see handle_column_name below)\n// * `data` is NULL for text columns (see handle_text_value below)\nstatic int handle_column(const char *name, rdata_type_t type,\n                         void *data, long count, void *ctx) {\n    /* Do something... */\n    return 0;\n}\n\n// Some column names appear in the file after the data\nstatic int handle_column_name(const char *name, int index, void *ctx) {\n    if (debug) printf(\"Read column name: %s\\n\", name);\n    /* Do something... */\n    return 0;\n}\n\n// Called once per row for a text column\nstatic int handle_text_value(const char *value, int index, void *ctx) {\n    /* Do something... */\n    return 0;\n}\n\n// Called for factor variables, once for each level\nstatic int handle_value_label(const char *value, int index, void *ctx) {\n    /* Do so",
    "url": "https://github.com/WizardMac/librdata",
    "last_updated": "2025-08-16T09:00:32+00:00"
  },
  {
    "full_name": "fcambus/ansiweather",
    "name": "ansiweather",
    "description": "Weather in terminal, with ANSI colors and Unicode symbols",
    "language": "Shell",
    "topics": [
      "weather",
      "terminal",
      "ansi",
      "unicode"
    ],
    "readme": "## Description\n\nAnsiWeather is a Shell script for displaying the current weather conditions\nin your terminal, with support for ANSI colors and Unicode symbols.\n\n![AnsiWeather Screenshot][1]\n\nWeather data comes from the `OpenWeatherMap` free weather API.\n\n\n\n## Requirements\n\nAnsiWeather requires the following dependencies:\n\n- A command to fetch HTTP data such as FTP, cURL or wget\n- [jq][2] (lightweight and flexible command-line JSON processor)\n- [bc][3] (arbitrary precision numeric processing language), for doing float\n  arithmetic\n\n\n\n## Installation\n\nAfter cloning the repository, simply invoke the script by typing:\n\n\t./ansiweather\n\nAnsiWeather packages are available for:\n\n- [OpenBSD][4]\n- [NetBSD][5]\n- [FreeBSD][6]\n- [Debian][7]\n- [Ubuntu][8]\n- [Homebrew][9]\n- [Haiku][10]\n- [Gentoo][11]\n- [Alpine Linux][12]\n\n\n\n## Usage\n\n### Synopsis\n\n\tansiweather [-l location] [-u system] [-f days] [-F] [-a value]\n\t            [-s value] [-k key] [-i value] [-w value] [-h value]\n\t            [-H value] [-p value] [-d value] [-v]\n\n### Options\n\n\t-l location\n\t        Specify location.\n\t\n\t-u system\n\t        Specify unit system to use ( metric or imperial ).\n\t\n\t-f days\n\t        Toggle forecast mode for the specified number of upcoming days.\n\t\n\t-F      Toggle forecast mode for the next five days.\n\t\n\t-a value\n\t        Toggle ANSI colors display ( true or false ).\n\t\n\t-s value\n\t        Toggle symbols display ( true or false ).\n\t\n\t-k key  Specify OpenWeatherMap API key.\n\t\n\t-i value\n\t        Toggle UV Index display ( true or false ).\n\t\n\t-w value\n\t        Toggle wind data display ( true or false ).\n\t\n\t-h value\n\t        Toggle humidity data display ( true or false ).\n\t\n\t-H value\n\t        Toggle Feels like display ( true or false ).\n\t\n\t-p value\n\t        Toggle pressure data display ( true or false ).\n\t\n\t-d value\n\t        Toggle daylight data display ( true or false ).\n\t\n\t-v      Display version.\n\n### Examples\n\nDisplay forecast using metric units for the next five days (showing symbols\nand daylight",
    "url": "https://github.com/fcambus/ansiweather",
    "last_updated": "2025-08-31T03:01:52+00:00"
  },
  {
    "full_name": "piskvorky/gensim-data",
    "name": "gensim-data",
    "description": "Data repository for pretrained NLP models and NLP corpora.",
    "language": "Python",
    "topics": [
      "dataset",
      "gensim",
      "word2vec-model",
      "corpora",
      "pretrained-models",
      "lsi-model",
      "glove-model",
      "lda-model"
    ],
    "readme": "# What is Gensim-data for?\n\nResearch datasets regularly disappear, change over time, become obsolete or come without a sane implementation to handle the data format reading and processing.\n\nFor this reason, [Gensim](https://github.com/RaRe-Technologies/gensim) launched its own dataset storage, committed to long-term support, a sane standardized usage API and focused on datasets for **unstructured text processing** (no images or audio). This [Gensim-data](https://github.com/RaRe-Technologies/gensim-data) repository serves as that storage.\n\n**There's no need for you to use this repository directly**. Instead, simply install Gensim and use its download API (see the Quickstart below). It will \"talk\" to this repository automagically.\n\n💡 When you use the Gensim download API, all data is stored in your `~/gensim-data` home folder.\n\nRead more about the project rationale and design decisions in this article: [New Download API for Pretrained NLP Models and Datasets](https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/).\n\n# How does it work?\n\nTechnically, the actual (sometimes large) corpora and model files are being stored as [release attachments](https://github.com/RaRe-Technologies/gensim-data/releases) here on Github. Each dataset (and each new version of each dataset) gets its own release, forever immutable.\n\nEach release is accompanied by a usage example and release notes, for example: [Corpus of USPTO Patents from 2017](https://github.com/RaRe-Technologies/gensim-data/releases/tag/patent-2017); [English Wikipedia from 2017 with plaintext section](https://github.com/RaRe-Technologies/gensim-data/releases/tag/wiki-english-20171001).\n\n🔴 **Each dataset comes with its own license, which the users should study carefully before using the dataset!**\n\n----\n\n## Quickstart\n\nTo load a model or corpus, use either the Python or command line interface of [Gensim](https://github.com/RaRe-Technologies/gensim) (you'll need Gensim installed firs",
    "url": "https://github.com/piskvorky/gensim-data",
    "last_updated": "2025-08-20T09:43:01+00:00"
  },
  {
    "full_name": "dgrtwo/splittestr",
    "name": "splittestr",
    "description": "Functions for Bayesian A/B Testing Post",
    "language": "R",
    "topics": [],
    "readme": "Bayesian and frequentist A/B split testing\n--------------------\n\nFunctions for Bayesian and frequentist A/B split testing.\n\nThe main purpose of this package is to provide functions and support for [this blog post about Bayesian A/B testing](http://varianceexplained.org/r/bayesian-ab-testing/). The functions thus focus on particular kinds of simulation and approximations useful in the post. It is not intended or documented to be used by others, though it certainly can be.\n",
    "url": "https://github.com/dgrtwo/splittestr",
    "last_updated": "2024-01-27T01:45:18+00:00"
  },
  {
    "full_name": "Andrew-Dickinson/bird-ospf-link-db-parser",
    "name": "bird-ospf-link-db-parser",
    "description": "Parse the text output from the BIRD Routing Daemon's OSPF link database into machine readable JSON",
    "language": "Python",
    "topics": [],
    "readme": "\n# Bird OSFP Link Database Parser\n\nParses the output of the BIRD Routing Daemon's `birdc show ospf state` command into a machine-readable JSON string.\n\n```sh\n> birdc show ospf state | parse-bird-link-db - | jq | less\n{\n  \"areas\": {\n    \"0.0.0.0\": {\n      \"routers\": {\n        \"10.68.29.50\": {\n          \"links\": {\n            \"router\": [\n              {\n                \"id\": \"10.69.7.31\",\n                \"metric\": 10\n              }\n            ],\n            \"stubnet\": [\n              {\n                \"id\": \"10.69.29.50/32\",\n                \"metric\": 0\n              },\n              {\n                \"id\": \"10.68.29.50/32\",\n                \"metric\": 0\n              }\n            ],\n            \"external\": [\n              {\n                \"id\": \"10.70.174.0/24\",\n                \"metric\": 20\n              }\n            ]\n          }\n        },\n        \"10.68.73.125\": {\n          \"links\": {\n            \"router\": [\n              {\n                \"id\": \"10.69.73.25\",\n                \"metric\": 10\n              },\n              {\n                \"id\": \"10.69.52.83\",\n                \"metric\": 30\n              },\n              {\n                \"id\": \"10.69.73.25\",\n                \"metric\": 30\n              }\n            ],\n            \"stubnet\": [\n              {\n                \"id\": \"10.69.73.125/32\",\n                \"metric\": 0\n              },\n              {\n                \"id\": \"10.68.73.125/32\",\n                \"metric\": 0\n              }\n            ]\n          }\n        },\n        ...\n      }\n    }\n  }\n}\n```\n\n## Output Format\n\nThe output format is detailed using [JSON Schema](https://json-schema.org/) in `src/bird_parser/output_schema.json`\n\n## Usage\n\nPre-requisites: `python3` available via the shell\n\nFirst, install the CLI via pip:\n```shell\npip install bird-ospf-link-db-parser\n```\n\nthen invoke the tool with the CLI command:\n```shell\nbirdc show ospf state | parse-bird-link-db -\n```\n\nBut you probably want to use `jq` and `less` to make this output a bit more mana",
    "url": "https://github.com/Andrew-Dickinson/bird-ospf-link-db-parser",
    "last_updated": "2025-01-15T15:30:24+00:00"
  },
  {
    "full_name": "yalattas/mindmate",
    "name": "mindmate",
    "description": "MindMate is a command-line tool that leverages the power of AI platforms to offer different use-cases to developers",
    "language": "Python",
    "topics": [
      "ai",
      "cli",
      "ml",
      "nlp",
      "productivity"
    ],
    "readme": "# install package\n\n## virtual environment\n```\npip install mindmate\n```\n> It's not recommended to install in virtual environment _(except for  testing)_ try it with default `pip`\n\n## operating system level\n```\nsudo apt update\nsudo apt install -y python3-pip\nexport PATH=\"$PATH:/home/$USER/.local/bin\"\npip install mindmate\n```\n# usage\n```\n$ mindmate [ARGUMENT] [OPTIONS] [OPTIONS] [OPTIONS] --help\n```\n\n## examples\n```\n$ mindmate configure\n$ mindmate directory prompting list\n\n$ mindmate chat --platform openai \\\n  --model text-davinci-003 \\\n  --stream true \\\n  --max-tokens 500 \\\n  --prompt \"Act as a professional developer, provide best file structure for fastAPI framework\"\n\n$ mindmate image create -p \"mindmate written on the background in a garden and friends playing around\"\n```\n\n# compatibility\n\n__Not tested__ yet, but should be compatible with any Python >= 3.8\n",
    "url": "https://github.com/yalattas/mindmate",
    "last_updated": "2025-01-15T15:30:21+00:00"
  },
  {
    "full_name": "leonawicz/rtrek",
    "name": "rtrek",
    "description": "R package for Star Trek datasets and related R functions.",
    "language": "R",
    "topics": [
      "r-package",
      "star-trek",
      "stapi",
      "data"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# rtrek <img src=\"man/figures/logo.png\" style=\"margin-left:10px;margin-bottom:5px;\" width=\"120\" align=\"right\">\n\n<!-- badges: start -->\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)\n[![R-CMD-check](https://github.com/leonawicz/rtrek/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/leonawicz/rtrek/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/leonawicz/rtrek/graph/badge.svg)](https://app.codecov.io/gh/leonawicz/rtrek)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/rtrek)](https://CRAN.R-project.org/package=rtrek)\n[![CRAN\ndownloads](https://cranlogs.r-pkg.org/badges/grand-total/rtrek)](https://cran.r-project.org/package=rtrek)\n[![Github\nStars](https://img.shields.io/github/stars/leonawicz/rtrek.svg?style=social&label=Github)](https://github.com/leonawicz/rtrek)\n<!-- badges: end -->\n\nThe `rtrek` package provides datasets related to the Star Trek fictional\nuniverse and functions for working with those datasets. It interfaces\nwith the [Star Trek API](http://stapi.co/) (STAPI), [Memory\nAlpha](https://memory-alpha.fandom.com/wiki/Portal:Main) and [Memory\nBeta](https://memory-beta.fandom.com/wiki/Main_Page) to retrieve data,\nmetadata and other information relating to Star Trek.\n\nThe package also contains several local datasets covering a variety of\ntopics such as Star Trek timeline data, universe species data and\ngeopolitical data. Some of these are more information rich, while others\nare toy examples useful for simple demonstrations. The bulk of Star Trek\ndata is accessed from external sources by API. A future version of\n`rtrek` will also include summary datasets resulting from text mining\nanalyses of Star Trek novels.\n\n<p style=\"text-align:center;\">\n\n<img src=\"https://github.com/leonawicz/rtrek/bl",
    "url": "https://github.com/leonawicz/rtrek",
    "last_updated": "2025-06-19T21:23:53+00:00"
  },
  {
    "full_name": "ramnathv/robservable",
    "name": "robservable",
    "description": "R package that brings observables to htmlwidgets, allowing for shiny-like interactivity in the browser",
    "language": "",
    "topics": [],
    "readme": "# robservable\n\n`robservable` is an R package that brings observables to R. It provides a generic framework to create reactive widgets that can interact with each other using a shiny-like API. This will allow R users to build entire web apps with interactive widgets that can be rendered purely in a browser. \n\n![caution](https://cdn1.iconfinder.com/data/icons/Koloria-Icon-Set/30/Error.png)\n__CAUTION: This is just an experiment at this point. The API is highly likely to change. So please DONT use it yet.__\n\n[![Imgur](http://i.imgur.com/QbkhTzT.png)](https://www.youtube.com/watch?v=IEjYznTDRFo)\n\n\n## Installation\n\nYou would need the development version of `htmlwidgets` and `robservable`\n\n```r\ndevtools::install_github('ramnathv/htmlwidgets')\ndevtools::install_github('ramnathv/robservable')\n```\n\n## Example 1\n\nIn this example, `rangeInput` and `colorInput` communicate with `d3circles`. This interaction is enabled using an observable named `input` that updates on changes to keep the various elements in sync. Note that this API is heavily inspired by `shiny`, and is implemented using the awesome [mobservable](http://mweststrate.github.io/mobservable/) library.\n\n\n\n```\nlibrary(robservable)\nlibrary(magrittr)\nmyscript <- \"\n  document.body.addEventListener('allwidgetsrendered', function(){\n    console.log('All Widgets Rendered')\n  });\n\"\n\nrenderMessage = function(x){\n  onRender(x, \"function(el, x){\n    console.log(el.id + ' is rendered')\n  }\")\n}\n\n\napp1 <- App(\n  colorInput('#ff0000', elementId = 'fill', width = 60),\n  htmltools::h3(\"Reactive Widgets\"),\n  rangeInput(5, min = 2, max = 10, elementId = 'radius1', grid = TRUE),\n  d3circles(\n    radius = input('radius'),\n    fill = input('fill')\n  ),\n  rangeInput(5, min = 2, max = 10, elementId = 'radius2', grid = TRUE),\n  reactiveVar('radius', 'input.radius1() + input.radius2()')\n)\n\napp1\n```\n\n\n\n## Example 2\n\nIn this example, we introduce the idea of reactive variables, which are essentially computed from other inputs. The variable `radi",
    "url": "https://github.com/ramnathv/robservable",
    "last_updated": "2021-09-14T22:27:14+00:00"
  },
  {
    "full_name": "nimble-dev/nimble",
    "name": "nimble",
    "description": "The base NIMBLE package for R",
    "language": "C++",
    "topics": [
      "bayesian-inference",
      "mcmc",
      "hierarchical-models",
      "probabilistic-programming",
      "bayesian-methods",
      "r"
    ],
    "readme": "# NIMBLE\n[![Build Status](https://github.com/nimble-dev/nimble/actions/workflows/ci.yaml/badge.svg?branch=devel)](https://github.com/nimble-dev/nimble/actions/workflows/ci.yaml)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/nimble-dev/nimble?branch=devel&svg=true)](https://ci.appveyor.com/project/nimble-dev/nimble)\n[![CRAN](https://www.r-pkg.org/badges/version/nimble)](https://CRAN.R-project.org/package=nimble)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1211190.svg)](https://zenodo.org/record/1211190)\n[![Google Group](https://img.shields.io/badge/google-group-blue.svg)](https://groups.google.com/forum/#!forum/nimble-users)\n\n[Website](https://r-nimble.org/) |\n[Documentation](https://r-nimble.org/manuals/NimbleUserManual.pdf) |\n[Examples](https://r-nimble.org/examples) |\n[Developing](https://nimble-dev.github.io/nimble-docs) |\n[Workshop materials](https://github.com/nimble-training)\n\nNIMBLE is an R package for hierarchical statistical modeling (aka\ngraphical modeling).  It enables writing general models along with\nmethods such as Markov chain Monte Carlo (MCMC), particle filtering\n(aka sequential Monte Carlo), Laplace approximation and other general methods.\n\nFor writing statistical models, NIMBLE adopts and extends the BUGS\nlanguage, making it largely compatible with\n[BUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs) and\n[JAGS](http://mcmc-jags.sourceforge.net/).  NIMBLE makes BUGS\nextensible, allowing users to add new functions and new distributions.\n\nFor writing algorithms (aka analysis methods), NIMBLE provides a\nmodel-generic programming system embedded within R.  This provides\ncontrol over models as generic objects and mathematical manipulation\nof model variables. In this way, NIMBLE's programming paradigm treats\nprobabilistic graphical models as a basic programming construct.\n\nBoth models and algorithms are compiled via generating customized C++\nand providing seamless interfaces to compiled C++ from R.\n\nNIMBLE's most dev",
    "url": "https://github.com/nimble-dev/nimble",
    "last_updated": "2025-09-01T14:38:06+00:00"
  },
  {
    "full_name": "yixuan/spectra",
    "name": "spectra",
    "description": "A header-only C++ library for large scale eigenvalue problems",
    "language": "C++",
    "topics": [
      "eigenvalues",
      "arpack",
      "linear-algebra",
      "spectra",
      "header-only"
    ],
    "readme": "# <a href=\"https://spectralib.org\"><img src=\"https://spectralib.org/img/logo.png\" width=\"200px\" /></a>\n\n![Basic CI](https://github.com/yixuan/spectra/workflows/Basic%20CI/badge.svg) [![codecov](https://codecov.io/gh/yixuan/spectra/branch/master/graph/badge.svg)](https://codecov.io/gh/yixuan/spectra)\n\n> **NOTE**: Spectra 1.0.0 was released in 2021-07-01, with a lot of\n> API-breaking changes. Please see the [migration guide](MIGRATION.md)\n> for a smooth transition to the new version.\n\n> **NOTE**: If you are interested in the future development of Spectra, please join\n> [this thread](https://github.com/yixuan/spectra/issues/92) to share your comments and suggestions.\n\n[**Spectra**](https://spectralib.org) stands for **Sp**arse **E**igenvalue **C**omputation **T**oolkit\nas a **R**edesigned **A**RPACK. It is a C++ library for large scale eigenvalue\nproblems, built on top of [Eigen](http://eigen.tuxfamily.org),\nan open source linear algebra library.\n\n**Spectra** is implemented as a header-only C++ library, whose only dependency,\n**Eigen**, is also header-only. Hence **Spectra** can be easily embedded in\nC++ projects that require calculating eigenvalues of large matrices.\n\n## Relation to ARPACK\n\n[ARPACK](https://www.arpack.org/) is a software package written in\nFORTRAN for solving large scale eigenvalue problems. The development of\n**Spectra** is much inspired by ARPACK, and as the full name indicates,\n**Spectra** is a redesign of the ARPACK library using the C++ language.\n\nIn fact, **Spectra** is based on the algorithm described in the\n[ARPACK Users' Guide](http://li.mit.edu/Archive/Activities/Archive/CourseWork/Ju_Li/MITCourses/18.335/Doc/ARPACK/Lehoucq97.pdf),\nthe implicitly restarted Arnoldi/Lanczos method. However,\nit does not use the ARPACK code, and it is **NOT** a clone of ARPACK for C++.\nIn short, **Spectra** implements the major algorithms in ARPACK,\nbut **Spectra** provides a completely different interface, and it does not\ndepend on ARPACK.\n\n## Common Usage\n\n**S",
    "url": "https://github.com/yixuan/spectra",
    "last_updated": "2025-08-31T05:55:52+00:00"
  },
  {
    "full_name": "ipinfo/python",
    "name": "python",
    "description": "Official Python Library for IPinfo API (IP geolocation and other types of IP data)",
    "language": "Python",
    "topics": [
      "ipinfo",
      "ip-address",
      "ip-geolocation",
      "ip-data",
      "python"
    ],
    "readme": "# [<img src=\"https://ipinfo.io/static/ipinfo-small.svg\" alt=\"IPinfo\" width=\"24\"/>](https://ipinfo.io/) IPinfo Python Client Library\n\nThis is the official Python client library for the IPinfo.io IP address API, allowing you to look up your own IP address, or get any of the following details for an IP:\n\n- [IP geolocation](https://ipinfo.io/ip-geolocation-api) (city, region, country, postal code, latitude, and longitude)\n- [ASN details](https://ipinfo.io/asn-api) (ISP or network operator, associated domain name, and type, such as business, hosting, or company)\n- [Firmographics data](https://ipinfo.io/ip-company-api) (the name and domain of the business that uses the IP address)\n- [Carrier information](https://ipinfo.io/ip-carrier-api) (the name of the mobile carrier and MNC and MCC for that carrier if the IP is used exclusively for mobile traffic)\n\n## Getting Started\n\nYou'll need an IPinfo API access token, which you can get by signing up for a free account at [https://ipinfo.io/signup](https://ipinfo.io/signup).\n\nThe free plan is limited to 50,000 requests per month, and doesn't include some of the data fields such as IP type and company data. To enable all the data fields and additional request volumes see [https://ipinfo.io/pricing](https://ipinfo.io/pricing)\n\nThe library also supports the Lite API, see the [Lite API section](#lite-api) for more info.\n\n### Installation\n\nThis package works with Python 3.5 or greater. However, we only officially\nsupport non-EOL Python versions.\n\n```bash\npip install ipinfo\n```\n\n### Quick Start\n\n```python\n>>> import ipinfo\n>>> access_token = '123456789abc'\n>>> handler = ipinfo.getHandler(access_token)\n>>> ip_address = '216.239.36.21'\n>>> details = handler.getDetails(ip_address)\n>>> details.city\n'Mountain View'\n>>> details.loc\n'37.3861,-122.0840'\n```\n\n#### Async/Await\n\nAn asynchronous handler is available as well, and can be accessed and used in\nalmost the same exact way as the synchronous handler:\n\n```python\n>>> import ipinfo\n>>> access",
    "url": "https://github.com/ipinfo/python",
    "last_updated": "2025-08-12T05:36:50+00:00"
  },
  {
    "full_name": "soodoku/typecast",
    "name": "typecast",
    "description": "Replication Materials for Typecast",
    "language": "TeX",
    "topics": [],
    "readme": "## Replication Materials for Typecast\n\nParty stereotyping inflames polarization. What fuels party stereotyping? We explore the extent to which a common mental shortcut---the representativeness heuristic---yields biased mental images of the parties. First, we show that people commit the conjunction fallacy---a logical error associated with representativeness bias---at higher rates when evaluating others with party-representative characteristics. Second, when we inform people of the percentage\nof partisans in groups, the least numerate use this information to infer party composition, consistent with the representativeness heuristic. Finally, we show that people's party stereotypes become more biased when we increase cognitive load, though stereotyping occurs even in relatively \"easy\" contexts. The epresentativeness heuristic appears to exacerbate party stereotyping, and the way that media informs people about the relationship between social groups and parties may encourage reliance on representativeness. More broadly, reducing stereotyping requires reckoning with our built-in machinery for simplifying theworld around us.\n\n## Replication Materials for Typecast\n\n### Data\n\n* [Bayesian Cues](data/bayesian_cues.csv)\n* [Linda](data/linda_clean.csv)\n* [James](data/james_ff_clean.csv)\n* [Time Pressure](data/timing.dta)\n* [Avatars](data/turk_recoded_old.dta)\n\n(Some of the screenshots of the treatments can be found [here](figs/))\n\n### Scripts\n\n* [Linda + James Max Contrast. Figures 1 and 2](scripts/01_linda_james_max_contrast_fig_1_2.R)\n* [Bayesian Cues. Table 2 and Figure 4.](scripts/02_bayes_tab_2_fig_4.R)\n* [James Full Factorial. Figure 3 and SI 3](scripts/03_james_full_factorial_fig_3_si_3.R)\n* [Time Pressure. Figure 5.](scripts/04_timing_fig_5_pre.R)\n* [Avatars. SI 4.](scripts/05_avatars_si_4.R)\n\n### Figures\n\n* [Figures](figs/)\n\n### Tables\n\n* [Tables](tabs/)\n\n### Manuscript and SI\n\n* [manuscript](ms/)\n\n### Authors\n\nDoug Ahler and Gaurav Sood\n\n### Suggested Citation\n\nAhler,",
    "url": "https://github.com/soodoku/typecast",
    "last_updated": "2022-01-13T08:39:03+00:00"
  },
  {
    "full_name": "eddelbuettel/pgapack",
    "name": "pgapack",
    "description": "A general-purpose, data-structure-neutral, and parallel genetic algorithm library",
    "language": "C",
    "topics": [],
    "readme": "## About\n\nPGAPack is a general-purpose, data-structure-neutral, parallel genetic\nalgorithm library developed at Argonne National Laboratory.\n\nKey features are:\n\n- Callable from Fortran or C.\n- Runs on uniprocessors, parallel computers, and workstation networks.\n- Binary-, integer-, real-, and character-valued native data types.\n- Object-oriented data structure neutral design.\n- Parameterized population replacement.\n- Multiple choices for selection, crossover, and mutation operators.\n- Easy integration of hill-climbing heuristics.\n- Easy-to-use interface for novice and application users.\n- Fully extensible to support custom operators and new data types.\n- Extensive debugging facilities.\n- A large set of example problems.\n- It is released under the MPICH2 license (also used by the MPICH2 MPI implementation from Argonne National Laboratory).\n\n## History\n\nDavid Levine is the principal author of pgagpack and wrote most of the code\nduring the mid-1990s. Dirk Eddelbuettel became its Debian maintainer in 2008,\norganised a relicensing by Argonne National Laboratories under the MPICH2\nlicense and is currently also the effective upstream maintainer.\n",
    "url": "https://github.com/eddelbuettel/pgapack",
    "last_updated": "2025-08-08T02:17:29+00:00"
  },
  {
    "full_name": "soodoku/partisan_vision",
    "name": "partisan_vision",
    "description": "Partisan Bias in Simple Visual Evaluations",
    "language": "TeX",
    "topics": [],
    "readme": "## Partisan Vision? Partisan Bias in Simple Visual Evaluations\n\nDo partisans `see' different things? We test the hypothesis using simple evaluation tasks. We manipulate the perceived partisanship of people who have [written a piece of text](data/treats/Mistakes_Dem.png) or [parked the cars](data/treats/Parking_Lot_Dems.png) and ask respondents to estimate the number of errors. \n\nWe find that partisan bias is generally small. In the CCES survey, the average number of mistakes found by Democrats in Democratic texts is 9.7 (.2) vs. 9.9 (.2) in Republican texts. On the flip side, Republicans on average find 8.40 (.3) mistakes in Democratic texts and 8.1 (.3) in Republican texts. Moving to judgments about errors in parking, we find that Democrats on average find 6.1 (.4) misparked cars when they think Democrats parked the cars vs. 8.72 (.6) when they think Republicans parked the cars. The difference here is statistically significant but the sharply smaller difference between the medians suggests that the effect is due to a small number of extreme observations. Moving to Republicans, they find on average 8.4 (.6) misparked cars when they think Democrats parked the cars vs. 8.1 (.7) when they think Republicans parked the cars. \n\nWhen we replicated the experiment on finding errors in text on Lucid, we found a similar pattern. Democrats find 5.47 (.61) errors on average when they think Democrats wrote the text vs. 5.91 (.79) when they think Republicans wrote it. On the Republican side, the corresponding numbers are 5.58 (.89) (when they think Democrats wrote the text) vs. 4.86 (.26) (when they think Republicans wrote it).\n\nLastly, on a survey on Amazon Mturk, we asked respondents to count [how many people are wearing a mask at a Trump rally](data/treats/trump_rally.mp4). Because of some absurd estimates, e.g., 80,000, we winsorized the data. Democrats on average spot 5.4 (.3) masks and Republicans spot 5.9 (.3) masks. The 25th percentile of Democrats' and Republicans' estima",
    "url": "https://github.com/soodoku/partisan_vision",
    "last_updated": "2022-11-28T07:41:26+00:00"
  },
  {
    "full_name": "google-deepmind/learning-to-learn",
    "name": "learning-to-learn",
    "description": "Learning to Learn in TensorFlow",
    "language": "Python",
    "topics": [
      "machine-learning",
      "artificial-intelligence",
      "neural-networks",
      "deep-learning"
    ],
    "readme": "# [Learning to Learn](https://arxiv.org/abs/1606.04474) in TensorFlow\n\n\n## Dependencies\n\n* [TensorFlow >=1.0](https://www.tensorflow.org/)\n* [Sonnet >=1.0](https://github.com/deepmind/sonnet)\n\n\n## Training\n\n```\npython train.py --problem=mnist --save_path=./mnist\n```\n\nCommand-line flags:\n\n* `save_path`: If present, the optimizer will be saved to the specified path\n    every time the evaluation performance is improved.\n* `num_epochs`: Number of training epochs.\n* `log_period`: Epochs before mean performance and time is reported.\n* `evaluation_period`: Epochs before the optimizer is evaluated.\n* `evaluation_epochs`: Number of evaluation epochs.\n* `problem`: Problem to train on. See [Problems](#problems) section below.\n* `num_steps`: Number of optimization steps.\n* `unroll_length`: Number of unroll steps for the optimizer.\n* `learning_rate`: Learning rate.\n* `second_derivatives`: If `true`, the optimizer will try to compute second\n    derivatives through the loss function specified by the problem.\n\n\n## Evaluation\n\n```\npython evaluate.py --problem=mnist --optimizer=L2L --path=./mnist\n```\n\nCommand-line flags:\n\n* `optimizer`: `Adam` or `L2L`.\n* `path`: Path to saved optimizer, only relevant if using the `L2L` optimizer.\n* `learning_rate`: Learning rate, only relevant if using `Adam` optimizer.\n* `num_epochs`: Number of evaluation epochs.\n* `seed`: Seed for random number generation.\n* `problem`: Problem to evaluate on. See [Problems](#problems) section below.\n* `num_steps`: Number of optimization steps.\n\n\n## Problems\n\nThe training and evaluation scripts support the following problems (see\n`util.py` for more details):\n\n* `simple`: One-variable quadratic function.\n* `simple-multi`: Two-variable quadratic function, where one of the variables\n    is optimized using a learned optimizer and the other one using Adam.\n* `quadratic`: Batched ten-variable quadratic function.\n* `mnist`: Mnist classification using a two-layer fully connected network.\n* `cifar`: Cifar10 classification u",
    "url": "https://github.com/google-deepmind/learning-to-learn",
    "last_updated": "2025-08-23T12:07:12+00:00"
  },
  {
    "full_name": "cavaunpeu/statistical-rethinking",
    "name": "statistical-rethinking",
    "description": "Solutions for the practice problems",
    "language": "R",
    "topics": [],
    "readme": "Notes and homework problems for Richard McElreath's [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/).\n",
    "url": "https://github.com/cavaunpeu/statistical-rethinking",
    "last_updated": "2025-03-22T11:00:50+00:00"
  },
  {
    "full_name": "iamlemec/fastpat",
    "name": "fastpat",
    "description": "Parse and cluster USPTO patent data. Includes applications, grants, assignments, and maintenance.",
    "language": "Python",
    "topics": [],
    "readme": "## Fastpat\n\nFetch and parse patent application, grant, assignment, and maintenance info from [USPTO Bulk Data](https://bulkdata.uspto.gov/). This handles all patent formats and outputs to pure CSV. Clusters patents by firm name, first filtering using locality-sensitive hashing, then finding components induced by a Levenshtein distance threshhold.\n\n### Requirements\n\nIn general, you'll need the `fire` library. For parsing, you'll need: `numpy`, `pandas`, and `lxml`. For firm clustering, you'll additionally need: `xxhash`, `editdistance`, `networkx`, and `Cython`. All of these are available through both `pip` and `conda`. You can install all the requirements with `pip` by running: `pip install -r requirements.txt`.\n\n### Usage\n\nMost common tasks can be executed through the `fastpat` command. For more advanced usage, you can also directly call the functions in the library itself. When using `fastpat` you have to specify the data directory. You can either do this by passing the `--datadir` flag directly or by setting the environment variable `FASTPAT_DATADIR`. If you've cloned the repository locally, you have to run `python3 -m fastpat` instead of `fastpat`.\n\n#### Downloading Data\n\nThe following USPTO data sources are supported\n- `grant`: patent grants\n- `apply`: patent applications\n- `assign`: patent resassignments\n- `maint`: patent maintenance events\n- `tmapply`: trademark applications (preliminary)\n\nTo download the files for data source `SOURCE`, run the command\n``` bash\nfastpat fetch SOURCE\n```\n\nThis library ships with a list of source files for each type, however this will become out of date over time. As such, you can also specify your own metadata path containing these files. You can do this by passing the `--metadir` flag directly or by setting the `FASTPAT_METADIR` environment variable. If you've cloned this repository locally, you can also update the files in `fastpat/meta`.\n\n#### Parsing Data\n\nParsing works similarly to fetching. Simply run\n``` bash\nfastpat par",
    "url": "https://github.com/iamlemec/fastpat",
    "last_updated": "2025-07-28T02:12:20+00:00"
  },
  {
    "full_name": "mllg/base64url",
    "name": "base64url",
    "description": "Fast and url-safe base64 encoder and decoder for R",
    "language": "C",
    "topics": [
      "cran",
      "base64",
      "base64url",
      "base32",
      "r"
    ],
    "readme": "# base64url\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/base64url)](https://cran.r-project.org/package=base64url)\n[![Build Status](https://travis-ci.org/mllg/base64url.svg?branch=master)](https://travis-ci.org/mllg/base64url)\n[![Build status](https://ci.appveyor.com/api/projects/status/5329u3dk9vanak0p/branch/master?svg=true)](https://ci.appveyor.com/project/mllg/base64url/branch/master)\n[![Coverage Status](https://coveralls.io/repos/github/mllg/base64url/badge.svg?branch=master)](https://coveralls.io/github/mllg/base64url?branch=master)\n\nIn contrast to base64 RFC3548, the 62nd character (`'+'`) is replaced with `'-'`, the 63rd character (`'/'`) is replaced with `'_'`.\nFurthermore, the encoder does not fill the string with trailing `'='`.\nThe resulting encoded strings comply to the regular expression pattern `'[A-Za-z0-9_-]'` and thus are safe to use in URLs or for file names.\n\nFor a small benchmark, see the [vignette](https://cran.r-project.org/package=base64url/vignettes/Benchmarks.html).\n\nAs of version 1.1, this package also ships with a simple base32 encoder/decoder suited to mangle file names on case insensitive file systems.\n\n## Installation\nFor the stable release, just install the latest version from [CRAN](https://cran.r-project.org/package=base64url):\n```{R}\ninstall.packages(\"base64url\")\n```\nFor the development version, use [devtools](https://cran.r-project.org/package=devtools):\n```{R}\ndevtools::install_github(\"mllg/base64url\")\n```\n",
    "url": "https://github.com/mllg/base64url",
    "last_updated": "2025-01-30T15:51:18+00:00"
  },
  {
    "full_name": "potterhsu/SVHNClassifier-PyTorch",
    "name": "SVHNClassifier-PyTorch",
    "description": "A PyTorch implementation of Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks (http://arxiv.org/pdf/1312.6082.pdf)",
    "language": "Jupyter Notebook",
    "topics": [
      "deep-learning",
      "pytorch",
      "svhn"
    ],
    "readme": "# SVHNClassifier-PyTorch\n\nA PyTorch implementation of [Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks](http://arxiv.org/pdf/1312.6082.pdf) \n\n> If you're interested in C++ inference, move [HERE](cpp)\n\n## Results\n\n<table>\n    <tr>\n        <th>Steps</th>\n        <th>GPU</th>\n        <th>Batch Size</th>\n        <th>Learning Rate</th>\n        <th>Patience</th>\n        <th>Decay Step</th>\n        <th>Decay Rate</th>\n        <th>Training Speed (FPS)</th>\n        <th>Accuracy</th>\n    </tr>\n    <tr>\n        <td>\n            <a href=\"https://drive.google.com/open?id=1DSg3F5GpouEvU9n7YSPdUKH1CSmkdwSw\">\n                54000\n            </a>\n        </td>\n        <td>GTX 1080 Ti</td>\n        <td>512</td>\n        <td>0.16</td>\n        <td>100</td>\n        <td>625</td>\n        <td>0.9</td>\n        <td>~1700</td>\n        <td>95.65%</td>\n    </tr>\n</table>\n\n### Sample\n\n![](images/test-75.png)\n```\n$ python infer.py -c=./logs/model-54000.pth ./images/test-75.png\nlength: 2\ndigits: 7 5 10 10 10\n```\n\n![](images/test-190.png)\n```\n$ python infer.py -c=./logs/model-54000.pth ./images/test-190.png\nlength: 3\ndigits: 1 9 0 10 10\n```\n\n\n### Loss\n\n![](images/loss.png)\n\n## Requirements\n\n* Python 3.6\n* torch 1.0\n* torchvision 0.2.1\n* visdom\n    ```\n    $ pip install visdom\n    ```\n    \n* h5py\n    ```\n    In Ubuntu:\n    $ sudo apt-get install libhdf5-dev\n    $ sudo pip install h5py\n    ```\n\n* protobuf\n    ```\n    $ pip install protobuf\n    ```\n\n* lmdb\n    ```\n    $ pip install lmdb\n    ```\n\n## Setup\n\n1. Clone the source code\n\n    ```\n    $ git clone https://github.com/potterhsu/SVHNClassifier-PyTorch\n    $ cd SVHNClassifier-PyTorch\n    ```\n\n2. Download [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/) format 1\n\n3. Extract to data folder, now your folder structure should be like below:\n    ```\n    SVHNClassifier\n        - data\n            - extra\n                - 1.png \n                - 2.png\n                - ...\n                - digitStru",
    "url": "https://github.com/potterhsu/SVHNClassifier-PyTorch",
    "last_updated": "2025-09-01T14:31:41+00:00"
  },
  {
    "full_name": "andrewflowers/how-to-make-mistakes-in-R",
    "name": "how-to-make-mistakes-in-R",
    "description": "How To Make Mistakes in R",
    "language": "",
    "topics": [],
    "readme": "## What are your “favorite” R mistakes?\n\n### R is great, but it’s also weird. \n\nR was built by and for statisticians, so it’s not like other programming languages. Its idiosyncrasies can be a source of deep frustration for beginners. But I’d argue there is no better tool for data analysis.\n\nThat’s why I’m writing a free ebook *How To Make Mistakes In R* for O’Reilly. It’s modeled after the excellent [*How To Make Mistakes In Python*](http://www.oreilly.com/programming/free/how-to-make-mistakes-in-python.csp), by [Mike Pirnat](http://mike.pirnat.com/). \n\nThe target audience is all R coders, from those just starting out all the way to the advanced developers. It’ll cover mistakes in set-up, style, and statistics -- and other surprises, too. I’m especially qualified to write this book because I’ve made so many R mistakes in my own work.\n\nIt's an exciting project, but I need your help. **What are your “favorite” R mistakes?**\n\nI’m looking for all types, ranging from the dead-simple, beginner-level screwups to the subtle, advanced bugs you’ve encountered. Here are a few examples of mistakes I plan to address:\n\n- Function masking due to conflicting packages (e.g., dplyr and plyr)\n- Repeatedly typing `stringsAsFactors = FALSE`\n- The default `table()` function masking NA values\n- Not using piping (`%>%`) to improve code readability\n- Not using the `broom` package to standardize the output of statistical models\n- Not using GitHub and/or RStudio Projects for collaboration\n\nSend them to me, via [email](mailto:andrew.w.flowers@gmail.com), a [GitHub pull request](https://github.com/andrewflowers/how-to-make-mistakes-in-R) or on [Twitter](https://twitter.com/andrewflowers). The more the merrier. Feel free to contact me multiple times, as you recall your “favorite” R mistakes. \n\n\n",
    "url": "https://github.com/andrewflowers/how-to-make-mistakes-in-R",
    "last_updated": "2023-09-09T19:43:22+00:00"
  },
  {
    "full_name": "njtierney/ukpolice",
    "name": "ukpolice",
    "description": ":uk: :police_car: R package to pull police data from the uk police data repository :police_car: :uk: ",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# ukpolice <img src=\"man/figures/ukpolice-hex-small.png\" align=\"right\" />\n\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/njtierney/ukpolice?branch=master&svg=true)](https://ci.appveyor.com/project/njtierney/ukpolice)[![Travis-CI\nBuild\nStatus](https://travis-ci.org/njtierney/ukpolice.svg?branch=master)](https://travis-ci.org/njtierney/ukpolice)[![Coverage\nStatus](https://img.shields.io/codecov/c/github/njtierney/ukpolice/master.svg)](https://codecov.io/github/njtierney/ukpolice?branch=master)\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n\nukpolice is an R package that facilitates retrieving data from the [UK\npolice database.](https://data.police.uk/). The data provided by the API\ncontains public sector information licensed under the [Open Government\nLicence\nv3.0.](http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/)\n\n# Installation\n\nInstall from GitHub\n\n``` r\n\n#install.packages(\"remotes\")\nremotes::install_github(\"njtierney/ukpolice\")\n```\n\n# Get the crime data with `ukp_crime`\n\n`ukp_crime()` draws crimes from within a one mile radius of the\nlocation. When no date is specified, it uses the latest month available,\nwhich can be found using `ukp_last_update()`.\n\n``` r\nlibrary(ukpolice)\n\ncrime_data <- ukp_crime(lat = 52.629729, lng = -1.131592)\n#> No encoding supplied: defaulting to UTF-8.\n\nhead(crime_data)\n#> # A tibble: 6 x 12\n#>   category persistent_id date    lat  long street_id street_name context\n#>   <chr>    <chr>         <chr> <dbl> <dbl> <chr>     <chr>       <chr>  \n#> 1 anti-so… \"\"            2018…  52.6 -1.13 883389    On or near… \"\"     \n#> 2 anti-so… \"\"            2018…  52.6 -1.13 883356    On or near… \"\"     \n#> 3 anti-so… \"\"            2018…  52.6 -1.14 883430    On or near… \"\"     \n#> 4 anti-so… \"\"            2018…  52.6 -1.13 883378    On or near",
    "url": "https://github.com/njtierney/ukpolice",
    "last_updated": "2025-06-24T18:32:50+00:00"
  },
  {
    "full_name": "iamaziz/PyDataset",
    "name": "PyDataset",
    "description": "Instant access to many datasets in Python.",
    "language": "Python",
    "topics": [
      "python",
      "datasets",
      "data-science"
    ],
    "readme": "## PyDataset\n [![PyPI version](https://badge.fury.io/py/pydataset.svg)](http://badge.fury.io/py/pydataset)\n\nProvides instant access to many datasets right from Python (in pandas DataFrame structure).\n\n### What?\n\nThe idea is simple. There are various datasets available out there, but they are scattered in different places over the web.\nIs there a quick way (in Python) to access them instantly without going through the hassle of searching, downloading, and reading ... etc?\nPyDataset tries to address that question :)\n\n\n### Usage:\n\nStart with importing `data()`:\n```python\nfrom pydataset import data\n```\n- To load a dataset:\n```python\ntitanic = data('titanic')\n```\n- To display the documentation of a dataset:\n```python\ndata('titanic', show_doc=True)\n```\n- To see the available datasets:\n```python\ndata()\n```\n\nThat's it.\nSee more [examples](examples).\n\n\n### Why?\n\nIn `R`, there is a very easy and immediate way to access multiple statistical datasets,\nin almost no effort. All it takes is one line ` > data(dataset_name)`.\nThis makes the life easier for quick prototyping and testing.\nWell, I am jealous that Python does not have a similar functionality.\nThus, the aim of `pydataset` is to fill that gap.\n\nCurrently, `pydataset` has about 757 (mostly numerical-based) datasets, that are based on `RDatasets`.\nIn the future, I plan to scale it to include a larger set of datasets.\nFor example,\n1) include textual data for NLP-related tasks, and\n2) allow adding a new dataset to the in-module repository.\n\n\n### Installation:\n\n`$ pip install pydataset`\n\n#### Uninstall:\n\n- `$ pip uninstall pydataset`\n- `$ rm -rf $HOME/.pydataset`\n\n### Changelog\n\n**0.2.0**\n\n- Add search dataset by name similarity.\n- Example:\n\n```python\n>>> data('heat')\nDid you mean:\nWheat, heart, Heating, Yeast, eidat, badhealth, deaths, agefat, hla, heptathlon, azt\n```\n\n**0.1.1**\n\n- Fix: add support to Windows and fix filepaths, issue #1\n\n### Dependency:\n- pandas\n\n### Miscellaneous:\n\n- Tested on OSX and Linux (debian).\n- Suppo",
    "url": "https://github.com/iamaziz/PyDataset",
    "last_updated": "2025-08-26T03:23:24+00:00"
  },
  {
    "full_name": "dodger487/dplython",
    "name": "dplython",
    "description": "dplyr for python",
    "language": "Python",
    "topics": [],
    "readme": "# Dplython: Dplyr for Python\n\n[![Build Status](https://travis-ci.org/dodger487/dplython.svg?branch=master)](https://travis-ci.org/dodger487/dplython)\n\nWelcome to Dplython: Dplyr for Python.\n\nDplyr is a library for the language R designed to make data analysis fast and easy.\nThe philosophy of Dplyr is to constrain data manipulation to a few simple functions that correspond to the most common tasks.\nThis maps thinking closer to the process of writing code, helping you move closer to analyze data at the \"speed of thought\".\n\nThe goal of this project is to implement the functionality of the R package Dplyr on top of Python's pandas.\n\n* Dplyr: [Click here](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)\n* Pandas: [Click here](http://pandas.pydata.org/pandas-docs/stable/10min.html)\n\nThis is version 0.0.7.\nIt's experimental and subject to change.\n\n## Introductory Video\nHere is a 20 minute video explaining dplython, given at PyGotham 2016.\n\n[![PyGotham Dplython video](http://img.youtube.com/vi/4YAcwCe1mAE/0.jpg)](https://www.youtube.com/watch?v=4YAcwCe1mAE \"PyGotham Dplython Video\")\n\nClick the awkward picture above to see the talk!\nNote that sound doesn't start until about 1 minute in due to microphone issues.\n\n## Installation\nTo install, use pip:\n```\npip install dplython\n```\n\nTo get the latest development version, you can clone this repo or use the command:\n```\npip install git+https://github.com/dodger487/dplython.git\n```\n\n## Contributing\nWe welcome your feature requests, open issues, bug reports, and pull requests!\nPlease use GitHub's interface.\nAlso consider joining the [dplython mailing list](https://groups.google.com/forum/#!forum/dplython).\n\n## Example usage\n```python\nimport pandas\nfrom dplython import (DplyFrame, X, diamonds, select, sift, sample_n,\n    sample_frac, head, arrange, mutate, group_by, summarize, DelayFunction) \n\n# The example `diamonds` DataFrame is included in this package, but you can \n# cast a DataFrame to a DplyFrame in this ",
    "url": "https://github.com/dodger487/dplython",
    "last_updated": "2025-08-01T15:48:57+00:00"
  },
  {
    "full_name": "dpressel/dliss-tutorial",
    "name": "dliss-tutorial",
    "description": "Tutorial for International Summer School on Deep Learning, 2019",
    "language": "Jupyter Notebook",
    "topics": [
      "nlp",
      "machine-learning",
      "deep-learning"
    ],
    "readme": "# dliss-tutorial\nTutorial for [International Summer School\non Deep Learning, 2019](http://dl-lab.eu/) in Gdansk, Poland\n\n## Sections\n\n### Overview Talk\n\nhttps://docs.google.com/presentation/d/1DJI1yX4U5IgApGwavt0AmOCLWwso7ou1Un93sMuAWmA/\n\n### Tutorial\nThere are currently 3 hands-on sections to this tutorial.\n\n- The [first section](1_pretrained_vectors.ipynb) covers pre-trained word embeddings [(colab)](https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb)\n\n- The [second section](2_context_vectors.ipynb) covers pre-trained contextual emeddings [(colab)](https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/2_context_vectors.ipynb)\n- The [third section](3_finetuning.ipynb) covers fine-tuning a pre-trained model [(colab)](https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/3_finetuning.ipynb)\n\n### Updates\n\n- *April 2022* If you are interested in learning how to build different Transformer architectures from the ground up, I have a [new set of tutorials](https://github.com/dpressel/tfs) with in-depth details and full implementations of several popular Transformer models.  They show how to build models step by step, how to pretrain them, and how to use them for downstream tasks.  There is an accompanying Python package that contains all of the tutorial pieces put together\n\n\n\n- *July 2020* I have posted a set of [Colab tutorials](https://github.com/dpressel/mead-tutorials) using [MEAD](https://github.com/dpressel/mead-baseline) which is referenced in these tutorials.  This new set of notebooks covers similar material, including transfer learning for classification and taggers, as well as training Transformer-based models from scratch using the [MEAD API](https://github.com/dpressel/mead-baseline/tree/master/layers) with TPUs.  MEAD makes it easy to train lots of powerful models for NLP using a simple YAML configuration and makes it easy to extend the code with new models while ",
    "url": "https://github.com/dpressel/dliss-tutorial",
    "last_updated": "2024-12-03T09:47:59+00:00"
  },
  {
    "full_name": "PMassicotte/gtrendsR",
    "name": "gtrendsR",
    "description": "R functions to perform and display Google Trends queries",
    "language": "R",
    "topics": [
      "google",
      "google-trends"
    ],
    "readme": "\n## gtrendsR \n[![GitHub Actions Build Status](https://github.com/PMassicotte/gtrendsR/actions/workflows/ci.yaml/badge.svg)](https://github.com/PMassicotte/gtrendsR/actions/workflows/ci.yaml)\n[![License](https://eddelbuettel.github.io/badges/GPL2+.svg)](https://www.gnu.org/licenses/gpl-2.0.html) \n[![CRAN](https://www.r-pkg.org/badges/version/gtrendsR)](https://cran.r-project.org/package=gtrendsR) \n[![Downloads](https://cranlogs.r-pkg.org/badges/gtrendsR?color=brightgreen)](https://www.r-pkg.org:443/pkg/gtrendsR)\n\n`gtrendsR` provides an interface for retrieving and displaying Google Trends information. \n\nTrends (number of hits) over time as well as geographic representation of the results can be displayed.\n\n### Example\n\nIn this simple example, trends for keywords `nhl`, `nba` are retrieved for Canada and USA and then plotted from R.\n\n``` {.r}\nlibrary(gtrendsR)\n\nres <- gtrends(c(\"nhl\", \"nba\"), geo = c(\"CA\", \"US\"))\nplot(res)\n```\n\n### Installation\n\nSince release 1.3.0, the package is on [CRAN](https://cran.r-project.org) and\ncan be installed via\n\n``` {.r}\ninstall.packages(\"gtrendsR\")\n```\n\nRelease-candidate packages are available in the [ghrr drat repository](https://ghrr.github.io/drat/)\nand can installed via\n\n```r\ninstall.packages(\"drat\")       # easier repo access + creation\ndrat:::add(\"ghrr\")             # make it known\ninstall.packages(\"gtrendsR\")   # install it\n```\n\nDevelopment version (which may be less stable) can be installed directly from this repository via\n\n``` {.r}\nif (!require(\"devtools\")) install.packages(\"devtools\")\ndevtools::install_github(\"PMassicotte/gtrendsR\")\n```\n\n## Using gtrendsR behind a PROXY.\n\nIf gtrendsR should be used behind a proxy, especially with NTLM authentication mode,\nyou need to set the proxy parameters using \"setHandleParameters\" function\n\n### Example\n\n``` {.r}\nlibrary(gtrendsR)\n\nsetHandleParameters(user = \"xxxx\", password = \"*******\", domain = \"mydomain\", proxyhost = \"10.111.124.113\", proxyport = 8080)\nres <- gtrends(c(\"nhl\", \"nba\"), ",
    "url": "https://github.com/PMassicotte/gtrendsR",
    "last_updated": "2025-08-27T12:08:13+00:00"
  },
  {
    "full_name": "clauswilke/multiscales",
    "name": "multiscales",
    "description": "Multivariate scales for ggplot2",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# multiscales\n\nMultivariate scales for ggplot2, written by Claus O. Wilke\n\n## Installation\n\nThis package can be installed from github. It requires the development\nversion of the colorspace package. It also requires ggplot2 3.0.0, which\nwas released on July 3,\n    2018.\n\n    install.packages(\"colorspace\", repos = \"http://R-Forge.R-project.org\")\n    devtools::install_github(\"clauswilke/multiscales\")\n\nThis is an experimental package. Use at your own risk. API is not\nstable. No user support provided.\n\n## Examples\n\nVisualizing the lead/lag of Clinton vs. Trump in the 2016 presidential\nelection jointly with the uncertainty of the lead/lag estimates. This\nvisualization shows that for many states the outcome was difficult to\npredict. (Example taken from Correll et al., [Value-Suppressing\nUncertainty\nPalettes](https://idl.cs.washington.edu/files/2018-UncertaintyPalettes-CHI.pdf),\nCHI 2018.)\n\n``` r\nlibrary(ggplot2)\nlibrary(multiscales)\n\ncolors <- scales::colour_ramp(\n  colors = c(red = \"#AC202F\", purple = \"#740280\", blue = \"#2265A3\")\n)((0:7)/7)\n\nggplot(US_polling) + \n  geom_sf(aes(fill = zip(Clinton_lead, moe_normalized)), color = \"gray30\", size = 0.2) + \n  coord_sf(datum = NA) +\n  bivariate_scale(\"fill\",\n    pal_vsup(values = colors, max_desat = 0.8, pow_desat = 0.2, max_light = 0.7, pow_light = 1),\n    name = c(\"Clinton lead\", \"uncertainty\"),\n    limits = list(c(-40, 40), c(0, 1)),\n    breaks = list(c(-40, -20, 0, 20, 40), c(0, 0.25, 0.50, 0.75, 1.)),\n    labels = list(waiver(), scales::percent),\n    guide = \"colourfan\"\n  ) +\n  theme_void() +\n  theme(\n    legend.key.size = grid::unit(0.8, \"cm\"),\n    legend.title.align = 0.5,\n    plot.margin = margin(5.5, 20, 5.5, 5.5)\n  )\n```\n\n![](man/figures/README-unnamed-chunk-2-1.png)<!-- -->\n\nFor comparison, the same plot with a univariate color scale. Not it\nappears that in every state we know who is in the lead.\n\n``` r\nggplot(US_polling) + \n  geom_sf(aes(fill = ",
    "url": "https://github.com/clauswilke/multiscales",
    "last_updated": "2025-03-22T11:05:31+00:00"
  },
  {
    "full_name": "tabulapdf/tabula-java",
    "name": "tabula-java",
    "description": "Extract tables from PDF files",
    "language": "Java",
    "topics": [
      "extracting-tables",
      "pdfs",
      "extraction-engine"
    ],
    "readme": "tabula-java [![Build Status](https://travis-ci.org/tabulapdf/tabula-java.svg?branch=master)](https://travis-ci.org/tabulapdf/tabula-java)\n===========\n\n`tabula-java` is a library for extracting tables from PDF files — it is the table extraction engine that powers [Tabula](http://tabula.technology/) ([repo](http://github.com/tabulapdf/tabula)). You can use `tabula-java` as a command-line tool to programmatically extract tables from PDFs.\n\n© 2014-2020 Manuel Aristarán. Available under MIT License. See [`LICENSE`](LICENSE).\n\n## Download\n\nDownload a version of the tabula-java's jar, with all dependencies included, that works on Mac, Windows and Linux from our [releases page](../../releases).\n\n## Commandline Usage Examples\n\n`tabula-java` provides a command line application:\n\n```\n$ java -jar target/tabula-1.0.5-jar-with-dependencies.jar --help\nusage: tabula [-a <AREA>] [-b <DIRECTORY>] [-c <COLUMNS>] [-f <FORMAT>]\n       [-g] [-h] [-i] [-l] [-n] [-o <OUTFILE>] [-p <PAGES>] [-r] [-s\n       <PASSWORD>] [-t] [-u] [-v]\n\nTabula helps you extract tables from PDFs\n\n -a,--area <AREA>           -a/--area = Portion of the page to analyze.\n                            Example: --area 269.875,12.75,790.5,561.\n                            Accepts top,left,bottom,right i.e. y1,x1,y2,x2\n                            where all values are in points relative to the\n                            top left corner. If all values are between\n                            0-100 (inclusive) and preceded by '%', input\n                            will be taken as % of actual height or width\n                            of the page. Example: --area %0,0,100,50. To\n                            specify multiple areas, -a option should be\n                            repeated. Default is entire page\n -b,--batch <DIRECTORY>     Convert all .pdfs in the provided directory.\n -c,--columns <COLUMNS>     X coordinates of column boundaries. Example\n                            --columns 10.1,20.2,30.3. If all values are\n ",
    "url": "https://github.com/tabulapdf/tabula-java",
    "last_updated": "2025-08-31T14:35:22+00:00"
  },
  {
    "full_name": "setzler/eventStudy",
    "name": "eventStudy",
    "description": "R package and guide for performing event studies with heterogeneous dynamic effects.",
    "language": "R",
    "topics": [],
    "readme": "\n#### Warning: This package is now deprecated.\n\nThis package is now deprecated and we do not plan to maintain it going forward. We suggest one of the following alternatives:\n\n- The `did` package in R [(here)](https://bcallaway11.github.io/did/).\n- The `DiDforBigData` package in R [(here)](https://setzler.github.io/DiDforBigData/).\n\n\n\n\n\n\neventStudy: Perform an Event Study in R\n================\n\nCreated by David Novgorodsky and Bradley Setzler at the University of Chicago\n\nThe methods are explained in [our companion Practical Guide to Event Studies](https://github.com/setzler/eventStudy/blob/master/guide/event_study_guide.pdf).\n\n**Disclaimer:** By using this software, you accept the terms of the MIT license. This is a work in progress. It is updated frequently. Please let us know if you find any bugs or have questions that are not addressed in the documentation.\n\nOverview\n========\n\n### Purpose\n\n`eventStudy` is an R package for performing event studies. It has many capabilites:\n\n-   It maximizes sample size by using a \"stacked\" approach to match all possible control observations to any given treatment observation at any event time;\n-   It allows the estimated treatment effects to vary over time and by treatment cohort. It can also impose a common treatment effect across cohorts (extending results by Abraham & Sun, 2018);\n-   It has built-in tools to correct for anticipation (extending results by Fadlon & Nielsen, 2019) and deviations from parallel trends (either using covariate balancing in the spirit of Abadie, 2005, or modeling parametric deviations from parallel trends).\n\n### Installation\n\n**Preferred method:** The latest version of the package can be installed with the command `devtools::install_github(\"setzler/eventStudy/eventStudy\")`.\n\nInstallation without internet access: Clone this repo to your personal machine, use the command line approach of `R CMD BUILD eventStudy/` to build the file eventStudy\\_0.1.0.tar.gz, move this .tar.gz file to the location without i",
    "url": "https://github.com/setzler/eventStudy",
    "last_updated": "2025-04-26T16:43:50+00:00"
  },
  {
    "full_name": "scikit-learn-contrib/polylearn",
    "name": "polylearn",
    "description": "A library for factorization machines and polynomial networks for classification and regression in Python.",
    "language": "Python",
    "topics": [
      "machine-learning",
      "factorization-machines",
      "polynomial-regression",
      "polynomial-networks"
    ],
    "readme": ".. -*- mode: rst -*-\n\npolylearn\n=========\n\nA library for **factorization machines** and **polynomial networks**\nfor classification and regression in Python.\n\n`Github repository. <https://github.com/scikit-learn-contrib/polylearn/>`_\n\n.. image:: https://travis-ci.org/scikit-learn-contrib/polylearn.svg?branch=master\n    :target: https://travis-ci.org/scikit-learn-contrib/polylearn\n\n.. image:: https://ci.appveyor.com/api/projects/status/g9xnar9081l3vsw7/branch/master?svg=true\n    :target: https://ci.appveyor.com/project/vene/polylearn\n\n.. image:: https://coveralls.io/repos/scikit-learn-contrib/polylearn/badge.svg?branch=master&service=github\n    :target: https://coveralls.io/r/scikit-learn-contrib/polylearn\n\n.. image:: https://circleci.com/gh/scikit-learn-contrib/polylearn/tree/master.svg?style=shield&circle-token=:circle-token\n    :target: https://circleci.com/gh/scikit-learn-contrib/polylearn/\n\nFactorization machines and polynomial networks are machine learning models\nthat can capture **feature interaction** (co-occurrence) through polynomial terms.\nBecause feature interactions can be very sparse, it's common to use **low rank,\nfactorized representations**; this way, we can learn weights even for feature\nco-occurrences that haven't been observed at training time.\n\nFactorization machines are popular for recommender systems, as they are a\ngeneralization of matrix completion models.\n\nThis package provides:\n\n- coordinate descent algorithm for fitting factorization machines of degree 2 or 3,\n- coordinate descent algorithm for fitting polynomial networks of arbitrary degree,\n- `scikit-learn <http://scikit-learn.org>`_-compatible API,\n- `Cython <http://cython.org>`_ implementations for computationally intensive parts.\n\nInstallation\n------------\n\nBinary packages are not yet available.\n\nThe development version of polylearn can be installed from its git repository. In\nthis case it is assumed that you have a working\nC++ compiler.\n\n1. Obtain the sources by::\n\n    git clone https",
    "url": "https://github.com/scikit-learn-contrib/polylearn",
    "last_updated": "2025-08-29T20:46:29+00:00"
  },
  {
    "full_name": "wrathematics/ssvm",
    "name": "ssvm",
    "description": "Simple SVM, with R bindings.",
    "language": "C",
    "topics": [],
    "readme": "# ssvm\n\n* **Version:** 0.1-0\n* **Status:** [![Build Status](https://travis-ci.org/wrathematics/ssvm.png)](https://travis-ci.org/wrathematics/ssvm)\n* **License:** [![License](http://img.shields.io/badge/license-BSD%202--Clause-orange.svg?style=flat)](http://opensource.org/licenses/BSD-2-Clause)\n* **Author:** Drew Schmidt\n\n\n**ssvm** is the Simple SVM package.  It has a 2-class linear SVM implementation using the Pegasos algorithm, written on my day off for fun.  The design philosophy for the project is:\n\n1. Use a clean C backend to handle the heavy lifting.\n2. The implementation is reasonably efficient.  One could work a bit harder to make this faster, but I think all of the \"easy\" gains have already been achieved.\n3. The R interface doesn't use formulas.  Oh my god do not get me started on fucking formulas.  If I could ban one thing from R, it would be formulas and I wouldn't even have to stop and think about it.\n4. The secret ingredient is love.\n\n\n## Installation\n\n<!--To install the R package, run:-->\n\n<!--```r-->\n<!--install.package(\"ssvm\")-->\n<!-- ``` -->\n\nThe development version is maintained on GitHub, and can easily be installed by any of the packages that offer installations from GitHub:\n\n```r\n### Pick your preference\ndevtools::install_github(\"wrathematics/ssvm\")\nghit::install_github(\"wrathematics/ssvm\")\nremotes::install_github(\"wrathematics/ssvm\")\n```\n\n\n## Package Use\n\n\n```r\nlibrary(ssvm)\n\ny <- recode(iris$Species == \"setosa\")\nx <- as.matrix(iris[, -5])\n\nset.seed(1234)\nmdl <- pegasos(x, y)\nmdl\n## SVM model (Method=Pegasos, Iterations=1e+05)\n##    Intercept Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##   -0.08139028  -0.12708726  -0.45953076   0.68090852   0.33553786  \n\nmdl <- pegasos(x, y, intercept=FALSE)\nmdl\n## SVM model (Method=Pegasos, Iterations=1e+05)\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##   -0.1358138   -0.4659208    0.6824105    0.3382079 \n\np <- predict(mdl, x)\nsum(which(p != y))\n## 0\n```\n",
    "url": "https://github.com/wrathematics/ssvm",
    "last_updated": "2025-03-22T11:20:27+00:00"
  },
  {
    "full_name": "milvus-io/milvus",
    "name": "milvus",
    "description": "Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search",
    "language": "Go",
    "topics": [
      "anns",
      "nearest-neighbor-search",
      "faiss",
      "vector-search",
      "image-search",
      "hnsw",
      "vector-database",
      "embedding-database",
      "embedding-store",
      "vector-store",
      "vector-similarity",
      "embedding-similarity",
      "distributed",
      "golang",
      "llm",
      "cloud-native",
      "diskann",
      "rag"
    ],
    "readme": "<img src=\"https://github.com/user-attachments/assets/51e33300-7f85-43ff-a05a-3a0317a961f3\" alt=\"milvus banner\">\n\n<div class=\"column\" align=\"middle\">\n  <a href=\"https://github.com/milvus-io/milvus/blob/master/LICENSE\"><img height=\"20\" src=\"https://img.shields.io/github/license/milvus-io/milvus\" alt=\"license\"/></a>\n  <a href=\"https://milvus.io/docs/install_standalone-docker.md\"><img src=\"https://img.shields.io/docker/pulls/milvusdb/milvus\" alt=\"docker-pull-count\"/></a>\n  <a href=\"https://milvus.io/docs/roadmap.md\"><img src=\"https://img.shields.io/badge/2025-roadmap-orange\" alt=\"fully-managed-milvus\"/></a>\n  <a href=\"https://cloud.zilliz.com/signup?utm_source=partner&utm_medium=referral&utm_campaign=2024-11-04_web_github-readme_global\"><img src=\"https://img.shields.io/badge/fully_managed-milvus-blue\" alt=\"fully-managed-milvus\"/></a>\n  <a href=\"https://milvus.io/docs/tutorials-overview.md\"><img src=\"https://img.shields.io/badge/tutorials-green\" alt=\"tutorials\"/></a>\n  <a href=\"https://discord.gg/mKc3R95yE5\"><img height=\"20\" src=\"https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white\" alt=\"discord\"/></a>\n  <a href=\"https://x.com/milvusio\"><img src=\"https://img.shields.io/twitter/follow/milvusio\" alt=\"twitter\"/></a>\n</div>\n\n## What is Milvus?\n\n🐦 [Milvus](https://milvus.io/) is a high-performance vector database built for scale. It powers AI applications by efficiently organizing and searching vast amounts of unstructured data, such as text, images, and multi-modal information.\n\n🧑‍💻 Written in Go and C++, Milvus implements hardware accelaration for CPU/GPU to achieve best-in-class vector search performance. Thanks to its [fully-distributed and K8s-native architecture](https://milvus.io/docs/overview.md#What-Makes-Milvus-so-Scalable), Milvus can scale horizontally, handle tens of thousands of search queries on billions of vectors, and keep data fresh with real-time streaming updates. Milvus also supports [Standalone mode](https:/",
    "url": "https://github.com/milvus-io/milvus",
    "last_updated": "2025-09-02T09:45:45+00:00"
  },
  {
    "full_name": "Solar-Helix-Independent-Transport/allianceauth-site-claim",
    "name": "allianceauth-site-claim",
    "description": "Site tools for Authbot.",
    "language": "Python",
    "topics": [],
    "readme": "# Site Claim\n\n## Installation install from pypi or git\n\n- Install the app and pre-reqs\n\n```\n    pip install allianceauth-site-claim\n\nor\n\n    pip install git+https://github.com/Solar-Helix-Independent-Transport/allianceauth-site-claim.git\n```\n\n- add `'siteclaim',`\n  and `'solo',` ( if its not already there )\n  to your local.py\n- migrate\n- restart auth and authbot\n\n## Settings\n\n| Setting                   | Default | What it does                    |\n| ------------------------- | ------- | ------------------------------- |\n| `SITE_CLAIM_ENABLE_SITES` | True    | Enable or disable the Sites Cog |\n| `SITE_CLAIM_ENABLE_ESS`   | True    | Enable or disable the ESS Cog   |\n",
    "url": "https://github.com/Solar-Helix-Independent-Transport/allianceauth-site-claim",
    "last_updated": "2025-08-25T17:12:21+00:00"
  },
  {
    "full_name": "rstudio/reticulate",
    "name": "reticulate",
    "description": "R Interface to Python",
    "language": "R",
    "topics": [],
    "readme": "# R Interface to Python\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/rstudio/reticulate/workflows/R-CMD-check/badge.svg)](https://github.com/rstudio/reticulate/actions)\n\n<!-- badges: end -->\n\nThe **reticulate** package provides a comprehensive set of tools for\ninteroperability between Python and R. The package includes facilities\nfor:\n\n<img src=\"man/figures/reticulated_python.png\" alt=\"reticulated python\" width=\"200\" align=\"right\" style=\"margin-left: 15px;\"/>\n\n-   Calling Python from R in a variety of ways including R Markdown,\n    sourcing Python scripts, importing Python modules, and using Python\n    interactively within an R session.\n\n-   Translation between R and Python objects (for example, between R and\n    Pandas data frames, or between R matrices and NumPy arrays).\n\n-   Flexible binding to different versions of Python including virtual\n    environments and Conda environments.\n\nReticulate embeds a Python session within your R session, enabling\nseamless, high-performance interoperability. If you are an R developer\nthat uses Python for some of your work or a member of data science team\nthat uses both languages, reticulate can dramatically streamline your\nworkflow!\n\n### Getting started\n\n#### Installation\n\nInstall the **reticulate** package from CRAN as follows:\n\n``` r\ninstall.packages(\"reticulate\")\n```\n\n#### Python version\n\nBy default, reticulate uses an isolated python virtual environment named \"r-reticulate\".\n\nThe `use_python()` function enables you to specify an alternate python,\nfor example:\n\n``` r\nlibrary(reticulate)\nuse_python(\"/usr/local/bin/python\")\n```\n\nThe `use_virtualenv()` and `use_condaenv()` functions enable you to\nspecify versions of Python in virtual or Conda environments, for\nexample:\n\n``` r\nlibrary(reticulate)\nuse_virtualenv(\"myenv\")\n```\n\nSee the article on [Python Version\nConfiguration](https://rstudio.github.io/reticulate/articles/versions.html)\nfor additional details.\n\n#### Python packages\n\nYou can install any required Python ",
    "url": "https://github.com/rstudio/reticulate",
    "last_updated": "2025-08-28T04:45:59+00:00"
  },
  {
    "full_name": "Y2Z/monolith",
    "name": "monolith",
    "description": "⬛️ CLI tool and library for saving complete web pages as a single HTML file",
    "language": "Rust",
    "topics": [
      "save-the-internet",
      "procrastination",
      "its-mine",
      "no-more-404",
      "tab-rehab",
      "e-hoarding",
      "make-the-internet-great-again",
      "come-and-take-it"
    ],
    "readme": "[![monolith build status on GNU/Linux](https://github.com/Y2Z/monolith/workflows/GNU%2FLinux/badge.svg)](https://github.com/Y2Z/monolith/actions?query=workflow%3AGNU%2FLinux)\n[![monolith build status on macOS](https://github.com/Y2Z/monolith/workflows/macOS/badge.svg)](https://github.com/Y2Z/monolith/actions?query=workflow%3AmacOS)\n[![monolith build status on Windows](https://github.com/Y2Z/monolith/workflows/Windows/badge.svg)](https://github.com/Y2Z/monolith/actions?query=workflow%3AWindows)\n[![Monolith Actor on Apify](https://apify.com/actor-badge?actor=snshn/monolith)](https://apify.com/snshn/monolith?fpr=snshn)\n\n\n```\n _____    _____________   __________     ___________________    ___\n|     \\  /             \\ |          |   |                   |  |   |\n|      \\/       __      \\|    __    |   |    ___     ___    |__|   |\n|              |  |          |  |   |   |   |   |   |   |          |\n|   |\\    /|   |__|          |__|   |___|   |   |   |   |    __    |\n|   | \\__/ |          |\\                    |   |   |   |   |  |   |\n|___|      |__________| \\___________________|   |___|   |___|  |___|\n```\n\nA data hoarder’s dream come true: bundle any web page into a single HTML file. You can finally replace that gazillion of open tabs with a gazillion of .html files stored somewhere on your precious little drive.\n\nUnlike the conventional “Save page as”, `monolith` not only saves the target document, it embeds CSS, image, and JavaScript assets **all at once**, producing a single HTML5 document that is a joy to store and share.\n\nIf compared to saving websites with `wget -mpk`, this tool embeds all assets as data URLs and therefore lets browsers render the saved page exactly the way it was on the Internet, even when no network connection is available.\n\n\n---------------------------------------------------\n\n\n## Installation\n\n#### Using [Cargo](https://crates.io/crates/monolith) (cross-platform)\n\n```console\ncargo install monolith\n```\n\n#### Via [Homebrew](https://formulae.brew.sh",
    "url": "https://github.com/Y2Z/monolith",
    "last_updated": "2025-09-02T00:54:46+00:00"
  },
  {
    "full_name": "somewacko/deconvfaces",
    "name": "deconvfaces",
    "description": "Generating faces with deconvolution networks",
    "language": "Python",
    "topics": [
      "deep-learning",
      "keras",
      "animation"
    ],
    "readme": "# Generating Faces with Deconvolution Networks\n\n![Example generations](img/example.gif)\n\nThis repo contains code to train and interface with a deconvolution network adapted from [this paper][Chairs] to generate faces using data from the [Radboud Faces Database][RaFD]. Requires [Keras][Keras], [NumPy][NumPy], [SciPy][SciPy], and [tqdm][tqdm] with Python 3 to use.\n\n## Training New Models\n\nTo train a new model, simply run:\n\n    python3 faces.py train path/to/data\n    \nYou can specify the number of deconvolution layers with `-d` to generate larger images, assuming your GPU has the memory for it. You can play with the batch size and the number of kernels per layer (using `-b` and `-k` respectively) until it fits in memory, although this may result in worse results or longer training.\n\nUsing 6 deconvolution layers with a batch size of 8 and the default number of kernels per layer, a model was trained on an Nvidia Titan X card (12 GB) to generate 512x640 images in a little over a day.\n\n## Generating Images\n\nTo generate images using a trained model, you can specify parameters in a yaml file and run:\n\n    python3 faces.py generate -m path/to/model -o output/directory -f path/to/params.yaml\n\nThere are four different modes you can use to generate images:\n\n* `single`, produce a single image.\n* `random`, produce a set of random images.\n* `drunk`, similar to random, but produces a more contiguous sequence of images.\n* `interpolate`, animate between a set of specified keyframes.\n\nYou can find examples of these files in the `params` directory, which should give you a good idea of how to format these and what's available.\n\n## Examples\n\nInterpolating between identities and emotions:\n\n[![Interpolating between identities and emotions](http://img.youtube.com/vi/UdTq_Q-WgTs/0.jpg)](https://www.youtube.com/watch?v=UdTq_Q-WgTs)\n\nInterpolating between orientations: (which the model is unable to learn)\n\n[![Interpolating between orientation](http://img.youtube.com/vi/F4OFkN3EURk/0.jpg)](https",
    "url": "https://github.com/somewacko/deconvfaces",
    "last_updated": "2025-07-20T00:41:01+00:00"
  },
  {
    "full_name": "orgoro/coverage",
    "name": "coverage",
    "description": "GitHub Action for python coverage publish & analysis",
    "language": "TypeScript",
    "topics": [
      "github-actions",
      "coverage",
      "python",
      "action",
      "typescript"
    ],
    "readme": "# Python Coverage: The Esential Coverage Reporter GitHub Action for python\n\n> ☂️ parse and publish coverage xml to a PR, enforce coverage rate on new & modified files\n\n## Usage\n\nCreate a new workflow `.yml` file in the `.github/workflows/` directory.\n\nYou can create a coverage report using python:\n - pytest `$ pytest --cov-report xml:path/to/coverage.xml`\n - coverage `$ coverage xml path/to/coverage.xml`\n\n### Minimal Configuration\n```yml\nname: 'coverage'\non:\n    pull_request:\n        branches:\n            - master\n            - main\njobs:\n    coverage:\n        runs-on: ubuntu-latest\n        steps:\n          - name: Get Cover \n            uses: orgoro/coverage@v3.2\n            with:\n                coverageFile: path/to/coverage.xml\n                token: ${{ secrets.GITHUB_TOKEN }}\n```\n## PR Message & Job Summary 🆕\n\n![message](./images/pr-message.png)\n\n## Inputs\n\n| Input               | Optional  | Description                                      | Example                |\n|---------------------|-----------|--------------------------------------------------|------------------------|\n| `coverageFile`      |           | path to .xml coverage report                     | ./path/to/coverage.xml |\n| `token`             |           | your github token                                | 🤫                     |\n| `thresholdAll`      | ✅        | the minimal average line coverage                | 0.8                    |\n| `thresholdNew`      | ✅        | the minimal average new files line coverage      | 0.9                    |\n| `thresholdModified` | ✅        | the minimal average modified files line coverage | 0.0                    |\n| `passIcon`          | ✅        | the indicator to use for files that passed       | 🟢                      |\n| `failIcon`          | ✅        | the indicator to use for files that failed       | 🔴                      |\n| `sourceDir`         | ✅        | the directory to use as the source of the coverage report       | ./path/to/src        ",
    "url": "https://github.com/orgoro/coverage",
    "last_updated": "2025-09-01T13:44:11+00:00"
  },
  {
    "full_name": "gojiplus/search-and-replace",
    "name": "search-and-replace",
    "description": "Edit Distance Based Search and Replace",
    "language": "Python",
    "topics": [],
    "readme": "### Turbo Search and Replace \n\n[![Build Status](https://travis-ci.org/soodoku/search-and-replace.svg?branch=master)](https://travis-ci.org/soodoku/search-and-replace)\n[![Build status](https://ci.appveyor.com/api/projects/status/dd6weascqvw4wg5o?svg=true)](https://ci.appveyor.com/project/soodoku/search-and-replace)\n\n#### Functionality\n\n1. Removes extra blank lines.  \n2. Removes soft-hyphens followed by new line (this typically means multi-line words).  \n3. Searches and replaces a list of words:   \n   The script takes a csv ([replacelist.csv](replacelist.csv)) that carries words to be replaced, and replacement words.  \n4. Regular expression based replacement: \n   * Allows for 0-X consecutive errors within a word.  \n   * Takes [wordlist.csv](wordlist.csv) that carries words and X for each word    \n   * For instance if a row in wordlist.csv reads: Available,1    \n   * Av.{0,1}\\??[\\r\\n]*ilable ==> Available    \n   * Ava.{0,1}\\??[\\r\\n]*lable ==> Available \n\n### Installation\n\nClone this repository:\n\n`git clone https://github.com/soodoku/search-and-replace.git`\n\nNavigate to search-and-replace\n\nRun `python setup.py install`\n\n#### Running the script \n\nThe script expects the following two files in the same directory:  \n1. replacelist.csv -- carries word pairs (original_word, replace_with_this_word). (Sample [replacelist.csv](replacelist.csv).)  \n2. wordlist.csv -- carries the correct word, and number of consecutive errors tolerated. All the variously misspelled words will be replaced with the correct word. (Sample [wordlist.csv](wordlist.csv).)  \n\n#### Usage\n`postprocess.py [options] source_txt_directory`\n\n#### Command Line Options\n```\nOptions:\n  -h, --help            show this help message and exit\n  -o OUTDIR, --outdir=OUTDIR\n                        Text output directory (default: postprocessed)\n  -r, --resume          Resume postprocessing (Skip if existing) (default:\n                        False)\n```\n\n### Example:\n`python postprocess.py txt_dir`\n\nThe script will be post p",
    "url": "https://github.com/gojiplus/search-and-replace",
    "last_updated": "2025-04-16T22:11:15+00:00"
  },
  {
    "full_name": "eddelbuettel/asioheaders",
    "name": "asioheaders",
    "description": "R package providing Asio C++ library header files",
    "language": "C++",
    "topics": [],
    "readme": "## AsioHeaders: Asio Headers for R\n\n[![CI](https://github.com/eddelbuettel/asioheaders/workflows/ci/badge.svg)](https://github.com/eddelbuettel/asioheaders/actions?query=workflow%3Aci)\n[![License](https://img.shields.io/badge/license-BSL--1.0-brightgreen.svg?style=flat)](https://www.boost.org/users/license.html)\n[![CRAN](https://www.r-pkg.org/badges/version/AsioHeaders)](https://cran.r-project.org/package=AsioHeaders)\n[![r-universe](https://eddelbuettel.r-universe.dev/badges/AsioHeaders)](https://eddelbuettel.r-universe.dev/AsioHeaders)\n[![Dependencies](https://tinyverse.netlify.app/badge/AsioHeaders)](https://cran.r-project.org/package=AsioHeaders)\n[![Downloads](https://cranlogs.r-pkg.org/badges/AsioHeaders?color=brightgreen)](https://cran.r-project.org/package=AsioHeaders)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/asioheaders)](https://github.com/eddelbuettel/asioheaders)\n\n### About\n\nThis package provides [R](https://www.r-project.org) with access to\n[Asio](https://think-async.com/Asio/) header files.  [Asio](https://think-async.com/Asio/) \nprovides a cross-platform C++ library for network and low-level I/O\nprogramming. It is also included in [Boost](https://www.boost.org/) -- but\nrequires linking when used as part of [Boost](https://www.boost.org/). This\nstandalone version of [Asio](https://think-async.com/Asio/) is a header-only C++ library\nwhich can be used without linking (just like our [BH](http://dirk.eddelbuettel.com/code/bh.html)\npackage with parts of [Boost](https://www.boost.org/)).\n\nBy providing the [Asio](https://think-async.com/Asio/) library in this package, we\noffer a more efficient distribution system for [CRAN](https://cran.r-project.org) \nas replication of this code in the sources of other packages is avoided.\n\nTo use it, simply add it to the `LinkingTo:` field in the `DESCRIPTION` field of your R\npackage---and the R package infrastructure tools will then know how to set\ninclude flags correctly on all architectures su",
    "url": "https://github.com/eddelbuettel/asioheaders",
    "last_updated": "2025-06-18T13:15:18+00:00"
  },
  {
    "full_name": "reflex-dev/reflex",
    "name": "reflex",
    "description": "🕸️ Web apps in pure Python 🐍",
    "language": "Python",
    "topics": [
      "python",
      "framework",
      "open-source",
      "gui",
      "web"
    ],
    "readme": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg\" alt=\"Reflex Logo\" width=\"300px\">\n\n<hr>\n\n### **✨ Performant, customizable web apps in pure Python. Deploy in seconds. ✨**\n\n[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)\n![versions](https://img.shields.io/pypi/pyversions/reflex.svg)\n[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)\n[![PyPI Downloads](https://static.pepy.tech/badge/reflex)](https://pepy.tech/projects/reflex)\n[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&label=Discord)](https://discord.gg/T5WSbC2YtQ)\n[![Twitter](https://img.shields.io/twitter/follow/getreflex)](https://x.com/getreflex)\n\n</div>\n\n---\n\n[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [简体中文](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [繁體中文](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [Türkçe](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [हिंदी](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [Português (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [Español](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [한국어](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md) | [日本語](https://github.com/reflex-dev/reflex/blob/main/docs/ja/README.md) | [Deutsch](https://github.com/reflex-dev/reflex/blob/main/docs/de/README.md) | [Persian (پارسی)](https://github.com/reflex-dev/reflex/blob/main/docs/pe/README.md) | [Tiếng Việt](https://github.com/reflex-dev/reflex/blob/main/docs/vi/README.md)\n\n---\n\n> [!NOTE]\n> 🚀 **Try [Reflex Build](https://build.reflex.dev/)** – our AI-powered app builder that generates ful",
    "url": "https://github.com/reflex-dev/reflex",
    "last_updated": "2025-09-02T08:08:33+00:00"
  },
  {
    "full_name": "marknagelberg/mr-scraper",
    "name": "mr-scraper",
    "description": "An iPython notebook analyzing the data resulting from scraping Marginal Revolution blog (www.marginalrevolution.com)",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Marginal Revolution Scraped Blog Post Data Analysis\n\nThis repo is simply a Jupyter Notebook analyzing data from blog posts scraped\nfrom [Marginal Revolution](www.marginalrevolution.com). In March 2018, I scraped\nthe website to get all of the posts, including comments. I'm going to see how\nmuch intelligence I can gather through the posts using as many tools at my\ndisposal as I can. \n\nYou can find the corresponding blog series [here](http://www.marknagelberg.com/lets-scrape-a-blog-part-1/).\n",
    "url": "https://github.com/marknagelberg/mr-scraper",
    "last_updated": "2018-05-19T20:08:51+00:00"
  },
  {
    "full_name": "stewid/gix",
    "name": "gix",
    "description": "An R package to search a Git repository",
    "language": "R",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/stewid/gix.png)](https://travis-ci.org/stewid/gix)\n\n# Introduction\n\n`gix` is an R package that aims to index and search a Git repository\nusing the [Xapian](http://xapian.org/) search engine.\n\n## Index a repository\n\nIndex the content of a git repository to documents with the Xapian\nsearch engine. A `document` is the data returned from a search. The\nindexing creates a database located in the `.xapian` folder in the\nrepository.\n\n\n```r\nlibrary(gix)\n\n## Index the 'gix' repository. (Current directory)\ngixit(\".\", github = \"stewid/gix\")\n```\n\n## Search a repository\n\n\n```r\n## Search for author Widgren and display url to blob on GitHub\ngix(\".\", \"author:widgren\")$url\n```\n\n```\n##  [1] \"https://github.com/stewid/gix/blob/c6fc2f018ee81cf973495dc359995cdac6fb4641/DESCRIPTION\"\n##  [2] \"https://github.com/stewid/gix/blob/54f3f6f422a352184caf115dabe8567ed7295fd0/.Rbuildignore\"\n##  [3] \"https://github.com/stewid/gix/blob/54f3f6f422a352184caf115dabe8567ed7295fd0/NAMESPACE\"\n##  [4] \"https://github.com/stewid/gix/blob/54f3f6f422a352184caf115dabe8567ed7295fd0/NEWS\"\n##  [5] \"https://github.com/stewid/gix/blob/54f3f6f422a352184caf115dabe8567ed7295fd0/README.md\"\n##  [6] \"https://github.com/stewid/gix/blob/54f3f6f422a352184caf115dabe8567ed7295fd0/man/gix.Rd\"\n##  [7] \"https://github.com/stewid/gix/blob/8906a4a3ddbb7cd29e33d47b37cdbf0f5ab70c2a/DESCRIPTION\"\n##  [8] \"https://github.com/stewid/gix/blob/a489b0a4272be35043967b42e5caa6ed1b66ba26/.gitignore\"\n##  [9] \"https://github.com/stewid/gix/blob/eca0aa4931fd0d6547841a7737fd3f4ec728f9d9/man/gixit-methods.Rd\"\n## [10] \"https://github.com/stewid/gix/blob/7e5bac365cad12fc5a5e9cbd8f4030bb0ebb3031/DESCRIPTION\"\n```\n\n\n```r\n## Search for sha that starts with 'd7'\ngix(\".\", \"sha:d7*\")[, c(\"sha\", \"url\")]\n```\n\n```\n##                                        sha\n## 1 d7f105139782ab695d86613e343916f7372f4ac0\n##                                                                                   url\n## 1 https://github.com/stewid",
    "url": "https://github.com/stewid/gix",
    "last_updated": "2023-01-27T23:25:19+00:00"
  },
  {
    "full_name": "hrbrmstr/waffle",
    "name": "waffle",
    "description": ":maple_leaf: Make waffle (square pie) charts in R",
    "language": "R",
    "topics": [
      "r",
      "waffle-charts",
      "square-pie-charts",
      "rstats",
      "ggplot2",
      "datavisualization",
      "data-visualisation",
      "data-visualization"
    ],
    "readme": "\n# 🧇 waffle\n\nCreate Waffle Chart Visualizations\n\n## Description\n\nSquare pie charts (a.k.a. waffle charts) can be used to communicate\nparts of a whole for categorical quantities. To emulate the percentage\nview of a pie chart, a 10x10 grid should be used with each square\nrepresenting 1% of the total. Modern uses of waffle charts do not\nnecessarily adhere to this rule and can be created with a grid of any\nrectangular shape. Best practices suggest keeping the number of\ncategories small, just as should be done when creating pie charts. Tools\nare provided to create waffle charts as well as stitch them together,\nand to use glyphs for making isotype pictograms.\n\nIt uses ggplot2 and returns a ggplot2 object.\n\n## What’s Inside the Tin\n\nThe following functions are implemented:\n\n- `waffle`: Make waffle (square pie) charts\n\n- `draw_key_pictogram`: Legend builder for pictograms\n\n- `fa_grep`: Search Font Awesome glyph names for a pattern\n\n- `fa_list`: List all Font Awesome glyphs\n\n- `fa5_brand`: Font Awesome 5 Brand\n\n- `fa5_solid`: Font Awesome 5 Solid\n\n- `geom_pictogram`: Pictogram Geom\n\n- `geom_waffle`: Waffle (Square pie chart) Geom\n\n- `install_fa_fonts`: Install Font Awesome 5 Fonts\n\n- `iron`: Veritical, left-aligned layout for waffle plots\n\n- `scale_label_pictogram`: Used with geom_pictogram() to map Font\n  Awesome fonts to labels\n\n- `theme_enhance_waffle`: Waffle chart theme cruft remover that can be\n  used with any other theme\n\n## Installation\n\n``` r\ninstall.packages(\"waffle\") # NOTE: CRAN version is 0.7.0\n# or\nremotes::install_github(\"hrbrmstr/waffle\")\n```\n\nNOTE: To use the ‘remotes’ install options you will need to have the\n[{remotes} package](https://github.com/r-lib/remotes) installed.\n\n## Usage\n\n``` r\nlibrary(waffle)\nlibrary(magrittr)\nlibrary(hrbrthemes)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(waffle)\n\n# current verison\npackageVersion(\"waffle\")\n## [1] '1.0.2'\n```\n\n### Some new bits up first\n\n``` r\ndata.frame(\n  parts = factor(rep(month.abb[1:3], 3), levels=month.abb[1:",
    "url": "https://github.com/hrbrmstr/waffle",
    "last_updated": "2025-08-27T15:59:56+00:00"
  },
  {
    "full_name": "ZhangJianAo/safe-exit",
    "name": "safe-exit",
    "description": "Exit process safety",
    "language": "Python",
    "topics": [],
    "readme": "================\nSafe Exit\n================\n\nSafe Exit is a Python package that provides functionality to handle graceful process termination.\nThe package allows users to register functions that will be called when the program exits.\n\nDifference from atexit\n========================\n\nPython has a standard module called ``atexit`` that does something similar,\nbut ``atexit`` cannot handle cases where a program is killed by a signal not handled by Python.\n\nPython only handles the SIGINT signal and does not handle SIGTERM, SIGQUIT, and SIGHUP signals.\nOn Windows, programs can also be killed by SIGBREAK and CTRL_CLOSE_EVENT.\n\nSafe Exit can handle all these signals:\n\n* On POSIX systems: ``SIGINT``, ``SIGTERM``, ``SIGQUIT``, and ``SIGHUP``\n* On Windows:\n\n  - ``SIGINT``, ``SIGTERM``, ``SIGBREAK``\n\n  - ``CTRL_CLOSE_EVENT``, ``CTRL_LOGOFF_EVENT``, ``CTRL_SHUTDOWN_EVENT``\n\nWindows also has ``CTRL_C_EVENT`` and ``CTRL_BREAK_EVENT``\nwhich Python translate to ``SIGINT`` and ``SIGBREAK`` signals, respectively.\nOn windows, ``SIGTERM`` is implemented only  for the current process,\nthere is no way to send ``SIGTERM`` to other processes.\n\nInstallation\n============\n\nTo install Safe Exit, simply run:\n\n.. code-block:: bash\n\n    pip install safe-exit\n\nUsage\n=====\n\nJust register a cleanup function like you would with `atexit`:\n\n.. code-block:: python\n\n    import safe_exit\n\n    def cleanup_function():\n        # Perform cleanup tasks\n\n    safe_exit.register(cleanup_function)\n\nThe ``register`` function can also be used as a decorator:\n\n.. code-block:: python\n\n    @safe_exit.register\n    def cleanup_function():\n        # Perform cleanup tasks\n\nSignal handling is configurable.\nCall the ``config`` function before registering functions.\nThe following code configures ``safe_exit`` to handle SIGQUIT and SIGHUP signals:\n\n.. code-block:: python\n\n    from safe_exit import ConfigFlag, config, register\n    config(ConfigFlag.SIGQUIT | ConfigFlag.SIGHUP)\n\n    @register\n    def cleanup()\n        print(\"clea",
    "url": "https://github.com/ZhangJianAo/safe-exit",
    "last_updated": "2025-03-25T09:08:33+00:00"
  },
  {
    "full_name": "appeler/naampy",
    "name": "naampy",
    "description": "Infer Sociodemographic Characteristics from Names Using Indian Electoral Rolls",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# naampy: Infer Sociodemographic Characteristics from Indian Names\n\n[![image](https://github.com/appeler/naampy/actions/workflows/test.yml/badge.svg)](https://github.com/appeler/naampy/actions/workflows/test.yml)\n[![image](https://img.shields.io/pypi/v/naampy.svg)](https://pypi.python.org/pypi/naampy)\n[![image](https://readthedocs.org/projects/naampy/badge/?version=latest)](http://naampy.readthedocs.io/en/latest/?badge=latest)\n[![image](https://static.pepy.tech/badge/naampy)](https://pepy.tech/project/naampy)\n\nThe ability to programmatically and reliably infer the social attributes\nof a person from their name can be useful for a broad set of tasks, from\nestimating bias in coverage of women in the media to estimating bias in\nlending against certain social groups. But unlike the American Census\nBureau, which produces a list of last names and first names, which can\n(and are) used to infer the gender, race, ethnicity, etc., from names,\nthe Indian government produces no such commensurate datasets. Hence\ninferring the relationship between gender, ethnicity, language group,\netc., and names has generally been done with small datasets constructed\nin an ad-hoc manner.\n\nWe fill this yawning gap. Using data from the [Indian Electoral\nRolls](https://github.com/in-rolls/electoral_rolls) (parsed data\n[here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/MUEGDT)),\nwe estimate the proportion female, male, and [third sex]{.title-ref}\n(see [here](https://en.wikipedia.org/wiki/Third_gender)) for a\nparticular [first name, year, and state.]{.title-ref}\n\nPlease also check out [pranaam](https://github.com/appeler/pranaam) that\nuses land record data from Bihar to infer religion based on the name.\nThe package uses [indicate](https://github.com/in-rolls/indicate) to\ntransliterate Hindi to English.\n\n### Streamlit App\n\n<https://naampy.streamlit.app/>\n\n## Data\n\nIn all, we capitalize on information in the parsed electoral rolls from\nthe following 31 states and union terr",
    "url": "https://github.com/appeler/naampy",
    "last_updated": "2025-04-27T05:14:54+00:00"
  },
  {
    "full_name": "replicate/cog",
    "name": "cog",
    "description": "Containers for machine learning",
    "language": "Go",
    "topics": [
      "containers",
      "cuda",
      "deep-learning",
      "docker",
      "machine-learning",
      "pytorch",
      "tensorflow",
      "ai"
    ],
    "readme": "# Cog: Containers for machine learning\n\nCog is an open-source tool that lets you package machine learning models in a standard, production-ready container.\n\nYou can deploy your packaged model to your own infrastructure, or to [Replicate](https://replicate.com/).\n\n## Highlights\n\n- 📦 **Docker containers without the pain.** Writing your own `Dockerfile` can be a bewildering process. With Cog, you define your environment with a [simple configuration file](#how-it-works) and it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.\n\n- 🤬️ **No more CUDA hell.** Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.\n\n- ✅ **Define the inputs and outputs for your model with standard Python.** Then, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.\n\n- 🎁 **Automatic HTTP prediction server**: Your model's types are used to dynamically generate a RESTful HTTP API using [FastAPI](https://fastapi.tiangolo.com/).\n\n- 🥞 **Automatic queue worker.** Long-running deep learning models or batch processing is best architected with a queue. Cog models do this out of the box. Redis is currently supported, with more in the pipeline.\n\n- ☁️ **Cloud storage.** Files can be read and written directly to Amazon S3 and Google Cloud Storage. (Coming soon.)\n\n- 🚀 **Ready for production.** Deploy your model anywhere that Docker images run. Your own infrastructure, or [Replicate](https://replicate.com).\n\n## How it works\n\nDefine the Docker environment your model runs in with `cog.yaml`:\n\n```yaml\nbuild:\n  gpu: true\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n  python_version: \"3.12\"\n  python_packages:\n    - \"torch==2.3\"\npredict: \"predict.py:Predictor\"\n```\n\nDefine how predictions are run on your model with `predict.py`:\n\n```python\nfrom cog import BasePre",
    "url": "https://github.com/replicate/cog",
    "last_updated": "2025-09-02T09:18:32+00:00"
  },
  {
    "full_name": "TheUpshot/statement",
    "name": "statement",
    "description": "A Ruby gem that extracts press releases and statements by members of Congress.",
    "language": "HTML",
    "topics": [],
    "readme": "# Statement\n\nStatement parses RSS feeds and HTML pages containing press releases and other official statements from members of Congress, and produces hashes with information about those pages. It has been tested under Ruby 1.9.3 and 2.x.\n\n## Coverage\n\nStatement currently parses press releases for members of the House and Senate. For members with RSS feeds, you can pass the feed URL into Statement. For members without RSS feeds (or with broken ones), HTML scrapers are provided, as are methods for special groups, such as House Republicans. Suggestions are welcomed.\n\n## Installation\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem 'statement'\n```\n\nAnd then execute:\n\n```sh\n$ bundle\n```\n\nOr install it yourself as:\n\n```sh\n$ gem install statement\n```\n\n## Usage\n\nStatement provides access to press releases, Facebook status updates and tweets from members of Congress. Most congressional offices have RSS feeds but some require HTML scraping.\n\nTo configure Statement to pull from the Twitter and Facebook APIs, you can pass in configuration values via a hash or a `config.yml` file:\n\n```ruby\nrequire 'rubygems'\nrequire 'statement'\nStatement.configure(:oauth_token => token, :oauth_token_secret => secret, ...) # option 1\nStatement.configure_with(\"config.yml\") # option 2\n```\n\nIf you don't need to use the Twitter or Facebook APIs, you don't need to setup configuration.\n\n### Press Releases\n\nTo parse an RSS feed, simply pass the URL to Statement's Feed class:\n\n```ruby\nrequire 'rubygems'\nrequire 'statement'\n\nresults = Statement::Feed.from_rss('http://blumenauer.house.gov/index.php?option=com_bca-rss-syndicator&feed_id=1')\nputs results.first\n{:source=>\"http://blumenauer.house.gov/index.php?option=com_bca-rss-syndicator&feed_id=1\", :url=>\"http://blumenauer.house.gov/index.php?option=com_content&amp;view=article&amp;id=2203:blumenauer-qwe-need-a-national-system-that-speaks-to-the-transportation-challenges-of-todayq&amp;catid=66:2013-press-releases\", :title=>\"Blumenauer: &quot;We ne",
    "url": "https://github.com/TheUpshot/statement",
    "last_updated": "2024-07-03T15:05:37+00:00"
  },
  {
    "full_name": "kennethreitz/maya",
    "name": "maya",
    "description": "Datetimes for Humans™",
    "language": "Python",
    "topics": [
      "datetimes",
      "dates",
      "times",
      "date",
      "time",
      "python",
      "forhumans",
      "kennethreitz",
      "parsing"
    ],
    "readme": "Maya: Datetimes for Humans™\n===========================\n\n.. image:: https://img.shields.io/pypi/v/maya.svg\n    :target: https://pypi.python.org/pypi/maya\n\n.. image:: https://github.com/timofurrer/maya/workflows/Continuous%20Integration%20and%20Deployment/badge.svg\n    :target: https://github.com/timofurrer/maya/actions\n\n\nDatetimes are very frustrating to work with in Python, especially when dealing\nwith different locales on different systems. This library exists to make the\nsimple things **much** easier, while admitting that time is an illusion\n(timezones doubly so).\n\nDatetimes should be interacted with via an API written for humans.\n\nMaya is mostly built around the headaches and use-cases around parsing datetime data from websites.\n\n\n☤ Basic Usage of Maya\n---------------------\n\nBehold, datetimes for humans!\n\n.. code-block:: pycon\n\n    >>> now = maya.now()\n    <MayaDT epoch=1481850660.9>\n\n    >>> tomorrow = maya.when('tomorrow')\n    <MayaDT epoch=1481919067.23>\n\n    >>> tomorrow.slang_date()\n    'tomorrow'\n\n    >>> tomorrow.slang_time()\n    '23 hours from now'\n\n    # Also: MayaDT.from_iso8601(...)\n    >>> tomorrow.iso8601()\n    '2017-02-10T22:17:01.445418Z'\n\n    # Also: MayaDT.from_rfc2822(...)\n    >>> tomorrow.rfc2822()\n    'Fri, 10 Feb 2017 22:17:01 GMT'\n\n    # Also: MayaDT.from_rfc3339(...)\n    >>> tomorrow.rfc3339()\n    '2017-02-10T22:17:01.44Z'\n\n    >>> tomorrow.datetime()\n    datetime.datetime(2016, 12, 16, 15, 11, 30, 263350, tzinfo=<UTC>)\n\n    # Automatically parse datetime strings and generate naive datetimes.\n    >>> scraped = '2016-12-16 18:23:45.423992+00:00'\n    >>> maya.parse(scraped).datetime(to_timezone='US/Eastern', naive=True)\n    datetime.datetime(2016, 12, 16, 13, 23, 45, 423992)\n\n    >>> rand_day = maya.when('2011-02-07', timezone='US/Eastern')\n    <MayaDT epoch=1297036800.0>\n\n    # Maya speaks Python.\n    >>> m = maya.MayaDT.from_datetime(datetime.utcnow())\n    >>> print(m)\n    Wed, 20 Sep 2017 17:24:32 GMT\n\n    >>> m = maya.MayaDT.from_struct(",
    "url": "https://github.com/kennethreitz/maya",
    "last_updated": "2025-08-28T07:06:29+00:00"
  },
  {
    "full_name": "dbamman/characterRelations",
    "name": "characterRelations",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "# characterRelations\n\nThis repository contains 2,170 annotations of character relations in 109 literary texts, as documented in `characterRelations.pdf`.  Each annotation describes a character dyad along four dimensions of interest: **coarse-grained category** (social, familial, professional), **fine-grained category** (e.g., friend, lover, parent, rival, employer), and **affinity** (positive, negative, neutral).  Additionally, we do not assume that this relationship is static; we also collect judgments as to whether it **changes** at any point in the course of the text.\n\n`character_relation_annotations.txt` contains the full set of 2,170 annotations from Amazon Mechanical Turk (generally 20 annotations per text), without any adjudication of disagreements or filtering. Each annotation is paired with an (anonymized) identifier of the annotator who provided it.\n\nThis work is described in the following paper:\n\nPhilip Massey, Patrick Xia, David Bamman and Noah A. Smith (2015), Annotating Character Relations in Literary Texts, ArXiv.\n\n\n## Relation classes\n\n### Categories\n\n- Social\n    - friend\n    - enemy\n    - acquaintance\n    - lovers\n    - unrequited love interest (X is in love with Y, but Y is not in love with X)\n    - rivals\n- Professional\n    - employer\n    - employee\n    - colleague\n    - servant\n    - master\n    - student\n    - teacher\n    - client\n    - person offering service to client (e.g., lawyer)\n- Familial\n    - husband/wife\n    - brother/sister\n    - cousin\n    - uncle/aunt\n    - niece/nephew\n    - child\n    - parent\n    - grandchild\n    - grandparent\n    - orphan\n    - foster parent\n    - step-child\n    - step-parent\n    - in-law relation (e.g., mother-in-law)\n    - half relation (e.g., half-sister)\n\n### Affinity\n- Positive\n- Negative\n- Neutral\n\n\n## Texts\n\n|Author|Title|\n|----|----|\n|Alexandre Dumas|The Count of Monte Cristo|\n|Alexandre Dumas|The Three Musketeers|\n|Aristophanes|Lysistrata|\n|Bram Stoker|Dracula|\n|Charles Dickens|A Tale of Two Cities|\n|Cha",
    "url": "https://github.com/dbamman/characterRelations",
    "last_updated": "2025-04-18T12:36:11+00:00"
  },
  {
    "full_name": "ishanagr/ethnicity",
    "name": "ethnicity",
    "description": "Determines the ethnicity based on your last name",
    "language": "Python",
    "topics": [],
    "readme": "",
    "url": "https://github.com/ishanagr/ethnicity",
    "last_updated": "2017-12-06T08:29:59+00:00"
  },
  {
    "full_name": "rticulate/import",
    "name": "import",
    "description": "An Import Mechanism For R",
    "language": "R",
    "topics": [
      "r",
      "cran"
    ],
    "readme": "\n# import <a href=\"https://import.rticulate.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/import)](https://CRAN.R-project.org/package=import)\n[![CRAN status\nshields](https://img.shields.io/badge/Git-1.3.2.9001-success)](https://github.com/rticulate/import)\n[![R build\nstatus](https://github.com/rticulate/import/workflows/R-CMD-check/badge.svg)](https://github.com/rticulate/import/actions)\n<!-- badges: end -->\n\n# An Import Mechanism For R\n\nThe import package is intended to simplify the way in which functions\nfrom external packages or modules are made available for use in R\nscripts. Learn more on the [package\nwebsite](https://import.rticulate.org/), by reading\n[`vignette(\"import\")`](https://import.rticulate.org/articles/import.html),\nor using the help (`?import::from`).\n\n## Introduction\n\nThe typical way of using functionality exposed by a package in R scripts\nis to load (and attach) the entire package with `library()` (or\n`require()`). This can have the **undesirable effect of masking\nobjects** in the user’s search path and can also make it difficult and\n**confusing to identify** what functionality comes from which package\nwhen using several `library` statements.\n\nThe `import` package provides a simple alternative, allowing the user\nspecify in a concise way exactly which objects. For example, the `Hmisc`\npackage exposes over four hundred functions. Instead of exposing all of\nthose functions, someone who only needs access to, say the `impute()`\nand the `nomiss()` functions, can import those functions only:\n\n``` r\nimport::from(Hmisc, impute, nomiss)\n```\n\nFor more on the motivation behind the package, see\n[vignette(“import”)](https://import.rticulate.org/articles/import.html)\n\n## Installation\n\nInstall the release version of `import` from CRAN using `pak` or\n`install.packages()`:\n\n``` r\npak::pak(\"import\")\n  # or\ninstall.packages(\"import\")\n```\n\nInstall the development vers",
    "url": "https://github.com/rticulate/import",
    "last_updated": "2025-07-05T19:54:09+00:00"
  },
  {
    "full_name": "kaneplusplus/ioregression",
    "name": "ioregression",
    "description": "An R package for out-of-core regressions",
    "language": "R",
    "topics": [],
    "readme": "[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/ioregression)](http://cran.r-project.org/package=ioregression)\n[![Build Status](https://travis-ci.org/kaneplusplus/ioregression.png)](https://travis-ci.org/kaneplusplus/ioregression)\n[![Build status](https://ci.appveyor.com/api/projects/status/how1lupbnrkmbeun/branch/master?svg=true)](https://ci.appveyor.com/project/kaneplusplus/ioregression/branch/master)\n[![Coverage Status](https://coveralls.io/repos/kaneplusplus/ioregression/badge.svg?branch=master&service=github)](https://coveralls.io/github/kaneplusplus/ioregression?branch=master)\n\n",
    "url": "https://github.com/kaneplusplus/ioregression",
    "last_updated": "2020-07-14T19:20:31+00:00"
  },
  {
    "full_name": "ropensci/textreuse",
    "name": "textreuse",
    "description": "Detect text reuse and document similarity",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-package",
      "peer-reviewed"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# textreuse\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/textreuse)](https://cran.r-project.org/package=textreuse)\n[![CRAN\\_Downloads](http://cranlogs.r-pkg.org/badges/grand-total/textreuse)](https://cran.r-project.org/package=textreuse)\n[![Build\nStatus](https://travis-ci.org/ropensci/textreuse.svg?branch=master)](https://travis-ci.org/ropensci/textreuse)\n[![Build\nstatus](https://ci.appveyor.com/api/projects/status/9qwf0473xi8cyuoh/branch/master?svg=true)](https://ci.appveyor.com/project/lmullen/textreuse-6xljc/branch/master)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/ropensci/textreuse/master.svg)](https://codecov.io/github/ropensci/textreuse?branch=master)\n[![rOpenSci\nbadge](https://badges.ropensci.org/20_status.svg)](https://github.com/ropensci/onboarding/issues/20)\n\n## Overview\n\nThis [R](https://www.r-project.org/) package provides a set of functions\nfor measuring similarity among documents and detecting passages which\nhave been reused. It implements shingled n-gram, skip n-gram, and other\ntokenizers; similarity/dissimilarity functions; pairwise comparisons;\nminhash and locality sensitive hashing algorithms; and a version of the\nSmith-Waterman local alignment algorithm suitable for natural language.\nIt is broadly useful for, for example, detecting duplicate documents in\na corpus prior to text analysis, or for identifying borrowed passages\nbetween texts. The classes provides by this package follow the model of\nother natural language processing packages for R, especially the\n[NLP](https://cran.r-project.org/package=NLP) and\n[tm](https://cran.r-project.org/package=tm) packages. (However, this\npackage has no dependency on Java, which should make it easier to\ninstall.)\n\n### Citation\n\nIf you use this package for scholarly research, I would appreciate a\ncitation.\n\n    citation(\"textreuse\")\n    #> To cite package 'textreuse' in publications use:\n    #> \n    #>   Li Y",
    "url": "https://github.com/ropensci/textreuse",
    "last_updated": "2025-07-12T04:48:07+00:00"
  },
  {
    "full_name": "trinker/topicmodels_learning",
    "name": "topicmodels_learning",
    "description": "A repository of learning & R resources related to topic models ",
    "language": "R",
    "topics": [],
    "readme": "Topic Models Learning and R Resources [![Follow](https://img.shields.io/twitter/follow/tylerrinker.svg?style=social)](https://twitter.com/intent/follow?screen_name=tylerrinker)\n============\n\n\nThis is a collection documenting the resources I find related to topic\nmodels with an R flavored focus. A *topic model* is a type of\n[*generative*](http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm)\nmodel used to \"discover\" latent topics that compose a *corpus* or\ncollection of documents. Typically topic modeling is used on a\ncollection of text documents but can be used for other modes including\nuse as caption generation for images.\n\n![](inst/figure/topic-model.jpg)\n\n\nTable of Contents\n============\n\n-   [Just the Essentials](#just-the-essentials)\n-   [Key Players](#key-players)\n-   [Videos](#videos)\n    -   [Introductory](#introductory)\n    -   [Theory](#theory)\n    -   [Visualization](#visualization)\n-   [Articles](#articles)\n    -   [Applied](#applied)\n    -   [Theoretical](#theoretical)\n-   [Websites & Blogs](#websites--blogs)\n-   [R Resources](#r-resources)\n    -   [Package Comparisons](#package-comparisons)\n    -   [R Specific References](#r-specific-references)\n    -   [Example Modeling](#example-modeling)\n-   [Topic Modeling R Demo](#topic-modeling-r-demo)\n    -   [topicmodels Package](#topicmodels-package)\n-   [Contributing](#contributing)\n\nJust the Essentials\n============\n\n\nThis is my run down of the minimal readings, websites, videos, & scripts\nthe reader needs to become familiar with topic modeling. The list is in\nan order I believe will be of greatest use and contains a nice mix of\nintroduction, theory, application, and interpretation. As you want to\nlearn more about topic modeling, the other sections will become more\nuseful.\n\n1.  Boyd-Graber, J. (2013). [Computational Linguistics I: Topic\n    Modeling](https://www.youtube.com/watch?v=4p9MSJy761Y)  \n2.  Underwood, T. (2012). [Topic Modeling Made Just S",
    "url": "https://github.com/trinker/topicmodels_learning",
    "last_updated": "2025-08-17T05:52:54+00:00"
  },
  {
    "full_name": "tidyverse/reprex",
    "name": "reprex",
    "description": "Render bits of R code for sharing, e.g., on GitHub or StackOverflow.",
    "language": "R",
    "topics": [
      "r",
      "rmarkdown",
      "stackoverflow",
      "github",
      "reproducibility"
    ],
    "readme": "# reprex <a href=\"https://reprex.tidyverse.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt = \"\"/></a>\n\n<!-- badges: start -->\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/reprex)](https://cran.r-project.org/package=reprex)\n[![R-CMD-check](https://github.com/tidyverse/reprex/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/reprex/actions/workflows/R-CMD-check.yaml)\n[![Codecov test coverage](https://codecov.io/gh/tidyverse/reprex/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/reprex?branch=main)\n<!-- badges: end -->\n\n## Overview\n\nPrepare reprexes for posting to [GitHub\nissues](https://docs.github.com/issues/tracking-your-work-with-issues/about-issues),\n[StackOverflow](https://stackoverflow.com/questions/tagged/r), in Slack [messages](https://slack.com/intl/en-ca/help/articles/201457107-Send-and-read-messages) or [snippets](https://slack.com/intl/en-ca/help/articles/204145658-Create-a-snippet), or even to paste into PowerPoint or Keynote slides.\nWhat is a `reprex`? It’s a **repr**oducible **ex**ample, as coined by\nRomain Francois in a tweet from 2014.\n\n<a href=\"https://media.giphy.com/media/fdLR6LGwAiVNhGQNvf/giphy.gif\"><img src=\"man/figures/help-me-help-you.png\" align=\"right\" /></a>\n\nGiven R code on the clipboard, selected in RStudio, as an expression\n(quoted or not), or in a file …\n\n  - run it via `rmarkdown::render()`,\n  - with deliberate choices re: `render()` arguments, knitr options, and\n    Pandoc options.\n\nGet resulting runnable code + output as\n\n  - Markdown, suitable for GitHub or Stack Overflow or Slack, or as\n  - R code, augmented with commented output, or as\n  - Plain HTML or (experimental) Rich Text\n\nThe result is returned invisibly, written to a file and, if possible, placed on the clipboard.\nPreview an HTML version in RStudio viewer or default browser.\n\n## Installation\n\nInstall from CRAN:\n\n``` r\ninstall.packages(\"reprex\")\n```\n\nor get a development version from GitHub:\n\n```",
    "url": "https://github.com/tidyverse/reprex",
    "last_updated": "2025-08-25T16:31:50+00:00"
  },
  {
    "full_name": "soodoku/mixed_signals",
    "name": "mixed_signals",
    "description": "Using 48,613 average movie ratings from 12 platforms for which we have ratings for 100 or more movies, we estimate the correlation between ratings across platforms. The median correlation between average ratings of two platforms was .37. ",
    "language": "R",
    "topics": [
      "movies",
      "rating",
      "movie-ratings",
      "rotten-tomatoes"
    ],
    "readme": "## Mixed Signals: Movie Quality Assessments Across Platforms\n\n![Guardians](figs/goog.png)\n\nIs the difference between ratings across different platforms for *Guardians of Galaxy* the norm? And are the differences in ratings systematic, or are the ratings largely unrelated?\n\n## Data from 1950--2020\n\nTo shed light on the question, we scraped the list of American films on Wikipedia produced between 1950 and 2020. (See [here](https://en.wikipedia.org/wiki/List_of_American_films_of_2019), for example.)  We then scraped the Google Knowledge Panel using APIfy for each of the movies. (The scripts for scraping the data are posted [here](https://github.com/NoahFinberg/google_kg_movie_scraper).)\n\n## Analyses\n\nIn all, we have 16,319 movies and movie ratings from 19 platforms. For seven platforms, however, we have less than 100 movie reviews. We subset our initial analysis on 48,613 (average) ratings from the 12 platforms for which we have ratings for 100 or more movies. \n\n![Number of Movies Per Platform](figs/n_movies.png)\n\nWe estimate the correlation between the average platform ratings. (See below for the (Pearson's) correlation matrix.) The median correlation between average ratings of two platforms was .37. (The median Spearman's correlation was .35.)\n\n![Correlation Plot](figs/pearson-corplot.png)\n\n### Largest Differences\n\nMovies with the largest difference between Rotten Tomatoes and IMDb Rating.\n\n|title                        | rotten_tomatoes_rating| IMDb_rating|\n|:----------------------------|----------------------:|-----------:|\n|After Last Season            |                     92|         1.6|\n|The Catered Affair           |                     17|         7.5|\n|Sparkle                      |                     10|         6.9|\n|Only the Strong              |                      8|         6.7|\n|The Life of David Gale       |                     19|         7.6|\n|The Vanishing of Sidney Hall |                     11|         6.9|\n|I'll Never Forget You        |    ",
    "url": "https://github.com/soodoku/mixed_signals",
    "last_updated": "2024-12-28T06:55:34+00:00"
  },
  {
    "full_name": "paleolimbot/prettymapr",
    "name": "prettymapr",
    "description": "Scale Bar, North Arrow, and Pretty Margins in R",
    "language": "R",
    "topics": [],
    "readme": "Prettymapr: Tools for rapid, nice-looking maps in R\n================\n\nPrettymapr automates the process of creating a scale bar and north arrow\nin any package that uses base graphics to plot in R, or provides\nparameters that help to draw scale bars and north arrows in other\nplotting environments. Bounding box tools help find and manipulate\nextents, and geocoding tools help plot locations on maps. Finally, there\nis a function to automate the process of setting margins, plotting the\nmap, scale bar, and north arrow, and resetting graphic parameters upon\ncompletion.\n\n## Installation\n\nThe prettymapr package is available on CRAN, and can be installed using\n`install.packages(\"prettymapr\")`.\n\n## Future deprecation\n\nThe prettymapr package was written several years ago and better tools to\nmake publication-quality maps in R have been made available since its\nrelease. The prettymapr package should be considered deprecated and is\nin bugfix-only mode for the foreseeable future.\n",
    "url": "https://github.com/paleolimbot/prettymapr",
    "last_updated": "2025-03-22T08:14:16+00:00"
  },
  {
    "full_name": "keon/awesome-nlp",
    "name": "awesome-nlp",
    "description": ":book: A curated list of resources dedicated to Natural Language Processing (NLP)",
    "language": "",
    "topics": [
      "natural-language-processing",
      "deep-learning",
      "machine-learning",
      "language",
      "awesome",
      "awesome-list",
      "nlp",
      "text-mining"
    ],
    "readme": "# awesome-nlp\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of resources dedicated to Natural Language Processing\n\n![Awesome NLP Logo](/images/logo.jpg)\n\nRead this in [English](./README.md), [Traditional Chinese](./README-ZH-TW.md)\n\n_Please read the [contribution guidelines](contributing.md) before contributing. Please add your favourite NLP resource by raising a [pull request](https://github.com/keonkim/awesome-nlp/pulls)_\n\n## Contents\n\n* [Research Summaries and Trends](#research-summaries-and-trends)\n* [Prominent NLP Research Labs](#prominent-nlp-research-labs)\n* [Tutorials](#tutorials)\n  * [Reading Content](#reading-content)\n  * [Videos and Courses](#videos-and-online-courses)\n  * [Books](#books)\n* [Libraries](#libraries)\n  * [Node.js](#node-js)\n  * [Python](#python)\n  * [C++](#c++)\n  * [Java](#java)\n  * [Kotlin](#kotlin)\n  * [Scala](#scala)\n  * [R](#R)\n  * [Clojure](#clojure)\n  * [Ruby](#ruby)\n  * [Rust](#rust)\n  * [NLP++](#NLP++)\n  * [Julia](#julia)\n* [Services](#services)\n* [Annotation Tools](#annotation-tools)\n* [Datasets](#datasets)\n* [NLP in Korean](#nlp-in-korean)\n* [NLP in Arabic](#nlp-in-arabic)\n* [NLP in Chinese](#nlp-in-chinese)\n* [NLP in German](#nlp-in-german)\n* [NLP in Polish](#nlp-in-polish)\n* [NLP in Spanish](#nlp-in-spanish)\n* [NLP in Indic Languages](#nlp-in-indic-languages)\n* [NLP in Thai](#nlp-in-thai)\n* [NLP in Danish](#nlp-in-danish)\n* [NLP in Vietnamese](#nlp-in-vietnamese)\n* [NLP for Dutch](#nlp-for-dutch)\n* [NLP in Indonesian](#nlp-in-indonesian)\n* [NLP in Urdu](#nlp-in-urdu)\n* [NLP in Persian](#nlp-in-persian)\n* [NLP in Ukrainian](#nlp-in-ukrainian)\n* [NLP in Hungarian](#nlp-in-hungarian)\n* [NLP in Portuguese](#nlp-in-portuguese)\n* [Other Languages](#other-languages)\n* [Credits](#credits)\n\n## Research Summaries and Trends\n\n* [NLP-Overview](https://nlpoverview.com/) is an up-to-date overview of deep learning techn",
    "url": "https://github.com/keon/awesome-nlp",
    "last_updated": "2025-09-02T02:46:09+00:00"
  },
  {
    "full_name": "propublica/compas-analysis",
    "name": "compas-analysis",
    "description": "Data and analysis for 'Machine Bias'",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "                  Low  High\n                 +---------+\nDidn't Reoffend  |____|____|\nReoffended       |    |    |\n                 +---------+\n\n\nThis repository contains a Jupyter notebook and data for the ProPublica story \"Machine Bias.\"\n\nStory:\nhttps://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing/\n\nMethodology:\nhttps://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm/\n\nNotebook (you'll probably want to follow along in the methodology):\nhttps://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n\nMain Dataset:\ncompas.db - a sqlite3 database containing criminal history, jail and prison time, demographics and COMPAS risk scores for defendants from Broward County.\n\nOther files as needed for the analysis.\n",
    "url": "https://github.com/propublica/compas-analysis",
    "last_updated": "2025-08-26T08:59:38+00:00"
  },
  {
    "full_name": "lmcinnes/umap",
    "name": "umap",
    "description": "Uniform Manifold Approximation and Projection",
    "language": "Python",
    "topics": [
      "umap",
      "dimensionality-reduction",
      "visualization",
      "machine-learning",
      "topological-data-analysis"
    ],
    "readme": ".. -*- mode: rst -*-\n\n.. image:: doc/logo_large.png\n  :width: 600\n  :alt: UMAP logo\n  :align: center\n\n|pypi_version|_ |pypi_downloads|_\n\n|conda_version|_ |conda_downloads|_\n\n|License|_ |build_status|_ |Coverage|_\n\n|Docs|_ |joss_paper|_\n\n.. |pypi_version| image:: https://img.shields.io/pypi/v/umap-learn.svg\n.. _pypi_version: https://pypi.python.org/pypi/umap-learn/\n\n.. |pypi_downloads| image:: https://pepy.tech/badge/umap-learn/month\n.. _pypi_downloads: https://pepy.tech/project/umap-learn\n\n.. |conda_version| image:: https://anaconda.org/conda-forge/umap-learn/badges/version.svg\n.. _conda_version: https://anaconda.org/conda-forge/umap-learn\n\n.. |conda_downloads| image:: https://anaconda.org/conda-forge/umap-learn/badges/downloads.svg\n.. _conda_downloads: https://anaconda.org/conda-forge/umap-learn\n\n.. |License| image:: https://img.shields.io/pypi/l/umap-learn.svg\n.. _License: https://github.com/lmcinnes/umap/blob/master/LICENSE.txt\n\n.. |build_status| image:: https://dev.azure.com/TutteInstitute/build-pipelines/_apis/build/status/lmcinnes.umap?branchName=master\n.. _build_status: https://dev.azure.com/TutteInstitute/build-pipelines/_build/latest?definitionId=2&branchName=master\n\n.. |Coverage| image:: https://coveralls.io/repos/github/lmcinnes/umap/badge.svg\n.. _Coverage: https://coveralls.io/github/lmcinnes/umap\n\n.. |Docs| image:: https://readthedocs.org/projects/umap-learn/badge/?version=latest\n.. _Docs: https://umap-learn.readthedocs.io/en/latest/?badge=latest\n\n.. |joss_paper| image:: http://joss.theoj.org/papers/10.21105/joss.00861/status.svg\n.. _joss_paper: https://doi.org/10.21105/joss.00861\n\n====\nUMAP\n====\n\nUniform Manifold Approximation and Projection (UMAP) is a dimension reduction\ntechnique that can be used for visualisation similarly to t-SNE, but also for\ngeneral non-linear dimension reduction. The algorithm is founded on three\nassumptions about the data:\n\n1. The data is uniformly distributed on a Riemannian manifold;\n2. The Riemannian metric is locally cons",
    "url": "https://github.com/lmcinnes/umap",
    "last_updated": "2025-09-02T01:54:38+00:00"
  },
  {
    "full_name": "pablobarbera/POIR613-2019",
    "name": "POIR613-2019",
    "description": "Course materials: POIR 613 - Computational Social Science - USC Fall 2019",
    "language": "HTML",
    "topics": [],
    "readme": "# POIR 613 (Fall 2017)\n\nRepository containing materials and the website for POIR 613: Measurement Models and Statistical Computing (University of Southern California, Fall 2017).\n\nCourse website is at http://pablobarbera.com/POIR613/\n\nThis website is built with [Rmarkdown](http://rmarkdown.rstudio.com/rmarkdown_websites.html#overview).\n\nThe layout for this website was designed by [Jeffrey Arnold](http://www.jrnold.me/) (thanks!).\n\n## Usage\n\nTo build the site,\n```rconsole\n> source(\"build.R\")\n```\n\nTo serve the site locally and rebuild it dynamically with changes, run:\n```rconsole\n> source(\"serve.R\")\n```\n",
    "url": "https://github.com/pablobarbera/POIR613-2019",
    "last_updated": "2023-06-06T17:48:47+00:00"
  },
  {
    "full_name": "dbjohnson/numerai",
    "name": "numerai",
    "description": "model for prediction challenge at http://numer.ai",
    "language": "Python",
    "topics": [],
    "readme": "I competed in the [numerai](https://numer.ai/) data modeling challenge as user [Pequod](https://numer.ai/ai?pequod), finishing 59th out of 209 competitors.  Not great, but I'm satisfied with the result given the limited time I put into it.\n\nThe model is very simple:\n\n1. PCA dimension reduction from 14 to 8 continuous variables\n2. [Dummy code](http://www.psychstat.missouristate.edu/multibook/mlt08m.html) the categorical variable\n3. K-means clustering into 6 clusters\n4. Separate [ElasicNet CV linear estimators](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html) for each cluster.\n\n\nAt one point I was at the head of the leader board, and maintained the #2 spot until very near the end of the contest.  The competition organizers then made it clear that final rankings would be based on a separate dataset than that used for the leaderboard; at that point, it became very difficult to gauge how well my solution might do, because my results against the validation set often did not line up well with the data set used for leaderboard position.  In other words, my solution was not generalizing well.\n\nRather than trying to maintain leaderboard position, I used n-fold cross validation to try and identify the combination of model parameters that produced the highest average AUC score across the CV sets, while minimizing the spread between highest and lowest scoring sets.  I figured this way, I could have more confidence in how my solution would ultimately perform on the final test set.  I chose to use 8 principle components and 6 k-means derived clusters (see cyan highlights in image below)\n\n![](data/PCs_vs_clusters.png)\n\nMy final sumbission had AUC 0.53019 on the leaderboard test set.  Even though my highest ranking solution scored AUC of 0.54824, I kept the lower scoring solution posted in the hopes it would generalize well, and that many players above me would be sorely surprised by their ultimate results on the final test set because of over",
    "url": "https://github.com/dbjohnson/numerai",
    "last_updated": "2025-07-23T09:39:23+00:00"
  },
  {
    "full_name": "wbnicholson/HackingR",
    "name": "HackingR",
    "description": "Code examples from my blog: ",
    "language": "R",
    "topics": [],
    "readme": "HackingR\n========\n\nCode examples from my blog: wbnicholson.wordpress.com\n",
    "url": "https://github.com/wbnicholson/HackingR",
    "last_updated": "2022-03-31T03:34:36+00:00"
  },
  {
    "full_name": "dicook/useR.2015",
    "name": "useR.2015",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "# useR.2015\nThese are the slides from my talk at useR! 2015 in Aalborg, Denmark. There is a pdf, and a movie which captures the embedded movies in the slides is available at https://vimeo.com/132403391.\n",
    "url": "https://github.com/dicook/useR.2015",
    "last_updated": "2015-12-18T18:42:50+00:00"
  },
  {
    "full_name": "philipperemy/name-dataset",
    "name": "name-dataset",
    "description": "The Python library for names.",
    "language": "Python",
    "topics": [
      "python",
      "dataset",
      "named-entity-recognition",
      "name"
    ],
    "readme": "# First and Last Name Database\n\n[![Downloads](https://pepy.tech/badge/names-dataset)](https://pepy.tech/project/names-dataset)\n[![Downloads](https://pepy.tech/badge/names-dataset/month)](https://pepy.tech/project/names-dataset/month)\n\nThis Python library provides detailed insights about names, including:\n- Popularity (ranking by country)\n- Gender prediction\n- Country-specific statistics (105 countries supported)\n- Fuzzy search (e.g., search for \"ISABLE\" returns \"ISABEL\")\n- Autocomplete search (e.g., search for names starting with \"ISA\")\n\n\n\nIt can give you an answer to some of those questions:\n- Who is `Zoe`? Likely a `Female, United Kindgom`. \n- Knows `Philippe`? Likely a `Male, France`. And with the spelling `Philipp`? `Male, Germany`.\n- How about `Nikki`? Likely a `Female, United States`.\n\n📥 To download the raw CSV data for your analysis, browse [here](#full-dataset).\n\n## Composition\n\n730K first names and 983K last names, extracted from the [Facebook massive dump (533M users)](https://www.theguardian.com/technology/2021/apr/03/500-million-facebook-users-website-hackers).\n\n## Installation\n\nAvailable on *[PyPI](https://pypi.org/project/names-dataset/)*:\n\n```bash\npip install names-dataset\n```\n\n## Usage\n\n⚠️ Note: This library requires approximately 3.2 GB of RAM to load the full dataset into memory. Make sure your system has enough available memory to avoid `MemoryError`.\n\nOnce installed, you can run the following commands to get familiar with the library:\n\n```python\nfrom names_dataset import NameDataset, NameWrapper\n\n# The library takes time to initialize because the database is massive. A tip is to include its initialization in your app's startup process.\nnd = NameDataset()\n\nprint(NameWrapper(nd.search('Philippe')).describe)\n# Male, France\n\nprint(NameWrapper(nd.search('Zoe')).describe)\n# Female, United Kingdom\n\nprint(nd.search('Walter'))\n# {'first_name': {'country': {'Argentina': 0.062, 'Austria': 0.037, 'Bolivia, Plurinational State of': 0.042, 'Colombia': 0.096, '",
    "url": "https://github.com/philipperemy/name-dataset",
    "last_updated": "2025-08-28T20:26:44+00:00"
  },
  {
    "full_name": "spotify/luigi",
    "name": "luigi",
    "description": "Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. ",
    "language": "Python",
    "topics": [
      "python",
      "luigi",
      "orchestration-framework",
      "scheduling",
      "hadoop"
    ],
    "readme": ".. figure:: https://raw.githubusercontent.com/spotify/luigi/master/doc/luigi.png\n   :alt: Luigi Logo\n   :align: center\n\n.. image:: https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fspotify%2Fluigi%2Fbadge&label=build&logo=none&%3Fref%3Dmaster&style=flat\n    :target: https://actions-badge.atrox.dev/spotify/luigi/goto?ref=master\n\n.. image:: https://img.shields.io/codecov/c/github/spotify/luigi/master.svg?style=flat\n    :target: https://codecov.io/gh/spotify/luigi?branch=master\n\n.. image:: https://img.shields.io/pypi/v/luigi.svg?style=flat\n   :target: https://pypi.python.org/pypi/luigi\n\n.. image:: https://img.shields.io/pypi/l/luigi.svg?style=flat\n   :target: https://pypi.python.org/pypi/luigi\n\n.. image:: https://readthedocs.org/projects/luigi/badge/?version=stable\n    :target: https://luigi.readthedocs.io/en/stable/?badge=stable\n    :alt: Documentation Status\n\nLuigi is a Python (3.8, 3.9, 3.10, 3.11, 3.12 tested) package that helps you build complex\npipelines of batch jobs. It handles dependency resolution, workflow management,\nvisualization, handling failures, command line integration, and much more.\n\nGetting Started\n---------------\n\nRun ``pip install luigi`` to install the latest stable version from `PyPI\n<https://pypi.python.org/pypi/luigi>`_. `Documentation for the latest release\n<https://luigi.readthedocs.io/en/stable/>`__ is hosted on readthedocs.\n\nRun ``pip install luigi[toml]`` to install Luigi with `TOML-based configs\n<https://luigi.readthedocs.io/en/stable/configuration.html>`__ support.\n\nFor the bleeding edge code, ``pip install\ngit+https://github.com/spotify/luigi.git``. `Bleeding edge documentation\n<https://luigi.readthedocs.io/en/latest/>`__ is also available.\n\nBackground\n----------\n\nThe purpose of Luigi is to address all the plumbing typically associated\nwith long-running batch processes. You want to chain many tasks,\nautomate them, and failures *will* happen. These tasks can be anything,\nbut are typically long running thin",
    "url": "https://github.com/spotify/luigi",
    "last_updated": "2025-09-02T08:23:17+00:00"
  },
  {
    "full_name": "scrat-online/pySTARMA",
    "name": "pySTARMA",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# pySTARMA\n \n <div align=\"center\">\n  <img src=\"https://github.com/scrat-online/pySTARMA/blob/master/docs/pySTARMA_Logo.png\"><br>\n</div>\n\nWorks on python 2.7 and 3.x\n\npySTARMA is a Python library for modelling space-time-ARMA processes. \nIt follows the [STARIMA](https://www.jstor.org/stable/621846) model proposed by Pfeifer und Deutsch (1980).\nIt implements the estimation of the space-time autocorrelation function and the partial space-time autocorrelation function.\nIt also implements the model estimation by kalman filtering, based on [Cipra and Motyková(1987)](https://dml.cz/bitstream/handle/10338.dmlcz/106567/CommentatMathUnivCarol_028-1987-3_16.pdf?sequence=1) and [Cheysson (2016)](https://cran.r-project.org/web/packages/starma/starma.pdf).\n\nAll documentation can be found in the \"docs\" directory\n",
    "url": "https://github.com/scrat-online/pySTARMA",
    "last_updated": "2025-08-12T15:45:25+00:00"
  },
  {
    "full_name": "Filter-Bubble/Mobile-Network-Tracking-Review",
    "name": "Mobile-Network-Tracking-Review",
    "description": "Mobile network tracking review",
    "language": "",
    "topics": [],
    "readme": "",
    "url": "https://github.com/Filter-Bubble/Mobile-Network-Tracking-Review",
    "last_updated": "2019-06-18T13:25:59+00:00"
  },
  {
    "full_name": "ropensci/unconf15",
    "name": "unconf15",
    "description": "rOpenSci's San Francisco hackathon/unconf 2015",
    "language": "JavaScript",
    "topics": [
      "unconf15"
    ],
    "readme": "\n# rOpenSci 2015 unconference\n__(invitation only), March 26 - 27, 2015. San Francisco__\n\n![](http://i.imgur.com/TTnpSYS.png)\n\n\nWelcome to the repository for the 2015 unconference.  rOpenSci will be hosting its second major developer meeting and open science hackathon in San Francisco in March.\n\n* [Participants](https://github.com/ropensci/unconf/wiki/Participants)\n* [Logistics](https://github.com/ropensci/unconf/wiki/Logistics)\n* Please post ideas for projects, discussion topics, and sessions as [issues](https://github.com/ropensci/unconf/issues/) and move to the wiki and/or a new repo within rOpenSci's account as needed.\n\nEvent hashtag is `#runconf15`\n\n__Remote participants__  \nFor those interested in participating remotely, please email `remote-hacker at ropensci.org` for an invite to our chat room (but also just watch the repo for discussion happening in the issues).\n\n## Support  \nThis meeting is made possible by generous support from: \n\n[![Alfred P. Sloan foundation](http://i.imgur.com/GjPfx0d.png)](http://www.sloan.org/)   \n[![GitHub](http://i.imgur.com/3Pq3ZR5.png)](https://github.com/)  \n[![Google](http://i.imgur.com/IWYdmAG.jpg)](http://google.com/)  \n[![Plotly](http://i.imgur.com/XyZa7x6.png)](http://plot.ly/)  \n[![RevolutionAnalytics](http://i.imgur.com/f2cd4X5.png)](http://revolutionanalytics.com/)    \n\n\n",
    "url": "https://github.com/ropensci/unconf15",
    "last_updated": "2024-10-21T15:49:23+00:00"
  },
  {
    "full_name": "leon-vv/node-image-scraper",
    "name": "node-image-scraper",
    "description": "Node.js module for scraping images from the web.",
    "language": "JavaScript",
    "topics": [],
    "readme": "Node Image Scraper\n========================\nScrape images from the web easily.\nUsage\n------------------------\nThe image-scrape module provides a class which needs to be constructed with a url:\n```JavaScript\nvar Scraper = require(\"image-scraper\");\n\nvar scraper = new Scraper(\"https://apod.nasa.gov/apod/astropix.html\");\n```\nThe url can be changed easily:\n```JavaScript\nscraper.address = \"http://www.npmjs.org\";\n```\nThe scraper object provides the .scrape() method.\nThe scrape method accepts one optional argument: a callback function.\n\nThis callback function can also be set using the .on() method.\n\n```JavaScript\nscraper.on(\"image\", function(image){\n\n\t// Do something.\t\n});\n```\nAs soon as 'scraper.scrape()' is called, the callback will be fired with every image found on the webpage, note however that dynamically generated images will not be found.\n\nThe image object that is passed to the callback has the following properties and methods:\n```JavaScript\n// The attributes found in the image tag, which is parsed by Cheerio (https://npmjs.org/package/cheerio).\nimage.attributes;\n// The basename of the image.\nimage.name;\n// Absolute path to the folder the image will be saved to.\nimage.saveTo;\n// Extension of the image file.\nimage.extension;\n// The absolute URL of the image.\nimage.address;\n// The URL of the page the image is scraped from.\nimage.fromAddress;\n// Save the image.\nimage.save();\n```\nThe behaviour of image.save() can be changed by setting the name, saveTo, and extension properties.\nThe saveTo property is by default set to the current directory, and the other two properties are by default set to the data found in the src attribute of the img tag.\n\nThus, the smallest program that scans a webpage for images and saves them in the current directory looks as follows:\n```JavaScript\nvar Scraper = require('image-scraper');\nvar scraper = new Scraper('https://apod.nasa.gov/apod/astropix.html');\n\nscraper.scrape(function(image) { \n\timage.save();\n});\n```\n",
    "url": "https://github.com/leon-vv/node-image-scraper",
    "last_updated": "2025-03-30T09:39:03+00:00"
  },
  {
    "full_name": "igrigorik/gharchive.org",
    "name": "gharchive.org",
    "description": "GH Archive is a project to record the public GitHub timeline, archive it, and make it easily accessible for further analysis.",
    "language": "Ruby",
    "topics": [],
    "readme": "# GH Archive\n\n![Stats](https://www.stathat.com//graphs/39/33/0b63991416f6b680e69f017a2c12.png?1340405820)\n\nOpen-source developers all over the world are working on millions of projects: writing code & documentation, fixing & submitting bugs, and so forth. GH Archive is a project to **record** the public GitHub activity, **archive it**, and **make it easily accessible for** further analysis.\n\n* [Download and analyze event archives](https://www.gharchive.org/)\n* [Analyze event data with BigQuery](https://www.gharchive.org/#bigquery)\n* [Research, visualizations, talks...](https://www.gharchive.org/#resources)\n\nFor more details, see [www.gharchive.org](https://www.gharchive.org/).\n\n## Licenses\n\nMIT, for code and documentation in this repository, see [LICENSE.md](LICENSE.md).\n\nwww.gharchive.org website content (gh-pages branch) is also released under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/), which gives you permission to use the content for almost any purpose but does not grant you any trademark permissions, so long as you note the license and give credit, such as follows:\n\n> Content based on\n> <a href=\"https://www.gharchive.org/\">www.gharchive.org</a>\n> used under the\n> <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC-BY-4.0</a>\n> license.</a>\n\nNote this repository does _not_ contain the GH Archive dataset (event archives/data). The dataset includes material that may be subject to third party rights.\n",
    "url": "https://github.com/igrigorik/gharchive.org",
    "last_updated": "2025-09-02T04:32:55+00:00"
  },
  {
    "full_name": "notnews/vandy_tv_news_abstracts",
    "name": "vandy_tv_news_abstracts",
    "description": "TV News Abstracts and Metadata from Vanderbilt TV News Archive",
    "language": "Python",
    "topics": [
      "news",
      "tv-news",
      "vanderbilt-news-archive",
      "tv-news-abstracts",
      "dataset"
    ],
    "readme": "## Abstracts from Vanderbilt Broadcast TV News Archives \n\nWe provide scripts to scrape and parse the Vanderbilt Broadcast TV News Archives and link to a dataset that includes both the raw files and the parsed data. \n\n### Final Dataset\n\nOur final parsed dataset is at the program segment level. Each `program` has multiple `broadcast` segments. Each `broadcast` segment gets its own row. The dataset has the following columns:\n\n`date, program_title, program_duration, broadcast_title, broadcast_duration, broadcast_time, broadcast_order, broadcast_abstract, reporter(s)`\n\n#### Data Dictionary\n\n`reporter(s)`: For instance, for this [broadcast](https://tvnews.vanderbilt.edu/broadcasts/16), the reporters are: Cosell, Howard; Reynolds, Frank\n\n`broadcast_time`: For instance, for https://tvnews.vanderbilt.edu/programs/1, the broadcast time for `WORLD SERIES / MCLAIN / GIBSON #16` is `12:20:40 am — 12:22:50 am`\n\n`broadcast_order`: For instance, for https://tvnews.vanderbilt.edu/programs/1, the broadcast time for `WORLD SERIES / MCLAIN / GIBSON #16` is `16`\n\n`broadcast_abstract`: Actual text of the broadcast without the header or footer. \n\n### Scraping\n\nWe start from the [site index](https://tvnews.vanderbilt.edu/siteindex). And then download all the month-year pages.\n\nThe `program` and `broadcast` pages rely on a simple counter. So we iterate over all program and broadcast pages. There are a total of 1,119,648 broadcast pages.\n\nWe then parse the data to produce the CSV. \n\nWhen there is no relevant `broadcast` page, we leave the `broadcast_abstract`, `reporter(s)` fields empty.\n\n## Testing\n\nTo make sure, we downloaded all the files, we spot checked existence of various files and compared actual number of links to actual number of files.\n\n## Scripts\n\n[The scripts](vandy/vandy) to scrape and parse are written in Python as a spider using the Scrapy framework.\n\n### Installation\n\nWe strongly recommend installing `vandy` inside a Python virtual environment (see [venv documentation](https",
    "url": "https://github.com/notnews/vandy_tv_news_abstracts",
    "last_updated": "2023-09-20T19:40:30+00:00"
  },
  {
    "full_name": "soodoku/sonny_side",
    "name": "sonny_side",
    "description": "Son Bias in US: Evidence from Business Names",
    "language": "Jupyter Notebook",
    "topics": [
      "gender-bias",
      "son-bias"
    ],
    "readme": "## Son Bias in the US: Evidence from Business Names\n\nAre American businesses with the word \"son(s)\" more common than the word \"daughter(s)\"? To answer the question, we assemble data on business names. In the US, businesses are registered with the state. And to allow businesses to check whether a particular name is taken, etc., each state provides a way to search for business names. But most states do not allow for sophisticated searches or return all the relevant search results. Because of these reasons, we can only estimate a very conservative lower bound. In all, based on data from 36 states, across states, the median ratio of businesses with the word 'son(s)' in their name to the word 'daughter(s)' is 12:1.\n\n### Data, Scripts, Manuscript\n\n* [Data](data/)\n    - [All 50 SoS Business Entity Search Links](https://www.llcuniversity.com/50-secretary-of-state-sos-business-entity-search/)\n    - [Misc. notes on the data](states-work-tracker.csv)\n\n* [Scripts](scripts/)\n    - The ipython notebooks are for states where we needed to use a Selenium scraper. The R script is the analysis script. \n\n* [Manuscript](ms/)\n\n### Authors\n\nWalter Guillioli and Gaurav Sood\n",
    "url": "https://github.com/soodoku/sonny_side",
    "last_updated": "2020-10-01T01:06:43+00:00"
  },
  {
    "full_name": "anahm/inferring-population-preferences",
    "name": "inferring-population-preferences",
    "description": "Data and analysis code for Nahm et al. \"Inferring Population Preferences via Mixtures of Spatial Voting Models\", SocInfo 2016",
    "language": "Python",
    "topics": [],
    "readme": "# Inferring Population Preferences\n\nData and analysis code for Nahm et al. \"Inferring Population Preferences via Mixtures of Spatial Voting Models\", SocInfo 2016\n\n### Instructions:\n\nTo run the code, all instructions and descriptions of the individual files necessary are located in `meta_script.py`.\n\n### Summary of Sub-directories\n\n* `data/` - Original and processed data, as well as related works data\n* `shp_code/` - Code for pre-processing the original data for inference\n* `sim_code/` - Code for Metropolis-Hastings algorithm\n* `val_code/` - Code for validation and prediction comparing our results to related works\n* `vis_code/` - Code for visualizations in the paper\n\n\nCopyright (c) 2016 Ali Nahm\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "url": "https://github.com/anahm/inferring-population-preferences",
    "last_updated": "2016-11-06T02:25:08+00:00"
  },
  {
    "full_name": "kateto/R-Network-Visualization-Basics-to-Advanced",
    "name": "R-Network-Visualization-Basics-to-Advanced",
    "description": "Basic and advanced network visualization with R - code and tutorial from my Sunbelt 2016 workshop. ",
    "language": "R",
    "topics": [],
    "readme": "# R-Network-Visualization-Basics-to-Advanced\n\n<strong>Basic and advanced network visualization with R <br></strong>\n(code and tutorial from my Sunbelt 2016 workshop)<br>\n\nThe tutorial covers network visualization using the R language for statistical computing (<a href=\"http://cran.r-project.org\">cran.r-project.org</a>) and RStudio (<a href=\"rstudio.com\">rstudio.com</a>). It provides a brief overview of network formats, focusing on their structure and representation in key R packages. The manual includes a step-by-step guide describing (through series of examples) the path from raw data to graph visualization in the igraph and Statnet frameworks. The advanced portion of this manual introduces dynamic visualization for longitudinal networks and combining networks with geographic maps.\n",
    "url": "https://github.com/kateto/R-Network-Visualization-Basics-to-Advanced",
    "last_updated": "2023-11-08T14:29:46+00:00"
  },
  {
    "full_name": "matloff/partools",
    "name": "partools",
    "description": "Tools to aid coding in the R 'parallel' package.",
    "language": "R",
    "topics": [],
    "readme": "# partools \n\nMiscellaneous utilities for parallelizing large computations.  Alternative\nto MapReduce.  File splitting and distributed operations such as sort and\naggregate.  \"Software Alchemy\" method for parallelizing most statistical\nmethods, presented in N. Matloff, Parallel Computation for Data Science,\nChapman and Hall, 2015.  Includes a debugging aid.\n\n[![Travis-CI Build\nStatus](https://travis-ci.org/matloff/partools.svg?branch=master)](https://travis-ci.org/matloff/partools)\n",
    "url": "https://github.com/matloff/partools",
    "last_updated": "2024-08-16T04:39:30+00:00"
  },
  {
    "full_name": "m-bain/whisperX",
    "name": "whisperX",
    "description": "WhisperX:  Automatic Speech Recognition with Word-level Timestamps (& Diarization)",
    "language": "Python",
    "topics": [
      "asr",
      "speech",
      "speech-recognition",
      "speech-to-text",
      "whisper"
    ],
    "readme": "<h1 align=\"center\">WhisperX</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/m-bain/whisperX/stargazers\">\n    <img src=\"https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&colorB=orange&logo=github\"\n         alt=\"GitHub stars\">\n  </a>\n  <a href=\"https://github.com/m-bain/whisperX/issues\">\n        <img src=\"https://img.shields.io/github/issues/m-bain/whisperx.svg\"\n             alt=\"GitHub issues\">\n  </a>\n  <a href=\"https://github.com/m-bain/whisperX/blob/master/LICENSE\">\n        <img src=\"https://img.shields.io/github/license/m-bain/whisperX.svg\"\n             alt=\"GitHub license\">\n  </a>\n  <a href=\"https://arxiv.org/abs/2303.00747\">\n        <img src=\"http://img.shields.io/badge/Arxiv-2303.00747-B31B1B.svg\"\n             alt=\"ArXiv paper\">\n  </a>\n  <a href=\"https://twitter.com/intent/tweet?text=&url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX\">\n  <img src=\"https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social\" alt=\"Twitter\">\n  </a>      \n</p>\n\n<img width=\"1216\" align=\"center\" alt=\"whisperx-arch\" src=\"https://raw.githubusercontent.com/m-bain/whisperX/refs/heads/main/figures/pipeline.png\">\n\n<!-- <p align=\"left\">Whisper-Based Automatic Speech Recognition (ASR) with improved timestamp accuracy + quality via forced phoneme alignment and voice-activity based batching for fast inference.</p> -->\n\n<!-- <h2 align=\"left\", id=\"what-is-it\">What is it 🔎</h2> -->\n\nThis repository provides fast automatic speech recognition (70x realtime with large-v2) with word-level timestamps and speaker diarization.\n\n- ⚡️ Batched inference for 70x realtime transcription using whisper large-v2\n- 🪶 [faster-whisper](https://github.com/guillaumekln/faster-whisper) backend, requires <8GB gpu memory for large-v2 with beam_size=5\n- 🎯 Accurate word-level timestamps using wav2vec2 alignment\n- 👯‍♂️ Multispeaker ASR using speaker diarization from [pyannote-audio](https://github.com/pyannote/pyannote-audio) (speaker ID labels)\n- 🗣️ VAD preprocessing, red",
    "url": "https://github.com/m-bain/whisperX",
    "last_updated": "2025-09-02T09:13:54+00:00"
  },
  {
    "full_name": "psf/pypistats.org",
    "name": "pypistats.org",
    "description": "PyPI downloads analytics dashboard",
    "language": "Python",
    "topics": [
      "python",
      "pypi-packages",
      "download-counts",
      "python-packages",
      "pypi"
    ],
    "readme": "PyPI Stats\n==========\n\nA simple analytics dashboard for aggregate data on PyPI downloads. PyPI Stats is built using Flask with plotly.js.\n\n`PyPI Stats <https://pypistats.org/>`_\n\nGitHub OAuth\n------------\n\nPyPI Stats has an integration with GitHub so you can track install data on the packages you maintain.\n\n`User page <https://pypistats.org/user>`_\n\nJSON API\n--------\n\nPyPI Stats provides a simple JSON API to retrieve aggregate download stats and time histories of pypi packages.\n\n`JSON API <https://pypistats.org/api>`_\n\nDevelopment\n-----------\n\n1. Copy ``.env.example`` to ``.env`` and configure your environment variables:\n   \n   .. code-block:: bash\n   \n      cp .env.example .env\n      # Edit .env with your configuration\n\n2. Run ``make pypistats`` to launch a complete development environment using docker-compose.\n\n",
    "url": "https://github.com/psf/pypistats.org",
    "last_updated": "2025-09-01T08:50:35+00:00"
  },
  {
    "full_name": "BuzzFeedNews/2016-12-fake-news-survey",
    "name": "2016-12-fake-news-survey",
    "description": "Data, analytic code, and findings based on a large-scale survey conducted by Ipsos Public Affairs for BuzzFeed News.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Fake News Survey Data and Analysis\n\nThis repository contains data, analytic code, and findings based on a large-scale survey conducted by Ipsos Public Affairs for BuzzFeed News.\n\nThe findings support the BuzzFeed News article, \"[Most Americans Who See Fake News Believe It, New Survey Says](https://www.buzzfeed.com/craigsilverman/fake-news-survey),\" published December 6, 2016. That article also contains additional details about the survey design and context.\n\n## Data\n\n- General cross-tabs, compiled by Ipsos, and a data dictionary describing the variables, can be [found here](docs/).\n\n- The raw survey response data, courtesy of Ipsos, can be [found here](data/), as both CSV and SPSS files.\n\n- BuzzFeed News has also created a [simplified CSV](data/headline-responses.csv) containing the following main columns for each headline presented to each respondent:\n    - Respondent ID\n    - Headline ID (A-K)\n    - Whether this headline is one of the five fake-news headlines\n    - Order in which the respondent saw the headline (1-6)\n    - Whether the respondent recalled having seen or heard about the headline (`yes`/`no`/`unsure`)\n    - Whether the respondent believed the headline to be accurate (`very accurate`/`somewhat accurate`/`not very accurate`/`not at all accurate`)\n    - The respondent's survey weight, as determined by Ipsos\n\n## Analysis\n\nA notebook containing the calculations can be [found here](notebooks/survey-analysis.ipynb). It's written in Python, but the resulting tables should still be generally legible to non-prgrammers.\n\n## Feedback / Questions?\n\nContact Jeremy Singer-Vine at jeremy.singer-vine@buzzfeed.com.\n\nLooking for more from BuzzFeed News? [Click here for a list of our open-sourced projects, data, and code](https://github.com/BuzzFeedNews/everything).\n",
    "url": "https://github.com/BuzzFeedNews/2016-12-fake-news-survey",
    "last_updated": "2025-06-07T00:25:58+00:00"
  },
  {
    "full_name": "thomasp85/tweenr",
    "name": "tweenr",
    "description": "Interpolate your data",
    "language": "R",
    "topics": [
      "rstats",
      "animation",
      "plotting",
      "transition",
      "tweening"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tweenr <img src=\"man/figures/logo.png\" align=\"right\" />\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/thomasp85/tweenr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/thomasp85/tweenr/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Release_Badge](http://www.r-pkg.org/badges/version-ago/tweenr)](https://CRAN.R-project.org/package=tweenr)\n[![CRAN_Download_Badge](http://cranlogs.r-pkg.org/badges/tweenr)](https://CRAN.R-project.org/package=tweenr)\n<!-- badges: end -->\n\n## What is this?\n\n`tweenr` is a package for interpolating data, mainly for animations. It\nprovides a range of functions that take data of different forms and\ncalculate intermediary values. It supports all atomic vector types along\nwith `factor`, `Date`, `POSIXct`, characters representing colours, and\n`list`. `tweenr` is used extensibly by\n[`gganimate`](https://github.com/thomasp85/gganimate) to create smooth\nanimations, but can also be used by itself to prepare data for animation\nin another framework.\n\n## How do I get it?\n\n`tweenr` is available on CRAN and can be installed with\n`install.packages('tweenr')`. In order to get the development version\nyou can install it from github with `devtools`\n\n``` r\n#install.packages('devtools')\ndevtools::install_github('thomasp85/tweenr')\n```\n\n## An example\n\nFollowing is an example of using the pipeable `tween_state()` function\nwith our belowed iris data:\n\n``` r\nlibrary(tweenr)\nlibrary(ggplot2)\n\n# Prepare the data with some extra columns\niris$col <- c('firebrick', 'forestgreen', 'steelblue')[as.integer(iris$Species)]\niris$size <- 4\niris$alpha <- 1\niris <- split(iris, iris$Species)\n\n# Here comes tweenr\niris_tween <- iris$setosa %>% \n  tween_state(iris$versicolor, ease = 'cubic-in-out', nframes = 30) %>% \n  keep_state(10) %>% \n  tween_state(iris$virginica, ease = 'elastic-out', nframes = 30) %>% \n  keep_state(10) %>% \n  tween_state(iris$setosa, ease = 'quadratic-in', nfra",
    "url": "https://github.com/thomasp85/tweenr",
    "last_updated": "2025-08-28T20:35:16+00:00"
  },
  {
    "full_name": "clintpgeorge/ldamcmc",
    "name": "ldamcmc",
    "description": "Implements several Markov chain Monte Carlo (MCMC) algorithms for the latent Dirichlet allocation (LDA) model",
    "language": "C++",
    "topics": [],
    "readme": "Markov Chain Monte Carlo Algorithms for the Latent Dirichlet Allocation Model\n=============================================================================\n\nThis **R** package, **ldamcmc**, implements several Markov chain Monte Carlo (MCMC) algorithms for the latent Dirichlet allocation (LDA) model. This includes: \n\n* The augmented collapsed Gibbs sampling (ACGS, Griffiths and Steyvers 2004, George and Doss 2015) algorithm\n* The full Gibbs sampling (FGS, George and Doss 2015) algorithm\n* The serial tempering (George and Doss 2015, Geyer 2011) algorithm \n* Hyperparameter selection in the LDA model (George and Doss 2015) \n* Posterior predictive checking (PPC, Chen and Doss 2015)\n\nFor package documentation run \n\n``` help(\"ldamcmc\") ```\n\nin an R console. All major functions and datasets are documented and linked to the package index. Raw data files for each dataset are available in the **data-raw** folder. To load raw data see ``` demo/load_raw_data.R  ```.    \n\nTo see all demo R scripts available in this package, run \n\n``` demo(package=\"ldamcmc\") ```\n\nin an R console. Some scripts can be executed via running  \n\n``` demo(file-name, package=\"ldamcmc\") ```\n\nin an R console. The rest of them may require commandline arguments for execution. Please see the documentation provided in each script before execution.    \n\nAuthors\n----------------------------\n* [Clint P. George](https://www.iitgoa.ac.in/~clint) (Please contact for questions and comments)\n* [Hani Doss](http://www.stat.ufl.edu/~doss) \n\nDependencies\n----------------------------\n\nThis package uses the following R packages, which are already included in this R package.   \n* **Rcpp**\n* **RcppArmadillo** based on the **Armadillo** C++ package \n* **lattice**\n\nInstallation Guide \n------------------\n\n* Download the package source from [Git Download Link](https://github.com/clintpgeorge/ldamcmc/archive/master.zip)\n* Unzip the dowloaded file and rename the folder **ldamcmc-master** to **ldamcmc** \n* To install **ldamcmc** run ",
    "url": "https://github.com/clintpgeorge/ldamcmc",
    "last_updated": "2023-09-07T14:19:05+00:00"
  },
  {
    "full_name": "pola-rs/polars",
    "name": "polars",
    "description": "Dataframes powered by a multithreaded, vectorized query engine, written in Rust",
    "language": "Rust",
    "topics": [
      "dataframe-library",
      "dataframe",
      "dataframes",
      "rust",
      "arrow",
      "python",
      "out-of-core",
      "polars"
    ],
    "readme": "<h1 align=\"center\">\n  <a href=\"https://pola.rs\">\n    <img src=\"https://raw.githubusercontent.com/pola-rs/polars-static/master/banner/polars_github_banner.svg\" alt=\"Polars logo\">\n  </a>\n</h1>\n\n<div align=\"center\">\n  <a href=\"https://crates.io/crates/polars\">\n    <img src=\"https://img.shields.io/crates/v/polars.svg\" alt=\"crates.io Latest Release\"/>\n  </a>\n  <a href=\"https://pypi.org/project/polars/\">\n    <img src=\"https://img.shields.io/pypi/v/polars.svg\" alt=\"PyPi Latest Release\"/>\n  </a>\n  <a href=\"https://www.npmjs.com/package/nodejs-polars\">\n    <img src=\"https://img.shields.io/npm/v/nodejs-polars.svg\" alt=\"NPM Latest Release\"/>\n  </a>\n  <a href=\"https://community.r-multiverse.org/polars\">\n    <img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fcommunity.r-multiverse.org%2Fapi%2Fpackages%2Fpolars&query=%24.Version&label=r-multiverse\" alt=\"R-multiverse Latest Release\"/>\n  </a>\n  <a href=\"https://doi.org/10.5281/zenodo.7697217\">\n    <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.7697217.svg\" alt=\"DOI Latest Release\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <b>Documentation</b>:\n  <a href=\"https://docs.pola.rs/api/python/stable/reference/index.html\">Python</a>\n  -\n  <a href=\"https://docs.rs/polars/latest/polars/\">Rust</a>\n  -\n  <a href=\"https://pola-rs.github.io/nodejs-polars/index.html\">Node.js</a>\n  -\n  <a href=\"https://pola-rs.github.io/r-polars/index.html\">R</a>\n  |\n  <b>StackOverflow</b>:\n  <a href=\"https://stackoverflow.com/questions/tagged/python-polars\">Python</a>\n  -\n  <a href=\"https://stackoverflow.com/questions/tagged/rust-polars\">Rust</a>\n  -\n  <a href=\"https://stackoverflow.com/questions/tagged/nodejs-polars\">Node.js</a>\n  -\n  <a href=\"https://stackoverflow.com/questions/tagged/r-polars\">R</a>\n  |\n  <a href=\"https://docs.pola.rs/\">User guide</a>\n  |\n  <a href=\"https://discord.gg/4UfP5cfBE7\">Discord</a>\n</p>\n\n## Polars: Blazingly fast DataFrames in Rust, Python, Node.js, R, and SQL\n\nPolars is a DataFrame interface on top of an OLAP Q",
    "url": "https://github.com/pola-rs/polars",
    "last_updated": "2025-09-02T09:56:54+00:00"
  },
  {
    "full_name": "Lightning-Universe/stable-diffusion-deploy",
    "name": "stable-diffusion-deploy",
    "description": "Learn to serve Stable Diffusion models on cloud infrastructure at scale. This Lightning App shows load-balancing, orchestrating, pre-provisioning, dynamic batching, GPU-inference, micro-services working together via the Lightning Apps framework.",
    "language": "Python",
    "topics": [
      "model-serving",
      "stable-diffusion"
    ],
    "readme": "<div align=\"center\">\n    <h1>\n        <img src=\"https://lightningaidev.wpengine.com/wp-content/uploads/2022/10/image-21.png\">\n        <br>\n        Use AI to inspire your art\n        </br>\n    </h1>\n\n<div align=\"center\">\n\n<p align=\"center\" style=\"color:grey\"><a href=\"https://lightning.ai/muse\">Muse is live here</a></p>\n\n<p align=\"center\">\n  <a href=\"#run-your-own\">Run your own</a> •\n  <a href=\"https://www.lightning.ai/\">Lightning AI</a> •\n  <a href=\"https://www.lightning.ai/muse\">Use Muse Live</a> •\n  <a href=\"https://lightning.ai/pages/community/tutorial/deploy-diffusion-models/\">Full Tutorial</a>\n</p>\n\n[![ReadTheDocs](https://readthedocs.org/projects/pytorch-lightning/badge/?version=stable)](https://lightning.ai/lightning-docs/)\n[![Slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/lightning/blob/master/LICENSE)\n\n</div>\n</div>\n\n______________________________________________________________________\n\n# Muse\n\nOpen source, stable-diffusion production server to show how to deploy diffusion models in a real production environment with: load-balancing, gpu-inference, performance-testing, micro-services orchestration and more. All handled easily with the [Lightning Apps framework](https://lightning.ai/lightning-docs/).\n\n[The app is live here](https://lightning.ai/muse).\n\n[Full tutorial on how to build this app](https://lightning.ai/pages/community/tutorial/deploy-diffusion-models/).\n\n<img width=\"1246\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3640001/195984024-788255e7-d01b-4522-9655-2a3ba56e80aa.png\">\n\n## Model\n\nMuse uses the opensource Stable Diffusion model made available by [stability AI](https://stability.ai/blog/stable-diffusion-public-release).\nWe apply a few fancy tricks to make the inference super fast.\n\nHere's a small snippet showing our [model server](https://github.com/Lightning-A",
    "url": "https://github.com/Lightning-Universe/stable-diffusion-deploy",
    "last_updated": "2025-06-03T07:44:14+00:00"
  },
  {
    "full_name": "kvasilaky/InverseProblem",
    "name": "InverseProblem",
    "description": "This function inverts ill conditioned matrices using an iterative solution to the Tikhonov regularization problem. It takes three arguments: A, the matrix, l, lambda the contraint, and k, the number of iterations. In this iterative Tikhonov regularization model, also known as ridge regression, I introduce an iterative solution to the ill-posed linear inverse problem. My approach to the inverse problem can be viewed as a generalization of existing methods, where, in addition to the regularization parameter, I introduce a second regularization parameter as the number if iterations. This work is motivated by the fact that the least squares solution does not give a reasonable result when the data matrix is singular or ill-conditioned. Test cases show that the approach is either better or significantly better than existing L2 regularization methods. ",
    "language": "Python",
    "topics": [],
    "readme": "# InverseProblem\n\nip.optimalk(A) #this will print out optimal k\n\nip.invert(A,be,k,l) #this will invert your A matrix, where be is noisy be, k is the no. of iterations, and lambda is your dampening effect (best set to 1)\n",
    "url": "https://github.com/kvasilaky/InverseProblem",
    "last_updated": "2025-02-28T08:14:06+00:00"
  },
  {
    "full_name": "aryamanarora/india-census-2011",
    "name": "india-census-2011",
    "description": "D3.js visualization of the language parts of the 2011 Census of India.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Indian Language Map\n\nData is from the 2011 Census of India. Shapefiles are from Subhodip Mukherjee (https://www.researchgate.net/post/Can_anyone_provide_a_shape_file_of_Indian_Districts).",
    "url": "https://github.com/aryamanarora/india-census-2011",
    "last_updated": "2025-06-26T01:46:41+00:00"
  },
  {
    "full_name": "paultopia/murdersky",
    "name": "murdersky",
    "description": "The Sky Wants to Murder You: A Californian's Guide to Surviving Iowa Winters",
    "language": "HTML",
    "topics": [],
    "readme": "Paul Gowder's guide to surviving Iowa winters, for Californians and other fellow weaklings. Available at [murdersky.com](https://murdersky.com).\n\n(c) Paul Gowder. Licensed under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.](http://creativecommons.org/licenses/by-nc-nd/4.0/) \n\nSee a problem/have a suggestion? File an issue!\n",
    "url": "https://github.com/paultopia/murdersky",
    "last_updated": "2025-06-17T03:17:57+00:00"
  },
  {
    "full_name": "csvy/csvy.github.io",
    "name": "csvy.github.io",
    "description": "CSVY yaml frontmatter for csv file format",
    "language": "CSS",
    "topics": [],
    "readme": "# csvy.org\n\nThis repo contains the specs of yaml frontmatter for csv file format on <http://csvy.org>.\n",
    "url": "https://github.com/csvy/csvy.github.io",
    "last_updated": "2025-03-22T08:14:15+00:00"
  },
  {
    "full_name": "unitedstates/congress",
    "name": "congress",
    "description": "Public domain data collectors for the work of Congress, including legislation, amendments, and votes.",
    "language": "Python",
    "topics": [],
    "readme": "## unitedstates/congress\n\nThis is a community-run project to develop Python tools to collect data about the bills, amendments, roll call votes, and other core data about the U.S. Congress into simple-to-use structured data files.\n\nThe tools include:\n\n* Downloading the [official bulk bill status data](https://github.com/usgpo/bill-status) from Congress, the official source of information on the life and times of legislation, and converting the data to an easier-to-use format.\n\n* Scrapers for House and Senate roll call votes.\n\n* A document fetcher for GovInfo.gov, which holds bill text, bill status, and other official documents, and which downloads only newly updated files.\n\n* A defunct THOMAS scraper for presidential nominations in Congress.\n\nRead about the contents and schema in the [documentation](https://github.com/unitedstates/congress/wiki) in the github project wiki.\n\nThis repository was originally developed by [GovTrack.us](https://www.govtrack.us) and the Sunlight Foundation in 2013 (see [Eric's blog post](https://sunlightfoundation.com/blog/2013/08/20/a-modern-approach-to-open-data/)) and is currently maintained by GovTrack.us and other contributors. For more information about data in Congress, see the [Congressional Data Coalition](https://congressionaldata.org/).\n\n\n### Setting Up\n\nThis project is tested using Python 3.\n\n**System dependencies**\n\nOn Ubuntu, you'll need `wget`, `pip`, and some support packages:\n\n```bash\nsudo apt-get install git python3-dev libxml2-dev libxslt1-dev libz-dev python3-pip python3-venv\n```\n\nOn OS X, you'll need developer tools installed ([XCode](https://developer.apple.com/xcode/)), and `wget`.\n\n```bash\nbrew install wget\n```\n\n**Python dependencies**\n\nIt's recommended you use a `virtualenv` (virtual environment) for development. Create a virtualenv for this project:\n\n```bash\npython3 -m venv env\nsource env/bin/activate\n```\nFinally, with your virtual environment activated, install the package, which\nwill automatically pull in the Pyt",
    "url": "https://github.com/unitedstates/congress",
    "last_updated": "2025-09-02T05:27:26+00:00"
  },
  {
    "full_name": "datahoarder/fec_individual_donors",
    "name": "fec_individual_donors",
    "description": "A how-to do a mass collection of FEC data using the command-line and regular expressions",
    "language": "",
    "topics": [],
    "readme": "# Gathering 25+ years of Federal Election Commission bulk detailed data using the command line and regular expressions\n\n[The Federal Election Commission contains a massive archive](http://www.fec.gov/finance/disclosure/ftpdet.shtml) of U.S. campaign finance data. The data is stored as flat, delmited text files. \n\nThe FEC has many pages about data; the one in question is titled:[Detailed Files About Candidates, Parties and Other Committees](http://www.fec.gov/finance/disclosure/ftpdet.shtml)\n\nAmong the most interesting datasets are the individual contributions to campaigns -- the FEC has this from 1980 to the current election cycle. This dataset, which includes the identities and self-proclaimed occupations and residences of individual donors, contains a lot of potential insights about the kind of people who donate to particular campaigns. If you've ever used OpenSecrets, [this is where you can see the raw data](http://www.opensecrets.org/indivs/index.php).\n\n\n## The problem\n\nHowever, the data is separated by cycle, with each file stored as a separate zip file on the FEC's FTP server. When the data file is unpacked, the text is delimited with __pipes__, and the files are headerless, which means we have to attach the [data headers to each file ourselves](http://www.fec.gov/finance/disclosure/metadata/DataDictionaryContributionsbyIndividuals.shtml).\n\nAnd that's before we've even inserted it into a database, nevermind done any actual analysis.\n\nBut if we could efficiently collect all the data, starting from 1980 to 2016, then we would have dozens of millions of records to analyze. Stories, such as this [LA Times feature into what Hollywood is donating in this cycle](http://graphics.latimes.com/2016-election-entertainment-donors/), could be repeated across every election cycle.\n\n## The command-line solution\n\nAll your database skills won't matter if you can't even get the data into the database. There's plenty of ways to approach this programmatically, but in this writeup,",
    "url": "https://github.com/datahoarder/fec_individual_donors",
    "last_updated": "2024-10-13T13:09:41+00:00"
  },
  {
    "full_name": "cisagov/github_org_name_change",
    "name": "github_org_name_change",
    "description": "Bash script to help folks update git remote URLs when a GitHub organization changes names",
    "language": "Shell",
    "topics": [],
    "readme": "# GitHub Organization Name Change Script :articulated_lorry: #\n\n## What is this? ##\nThis is a simple bash script intended to help folks update their git\nremotes when a GitHub organization undergoes a name change.  The tool\nupdates the default git remote (`origin`) for all git repositories\ndirectly inside the specified directory.  The git remotes are updated\nby replacing instances of `old_org_name` with `new_org_name`.\n\n```\nupdate_git_repos.sh old_org_name new_org_name directory\n```\n\n## License ##\n\nThis project is in the worldwide [public domain](LICENSE.md).\n\nThis project is in the public domain within the United States, and\ncopyright and related rights in the work worldwide are waived through\nthe [CC0 1.0 Universal public domain\ndedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\nAll contributions to this project will be released under the CC0\ndedication. By submitting a pull request, you are agreeing to comply\nwith this waiver of copyright interest.\n\n",
    "url": "https://github.com/cisagov/github_org_name_change",
    "last_updated": "2025-01-14T14:21:37+00:00"
  },
  {
    "full_name": "LibraryOfCongress/newspaper-navigator",
    "name": "newspaper-navigator",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "\n# *Newspaper Navigator*\n\n## By Benjamin Charles Germain Lee (2020 Library of Congress Innovator in Residence)\n\n<i>This project is an experiment and there is no active development and/or maintenance of the codebase. Fork at your own risk! In the unlikely event that there are further updates, the LC Labs team will announce it through our communication channels. Sign up for our listserv on labs.loc.gov, follow us on Twitter @LC_Labs, and watch #NewspaperNavigator . </i>\n\n## Introduction\n\nThe goal of *Newspaper Navigator* is to re-imagine searching over the visual content in [*Chronicling America*](https://chroniclingamerica.loc.gov/about/). The project consists of two stages:\n- Creating the [*Newspaper Navigator* dataset](https://news-navigator.labs.loc.gov/) by extracting headlines, photographs, illustrations, maps, comics, cartoons, and advertisements from 16.3 million historic newspaper pages in Chronicling America using emerging machine learning techniques.  In addition to the visual content, the dataset includes captions and other relevant text derived from the METS/ALTO OCR, as well as image embeddings for fast similarity querying.\n- Creating an [exploratory search application](https://news-navigator.labs.loc.gov/search) for the *Newspaper Navigator* dataset in order to enable new ways for the American public to navigate Chronicling America.\n\nThis repo contains the code for both steps of the project, as well as a list of *Newspaper Navigator* resources.\n\n## Updates\n\n**Update (09/10/2020):**\n\n- The development of the *Newspaper Navigator* search application is complete! You can find the search application at: [https://news-navigator.labs.loc.gov/search](https://news-navigator.labs.loc.gov/search).\n\n- The *Newspaper Navigator* data archaeology is now available at: [http://dx.doi.org/10.17613/k9gt-6685](http://dx.doi.org/10.17613/k9gt-6685). The data archaeology examines the ways in which a *Chronicling America* newspaper page is transmuted and decontextualized dur",
    "url": "https://github.com/LibraryOfCongress/newspaper-navigator",
    "last_updated": "2025-09-01T09:52:24+00:00"
  },
  {
    "full_name": "rdrivers/mrp-aapor",
    "name": "mrp-aapor",
    "description": "Short course on MRP at 2018 AAPOR Conference",
    "language": "R",
    "topics": [],
    "readme": "# mrp-aapor\nShort course on MRP at 2018 AAPOR Conference\n",
    "url": "https://github.com/rdrivers/mrp-aapor",
    "last_updated": "2025-02-13T12:57:17+00:00"
  },
  {
    "full_name": "laurenancona/twimoji",
    "name": "twimoji",
    "description": "analysis of emoji in tweets by location",
    "language": "JavaScript",
    "topics": [
      "tweets",
      "analysis",
      "emoji",
      "sentiment-analysis",
      "r"
    ],
    "readme": "# twimoji\na mostly-for-fun exercise in analyzing location-enabled tweets with emoji in Philadelphia\n\n![](presentation/img/alltweets.gif)\n\n## Data\n#### Collection\nThis project uses the simplest method to aggregate tweets based on simple parameters:\n- This IFTTT job to collect tweets _with location attributes_ in a given radius around Philadelphia:\n\n<a href=\"https://ifttt.com/view_embed_recipe/305098-collect-tweets-with-location\" target = \"_blank\" class=\"embed_recipe embed_recipe-l_28\" id= \"embed_recipe-305098\"><img src= '/web/images/ifttt.png' alt=\"IFTTT Recipe: Collect Tweets with Location connects twitter to google-drive\" width=\"370px\" style=\"max-width:100%\"/></a>\n- Saves to a Google Spreadsheet, when full begins new sheet\n\n#### Cleanup & Reshape\n- Tokenize words from tweet text for possible later NLP\n- Extract latitude & longitude from link to map field string\n\n#### Analysis\n- Emoji lookup table\n- Map unicode and bytes from table\n- Calculate word frequency\n- Only count 1st occurence of an emoji per tweet\n\n## Visualization\n#### Geocoding\n- CartoDB\n\n#### Location Filtering\n- Uses city boundaries file (link)\n- Add geodata for additional demographic analysis:\n  - Census tracts or Block segments\n  - Zip codes?\n\n#### Time-based animation\n- Torque.js\n\n\n",
    "url": "https://github.com/laurenancona/twimoji",
    "last_updated": "2025-05-11T05:35:46+00:00"
  },
  {
    "full_name": "kayzhu/LSHash",
    "name": "LSHash",
    "description": "A fast Python implementation of locality sensitive hashing. ",
    "language": "Python",
    "topics": [],
    "readme": "======\nLSHash\n======\n\n:Version: 0.0.4dev\n\nA fast Python implementation of locality sensitive hashing with persistance\nsupport.\n\nHighlights\n==========\n\n- Fast hash calculation for large amount of high dimensional data through the use of `numpy` arrays.\n- Built-in support for persistency through Redis.\n- Multiple hash indexes support.\n- Built-in support for common distance/objective functions for ranking outputs.\n\nInstallation\n============\n``LSHash`` depends on the following libraries:\n\n- numpy\n- redis (if persistency through Redis is needed)\n- bitarray (if hamming distance is used as distance function)\n\nTo install:\n\n.. code-block:: bash\n\n    $ pip install lshash\n\nQuickstart\n==========\nTo create 6-bit hashes for input data of 8 dimensions:\n\n.. code-block:: python\n\n    >>> from lshash import LSHash\n\n    >>> lsh = LSHash(6, 8)\n    >>> lsh.index([1,2,3,4,5,6,7,8])\n    >>> lsh.index([2,3,4,5,6,7,8,9])\n    >>> lsh.index([10,12,99,1,5,31,2,3])\n    >>> lsh.query([1,2,3,4,5,6,7,7])\n    [((1, 2, 3, 4, 5, 6, 7, 8), 1.0),\n     ((2, 3, 4, 5, 6, 7, 8, 9), 11)]\n\n\nMain Interface\n==============\n\n- To initialize a ``LSHash`` instance:\n\n.. code-block:: python\n\n    LSHash(hash_size, input_dim, num_of_hashtables=1, storage=None, matrices_filename=None, overwrite=False)\n\nparameters:\n\n``hash_size``:\n    The length of the resulting binary hash.\n``input_dim``:\n    The dimension of the input vector.\n``num_hashtables = 1``:\n    (optional) The number of hash tables used for multiple lookups.\n``storage = None``:\n    (optional) Specify the name of the storage to be used for the index\n    storage. Options include \"redis\".\n``matrices_filename = None``:\n    (optional) Specify the path to the .npz file random matrices are stored\n    or to be stored if the file does not exist yet\n``overwrite = False``:\n    (optional) Whether to overwrite the matrices file if it already exist\n\n- To index a data point of a given ``LSHash`` instance, e.g., ``lsh``:\n\n.. code-block:: python\n\n    lsh.index(input_point, extr",
    "url": "https://github.com/kayzhu/LSHash",
    "last_updated": "2025-08-07T02:23:18+00:00"
  },
  {
    "full_name": "google/re2",
    "name": "re2",
    "description": "RE2 is a fast, safe, thread-friendly alternative to backtracking regular expression engines like those used in PCRE, Perl, and Python. It is a C++ library.",
    "language": "C++",
    "topics": [],
    "readme": "# RE2, a regular expression library\n\nRE2 is an efficient, principled regular expression library\nthat has been used in production at Google and many other places\nsince 2006.\n\n_**Safety is RE2's primary goal.**_\n\nRE2 was designed and implemented with an explicit goal of being able\nto handle regular expressions from untrusted users without risk.\nOne of its primary guarantees is that the match time is linear in the\nlength of the input string. It was also written with production concerns in mind:\nthe parser, the compiler and the execution engines limit their memory usage\nby working within a configurable budget—failing gracefully when exhausted—and\nthey avoid stack overflow by eschewing recursion.\n\nIt is not a goal to be faster than all other engines under all circumstances.\nAlthough RE2 guarantees a running time that is asymptotically linear in\nthe length of the input, more complex expressions may incur larger constant factors;\nlonger expressions increase the overhead required to handle those expressions safely.\nIn a sense, RE2 is pessimistic where a backtracking engine is optimistic:\nA backtracking engine tests each alternative sequentially, making it fast when the first alternative is common.\nBy contrast RE2 evaluates all alternatives in parallel, avoiding the performance penalty for the last alternative,\nat the cost of some overhead. This pessimism is what makes RE2 secure.\n\nIt is also not a goal to implement all of the features offered by Perl, PCRE and other engines.\nAs a matter of principle, RE2 does not support constructs for which only backtracking solutions are known to exist.\nThus, backreferences and look-around assertions are not supported.\n\nFor more information, please refer to Russ Cox's articles on regular expression theory and practice:\n\n* [Regular Expression Matching Can Be Simple And Fast](https://swtch.com/~rsc/regexp/regexp1.html)\n* [Regular Expression Matching: the Virtual Machine Approach](https://swtch.com/~rsc/regexp/regexp2.html)\n* [Regular Expres",
    "url": "https://github.com/google/re2",
    "last_updated": "2025-09-02T03:41:47+00:00"
  },
  {
    "full_name": "raylene/eng-handbook",
    "name": "eng-handbook",
    "description": "A developer's guide to management: an open-sourced handbook for leading software engineering teams.",
    "language": "",
    "topics": [],
    "readme": "# The Eng Team Handbook (eng-handbook)\n*A developer's guide to management: an open-sourced handbook for leading engineering teams*\n\n## Background\n\nWhen it comes to building software, there are a lot of resources out there that help you get started quickly, from open-source libraries to full-stack tools and platforms. But when it comes to building engineering _teams_, it's hard to find resources that you can link to, clone, or integrate with and start using right away.\n\nThis project takes what we've already done with code (open-source collaboration, modules, templating), and applies it to resources that no engineering team should need to rediscover or reinvent.\n\nEach guide is written to be self-explanatory and usable on its own, combining both \"how to\" instructional elements as well as templates that can be adapted to your own needs. Since no engineering team is created equal, you can pick and choose to compose modules into a customized handbook that works for you.\n\n## Usage\n\nIf you're on an engineering team and looking for some easy ways to get started when it comes to 1-1s, performance reviews, hosting a team offsite, and more:\n* Browse our guides see if you find them useful—<kbd>Quickstart</kbd> headers tell you who they’re most relevant for, e.g. [<kbd>Quickstart</kbd> *Applicable to most managers*]\n* Use them directly as internal references, or fork to customize them to better suit your team's needs\n* Give feedback and make edits to help improve these guides for others\n* Watch to stay tuned for updates and new guides\n\nIf you've developed content of your own that you'd like to share, consider [contributing](#contribute).\n\n## Table of Contents\n\n**Engineering Management**\n\n* <a href=\"management/guide-to-1-1s.md\">Guide to 1-1s</a>\n* <a href=\"management/manager-changes.md\">Guide to Manager Changes</a>\n* <a href=\"management/role-transitions.md\">Engineering Role Transitions (IC<>EM)</a>\n\n**Performance Reviews & Feedback**\n\n* <a href=\"feedback/perf-review-templates.md\">",
    "url": "https://github.com/raylene/eng-handbook",
    "last_updated": "2025-08-22T11:35:19+00:00"
  },
  {
    "full_name": "raphael-susewind/india-election-data",
    "name": "india-election-data",
    "description": "To map publicly available datasets related to General Assembly (Lok Sabha) elections in India.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "India Election Data\n===================\n\nTo map publicly available datasets related to General Assembly (Lok Sabha) elections in India.\n\n**License:** Datasets are protected under their respective licenses. Documentation by DataMeet members is shared under [Creative Commons Attribution-ShareAlike 3.0 Unported](http://creativecommons.org/licenses/by-sa/3.0/) license.\n",
    "url": "https://github.com/raphael-susewind/india-election-data",
    "last_updated": "2021-09-21T16:11:08+00:00"
  },
  {
    "full_name": "kotartemiy/newscatcher",
    "name": "newscatcher",
    "description": "Programmatically collect normalized news from (almost) any website.",
    "language": "Python",
    "topics": [],
    "readme": "# Newscatcher\n**Programmatically collect normalized news from (almost) any website.**\n\nFilter by **topic**, **country**, or **language**.\n\nCreated by [newscatcherapi.com](https://www.newscatcherapi.com) but you do not need anything from us or from anyone else to get the software going, it just works out of the box.\n\n## Demo\n![](newscatcherdemo.gif)\n\n## Motivation\nWhile working on [newscatcherapi](https://newscatcherapi.com/) - JSON API \nto query news articles,\nI came up with an idea to make a simple Python package that would allow\nto easily grab the live news data. \n\nWhen I used to be a junior data scientist working on my own side projects,\nit was difficult for me to operate with external data sources. I knew Python\nquite well, but in most cases it was not enough to build proper data pipelines\nthat required gathering data on my own. I hope that this package will help you \nwith your next project. \n\nEven though I do not recommend to use this package for any production systems, \nI believe that it should be enough to test your assumptions and build some MVPs.\n\n## Installation\n`pip install newscatcher --upgrade` \n\n\n## Quick Start\n```python\nfrom newscatcher import Newscatcher\n```\n\nGet the latest news from [nytimes.com](https://www.nytimes.com/) \n(_we support thousands of news websites, try yourself!_) main news feed\n```python\nnc = Newscatcher(website = 'nytimes.com')\nresults = nc.get_news()\n\n# results.keys()\n# 'url', 'topic', 'language', 'country', 'articles'\n\n# Get the articles\narticles = results['articles']\n\nfirst_article_summary = articles[0]['summary']\nfirst_article_title = articles[0]['title']\n```\n\nGet the latest news from [nytimes.com](https://www.nytimes.com/) **politics** feed\n\n```python\nnc = Newscatcher(website = 'nytimes.com', topic = 'politics')\n\nresults = nc.get_news()\narticles = results['articles']\n```\n\nThere is a limited set of topic that you might find:\n\n``` 'tech', 'news', 'business', 'science', 'finance', 'food', 'politics', 'economics', 'travel', 'entert",
    "url": "https://github.com/kotartemiy/newscatcher",
    "last_updated": "2025-08-30T04:57:07+00:00"
  },
  {
    "full_name": "toml-lang/toml",
    "name": "toml",
    "description": "Tom's Obvious, Minimal Language",
    "language": "",
    "topics": [],
    "readme": "<img align=\"right\" src=\"logos/toml-200.png\" alt=\"TOML logo\">\n\n# TOML\n\nTom's Obvious, Minimal Language.\n\nBy Tom Preston-Werner, Pradyun Gedam, et al.\n\n> This repository contains the in-development version of the TOML specification.\n> You can find the released versions at https://toml.io.\n\n## Objectives\n\nTOML aims to be a minimal configuration file format that's easy to read due to\nobvious semantics. TOML is designed to map unambiguously to a hash table. TOML\nshould be easy to parse into data structures in a wide variety of languages.\n\n## Example\n\n```toml\n# This is a TOML document.\n\ntitle = \"TOML Example\"\n\n[owner]\nname = \"Tom Preston-Werner\"\ndob = 1979-05-27T07:32:00-08:00 # First class dates\n\n[database]\nserver = \"192.168.1.1\"\nports = [ 8000, 8001, 8002 ]\nconnection_max = 5000\nenabled = true\n\n[servers]\n\n  # Indentation (tabs and/or spaces) is allowed but not required\n  [servers.alpha]\n  ip = \"10.0.0.1\"\n  dc = \"eqdc10\"\n\n  [servers.beta]\n  ip = \"10.0.0.2\"\n  dc = \"eqdc10\"\n\n[clients]\ndata = [ [\"gamma\", \"delta\"], [1, 2] ]\n\n# Line breaks are OK when inside arrays\nhosts = [\n  \"alpha\",\n  \"omega\"\n]\n```\n\n## Comparison with Other Formats\n\nTOML shares traits with other file formats used for application configuration\nand data serialization, such as YAML and JSON. TOML and JSON both are simple and\nuse ubiquitous data types, making them easy to code for or parse with machines.\nTOML and YAML both emphasize human readability features, like comments that make\nit easier to understand the purpose of a given line. TOML differs in combining\nthese, allowing comments (unlike JSON) but preserving simplicity (unlike YAML).\n\nBecause TOML is explicitly intended as a configuration file format, parsing it\nis easy, but it is not intended for serializing arbitrary data structures. TOML\nalways has a hash table at the top level of the file, which can easily have data\nnested inside its keys, but it doesn't permit top-level arrays or floats, so it\ncannot directly serialize some data. There is also no st",
    "url": "https://github.com/toml-lang/toml",
    "last_updated": "2025-09-02T03:57:09+00:00"
  },
  {
    "full_name": "alexbyrnes/FCC-Political-Ads_The-Code",
    "name": "FCC-Political-Ads_The-Code",
    "description": "Code for extracting data from a large number of PDFs, particularly FCC political ad documents",
    "language": "HTML",
    "topics": [],
    "readme": "## Code Repository for the FCC Political Ad Archive\n \nCode for extracting data from a large number of PDFs, particularly FCC Political Ad documents\n\n\nMain requirements:\n* ImageMagick\n* GNU Parallel\n* PostgreSQL\n* Python\n* Tesseract (3.03)\n* Ghostscript\n\n(Optional)\n* Cuneiform\n\nPython are dependencies in requirements.txt. `pip install -r requirements.txt`\n\nSome python dependencies may be installed with Debian packages:\n\n* python-lxml\n* python-numpy\n* python-scipy\n* python-skimage\n\n\n##### Create database\n\n    createdb fcc\n    psql -d fcc -f schema.sql\n\n##### Edit settings.py\n```\n\nCONNECTION = \"dbname='fcc' user='<user>' host='localhost' password='<pw>'\"\n\npython_bin = '/path/to/scripts/'\nraw_data_dir = 'rawdata/'\ninvalid_data_dir = 'invalid/'\nto_validate_dir = 'to_validate/'\n\n# Directory where pdfs and html directories should be located.\nbasepath = '/path/for/files/'\n```\n\n\n##### Run the tests\n\n    cd fpa\n    nosetests\n\n##### Download files\n\n    python station_dloader.py\n    \n\n##### Get help for main scripts\n\n    python pq.py --help\n    python run_parallel.py --help\n    python run_extract.py --help\n    # (cropbox.py, merge_clusters.py, standardize.py, threshold.py)\n\n##### Identify text PDFs\n\n    python pq.py markCommonFromLocalText\n\n##### Extract data from text PDFs\n\n    python pq.py parseTextInvoices\n    python pq.py parseTextContracts\n    python pq.py parseTextOrders\n\n\n##### Get bounding boxes\n\n    ./get_bboxes.sh\n\n##### Detect document types\n\n    python run_parallel.py --help\n    python run_parallel.py -s=35 -r=420 --psm=3 --targetfield=doctype --outfile=\"doctype.tsv\"\n\n\n##### Extract data from one type\n\n    python run_parallel.py -s=50 -r=400 --targetfield=contract --crop\n\n\n##### Extract using multiple trials and validation \n\n    python run_extract.py --help\n    python run_extract.py --targetfield=contract --crop\n\n\n#### Other Useful Commands\n\n##### Print all Parallel commands  \n\nThis can be used to run extractions on other machines, or without python dependencies.\n\n ",
    "url": "https://github.com/alexbyrnes/FCC-Political-Ads_The-Code",
    "last_updated": "2021-04-29T17:46:39+00:00"
  },
  {
    "full_name": "google/gin-config",
    "name": "gin-config",
    "description": "Gin provides a lightweight configuration framework for Python",
    "language": "Python",
    "topics": [
      "python",
      "configuration-management",
      "tensorflow",
      "tensorflow-experiments"
    ],
    "readme": "# Gin Config\n\n\n**Authors**: Dan Holtmann-Rice, Sergio Guadarrama, Nathan Silberman\n**Contributors**: Oscar Ramirez, Marek Fiser\n\n<!---->\n\nGin provides a lightweight configuration framework for Python, based on\ndependency injection. Functions or classes can be decorated with\n`@gin.configurable`, allowing default parameter values to be supplied from a\nconfig file (or passed via the command line) using a simple but powerful syntax.\nThis removes the need to define and maintain configuration objects (e.g.\nprotos), or write boilerplate parameter plumbing and factory code, while often\ndramatically expanding a project's flexibility and configurability.\n\nGin is particularly well suited for machine learning experiments (e.g. using\nTensorFlow), which tend to have many parameters, often nested in complex ways.\n\nThis is not an official Google product.\n\n## Table of Contents\n\n[TOC]\n\n## Basic usage\n\nThis section provides a high-level overview of Gin's main features, ordered\nroughly from \"basic\" to \"advanced\". More details on these and other features can\nbe found in the [user guide].\n\n[user guide]: https://github.com/google/gin-config/tree/master/docs/index.md\n\n### 1. Setup\n\n\nInstall Gin with pip:\n\n```shell\npip install gin-config\n```\n\nInstall Gin from source:\n\n```shell\ngit clone https://github.com/google/gin-config\ncd gin-config\npython -m setup.py install\n```\n\nImport Gin (without TensorFlow functionality):\n\n```python\nimport gin\n```\n\nImport additional TensorFlow-specific functionality via the `gin.tf` module:\n\n```python\nimport gin.tf\n```\n\nImport additional PyTorch-specific functionality via the `gin.torch` module:\n\n```python\nimport gin.torch\n```\n\n\n### 2. Configuring default values with Gin (`@gin.configurable` and \"bindings\")\n\nAt its most basic, Gin can be seen as a way of providing or changing default\nvalues for function or constructor parameters. To make a function's parameters\n\"configurable\", Gin provides the `gin.configurable` decorator:\n\n```python\n@gin.configurable\ndef dnn(input",
    "url": "https://github.com/google/gin-config",
    "last_updated": "2025-08-27T12:17:10+00:00"
  },
  {
    "full_name": "mkearney/rmd2jupyter",
    "name": "rmd2jupyter",
    "description": "Convert Rmd (rmarkdown) to ipynb (Jupyter notebook)",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "jupyter-notebook",
      "python",
      "ipynb-notebook",
      "python-notebook",
      "rmd",
      "rmarkdown"
    ],
    "readme": "\nrmd2jupyter\n-----------\n\nConvert rmarkdown (.Rmd) files into jupyter notebooks (.ipynb).\n\n### Example\n\nCreate a .Rmd file.\n\n``` r\nrmd <- \"# Module 1. Working with web APIs\n\n## Gathering publicly available web data\n\nThe two most common ways to collect publicly available web data are (1) web scraping and (2) interacting with Application Program Interfaces (API).\n\n## Web scraping\n\n- A normal workflow goes something like this\n    - Extract website source code, known as Extensible Markup Language (XML). XML is similar to HTML only designed for storing not displaying data.\n    - Although XML trees contain elements, tags, and text, data collected via web scraping is almost always unstructred\n\n### Web scraping in R\n- I recommend the {rvest} package.\n\n\\`\\`\\`{r}\nlibrary(rvest)\n\n## population statistics\npopulation_url <- \\\"https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)\\\"\n\n## read in page\npop <- read_html(population_url)\n\\`\\`\\`\"\n\n## save as temp file\ntmp <- tempfile(fileext = \".Rmd\")\ncat(rmd, file = tmp)\n\n## render html\nrmarkdown::render(tmp)\n\n## output output\nbrowseURL(gsub(\"\\\\.Rmd$\", \".html\", tmp))\n```\n\n#### Rmd -&gt; html\n\nScreen capture of output.\n\n![](tools/readme/ss_rmd.png)\n\n#### Rmd -&gt; ipynb.\n\nNow convert to an ipython notebook.\n\n``` r\n## install and load rmd2jupyter\ndevtools::install_github(\"mkearney/rmd2jupyter\")\nlibrary(rmd2jupyter)\n\n## convert\nrmd2jupyter(tmp)\n\n## open via your jupyter notebook method\n```\n\nScreen capture of jupyter notebook.\n\n![](tools/readme/ss_ipynb.png)\n",
    "url": "https://github.com/mkearney/rmd2jupyter",
    "last_updated": "2025-05-13T07:19:47+00:00"
  },
  {
    "full_name": "whotracksme/whotracks.me",
    "name": "whotracks.me",
    "description": "Data from the largest and longest measurement of online tracking.",
    "language": "Jupyter Notebook",
    "topics": [
      "tracking",
      "trackers",
      "transparency",
      "privacy",
      "privacy-tools",
      "cliqz",
      "ghostery"
    ],
    "readme": "&nbsp;\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/ghostery/whotracks.me/master/static/img/who-tracksme-logo.png\" width=\"300px\" alt=\"WhoTracks.Me\" />\n</p>\n<h3 align=\"center\">Bringing Transparency to Online Tracking</h3>\n\n<p align=\"center\">\n  <em>\n    Transparency\n    · Privacy\n    · Tracking landscape\n    · Built by Ghostery\n  </em>\n  <br />\n  <em>\n    <a href=\"https://www.ghostery.com/whotracksme/trackers\" target=\"_blank\" rel=\"noopener noreferrer\">Trackers</a>\n    · <a href=\"https://www.ghostery.com/whotracksme/websites\" target=\"_blank\" rel=\"noopener noreferrer\">Websites</a>\n    · <a href=\"https://www.ghostery.com/whotracksme/explorer\" target=\"_blank\" rel=\"noopener noreferrer\">Explorer</a>\n  </em>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.ghostery.com\" target=\"_blank\" rel=\"noopener noreferrer\">\n    <img alt=\"powered by Ghostery\" src=\"https://img.shields.io/badge/ghostery-powered-blue?logo=ghostery&style=flat-square\">\n  </a>\n  <a href=\"https://github.com/cliqz-oss/adblocker/blob/master/LICENSE\">\n    <img alt=\"License Badge\" src=\"https://img.shields.io/github/license/ghostery/whotracks.me?style=flat-square\"></a>\n</p>\n\n# Downloading the data\n\nEach month, we release a new version of the web site. The data from the last month can be directly [accessed through the website](https://www.ghostery.com/whotracksme/explorer).\n\nThe raw data, from which the graphs have been computed, is also available as an open data set (updated every month). You can also\ndownload historical data. More information on the raw data can be found [here](whotracksme/data/Readme.md).\n\nWhoTracks.me also builts heavily on another open source project called [TrackerDB](https://github.com/ghostery/trackerdb);\nall meta data (e.g. company descriptions) is maintained there.\n\n# Using the data\n\nYou can directly use the [raw data](whotracksme/data/Readme.md), which are all text files. As an alternative, you an also\ndownload it locally and use the Python API:\n\n```\npython3.11 -m venv ",
    "url": "https://github.com/whotracksme/whotracks.me",
    "last_updated": "2025-09-01T16:24:24+00:00"
  },
  {
    "full_name": "minimaxir/movie-gender",
    "name": "movie-gender",
    "description": "Data and code for analyzing Movie Lead Gender.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# movie-gender\nData and code for analyzing Movie Lead Gender. This notebook is a complement to my blog post \"Blockbuster Movies with Male Leads Earn More Than Those with Female Leads.\"\n\nThe movie data is from [OMDb](http://www.omdbapi.com) (result data was licensed under CC-BY 4.0.) Note that raw processing cannot be reproduced since that data is not included.\n\n## Credits\n\n* `actors_list.csv` via [Matt Daniels](https://twitter.com/matthew_daniels) on [his GitHub](https://github.com/matthewfdaniels/scripts/blob/master/data/actor_list.csv)\n* `male_names.txt` and `female_names.txt` from [male](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/male.txt) and [female](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/female.txt) lists via Carnegie-Mellon University.\n* [R Explorations](https://rexplorations.wordpress.com/2015/09/05/simple-outlier-detection-in-r/) for reminder how to use the `outliers` package in R.\n* [r-statistics.co](http://r-statistics.co/Statistical-Tests-in-R.html) for reminder how to implement and parse various statistical tests in R.  \n* [This Stack Overflow answer](http://stackoverflow.com/questions/20689650/how-to-append-rows-to-an-r-data-frame) for answering the best way to append rows of data to a data frame in R.\n",
    "url": "https://github.com/minimaxir/movie-gender",
    "last_updated": "2022-05-03T09:40:30+00:00"
  },
  {
    "full_name": "kosukeimai/qss-swirl",
    "name": "qss-swirl",
    "description": "Swirl Lessons for ``Quantitative Social Science: An Introduction''",
    "language": "R",
    "topics": [],
    "readme": "# qss-swirl [![Build Status](https://travis-ci.org/kosukeimai/qss-swirl.svg?branch=master)](https://travis-ci.org/kosukeimai/qss-swirl)\n\n*News*: See [Interactive Tutorials for QSS by Matt Blackwell](https://github.com/mattblackwell/qsslearnr) which builds on `qss-swirl` but significantly improves it.\n\n\nSwirl Lessons for the book, ``Quantitative Social Science: An Introduction'' (QSS), published by Princeton University Press in 2017. See also the main repo, [qss](../../../qss), for other supplementary materials.  \n\nI welcome your contributions.  Please improve these questions and add some new ones so that others can use them!\n\nIf you are an instructor, you may also be interested in [Socratic Swirl](https://github.com/dimagor/socraticswirl), which can be used to monitor and analyze your students' progress in these swirl lessons with an easy-to-use [dashboard](https://github.com/dimagor/socraticswirlInstructor).\n\n* Install the `swirl` package:\n```\ninstall.packages(\"swirl\")\n```\n\n* Install the `qss-swirl` lessons:\n```\nlibrary(swirl) # load the swirl package\ninstall_course_github(\"kosukeimai\", \"qss-swirl\")\n```\n\n* Start a `qss-swirl` lesson (after loading the `swirl` package):\n```\nswirl()\n```\n\n* Uninstall the `qss-swirl` lessons (after loading the `swirl` package):\n```\nuninstall_course(\"qss-swirl\")\n```\n\n* Update the `qss-swirl` lessons (after loading the `swirl` package):  \n```\nuninstall_course(\"qss-swirl\") # uninstall the course\ninstall_course_github(\"kosukeimai\", \"qss-swirl\") # reinstall the course\n```\n",
    "url": "https://github.com/kosukeimai/qss-swirl",
    "last_updated": "2025-08-26T17:08:09+00:00"
  },
  {
    "full_name": "yinanxu0/mountaintop",
    "name": "mountaintop",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# mountaintop\n[![Python-Version](https://img.shields.io/badge/Python-3.7%7C3.8-brightgreen)](https://github.com/yinanxu0/mountaintop)\n\nEasy to train a model. Will support more model in the future\n\n## Install\n```\npip3 install mountaintop\n```\n\n## Document\nWe afford python package and bin mode. For more details, please check `mountaintop -h`. \n```\nusage: mountaintop [-h] [-v] {train,average,decode,wer,export,visual,eval} ...\n\nMountainTop Line Interface\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         show MountainTop version\n\nsubcommands:\n  use \"mountaintop [sub-command] --help\" to get detailed information about each sub-command\n\n  {train,average,decode,wer,export,visual,eval}\n    train               👋 train a pytorch model\n    average             👋 average a pytorch model\n    decode              👋 decode a pytorch model\n    wer                 👋 compute wer of reference and hypotheses\n    export              👋 export a pytorch model\n    visual              👋 visualize a pytorch model\n    eval                👋 evaluate a pytorch model\n\nmountaintop v0.1.1, a toolkit based on pytorch. Visit https://github.com/yinanxu0/mountaintop for tutorials and documents.\n```\nFor convenience, you can use `mt` instead of `mountaintop`, like `mt -h`.\n\n### Train model\n```\ntorchrun --nnodes=${num_nodes} --nproc_per_node=${num_gpus} --no_python \\\n    mt train --config ${train_config} --model_dir ${model_dir} \\\n    --world_size $num_gpus --dist_backend $dist_backend \\\n    --num_workers 32 --tensorboard_dir ${tensorboard_dir} --pin_memory\n```\n`torchrun` is recommended to easily use multi-gpu and multi-machine.\n\nMore details of parameters in help mode.\n```\nmt train -h\n```\n\n\n### Evaluate model\n```\nmt average --model_dir ${model_dir} --val_best --num ${avg_num} --dst_model ${avg_pt}\nfi\n\nmt decode --config ${train_config} --mode ${mode} --gpu 0 --dict ${dict} \\\n    --checkpoint ${avg_pt} --num_workers ${num_workers} --ctc_weight ${ctc_weight} \\",
    "url": "https://github.com/yinanxu0/mountaintop",
    "last_updated": "2025-01-15T15:28:04+00:00"
  },
  {
    "full_name": "jennybc/code-smells-and-feels",
    "name": "code-smells-and-feels",
    "description": "Talk on code smells and feels and how to change that via refactoring",
    "language": "R",
    "topics": [
      "r",
      "talk",
      "refactoring"
    ],
    "readme": "# Code Smells and Feels\n\nTalk initially prepared for [useR!2018](https://user2018.r-project.org) Brisbane. Also delivered elsewhere, such as the [First Mexican Statistical Association School in Data Science](https://amestad.mx/escuela/1/)  \nby Jenny Bryan  \n[jennybryan.org](https://jennybryan.org)  \nTwitter: [@jennyBryan](https://twitter.com/jennyBryan/)  \nGitHub: [@jennybc](https://github.com/jennybc)  \n\n> \"Code smell\" is an evocative term for that vague feeling of unease we get when reading certain bits of code. It's not necessarily wrong, but neither is it obviously correct. We may be reluctant to work on such code, because past experience suggests it's going to be fiddly and bug-prone. In contrast, there's another type of code that just feels good to read and work on. What's the difference? If we can be more precise about code smells and feels, we can be intentional about writing code that is easier and more pleasant to work on. I've been fortunate to spend the last couple years embedded in a group of developers working on the tidyverse and r-lib packages. Based on this experience, I'll talk about specific code smells and deodorizing strategies for R.\n\n## Link to this repo\n\n[rstd.io/code-smells](https://rstd.io/code-smells) is a shortlink to HERE\n\n## Slides\n\n<a href=\"https://speakerdeck.com/jennybc/code-smells-and-feels\"><img src=\"2018-07_user-brisbane-400.jpeg\"></a>\n\nSlides [on SpeakerDeck](https://speakerdeck.com/jennybc/code-smells-and-feels)\n\nSlides [as PDF file](2018-07_user-brisbane-bryan.pdf) here in this repo\n\n## Video\n\nVideo is available on YouTube:  \n<https://www.youtube.com/watch?v=7oyiPBjLAWY>\n\n## Credits and resources\n\nAnnotated and hyperlink-y list of resources mentioned in the slides, in roughly the same order.\n\n---\n\nDo useRs have less formal training in CS/programming than others writing code?\n\n2018 Stack Overflow Annual Developer Survey: <https://insights.stackoverflow.com/survey>\n\nAdapted from original code by [Julia Silge](https://juliasilge.c",
    "url": "https://github.com/jennybc/code-smells-and-feels",
    "last_updated": "2025-08-28T22:04:54+00:00"
  },
  {
    "full_name": "hannawallach/python-lda",
    "name": "python-lda",
    "description": "LDA in Python.",
    "language": "Python",
    "topics": [],
    "readme": "",
    "url": "https://github.com/hannawallach/python-lda",
    "last_updated": "2025-04-15T12:12:35+00:00"
  },
  {
    "full_name": "samcarlos/Rstan-Neural-Net",
    "name": "Rstan-Neural-Net",
    "description": "Simple 3 Hidden Layers with 10 nodes each to predict on Iris Dataset",
    "language": "R",
    "topics": [],
    "readme": "# Rstan-Neural-Net\nSimple 3 Hidden Layers with 10 nodes each to predict on Iris Dataset\n",
    "url": "https://github.com/samcarlos/Rstan-Neural-Net",
    "last_updated": "2020-10-28T16:28:44+00:00"
  },
  {
    "full_name": "RevolutionAnalytics/dplyr-spark",
    "name": "dplyr-spark",
    "description": "spark backend for dplyr",
    "language": "R",
    "topics": [],
    "readme": "\n\n\n# dplyr.spark\n\nThis package implements a [`spark`](http://spark.apache.org/) backend for the [`dplyr`](http://github.com/hadley/dplyr) package, providing a powerful and intuitive DSL to manipulate large datasets on a powerful big data platform.  It is a simple package: simple to learn if you have any familiarity with `dplyr` or even just R and SQL, simple to deploy: just a few packages to install on a single machine, as long as your Spark installation comes with JDBC support -- or build it in, instructions below.\nThe current state of the project is:\n\n - most `dplyr` features supported\n - adds some `spark`-specific goodies, like *caching* tables.\n - can go succesfully through tutorials for `dplyr` like any other database backend^[with the exception of one bug to avoid which you need to run Spark from trunk or wait for version 1.5, see [SPARK-9221](https://issues.apache.org/jira/browse/SPARK-9921)]. \n - not yet endowed with a thorugh test suite. Nonetheless we expect it to inherit much of its correctness, scalability and robustness from its main dependencies, `dplyr` and `spark`.\n - we don't recommend production use yet\n\n## Installation\n\nYou need to [download spark](https://spark.apache.org/downloads.html) and [build it](https://spark.apache.org/docs/latest/building-spark.html) as follows\n\n```\ncd <spark root>\nbuild/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests -Phive -Phive-thriftserver clean package\n```\n\nIt may work with other hadoop versions, but we need the hive and hive-thriftserver support. The package is able to start the thirft server but can also connect to a running one.\n\n`dplyr.spark` has a few dependencies: get them with\n\n```\ninstall.packages(c(\"RJDBC\", \"dplyr\", \"DBI\", \"devtools\"))\ndevtools::install_github(\"hadley/purrr\")\n```\n\nIndirectly `RJDBC` needs `rJava`. Make sure that you have `rJava` working with:\n\n\n```r\nlibrary(rJava)\n.jinit()\n```\n\nThis is only a test, in general you don't need it before loading `dplyr.spark`.\n\n----------------\n\n###",
    "url": "https://github.com/RevolutionAnalytics/dplyr-spark",
    "last_updated": "2023-06-21T08:27:16+00:00"
  },
  {
    "full_name": "stevencarlislewalker/lme4ord",
    "name": "lme4ord",
    "description": "Ecological mixed-effects ordination with lme4",
    "language": "R",
    "topics": [],
    "readme": "lme4ord (l-m-e-ford)\n====================\n\n[New website](http://stevencarlislewalker.github.io/lme4ord/)\n\nGeneralized linear mixed-effects models with flexible (co)variance\nstructure.  See the currently\nevolving [mission statement](https://github.com/stevencarlislewalker/lme4ord/issues/1).\n\nThis package is not at all stable ... but getting more so!\n\n",
    "url": "https://github.com/stevencarlislewalker/lme4ord",
    "last_updated": "2025-02-09T21:38:35+00:00"
  },
  {
    "full_name": "dbamman/book-segmentation",
    "name": "book-segmentation",
    "description": "Labeled segmentation for the document structure of printed books",
    "language": "Python",
    "topics": [],
    "readme": "# book-segmentation\n\nData, code and trained models to segment the document structure of printed books and label each segment according to ten categories:\n\n* Title page (including half titles)\n* Ad card (advertisements)\n* Publisher information\n* Dedication\n* Preface\n* Table of contents\n* Text\n* Appendix\n* Index\n* N/A\n\nData, categorization system, and models described in more detail here:\n\nLara McConnaughey, Jennifer Dai and David Bamman (2017), \"[The Labeled Segmentation of Printed Books](http://people.ischool.berkeley.edu/~dbamman/pubs/pdf/emnlp2017_book_segmentation.pdf)\" (EMNLP 2017) \n\n\nThis model makes use of data from Ted Underwood's [DataMunging repo](https://github.com/tedunderwood/DataMunging)\n\n## Usage\n\nTo segment a book from the HathiTrust named `book.zip` using the default model:\n`python code/segment_book.py book.zip models/labseg10/`\n\nThis should output a list of page numbers and labels for all pages in book.zip.\n\n## Dependencies\n\nNumpy (`pip install numpy --user`), scipy (`pip install scipy --user`) and [Tensorflow 1.0](https://www.tensorflow.org/install/)\n",
    "url": "https://github.com/dbamman/book-segmentation",
    "last_updated": "2025-07-18T23:38:53+00:00"
  },
  {
    "full_name": "raphael-susewind/name2community",
    "name": "name2community",
    "description": "Probabilistic inference of religious community from South Asian names",
    "language": "Perl",
    "topics": [],
    "readme": "# name2community\nProbabilistic inference of religious community from South Asian names\n\nThis software is a sample implementation (under a GNU Affero General Public license; see LICENSE) of an algorithm that infers likely religious community from South Asian names:\n\n> Susewind, Raphael (2015). What's in a name? Probabilistic inference of religious community from South Asian names. Field Methods 27(4), 319-332. http://dx.doi.org/10.1177/1525822X14564275\n\nWhile the version of reference is available under the abovementioned link as supplemental material to the journal article, the software might be updated from time to time - the purpose of this repository (see [releases](https://github.com/raphael-susewind/name2community/releases)). Links to empirical applications and new derivative datasets can be found at:  http://data.raphael-susewind.de\n\nPlease read the original article as well as the blog posts mentioned on my website to get a clear picture of the purpose and limitations of this algorithm. You will likely need to produce a sample list of names with known community affiliation drawn from your area of study in order to properly establish the accuracy of the algorithm for your purpose. Likewise, you will need to generate your own master name list using the createnamedb.pl script before you can use guesscommunity.pl itself.\n\nBoth scripts are implemented in Perl and run best under Linux, though I have heard of successful runs under Windows as well. They need a fully unicode aware version of Perl (i.e. above 5.8) as well as several modules (in particular WWW::Mechanize and DBD::SQlite). The fuzzy soundex matching relies on python (version 2) scripts developed by Swathanthra Indian Language Processing Applications: http://silpa.org.in/\n\nI am happy to answer your questions about this sample implementation and to hear of your successes and failures. You can reach me at mail@raphael-susewind.de.\n",
    "url": "https://github.com/raphael-susewind/name2community",
    "last_updated": "2025-06-10T20:44:29+00:00"
  },
  {
    "full_name": "bwallace/ACL-2014-irony",
    "name": "ACL-2014-irony",
    "description": "Code and data from our ACL 2014 paper \"Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)\"",
    "language": "Python",
    "topics": [],
    "readme": "README\n---\nCode & data to reproduce the analyses in our ACL 2014 paper: \n\n    Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)\n        Byron C Wallace, Do Kook Choe, Laura Kertz, and Eugene Charniak\n\nMade possible by support from the Army Research Office (ARO), grant 64481-MA / W9111F-13-1-0406\n\"Sociolinguistically Informed Natural Language Processing: Automating Irony Detection\"\n\nContact: Byron Wallace (byron.wallace@gmail.com)\n\nWe make the data available in several ways; if you just want a simple CSV, see below! \n\n* The actual database - a flat sqlite file - is ironate.db.zip, it needs to be unzipped, of course. \n* The database-schema.txt file (in this directory) contains information regarding the database.\n* See irony_stats.py for instructions on how to reproduce our analyses. This also gives examples on working with the database in python (and in SQL, since we issue queries directly). Note that this requires sklearn, numpy, & statsmodels modules to be installed.\n\n\n**** \nWe are also making the data available as a simple CSV, with each row corresponding to a comment and a summary label provided. This label is 1 if any annotator (of the three) labeled any segment in the corresponding comment as 1. This is therefore a very liberal definition of 'irony', and may or may not be appropriate, depending on the aims and task.\n",
    "url": "https://github.com/bwallace/ACL-2014-irony",
    "last_updated": "2024-01-04T15:56:40+00:00"
  },
  {
    "full_name": "dagster-io/fake-star-detector",
    "name": "fake-star-detector",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# fake-star-detector: A Dagster tutorial\n\nThis is a simple Dagster project to analyze the number of fake GitHub stars on any GitHub repository.  It is a companion to the blog post found [on the Dagster blog](https://dagster.io/blog/fake-stars).\n\n\nThis project consists of two models:\n- [Simpler model](#trying-the-simpler-model-using-data-from-the-github-api): A simple model running “low activity” heuristic. This simple heuristic can detect many (but hardly all) suspected fake accounts that starred the same set of repositories, using nothing but data from the GitHub REST API (via [pygithub](https://github.com/PyGithub/PyGithub)).\n\n- [Complex detector](#running-the-complex-model-using-bigquery-archive-data): An alternative detection model which runs a sophisticated clustering algorithm as well as the heuristic, using the public [GH Archive](https://www.gharchive.org) available in Bigquery. This model is written in SQL and uses [dbt](https://github.com/dbt-labs/dbt-core) alongside Dagster.\n  * *Note: You can run this within the limits of a free-tier BQ account, but the analysis will be reduced in scope. By default, this model only scans data in 2023 on a small repository, in order to make it stay within the free-tier quota.*\n\n<p align=\"center\">\n    <img width=\"600\" alt=\"global-asset-lineage\" src=\"./screenshots/global-asset-lineage.png\">\n</p>\n\n## Table of contents\n- [Table of contents](#table-of-contents)\n- [Getting started](#getting-started)\n  - [Install instructions](#install-instructions)\n    - [Getting GitHub Access Token for the simpler model](#getting-github-access-token-for-the-simpler-model)\n    - [Creating Google Service Account for the complex model](#creating-google-service-account-for-the-complex-model)\n  - [Setting up your local environment](#setting-up-your-local-environment)\n    - [Cloning the repo](#cloning-the-repo)\n  - [Building a virtual environment](#building-a-virtual-environment)\n  - [Running Dagster locally](#running-dagster-locally)\n  - [Running D",
    "url": "https://github.com/dagster-io/fake-star-detector",
    "last_updated": "2025-08-26T09:08:35+00:00"
  },
  {
    "full_name": "gojiplus/ycombo",
    "name": "ycombo",
    "description": "ycombo: Post a Random GitHub Repo to Hacker News",
    "language": "Python",
    "topics": [
      "github",
      "hacker-news",
      "ycombinator"
    ],
    "readme": "# 🚀 ycombo: Post a Random GitHub Repo. to Hacker News\n\nThis GitHub Action picks a random public repo from your GitHub profile (or orgs) and submits it to [Hacker News](https://news.ycombinator.com/submit) via a headless browser using your session cookie.\n\nPerfect for passive social proof, open-source visibility, or quietly testing demand.\n\n---\n\n## ✨ Features\n\n- Picks repos from your GitHub username and any orgs you specify\n- Filters to repos with ≥5 stars\n- Generates a `Show HN: {repo}` title\n- Submits to Hacker News using Playwright and your HN session cookie\n\n---\n\n## 🔧 Setup\n\n### 1. Secrets\nAdd the following GitHub Action secrets:\n\n| Name            | Description                                  |\n|-----------------|----------------------------------------------|\n| `HN_USER_COOKIE` | Your `user` session cookie from Hacker News |\n\n### 2. Example workflow\n\n```yaml\nname: Post to HN\n\non:\n  schedule:\n    - cron: '0 17 * * 1'  # every Monday\n  workflow_dispatch:\n\njobs:\n  post:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - uses: ./.github/actions/post-to-hn\n        with:\n          github-username: soodoku\n          github-orgs: appeler,recite,gojiplus\n          hn-cookie: ${{ secrets.HN_USER_COOKIE }}\n```\n\n---\n\n## 🧠 Notes\n\n- Repos must have at least 5 stars to be considered.\n- The Hacker News `user` cookie should be stable unless you log out or change your password.\n- You can monitor your posts at: `https://news.ycombinator.com/submitted?id=YOUR_USERNAME`\n\n---\n\n## 📦 Local Testing\nTo run it manually:\n```bash\nexport GITHUB_USERNAME=soodoku\nexport GITHUB_ORGS=appeler,recite\nexport HN_USER_COOKIE='your_cookie_here'\npython scripts/post_repo_to_hn.py\n```\n\n---\n\n## License\nMIT © Gojiplus\n\n",
    "url": "https://github.com/gojiplus/ycombo",
    "last_updated": "2025-06-14T22:41:18+00:00"
  },
  {
    "full_name": "ercexpo/us-news-domains",
    "name": "us-news-domains",
    "description": "A list of over 5000 US news domains and their social media accounts",
    "language": "",
    "topics": [],
    "readme": "# A list of over 5000 US news domains and their social media accounts\n\nThis repository contains a list of and information on 5,397 US news domains (5,400 domain in release v1.0.0). The domains overwhelmingly represent US-based organizations, but since the list was partly sourced from browsing data, it also includes some international domains visited by US study participants. The repository contains the following data sets:\n\n- `us-news-domains-v2.0.0.csv`: The main data set of news domains. We only include top-level domains, except when news is only a part of the organization's web site (e.g. yahoo.com/news, msn.com/en-us/news etc). The data set further contains a continuous variable `ideology` and a categorical variable `type` indicating whether the outlet is national, local or international. For coding see below.\n- `us-news-twitter-v1.0.0.csv`: The Twitter accounts linked to each domain, if available. Many news organizations have several Twitter accounts (and therefore several rows in this data set). \n- `us-news-facebook-v1.0.0.csv`: The Facebook accounts linked to each domain, if available.\n\n## Data sources\n\nThe main list of web domains was collected from the following sources:\n\n1. A list of Alexa's most popular 1,000 domains from 2018.\n2. The most frequented domains in the trace data collected through the ERC EXPO project between May-December 2019.\n3. The domains most often tweeted by US politicians at the time. \n\nThese domains were included only if manually labelled as news. To account for local news, the list was supplemented with:\n\n4. Domains from the website <https://usnpl.com>, which lists (overwhelmingly local) newspapers per US state. Newspapers with the same domain but a unique URL path (e.g. \"dothaneagle.com/enterprise_ledger\" and \"dothaneagle.com/eufaula_tribune\") were subsumed under their top-level domain, provided this was still a news domain. \n5. Domains from the website <https://www.officialusa.com/stateguides/media/television/>, which lists local t",
    "url": "https://github.com/ercexpo/us-news-domains",
    "last_updated": "2025-06-29T05:31:12+00:00"
  },
  {
    "full_name": "spotify/annoy",
    "name": "annoy",
    "description": "Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk",
    "language": "C++",
    "topics": [
      "c-plus-plus",
      "python",
      "nearest-neighbor-search",
      "locality-sensitive-hashing",
      "approximate-nearest-neighbor-search",
      "golang",
      "lua"
    ],
    "readme": "Annoy\n-----\n\n\n\n.. figure:: https://raw.github.com/spotify/annoy/master/ann.png\n   :alt: Annoy example\n   :align: center\n\n.. image:: https://github.com/spotify/annoy/actions/workflows/ci.yml/badge.svg\n    :target: https://github.com/spotify/annoy/actions\n\nAnnoy (`Approximate Nearest Neighbors <http://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor>`__ Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are `mmapped <https://en.wikipedia.org/wiki/Mmap>`__ into memory so that many processes may share the same data.\n\nInstall\n-------\n\nTo install, simply do ``pip install --user annoy`` to pull down the latest version from `PyPI <https://pypi.python.org/pypi/annoy>`_.\n\nFor the C++ version, just clone the repo and ``#include \"annoylib.h\"``.\n\nBackground\n----------\n\nThere are some other libraries to do nearest neighbor search. Annoy is almost as fast as the fastest libraries, (see below), but there is actually another feature that really sets Annoy apart: it has the ability to **use static files as indexes**. In particular, this means you can **share index across processes**. Annoy also decouples creating indexes from loading them, so you can pass around indexes as files and map them into memory quickly. Another nice thing of Annoy is that it tries to minimize memory footprint so the indexes are quite small.\n\nWhy is this useful? If you want to find nearest neighbors and you have many CPU's, you only need to build the index once. You can also pass around and distribute static files to use in production environment, in Hadoop jobs, etc. Any process will be able to load (mmap) the index into memory and will be able to do lookups immediately.\n\nWe use it at `Spotify <http://www.spotify.com/>`__ for music recommendations. After running matrix factorization algorithms, every user/item can be represented as a vector in f-dimensiona",
    "url": "https://github.com/spotify/annoy",
    "last_updated": "2025-08-31T14:11:47+00:00"
  },
  {
    "full_name": "guardian/online-abuse-analysis",
    "name": "online-abuse-analysis",
    "description": "",
    "language": "Scala",
    "topics": [],
    "readme": "# Online Abuse Analysis\n\nThis is the source code behind the Guardian's analysis of online abuse : [results](https://www.theguardian.com/technology/2016/apr/12/the-dark-side-of-guardian-comments), [methodology](https://www.theguardian.com/technology/2016/apr/12/how-we-analysed-70m-comments-guardian-website).\n\nThis is a Spark + Scala application, which runs on AWS.\n",
    "url": "https://github.com/guardian/online-abuse-analysis",
    "last_updated": "2023-04-12T14:51:26+00:00"
  },
  {
    "full_name": "johnmchambers/XRJulia",
    "name": "XRJulia",
    "description": "XR-style Interface to Julia (from \"Extending R\") ",
    "language": "R",
    "topics": [],
    "readme": "# XRJulia - An Interface from R to Julia\n\nThis package provides an interface from R to Julia, based on the XR\nstructure, as implemented in the XR package, in this repository.\n\nThe interface is designed as a basis for computations in R that use\nfunctions, objects and classes in Julia.\nIn particular, the design caters to programmers developing application\npackages.\nThe XR structure encourages definition of proxy functions and classes\nin \\R{}, which users of the package can treat essentially as they\nwould in R, without special programming imposed by the interface.\n\nThe interface structure is described in the book\n*Extending R* (John M. Chambers, 2016, Chapman & Hall).\nA pdf version of the XRJulia chapter from the book is included with the\ndocumentation of this package.  To open the pdf file from R:\n\n  `RShowDoc(\"Chapter_XRJulia\", package = \"XRJulia\")`\n\nVersion 0.76 of the package has been updated to deal with back-incompatible\nchanges in Julia (up to version 0.6).\n",
    "url": "https://github.com/johnmchambers/XRJulia",
    "last_updated": "2025-09-02T04:38:35+00:00"
  },
  {
    "full_name": "joshkatz/needs",
    "name": "needs",
    "description": "An R function for safe package loading / installation.",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from readme-src.Rmd. Please edit that file -->\n\n\n\n# needs\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/needs)](http://cran.r-project.org/package=needs)\n\n`needs` is a simple R package to make package loading / installation hassle-free &mdash; use it in place of `library` to attach packages and automatically install any that are missing. You can also supply a minimum version number, and it will update old packages as needed. No more changing your code to reinstall packages every time you update R &mdash; `needs` does it for you.\n\n\n```r\ninstall.packages(\"needs\")\n# for the dev version:\n# devtools::install_github(\"joshkatz/needs\", ref = \"development\")\nlibrary(needs)\n\n# answer \"yes\" when prompted, and you will never have\n# to type library or install.packages again. hooray.\n```\n\n### Usage\nOnce installed, use just as you would `library`. With the added bonus of being able to give multiple unquoted arguments in one single function call. Specify a required package version with a pairlist:\n\n\n```r\nneeds(foo,\n      bar = \"0.9.1\",\n      baz = \"0.4.3\")\n```\n\n\n### Rprofile\n`needs` can help make code-sharing easier. In your project directory:\n\n```r\nneeds::toProfile()\n```\nThis extracts the package contents and appends it to the Rprofile in your working directory. Now if someone else clones your project, your code runs without requiring any extra installation or throwing errors for uninstalled packages.\n",
    "url": "https://github.com/joshkatz/needs",
    "last_updated": "2024-06-20T07:02:56+00:00"
  },
  {
    "full_name": "duckdb/duckdb",
    "name": "duckdb",
    "description": "DuckDB is an analytical in-process SQL database management system",
    "language": "C++",
    "topics": [
      "sql",
      "database",
      "olap",
      "analytics",
      "embedded-database"
    ],
    "readme": "<div align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"logo/DuckDB_Logo-horizontal.svg\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"logo/DuckDB_Logo-horizontal-dark-mode.svg\">\n    <img alt=\"DuckDB logo\" src=\"logo/DuckDB_Logo-horizontal.svg\" height=\"100\">\n  </picture>\n</div>\n<br>\n\n<p align=\"center\">\n  <a href=\"https://github.com/duckdb/duckdb/actions\"><img src=\"https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main\" alt=\"Github Actions Badge\"></a>\n  <a href=\"https://discord.gg/tcvwpjfnZx\"><img src=\"https://shields.io/discord/909674491309850675\" alt=\"discord\" /></a>\n  <a href=\"https://github.com/duckdb/duckdb/releases/\"><img src=\"https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white\" alt=\"Latest Release\"></a>\n</p>\n\n## DuckDB\n\nDuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html).\n\nDuckDB is available as a [standalone CLI application](https://duckdb.org/docs/stable/clients/cli/overview) and has clients for [Python](https://duckdb.org/docs/stable/clients/python/overview), [R](https://duckdb.org/docs/stable/clients/r), [Java](https://duckdb.org/docs/stable/clients/java), [Wasm](https://duckdb.org/docs/stable/clients/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdb.org/docs/stable/clients/r#duckplyr-dplyr-api).\n\nFor more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/stable/).\n\n## Installation\n\nIf you",
    "url": "https://github.com/duckdb/duckdb",
    "last_updated": "2025-09-02T08:57:29+00:00"
  },
  {
    "full_name": "blindglobe/common-lisp-stat",
    "name": "common-lisp-stat",
    "description": "Common Lisp Statistics -- based on LispStat (Tierney) but updated for Common Lisp and incorporating lessons from R (http://www.r-project.org/).  See the google group for lisp stat / common lisp statistics for a mailing list.",
    "language": "Common Lisp",
    "topics": [],
    "readme": "\nTime-stamp: <2014-04-07 16:28:57 tony>\n\n* Current Status: IMPROVING\n\n  but we are rebuilding it.\n\n* Fast Start\n\n  Here's a general fast start approach for using this.   \n\n  1. Get access to a Common Lisp implementation on your platform.\n     Make sure you have BLAS, LAPACK, and their corresponding\n     development environments on your system.\n\n     For example, on a recent debian (testing), one may need:\n#+BEGIN_EXAMPLE\nlibffi6\nlibblas\nliblapack\nlibgsl\n#+END_EXAMPLE\n     and the corresponding /-dev/ packages.\n\n  2. if needed, install quicklisp (http://www.quicklisp.org)\n  3. if wanted, install git (http://www.git-scm.org/)\n  4. Use git to fetch common-lisp-statistics from the repository:\n        git://github.com/blindglobe/common-lisp-stat.git\n     and put it into your quicklisp local-projects directory\n     (you will need to put a few more projects there as well), \n     OR make sure you have an internet connection and go to step 5 and\n     fetch a not-so-bleeding-edge version from QuickLisp.\n\n     We suggest something like:\n\n#+begin_src shell\nmkdir ~/quicklisp/\nmkdir ~/quicklisp/local-projects\ncd ~/quicklisp/local-projects\ngit clone git://github.com/blindglobe/common-lisp-stat.git\ngit clone git://github.com/blindglobe/lisp-matrix.git\n#+end_src\n\n     (with the 5th and on lines representing stuff undergoing rapid\n     change, so possibly out of date on QUICKLISP).\n\n     You might need to grab XARRAY as well, and if behind a firewall,\n     might need to use HTTP or similar alternative transport.\n\n  5. Start up the Common Lisp implementation, and:\n\n#+name: loadIt\n#+begin_src lisp\n  (ql:register-local-projects)\n  (ql:quickload :antik)\n  (ql:quickload :cls)\n#+end_src\n\n     TODO: Unfortunately, it looks like ANTIK needs to be preloaded,\n     or it gets confused.  I don't yet know why.\n\n  6. get coffee, tea, water, beer, or a glass of wine, review the\n     Common Lisp Statistics mailing list, chat with friends, etc,\n     until it is done compiling and loading.\n  7. Report",
    "url": "https://github.com/blindglobe/common-lisp-stat",
    "last_updated": "2025-05-25T20:08:23+00:00"
  },
  {
    "full_name": "facebookresearch/mmf",
    "name": "mmf",
    "description": "A modular framework for vision & language multimodal research from Facebook AI Research (FAIR)",
    "language": "Python",
    "topics": [
      "pytorch",
      "vqa",
      "pretrained-models",
      "multimodal",
      "deep-learning",
      "captioning",
      "dialog",
      "textvqa",
      "hateful-memes",
      "multi-tasking"
    ],
    "readme": "\n<div align=\"center\">\n<img src=\"https://mmf.sh/img/logo.svg\" width=\"50%\"/>\n</div>\n\n#\n\n<div align=\"center\">\n  <a href=\"https://mmf.sh/docs\">\n  <img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/mmf/badge/?version=latest\"/>\n  </a>\n  <a href=\"https://circleci.com/gh/facebookresearch/mmf\">\n  <img alt=\"CircleCI\" src=\"https://circleci.com/gh/facebookresearch/mmf.svg?style=svg\"/>\n  </a>\n</div>\n\n---\n\nMMF is a modular framework for vision and language multimodal research from Facebook AI Research. MMF contains reference implementations of state-of-the-art vision and language models and has powered multiple research projects at Facebook AI Research. See full list of project inside or built on MMF [here](https://mmf.sh/docs/notes/projects).\n\nMMF is powered by PyTorch, allows distributed training and is un-opinionated, scalable and fast. Use MMF to **_bootstrap_** for your next vision and language multimodal research project by following the [installation instructions](https://mmf.sh/docs/). Take a look at list of MMF features [here](https://mmf.sh/docs/getting_started/features).\n\nMMF also acts as **starter codebase** for challenges around vision and\nlanguage datasets (The Hateful Memes, TextVQA, TextCaps and VQA challenges). MMF was formerly known as Pythia. The next video shows an overview of how datasets and models work inside MMF. Checkout MMF's [video overview](https://mmf.sh/docs/getting_started/video_overview).\n\n\n## Installation\n\nFollow installation instructions in the [documentation](https://mmf.sh/docs/).\n\n## Documentation\n\nLearn more about MMF [here](https://mmf.sh/docs).\n\n## Citation\n\nIf you use MMF in your work or use any models published in MMF, please cite:\n\n```bibtex\n@misc{singh2020mmf,\n  author =       {Singh, Amanpreet and Goswami, Vedanuj and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and\n                 Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},\n  title =        {MMF: A multimodal framework for vision and lang",
    "url": "https://github.com/facebookresearch/mmf",
    "last_updated": "2025-08-30T13:18:09+00:00"
  },
  {
    "full_name": "mukul13/RYandexTranslate",
    "name": "RYandexTranslate",
    "description": "R package to translate text using Yandex Translate API",
    "language": "R",
    "topics": [],
    "readme": "# RYandexTranslate\r\n\r\n[![Travis-CI Build Status](https://travis-ci.org/mukul13/RYandexTranslate.svg?branch=master)](https://travis-ci.org/mukul13/RYandexTranslate)\r\n[![Downloads](http://cranlogs.r-pkg.org/badges/grand-total/RYandexTranslate)](http://cran.r-project.org/package=RYandexTranslate)\r\n[![cran version](http://www.r-pkg.org/badges/version/RYandexTranslate)](http://cran.rstudio.com/web/packages/RYandexTranslate)\r\n\r\nR package to translate text using Yandex Translate API.\r\n\r\n##Installation\r\n\r\nTo install from CRAN repository:\r\n\r\n```R\r\ninstall.packages(\"RYandexTranslate\")\r\n```\r\n\r\nTo install from github:\r\n\r\n```R\r\nlibrary(devtools)\r\ninstall_github(\"mukul13/RYandexTranslate\")\r\n```\r\n\r\nTo get free API key, sign up [here](https://tech.yandex.com/translate/doc/dg/concepts/api-overview-docpage/)\r\n\r\n##Examples\r\n\r\nTo list all functions supported by RYandexTranslate package\r\n\r\n```R\r\nlibrary(RYandexTranslate)\r\nls(\"package:RYandexTranslate\")\r\n```\r\n```R\r\n#>\"detect_language\"   \"get_translation_direction\"   \"translate\"   \r\n```\r\n\r\nTo get a list of translation directions supported by the service\r\n\r\n```R\r\napi_key=\"YOUR API KEY\"\r\ndirections=get_translation_direction(api_key)\r\nhead(directions)\r\n```\r\n```\r\n#>\"az-ru\" \"be-bg\" \"be-cs\" \"be-de\" \"be-en\" \"be-es\"\r\n```\r\nTo detect the language of the specified text\r\n\r\n```R\r\ndata=detect_language(api_key,text=\"how are you?\")\r\ndata\r\n```\r\n```R\r\n#>\"en\"\r\n```\r\n\r\nTo translate text to the specified language\r\n\r\n```R\r\ndata=translate(api_key,text=\"how are you?\",lang=\"en-hi\"  )\r\n```\r\n```R\r\n#>$lang\r\n#>[1] \"en-hi\"\r\n#>\r\n#>$text\r\n#>[1] \"आप कैसे हैं?\"\r\n```\r\n##Resources\r\n* [Rpubs Blog link](http://www.rpubs.com/mukul13/RYandexTranslate)\r\n* [Yandex Translate API doc](https://tech.yandex.com/translate/doc/dg/concepts/api-overview-docpage/)\r\n",
    "url": "https://github.com/mukul13/RYandexTranslate",
    "last_updated": "2024-09-04T07:16:24+00:00"
  },
  {
    "full_name": "tidyverse/glue",
    "name": "glue",
    "description": "Glue strings to data in R. Small, fast, dependency free interpreted string literals.",
    "language": "R",
    "topics": [
      "r",
      "strings",
      "string-interpolation"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# glue <a href=\"https://glue.tidyverse.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"glue website\" /></a>\n\n<!-- badges: start -->\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/glue)](https://cran.r-project.org/package=glue)\n[![R-CMD-check](https://github.com/tidyverse/glue/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/glue/actions/workflows/R-CMD-check.yaml)\n[![test-coverage](https://github.com/tidyverse/glue/actions/workflows/test-coverage.yaml/badge.svg)](https://github.com/tidyverse/glue/actions/workflows/test-coverage.yaml)\n<!-- badges: end -->\n\nglue offers interpreted string literals that are small, fast, and\ndependency-free. glue does this by embedding R expressions in curly\nbraces, which are then evaluated and inserted into the string.\n\n## Installation\n\n<div class=\".pkgdown-release\">\n\n``` r\n# Install released version from CRAN\ninstall.packages(\"glue\")\n```\n\n</div>\n\n<div class=\".pkgdown-devel\">\n\n``` r\n# Install development version from GitHub\npak::pak(\"tidyverse/glue\")\n```\n\n</div>\n\n## Usage\n\n`glue()` makes it easy to interpolate data into strings:\n\n``` r\nlibrary(glue)\n\nname <- \"Fred\"\nglue(\"My name is {name}.\")\n#> My name is Fred.\n```\n\n`stringr::str_glue()` is an alias for `glue::glue()`. So if you’ve\nalready attached stringr (or perhaps the whole tidyverse), you can use\n`str_glue()` to access all of the functionality of `glue()`:\n\n``` r\nlibrary(stringr) # or library(tidyverse)\n\nname <- \"Wilma\"\nstr_glue(\"My name is {name}.\")\n#> My name is Wilma.\n```\n\nYou’re not limited to using a bare symbol inside `{}`; it can be any\nlittle bit of R code:\n\n``` r\nname <- \"Pebbles\"\nglue(\"Here is my name in uppercase and doubled: {strrep(toupper(name), 2)}.\")\n#> Here is my name in uppercase and doubled: PEBBLESPEBBLES.\n```\n\n### Data can come from various sources\n\nglue can interpolate values from the local environment or from data\npassed in `na",
    "url": "https://github.com/tidyverse/glue",
    "last_updated": "2025-08-14T21:40:45+00:00"
  },
  {
    "full_name": "e2b-dev/E2B",
    "name": "E2B",
    "description": "Open-source, secure environment with real-world tools for enterprise-grade agents.",
    "language": "MDX",
    "topics": [
      "ai",
      "gpt",
      "openai",
      "python",
      "software",
      "typescript",
      "react",
      "devtools",
      "llm",
      "nextjs",
      "development",
      "gpt-4",
      "javascript",
      "agent",
      "ai-agent",
      "ai-agents",
      "code-interpreter",
      "copilot"
    ],
    "readme": "<!-- <p align=\"center\">\n  <img width=\"100\" src=\"/readme-assets/logo-circle.png\" alt=\"e2b logo\">\n</p> -->\n\n![E2B SDK Preview](/readme-assets/e2b-sdk-light.png#gh-light-mode-only)\n![E2B SDK Preview](/readme-assets/e2b-sdk-dark.png#gh-dark-mode-only)\n\n<h4 align=\"center\">\n  <a href=\"https://pypi.org/project/e2b/\">\n    <img alt=\"Last 1 month downloads for the Python SDK\" loading=\"lazy\" width=\"200\" height=\"20\" decoding=\"async\" data-nimg=\"1\"\n    style=\"color:transparent;width:auto;height:100%\" src=\"https://img.shields.io/pypi/dm/e2b?label=PyPI%20Downloads\">\n  </a>\n  <a href=\"https://www.npmjs.com/package/e2b\">\n    <img alt=\"Last 1 month downloads for the JavaScript SDK\" loading=\"lazy\" width=\"200\" height=\"20\" decoding=\"async\" data-nimg=\"1\"\n    style=\"color:transparent;width:auto;height:100%\" src=\"https://img.shields.io/npm/dm/e2b?label=NPM%20Downloads\">\n  </a>\n</h4>\n\n<!---\n<img width=\"100%\" src=\"/readme-assets/preview.png\" alt=\"Cover image\">\n--->\n## What is E2B?\n[E2B](https://www.e2b.dev/) is an open-source infrastructure that allows you to run AI-generated code in secure isolated sandboxes in the cloud. To start and control sandboxes, use our [JavaScript SDK](https://www.npmjs.com/package/@e2b/code-interpreter) or [Python SDK](https://pypi.org/project/e2b_code_interpreter).\n\n> [!NOTE]\n> This repository contains the core E2B SDK that's used in our main [E2B Code Interpreter SDK](https://github.com/e2b-dev/code-interpreter).\n\n## Run your first Sandbox\n\n### 1. Install SDK\n\nJavaScript / TypeScript\n```\nnpm i @e2b/code-interpreter\n```\n\nPython\n```\npip install e2b-code-interpreter\n```\n\n### 2. Get your E2B API key\n1. Sign up to E2B [here](https://e2b.dev).\n2. Get your API key [here](https://e2b.dev/dashboard?tab=keys).\n3. Set environment variable with your API key\n```\nE2B_API_KEY=e2b_***\n```     \n\n### 3. Execute code with code interpreter inside Sandbox\n\nJavaScript / TypeScript\n```ts\nimport { Sandbox } from '@e2b/code-interpreter'\n\nconst sandbox = await Sandbox.create()\nawait sandb",
    "url": "https://github.com/e2b-dev/E2B",
    "last_updated": "2025-09-02T09:56:10+00:00"
  },
  {
    "full_name": "PyImageSearch/imutils",
    "name": "imutils",
    "description": "A series of convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.",
    "language": "Python",
    "topics": [],
    "readme": "# imutils\nA series of convenience functions to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and ***both*** Python 2.7 and Python 3.\n\nFor more information, along with a detailed code review check out the following posts on the [PyImageSearch.com](http://www.pyimagesearch.com) blog:\n\n- [http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/](http://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/)\n- [http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/](http://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/](http://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/)\n- [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/)\n- [http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/](http://www.pyimagesearch.com/2015/08/10/checking-your-opencv-version-using-python/)\n\n## Installation\nProvided you already have NumPy, SciPy, Matplotlib, and OpenCV already installed, the `imutils` package is completely `pip`-installable:\n\n<pre>$ pip install imutils</pre>\n\n## Finding function OpenCV functions by name\nOpenCV can be a big, hard to navigate library, especially if you are just getting started learning computer vision and image processing. The `find_function` method allows you to quickly search function names across modules (and optionally sub-modules) to find the function you are looking for.\n\n#### Example:\nLet's find all function names that contain the text `contour",
    "url": "https://github.com/PyImageSearch/imutils",
    "last_updated": "2025-08-30T16:09:06+00:00"
  },
  {
    "full_name": "sahilchinoy/django-irs-filings",
    "name": "django-irs-filings",
    "description": "A Django app to download and parse IRS political committee filings",
    "language": "Python",
    "topics": [],
    "readme": "A Django app for downloading and parsing IRS campaign finance data, inspired by the [New York Times Fech library](https://github.com/NYTimes/Fech).\n\n[![PyPI version](https://badge.fury.io/py/django-irs-filings.svg)](http://badge.fury.io/py/django-irs-filings)\n[![Build Status](https://travis-ci.org/sahilchinoy/django-irs-filings.svg?branch=master)](https://travis-ci.org/sahilchinoy/django-irs-filings)\n\nBackground\n---------------\nSome political committees report their contributions and expenditures to the IRS under § 527 of the U.S. tax code. The IRS publishes these disclosure forms as a bulk download. This app attempts to make sense of that archive.\n\nThe [archive](http://forms.irs.gov/app/pod/dataDownload/dataDownload) is updated every Sunday at 1:00 AM. \n\nGetting started\n---------------\n\nInstall it.\n\n```bash\n$ pip install django-irs-filings\n```\n\nAdd `irs` to your list of `INSTALLED_APPS` in `settings.py`.\n\n```python\nINSTALLED_APPS = (\n    ...\n    'irs',\n    ...\n)\n```\n\nMigrate your database.\n\n```bash\n$ python manage.py migrate irs\n```\n\nFinally, call the `updateIRS` command. This will download the latest zipped archive from the IRS website, unzip and parse it, and load it into the database.\n\n```bash\n$ python manage.py updateIRS\n```\n",
    "url": "https://github.com/sahilchinoy/django-irs-filings",
    "last_updated": "2022-12-03T22:57:39+00:00"
  },
  {
    "full_name": "TryGhost/Ghost",
    "name": "Ghost",
    "description": "Independent technology for modern publishing, memberships, subscriptions and newsletters.",
    "language": "JavaScript",
    "topics": [
      "journalism",
      "publishing",
      "blogging",
      "javascript",
      "web-application",
      "cms",
      "nodejs",
      "ghost"
    ],
    "readme": "&nbsp;\n<p align=\"center\">\n  <a href=\"https://ghost.org/#gh-light-mode-only\" target=\"_blank\">\n    <img src=\"https://user-images.githubusercontent.com/65487235/157884383-1b75feb1-45d8-4430-b636-3f7e06577347.png\" alt=\"Ghost\" width=\"200px\">\n  </a>\n  <a href=\"https://ghost.org/#gh-dark-mode-only\" target=\"_blank\">\n    <img src=\"https://user-images.githubusercontent.com/65487235/157849205-aa24152c-4610-4d7d-b752-3a8c4f9319e6.png\" alt=\"Ghost\" width=\"200px\">\n  </a>\n</p>\n&nbsp;\n\n<p align=\"center\">\n    <a href=\"https://ghost.org/\">Ghost.org</a> •\n    <a href=\"https://forum.ghost.org\">Forum</a> •\n    <a href=\"https://ghost.org/docs/\">Docs</a> •\n    <a href=\"https://github.com/TryGhost/Ghost/blob/main/.github/CONTRIBUTING.md\">Contributing</a> •\n    <a href=\"https://twitter.com/ghost\">Twitter</a>\n    <br /><br />\n    <a href=\"https://ghost.org/\">\n        <img src=\"https://img.shields.io/badge/downloads-100M+-brightgreen.svg\" alt=\"Downloads\" />\n    </a>\n    <a href=\"https://github.com/TryGhost/Ghost/releases/\">\n        <img src=\"https://img.shields.io/github/release/TryGhost/Ghost.svg\" alt=\"Latest release\" />\n    </a>\n    <a href=\"https://github.com/TryGhost/Ghost/actions\">\n        <img src=\"https://github.com/TryGhost/Ghost/workflows/CI/badge.svg?branch=main\" alt=\"Build status\" />\n    </a>\n    <a href=\"https://github.com/TryGhost/Ghost/contributors/\">\n        <img src=\"https://img.shields.io/github/contributors/TryGhost/Ghost.svg\" alt=\"Contributors\" />\n    </a>\n</p>\n\n&nbsp;\n\n> [!NOTE]\n> Love open source? We're hiring! Ghost is looking staff engineers to [join the team](https://careers.ghost.org) and work with us full-time\n\n<a href=\"https://ghost.org/\"><img src=\"https://user-images.githubusercontent.com/353959/169805900-66be5b89-0859-4816-8da9-528ed7534704.png\" alt=\"Fiercely independent, professional publishing. Ghost is the most popular open source, headless Node.js CMS which already works with all the tools you know and love.\" /></a>\n\n&nbsp;\n\n<a href=\"https://ghost.org/pricing/#",
    "url": "https://github.com/TryGhost/Ghost",
    "last_updated": "2025-09-02T08:29:01+00:00"
  },
  {
    "full_name": "dunovank/jupyter-themes",
    "name": "jupyter-themes",
    "description": "Custom Jupyter Notebook Themes",
    "language": "CSS",
    "topics": [
      "jupyter",
      "jupyter-notebook",
      "theme",
      "css",
      "syntax-highlighting",
      "jupyter-themes"
    ],
    "readme": "# jupyterthemes\n## Theme-ify your Jupyter Notebooks!\n\n|    Author    |                 Version                  |                  Status                  |                   Demo                   |\n| :----------: | :--------------------------------------: | :--------------------------------------: | :--------------------------------------: |\n| Kyle Dunovan | ![image](https://img.shields.io/pypi/v/jupyterthemes.svg) | ![image](https://travis-ci.org/dunovank/jupyter-themes.svg?branch=master) | [![Binder](http://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/dunovank/jupyter-themes/master?filepath=index.ipynb) |\n\n\n## Updates\n\n### NO LONGER SUPPORTED\n\n> This project is no longer active and the library is not being supported. ***Thank you*** to everyone who contributed to `jupyter-themes`!\n\n### *JupyterLab Themes*:\n\nJupyterLab themes with similar style/design conventions to `jupyter-themes`:\n\n##### JupyterLab v3.0\n* [**Legos Theme**](https://github.com/dunovank/jupyterlab_legos_ui).\n* [**Darkside Theme**](https://github.com/dunovank/jupyterlab_darkside_ui).\n\n##### JupyterLab v4.0\n* [**Darkside Theme**](https://github.com/dunovank/jupyterlab_darkside_theme).\n\n---\n\n#### JT Customizable Features\n\n#### **plotting style**\n![image](screens/onedork_reach_plots.png)\n\n#### **markdown/equations**\n![image](screens/oceans16_markdown.png)\n\n#### **pandas dataframes**\n![image](screens/grade3_table.png)\n\n#### **command palette**\n![image](screens/oceans16_command_palette.png)\n\n\n### Links\n* [jupyterthemes on PyPI](https://pypi.python.org/pypi/jupyterthemes/)\n* [jupyterthemes on GitHub](https://github.com/dunovank/jupyter-themes)\n\n### Requirements\n* Python 3.4, 3.5, 3.6, 3.7 and 3.8\n* Jupyter ([Anaconda](https://www.continuum.io/downloads) recommended)\n* matplotlib\n\n### Install with pip\n```sh\n# install jupyterthemes\npip install jupyterthemes\n\n# upgrade to latest version\npip install --upgrade jupyterthemes\n```\n\n### Install with conda\n```sh\n# install jupyterthemes\nconda install -c con",
    "url": "https://github.com/dunovank/jupyter-themes",
    "last_updated": "2025-08-30T07:02:46+00:00"
  },
  {
    "full_name": "pyg-team/pytorch-frame",
    "name": "pytorch-frame",
    "description": "Tabular Deep Learning Library for PyTorch",
    "language": "Python",
    "topics": [
      "data-frame",
      "deep-learning",
      "pytorch",
      "tabular-learning"
    ],
    "readme": "<div align=\"center\">\n\n<img height=\"175\" src=\"https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pytorch_frame_logo_text.png?sanitize=true\" />\n\n<br>\n<br>\n\n**A modular deep learning framework for building neural network models on heterogeneous tabular data.**\n\n______________________________________________________________________\n\n[![arXiv][arxiv-image]][arxiv-url]\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-frame)](https://pypi.org/project/pytorch-frame/)\n[![PyPI Version][pypi-image]][pypi-url]\n[![Testing Status][testing-image]][testing-url]\n[![Docs Status][docs-image]][docs-url]\n[![Contributing][contributing-image]][contributing-url]\n[![Slack][slack-image]][slack-url]\n\n</div>\n\n**[Documentation](https://pytorch-frame.readthedocs.io)** | **[Paper](https://arxiv.org/abs/2404.00776)**\n\nPyTorch Frame is a deep learning extension for [PyTorch](https://pytorch.org/), designed for heterogeneous tabular data with different column types, including numerical, categorical, time, text, and images. It offers a modular framework for implementing existing and future methods. The library features methods from state-of-the-art models, user-friendly mini-batch loaders, benchmark datasets, and interfaces for custom data integration.\n\nPyTorch Frame democratizes deep learning research for tabular data, catering to both novices and experts alike. Our goals are:\n\n1. **Facilitate Deep Learning for Tabular Data:** Historically, tree-based models (e.g., GBDT) excelled at tabular learning but had notable limitations, such as integration difficulties with downstream models, and handling complex column types, such as texts, sequences, and embeddings. Deep tabular models are promising to resolve the limitations. We aim to facilitate deep learning research on tabular data by modularizing its implementation and supporting the diverse column types.\n\n1. **Integrates with Diverse Model Architectures like Large Language Models:** ",
    "url": "https://github.com/pyg-team/pytorch-frame",
    "last_updated": "2025-09-01T20:00:32+00:00"
  },
  {
    "full_name": "jacobeisenstein/gt-nlp-class",
    "name": "gt-nlp-class",
    "description": "Course materials for Georgia Tech CS 4650 and 7650, \"Natural Language\"",
    "language": "TeX",
    "topics": [],
    "readme": "CS 4650 and 7650\n==========\n\n(**Note about registration**: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)\n\n- **Course**: Natural Language Understanding\n- **Instructor**: Jacob Eisenstein\n- **Semester**: Spring 2018\n- **Time**: Mondays and Wednesdays, 3:00-4:15pm\n- **TAs**: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun\n- [Schedule](https://docs.google.com/spreadsheets/d/1BuvRjPhfHmy7XAfpc5KoygdfqI3Cue3bbmiO6yYuX_E/edit?usp=sharing)\n- [Recaps](https://docs.google.com/document/d/1loefqZhmOaF2mP8yQPEx91jZ7BHylWixVtYlFhpIlGM/edit?usp=sharing) from previous classes\n\nThis course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which\nare especially relevant to natural language processing.\n\n- [Readings](#readings)\n- [Grading](#grading)\n- [Help](#help)\n- [Policies](#policies)\n\n# Learning goals\n<a name=\"learning\"/>\n\n- Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.\n- Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.\n- Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.\n- Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.\n- (76",
    "url": "https://github.com/jacobeisenstein/gt-nlp-class",
    "last_updated": "2025-09-01T13:51:52+00:00"
  },
  {
    "full_name": "serengil/deepface",
    "name": "deepface",
    "description": "A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python",
    "language": "Python",
    "topics": [
      "face-recognition",
      "vgg-face",
      "facenet",
      "openface",
      "facial-expression-recognition",
      "age-prediction",
      "gender-prediction",
      "race-classification",
      "emotion-recognition",
      "deep-learning",
      "machine-learning",
      "deepface",
      "face-analysis",
      "python",
      "deepid",
      "arcface",
      "facial-recognition"
    ],
    "readme": "# deepface\n\n<div align=\"center\">\n\n[![Downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&units=international_system&left_color=grey&right_color=blue&left_text=downloads)](https://pepy.tech/project/deepface)\n[![Stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&style=flat&label=%E2%AD%90%20stars)](https://github.com/serengil/deepface/stargazers)\n[![Pulls](https://img.shields.io/docker/pulls/serengil/deepface?logo=docker)](https://hub.docker.com/r/serengil/deepface)\n[![License](http://img.shields.io/:license-MIT-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/LICENSE)\n[![Tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)\n[![DOI](http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat)](https://doi.org/10.17671/gazibtd.1399077)\n\n[![Blog](https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&logo=wordpress)](https://sefiks.com)\n[![YouTube](https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&logo=youtube)](https://www.youtube.com/@sefiks?sub_confirmation=1)\n[![Twitter](https://img.shields.io/:follow-@serengil-blue.svg?style=flat&logo=x)](https://twitter.com/intent/user?screen_name=serengil)\n\n[![Patreon](https://img.shields.io/:become-patron-f96854.svg?style=flat&logo=patreon)](https://www.patreon.com/serengil?repo=deepface)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/serengil?logo=GitHub&color=lightgray)](https://github.com/sponsors/serengil)\n[![Buy Me a Coffee](https://img.shields.io/badge/-buy_me_a%C2%A0coffee-gray?logo=buy-me-a-coffee)](https://buymeacoffee.com/serengil)\n\n<div align=\"center\">\n  <a href=\"https://trendshift.io/repositories/4227\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4227\" alt=\"serengil%2Fdeepface | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n  <!--\n  <a href=\"https://www.produ",
    "url": "https://github.com/serengil/deepface",
    "last_updated": "2025-09-02T09:55:55+00:00"
  },
  {
    "full_name": "notnews/lacc_to_csv",
    "name": "lacc_to_csv",
    "description": "Los Angeles Closed-Caption Television News Archive Data to CSV",
    "language": "Python",
    "topics": [
      "closed-captions",
      "news",
      "local-news"
    ],
    "readme": "## Los Angeles Closed Caption News Data Archive to CSV\n\nConvert the text files from the infamous LA Closed-Caption Television News data---infamous because it was part of the LaCour scandal--- to a CSV.\n\n## Description\n\nA python script to parse the Closed-Caption files (text format) to a single CSV file.\n\n### Usage\n\n```\nParse CC files to a single CSV file\nUsage: lacc_to_csv.py [options] <directory of text files>\n\nOptions:\n  -h, --help            show this help message and exit\n  -o OUTFILE, --out=OUTFILE\n                        Output file in CSV (default: program.data.csv)\n```\n\n### Example\n\n```\npython lacc_to_csv.py -o output.csv ./tv\n```\n",
    "url": "https://github.com/notnews/lacc_to_csv",
    "last_updated": "2021-03-02T03:37:46+00:00"
  },
  {
    "full_name": "blindglobe/rclg",
    "name": "rclg",
    "description": "R <-> Common Lisp gateway (written by Rif, maintained by Tony Rossini)",
    "language": "",
    "topics": [],
    "readme": " -*- mode: text -*-\n\n(we are really in muse mode; but most people don't have it readily\navailable and we aren't forcing the use of Emacs/muse.   M-x muse-mode\nmakes reading easier IMHO).\n\n\n* License \n\nSee the file COPYING in the top level directory.\n\n* Interfacing Common Lisp and R\n\nThis library considers multiple approaches for connecting Common Lisp\nand R, particularly for embedding R within CL.  CLSR also attempts to\ndo the reverse.  RCL is a third approach, which provides more\nporcelain around the plumbing.  RCLG also tries to use R with the\nthreaded SBCL capability.  All of this will be commented on later (in\ntime, in this file).\n\n\n* Quick start\n\n** Requirements\n\nCommon Lisp Implementation:\n\nA. (WORKS!) SBCL 1.0 and later (known, might work on earlier versions)\n\nB. (Goal, but not working yet):  CLISP is a target, but there are a\n   few configuration issues to resolve.\n\nYou will need the following libraries available:\n\n1. ASDF   (system definition facility, for loading packages)\n2. CFFI   (Common foreign function interface: later than CFFI-060526)\n                                                        (May 26 2006)\n3. RCLG   (this library)\n\n\n\nOnce you have these, then the simple way to get started is to:\n\n1. add rclg.asd to the ASDF systems path.\n2. See  rclg-demo.lisp  for getting started.  It has incantations for:\n   a. compiling and loading cffi\n   b. compiling and loading rclg\n   c. basic R functions\n   d. basic data conversion.\n\n\n\n\n\n\n\n\n\n\n\n* Past and possibly present \"Issues\"\n\n1. Need to get it working (\"again\") with CLISP.\n\n** From the file formerly known as NOTES/06032006.rif (date contained)\n\n1. In the current version of cffi, the variable names get wrapped in\n   asterisks, so\n\n(defcvar \"R_CStackLimit\"  :unsigned-long)  ;; :unsigned long\n(defcvar \"R_SignalHandlers\" :unsigned-long) ;; :unsigned long\n\n   make variables named *R-CSTACKLIMIT* and *R-SIGNALHANDLERS*\n\n2. In \"rclg-demo.lisp\", I wanted to mention that rnb doesn't mean \"no\n   blocking\", it means ",
    "url": "https://github.com/blindglobe/rclg",
    "last_updated": "2021-04-17T16:21:07+00:00"
  },
  {
    "full_name": "keyuxing/tpfi",
    "name": "tpfi",
    "description": "Plot identification charts for Kepler, K2 and TESS",
    "language": "Python",
    "topics": [],
    "readme": "# tpfi\n[![PyPI version](https://badge.fury.io/py/tpfi.svg)](https://badge.fury.io/py/tpfi)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n`tpfi` is an easy-to-use visualization tool for astronomers to identify and analyze \nstars in the Kepler, K2, and TESS missions. The main focus of this project is on \ntwo functions: `plot_identification` and `plot_season`. These functions create \nplots to help visualize target stars and their surrounding environments.\n\n---\n\n**Plot identification charts for Kepler, K2 and TESS.**\n\n[![pPKxsjH.png](https://s1.ax1x.com/2023/08/14/pPKxsjH.png)](https://imgse.com/i/pPKxsjH)\n\n[![pPKxcDA.png](https://s1.ax1x.com/2023/08/14/pPKxcDA.png)](https://imgse.com/i/pPKxcDA)\n\n[![pPKx6ud.png](https://s1.ax1x.com/2023/08/14/pPKx6ud.png)](https://imgse.com/i/pPKx6ud)\n\nThe `plot_identification` function creates identification charts, which are useful \nfor determining if a target is contaminated by nearby stars. In each chart, the \nright panel overlays the Gaia DR3 catalog onto the TESS Target Pixel Files (TPF) \nwith the target marked by a cross symbol. The circle size represents the relative \nbrightness of the stars according to Gaia G magnitude. The left panel displays the \nsame sky coverage but taken from the \n[DSS2 Red survey](https://skyview.gsfc.nasa.gov/current/cgi/moreinfo.pl?survey=DSS2%20Red).\n\nThis function is revised based on \n[_tpfplotter_](https://github.com/jlillo/tpfplotter). \n\n---\n\n**Plot season charts for Kepler targets.**\n\n[![pPKxrge.png](https://s1.ax1x.com/2023/08/14/pPKxrge.png)](https://imgse.com/i/pPKxrge)\n\nThe Kepler Space Telescope was designed to observe a specific field of stars \ncontinuously, but it also needed to ensure its solar panels faced the Sun to power \nits operations. As Kepler orbited the Sun, it performed a 90-degree roll every three \nmonths (or every quarter) to keep its solar panels sunward. After four of these \nquarterly rolls, amounting to a full ye",
    "url": "https://github.com/keyuxing/tpfi",
    "last_updated": "2025-08-21T12:27:39+00:00"
  },
  {
    "full_name": "ntnu-arl/LKH_TSP",
    "name": "LKH_TSP",
    "description": "A set of tools to solve TSP problems using the LKH solver",
    "language": "Python",
    "topics": [],
    "readme": "# LKH_TSP\nA set of tools to solve TSP problems using the LKH solver\n\n**What is LKH**\n\nLKH is an effective implementation of the Lin-Kernighan heuristic for solving the Traveling Salesman Problem. The Lin-Kernighan solver (also called the Lin-Kernighan-Helsgaun solver) is among the most efficient solvers of the TSP and it is employing the concept of k-opt moves. An implementaton of this solver is found online following this link: \n\nhttp://www.akira.ruc.dk/~keld/research/LKH/ \n\nFrom this site you can download and compile the code following the relevant instructions. Considering that this process is done correctly, this repo provides simple tools to easily invoke this functionality. \n\n**Python interface**\nWithin *python* a python script called *InvokeLKH.py* interfaces a compiled version of the LKH TSP Solver and exports the solution in the form of a file. To run the script: \n\n    $ python InvokeLKH.py\n\n**MATLAB interface**\nWithin *matlab* a matlab function called *LKH_TSP.m* interfaces a compiled version of the LKH TSP Solver and exports the solution to its output argument. The function syntax is: \n\n    TSPsolution = LKH_TSP(CostMatrix,pars_struct,fname_tsp,LKHdir,TSPLIBdir)\n\nFeel free to use these simple functions which are released mostly to motivate those working on TSP to use the LKH solver. \n\n[Kostas Alexis](mailto:konstantinos.alexis@gmail.com)\n",
    "url": "https://github.com/ntnu-arl/LKH_TSP",
    "last_updated": "2025-08-10T16:16:26+00:00"
  },
  {
    "full_name": "inferodata/iSAX",
    "name": "iSAX",
    "description": "The iSA package for R",
    "language": "R",
    "topics": [],
    "readme": "## iSAX\niSAX is an R package which provides access to iSA algorithm technology presented in the paper \"iSA: A fast, scalable and accurate algorithm for sentiment analysis of social media content\", Information Sciences (2016). The paper is accessible following <a href=\"http://dx.doi.org/10.1016/j.ins.2016.05.052\" target=\"_blank\">this link</a>.\n\n## Getting the R package\n- Make sure to start a fresh R session and to have the devtools package installed, if not, please install it first with\n  - `install.packages(\"devtools\")`\n. Then type:\n  - `library(devtools)`\n  - `install.packages(c(\"tm\", \"BMS\", \"quadprog\", \"rJava\", \"parallel\", \"data.table\", \"entropy\"))`\n  - `install_github(\"blogsvoices/iSAX\")`\n- At this point you should have `iSAX` installed and can proceed with:\n  - `library(iSAX)`\n- You should be ready to go!\n- If you plan to use Chinese language, you'll also need the rmmseg4j package (easy)\n- If you plan to use Japanese language, you'll also need RMeCab package (painful in many systems)\n\nIf rJava fails on OSX, please try to run on the Terminal window after installing the latest version of Java and reinstalling rJava package:\n\nsudo R CMD javareconf\n\n",
    "url": "https://github.com/inferodata/iSAX",
    "last_updated": "2025-02-16T01:53:01+00:00"
  },
  {
    "full_name": "raghakot/keras-text",
    "name": "keras-text",
    "description": "Text Classification Library in Keras",
    "language": "Python",
    "topics": [
      "deep-learning",
      "tensorflow",
      "theano",
      "keras",
      "text-classification",
      "neural-network",
      "machine-learning"
    ],
    "readme": "# Keras Text Classification Library\n[![Build Status](https://travis-ci.org/raghakot/keras-text.svg?branch=master)](https://travis-ci.org/raghakot/keras-text)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/raghakot/keras-text/blob/master/LICENSE)\n[![Slack](https://img.shields.io/badge/slack-discussion-E01563.svg)](https://join.slack.com/t/keras-text/shared_invite/MjMzNDU3NDAxODMxLTE1MDM4NTg0MTktNzgxZTNjM2E4Zg)\n\nkeras-text is a one-stop text classification library implementing various state of the art models with a clean and \nextendable interface to implement custom architectures.\n\n## Quick start\n\n### Create a tokenizer to build your vocabulary\n\n- To represent you dataset as `(docs, words)` use `WordTokenizer`\n- To represent you dataset as `(docs, sentences, words)` use `SentenceWordTokenizer`\n- To create arbitrary hierarchies, extend `Tokenizer` and implement the `token_generator` method.\n\n```python\nfrom keras_text.processing import WordTokenizer\n\n\ntokenizer = WordTokenizer()\ntokenizer.build_vocab(texts)\n```\n\nWant to tokenize with character tokens to leverage character models? Use `CharTokenizer`.\n\n\n### Build a dataset\n\nA dataset encapsulates tokenizer, X, y and the test set. This allows you to focus your efforts on \ntrying various architectures/hyperparameters without having to worry about inconsistent evaluation. A dataset can be \nsaved and loaded from the disk.\n\n```python\nfrom keras_text.data import Dataset\n\n\nds = Dataset(X, y, tokenizer=tokenizer)\nds.update_test_indices(test_size=0.1)\nds.save('dataset')\n```\n\nThe `update_test_indices` method automatically stratifies multi-class or multi-label data correctly.\n\n### Build text classification models\n\nSee tests/ folder for usage.\n\n#### Word based models\n\nWhen dataset represented as `(docs, words)` word based models can be created using `TokenModelFactory`.\n\n```python\nfrom keras_text.models import TokenModelFactory\nfrom keras_text.models import YoonKimCNN, Att",
    "url": "https://github.com/raghakot/keras-text",
    "last_updated": "2025-08-28T12:09:56+00:00"
  },
  {
    "full_name": "hrbrmstr/tdigest",
    "name": "tdigest",
    "description": "Wicked Fast, Accurate Quantiles Using 't-Digests'",
    "language": "C",
    "topics": [
      "r",
      "rstats",
      "t-digest",
      "quantile"
    ],
    "readme": "\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-3%25-lightgrey.svg)\n\n[![cran\nchecks](https://cranchecks.info/badges/worst/tdigest.png)](https://cranchecks.info/pkgs/tdigest)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/tdigest.png)](https://www.r-pkg.org/pkg/tdigest)\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-3.5.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# tdigest\n\nWicked Fast, Accurate Quantiles Using ‘t-Digests’\n\n## Description\n\nThe t-Digest construction algorithm uses a variant of 1-dimensional\nk-means clustering to produce a very compact data structure that allows\naccurate estimation of quantiles. This t-Digest data structure can be\nused to estimate quantiles, compute other rank statistics or even to\nestimate related measures like trimmed means. The advantage of the\nt-Digest over previous digests for this purpose is that the t-Digest\nhandles data with full floating point resolution. The accuracy of\nquantile estimates produced by t-Digests can be orders of magnitude more\naccurate than those produced by previous digest algorithms. Methods are\nprovided to create and update t-Digests and retrieve quantiles from the\naccumulated distributions.\n\nSee [the original paper by Ted Dunning & Otmar\nErtl](https://arxiv.org/abs/1902.04023) for more details on t-Digests.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n- `as.list.tdigest`: Serialize a tdigest object to an R list or\n  unserialize a serialized tdigest list back into a tdigest object\n- `td_add`: Add a value to the t-Digest with the specified count\n- `td_create`: Allocate a new histogram\n- `td_merge`: Merge one t-Digest into",
    "url": "https://github.com/hrbrmstr/tdigest",
    "last_updated": "2025-09-02T02:08:40+00:00"
  },
  {
    "full_name": "r-lib/memoise",
    "name": "memoise",
    "description": "Easy memoisation for R",
    "language": "R",
    "topics": [
      "r",
      "memoise"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# memoise\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/memoise)](https://CRAN.R-project.org/package=memoise)\n[![R build\nstatus](https://github.com/r-lib/memoise/workflows/R-CMD-check/badge.svg)](https://github.com/r-lib/memoise/actions)\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/memoise/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-lib/memoise?branch=main)\n<!-- badges: end -->\n\nThe memoise package makes it easy to memoise R functions.\n**Memoisation** (<https://en.wikipedia.org/wiki/Memoization>) caches\nfunction calls so that if a previously seen set of inputs is seen, it\ncan return the previously computed output.\n\n# Installation\n\nInstall from CRAN with\n\n``` r\ninstall.packages(\"memoise\")\n```\n\n## Usage\n\nTo memoise a function, use `memoise()`:\n\n``` r\nlibrary(memoise)\nf <- function(x) {\n  Sys.sleep(1)\n  mean(x)\n}\nmf <- memoise(f)\n```\n\n``` r\nsystem.time(mf(1:10))\n#>    user  system elapsed\n#>   0.002   0.000   1.003\nsystem.time(mf(1:10))\n#>    user  system elapsed\n#>   0.000   0.000   0.001\n```\n\nYou can clear `mf`’s cache with:\n\n``` r\nforget(mf)\n```\n\nAnd you can test whether a function is memoised with `is.memoised()`.\n\n## Caches\n\nBy default, memoise uses an in-memory cache, using `cache_mem()` from\nthe [cachem](https://cachem.r-lib.org/) package. `cachem::cache_disk()`\nallows caching using files on a local filesystem.\n\nBoth `cachem::cache_mem()` and `cachem::cache_disk()` support automatic\npruning by default; this means that they will not keep growing past a\ncertain size, and eventually older items will be removed from the cache.\nThe default size `cache_mem()` is 512 MB, and the default size for a\n`cache_disk()` is 1 GB, but this can be customized by specifying\n`max_size`:\n\n``` r\n# 100 MB limit\ncm <- cachem::cache_mem(max_size = 100 * 1024^2)\n\nmf <- memoise(f, cache = cm)\n```\n\nYou can also change the maximum age of items in the cache with\n`max",
    "url": "https://github.com/r-lib/memoise",
    "last_updated": "2025-08-21T22:01:33+00:00"
  },
  {
    "full_name": "Lightning-AI/pytorch-lightning",
    "name": "pytorch-lightning",
    "description": "Pretrain, finetune ANY AI model of ANY size on multiple GPUs, TPUs with zero code changes.",
    "language": "Python",
    "topics": [
      "python",
      "deep-learning",
      "artificial-intelligence",
      "ai",
      "pytorch",
      "data-science",
      "machine-learning"
    ],
    "readme": "<div align=\"center\">\n\n<img alt=\"Lightning\" src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png\" width=\"800px\" style=\"max-width: 100%;\">\n\n<br/>\n<br/>\n\n**The deep learning framework to pretrain, finetune and deploy AI models.**\n\n**NEW- Deploying models? Check out [LitServe](https://github.com/Lightning-AI/litserve), the PyTorch Lightning for model serving**\n\n______________________________________________________________________\n\n<p align=\"center\">\n    <a href=\"#quick-start\" style=\"margin: 0 10px;\">Quick start</a> •\n  <a href=\"#examples\">Examples</a> •\n  <a href=\"#why-pytorch-lightning\">PyTorch Lightning</a> •\n  <a href=\"#lightning-fabric-expert-control\">Fabric</a> •\n  <a href=\"https://lightning.ai/\">Lightning AI</a> •   \n  <a href=\"#community\">Community</a> •\n  <a href=\"https://pytorch-lightning.readthedocs.io/en/stable/\">Docs</a>\n</p>\n\n<!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL -->\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)](https://pypi.org/project/pytorch-lightning/)\n[![PyPI Status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![Conda](https://img.shields.io/conda/v/conda-forge/lightning?label=conda&color=success)](https://anaconda.org/conda-forge/lightning)\n[![codecov](https://codecov.io/gh/Lightning-AI/pytorch-lightning/graph/badge.svg?token=SmzX8mnKlA)](https://codecov.io/gh/Lightning-AI/pytorch-lightning)\n\n[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/w/lightning-ai/lightning)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/pytorch-lightning/blob/master/LICENSE)\n\n<!--\n[![CodeFactor](https://www.codef",
    "url": "https://github.com/Lightning-AI/pytorch-lightning",
    "last_updated": "2025-09-02T09:15:47+00:00"
  },
  {
    "full_name": "Sroy20/machine-learning-interview-questions",
    "name": "machine-learning-interview-questions",
    "description": "This repository is to prepare for Machine Learning interviews. ",
    "language": "",
    "topics": [],
    "readme": "In different files, I list various questions that might be asked in a ML interview. Here is the table of contents:\n\n1. [Deep Learning Questions](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_deep_learning.md)\n1. [General Machine Learning Questions](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_machine_learning.md)\n1. [Mathematics for Machine Learning Questions](https://github.com/Sroy20/machine-learning-interview-questions/blob/master/list_of_questions_mathematics.md)\n",
    "url": "https://github.com/Sroy20/machine-learning-interview-questions",
    "last_updated": "2025-08-31T17:35:11+00:00"
  },
  {
    "full_name": "18F/2015-foia",
    "name": "2015-foia",
    "description": "Please check out https://github.com/18F/foia-hub/issues to track our work. This repo is for project wide discussion, blogging, and scratch space for 18F's FOIA modernization team. ",
    "language": "Python",
    "topics": [
      "unmaintained",
      "foia"
    ],
    "readme": "## FOIA Modernization\n\nThis is the main repository for [18F's](https://18f.gsa.gov) FOIA modernization\neffort. \n\nThe Freedom of Information Act (FOIA) grants the public the right to access\nrecords from the federal government. \n\nOur work is one of number of\n[commitments](http://www.whitehouse.gov/sites/default/files/docs/us_national_action_plan_6p.pdf)\ntowards the modernization of FOIA. \n\nWe use the [issues](https://github.com/18f/foia/issues) for project-wide\ndiscussions. \n\nAdditional news can be found on 18F's website:\n[18f.gsa.gov/tags/foia](http://18f.gsa.gov/tags/foia).\n\nOur primary focus right now is working on\n[foia-hub](https://github.com/18F/foia-hub). FOIA Hub is an online tool to\nguide users to up-to-date FOIA contact and request information across the\nfederal government. \n\nThis repository also contains the [bulk\ndata](https://github.com/18F/foia/tree/master/contacts/data)  that powers FOIA\nHub. \n\n## Public domain\n\nThis project is in the worldwide [public domain](LICENSE.md). As stated in [CONTRIBUTING](CONTRIBUTING.md):\n\n> This project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the [CC0 1.0 Universal public domain dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n>\n> All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest.\n\n",
    "url": "https://github.com/18F/2015-foia",
    "last_updated": "2024-09-01T20:16:09+00:00"
  },
  {
    "full_name": "ReviewNB/support",
    "name": "support",
    "description": "Issues and feature requests for ReviewNB",
    "language": "",
    "topics": [],
    "readme": "## ReviewNB Support Repository\n* If you want to report a problem, [open an issue](https://github.com/ReviewNB/support/issues/new/choose) or write to us at support@reviewnb.com\n* If you've a question, feedback, feature request or generally want to discuss something; [head over to discussion](https://github.com/reviewNB/support/discussions)\n* We're building in the open. You can view [upcoming features](https://github.com/ReviewNB/support/issues?q=is%3Aopen+is%3Aissue+label%3A%22Feature+Request%22+sort%3Acreated-desc), vote on them,  and even request a new one.\n\n[Visit the homepage](https://www.reviewnb.com/) for more information.\n",
    "url": "https://github.com/ReviewNB/support",
    "last_updated": "2025-08-25T16:44:18+00:00"
  },
  {
    "full_name": "Jefferson-Henrique/GetOldTweets-python",
    "name": "GetOldTweets-python",
    "description": "A project written in Python to get old tweets, it bypass some limitations of Twitter Official API.",
    "language": "Python",
    "topics": [],
    "readme": "# Get Old Tweets Programatically\nA project written in Python to get old tweets, it bypass some limitations of Twitter Official API.\n\n## Details\nTwitter Official API has the bother limitation of time constraints, you can't get older tweets than a week. Some tools provide access to older tweets but in the most of them you have to spend some money before.\nI was searching other tools to do this job but I didn't found it, so after analyze how Twitter Search through browser works I understand its flow. Basically when you enter on Twitter page a scroll loader starts, if you scroll down you start to get more and more tweets, all through calls to a JSON provider. After mimic we get the best advantage of Twitter Search on browsers, it can search the deepest oldest tweets.\n\n## Prerequisites\nThis package assumes using Python 2.x. The Python3 \"got3\" folder is maintained as experimental and is not officially supported.\n\nExpected package dependencies are listed in the \"requirements.txt\" file for PIP, you need to run the following command to get dependencies:\n```\npip install -r requirements.txt\n```\n\n## Components\n- **Tweet:** Model class to give some informations about a specific tweet.\n  - id (str)\n  - permalink (str)\n  - username (str)\n  - text (str)\n  - date (date)\n  - retweets (int)\n  - favorites (int)\n  - mentions (str)\n  - hashtags (str)\n  - geo (str)\n\n- **TweetManager:** A manager class to help getting tweets in **Tweet**'s model.\n  - getTweets (**TwitterCriteria**): Return the list of tweets retrieved by using an instance of **TwitterCriteria**. \n\n- **TwitterCriteria:** A collection of search parameters to be used together with **TweetManager**.\n  - setUsername (str): An optional specific username from a twitter account. Without \"@\".\n  - setSince (str. \"yyyy-mm-dd\"): A lower bound date to restrict search.\n  - setUntil (str. \"yyyy-mm-dd\"): An upper bound date to restrist search.\n  - setQuerySearch (str): A query text to be matched.\n  - setTopTweets (bool): If True only the T",
    "url": "https://github.com/Jefferson-Henrique/GetOldTweets-python",
    "last_updated": "2025-08-14T17:51:30+00:00"
  },
  {
    "full_name": "hrbrmstr/publicwww",
    "name": "publicwww",
    "description": " Query the 'PublicWWW' Source Code Search Engine in R",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "publicwww",
      "r-cyber"
    ],
    "readme": "\n# publicwww\n\nQuery the ‘PublicWWW’ Source Code Search Engine\n\n## Description\n\nThe ‘PublicWWW’ <https://publicwww.com/> source code search engine\nindexes the content of over 200 million web sites and provides a query\ninterface that lets the caller find any alphanumeric snippet, signature\nor keyword in the web pages ‘HTML’, ‘JavaScript’ and ‘CSS’ style sheet\ncode.\n\n## NOTE\n\nThe site requires an [API key](https://publicwww.com/pricing.html) and\nhas limited functionality under the free plan.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `source_code_search`: Find any alphanumeric snippet, signature or\n    keyword in the web pages HTML, JS and CSS code in the PublicWWW\n    database\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/publicwww\")\n```\n\n## Usage\n\n``` r\nlibrary(publicwww)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"publicwww\")\n```\n\n    ## [1] '0.1.0'\n\n### Sites using the WordPress theme “Twenty Sixteen”:\n\n``` r\nsource_code_search('\"/wp-content/themes/twentysixteen/\"')\n```\n\n    ## No encoding supplied: defaulting to UTF-8.\n\n    ## # A tibble: 2,847 x 2\n    ##    site                          rank\n    ##    <chr>                        <int>\n    ##  1 cyberciti.biz                 4738\n    ##  2 creativecommons.org           7217\n    ##  3 1337x.io                      8173\n    ##  4 bestgore.com                  8480\n    ##  5 mooma.sh                     23268\n    ##  6 educationteams.com           30091\n    ##  7 stih.su                      33197\n    ##  8 getitfree.us                 37142\n    ##  9 pagesuite-professional.co.uk 39135\n    ## 10 tagaloglang.com              40384\n    ## # ... with 2,837 more rows\n\n### Sites running the coinhive JS miner\n\n``` r\nsource_code_search('\"coin-hive.com/lib/coinhive.min.js\"')\n```\n\n    ## No encoding supplied: defaulting to UTF-8.\n\n    ## # A tibble: 501 x 2\n    ##    site                 rank\n    ##    <chr>               <int>\n    ##  1 seriesdanko.to       9955\n    ##  2",
    "url": "https://github.com/hrbrmstr/publicwww",
    "last_updated": "2025-03-22T11:07:30+00:00"
  },
  {
    "full_name": "bdilday/scrapeTvRatings",
    "name": "scrapeTvRatings",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "scrapeTvRatings\n===============\n\nThis code scrapes TV ratings data from the web. It was originally written to compare world series ratings across years, so it flags the data as \"baseball\", \"football\", or \"other\". If you run it, it will locally store about 1 GB of html files and output a csv file of about 8.7 MB.\n\nThe data, through March 2014, can be directly downloaded from Dropbox: <https://www.dropbox.com/s/yuleoyx326msrm9/scrapeTvRatings_0.csv.gz>\n",
    "url": "https://github.com/bdilday/scrapeTvRatings",
    "last_updated": "2016-11-27T05:22:30+00:00"
  },
  {
    "full_name": "Undertone0809/promptulate",
    "name": "promptulate",
    "description": "🚀Lightweight Large language model automation and Autonomous Language Agents development framework. Build your LLM Agent Application in a pythonic way!",
    "language": "Python",
    "topics": [
      "chatgpt",
      "gpt-4",
      "langchain",
      "llm",
      "python",
      "prompt",
      "prompt-engineering",
      "pne",
      "promptulate",
      "agent",
      "autogen",
      "language-agent"
    ],
    "readme": "<p align=\"center\">\n    <img src=\"./docs/public/banner.png\" alt=\"promptulate\" style=\"border-radius: 15px;\"/>\n</p>\n\n<p align=\"center\">\n    <a target=\"_blank\" href=\"\">\n        <img src=\"https://img.shields.io/github/license/Undertone0809/promptulate.svg?style=flat-square\" />\n    </a>\n    <a target=\"_blank\" href=''>\n        <img src=\"https://img.shields.io/github/release/Undertone0809/promptulate/all.svg?style=flat-square\"/>\n    </a>\n    <a href=\"https://pypi.org/project/promptulate\" target=\"_blank\">\n        <img src=\"https://img.shields.io/pypi/pyversions/promptulate.svg?color=%2334D058\" alt=\"Supported Python versions\">\n    </a>\n    <a href=\"https://t.me/zeeland0809\" target=\"_blank\">\n      <img src=\"https://img.shields.io/badge/Telegram-join%20chat-2CA5E0?logo=telegram&logoColor=white\" alt=\"chat on Telegram\">\n    </a>\n    <a target=\"_blank\" href=''>\n        <img src=\"https://static.pepy.tech/personalized-badge/promptulate?period=month&units=international_system&left_color=grey&right_color=blue&left_text=Downloads/Week\"/>\n    </a>\n</p>\n\n[English](/README.md) [中文](/README_zh.md)\n\n## Overview\n\n**Promptulate** is an AI Agent application development framework crafted by **Cogit Lab**, which offers developers an extremely concise and efficient way to build Agent applications through a Pythonic development paradigm. The core philosophy of Promptulate is to borrow and integrate the wisdom of the open-source community, incorporating the highlights of various development frameworks to lower the barrier to entry and unify the consensus among developers. With Promptulate, you can manipulate components like LLM, Agent, Tool, RAG, etc., with the most succinct code, as most tasks can be easily completed with just a few lines of code. 🚀\n\n## 💡 Features\n\n- 🐍 Pythonic Code Style: Embraces the habits of Python developers, providing a Pythonic SDK calling approach, putting everything within your grasp with just one `pne.chat` function to encapsulate all essential functionalities.\n- 🧠 Model",
    "url": "https://github.com/Undertone0809/promptulate",
    "last_updated": "2025-08-28T07:09:23+00:00"
  },
  {
    "full_name": "hrbrmstr/carbondater",
    "name": "carbondater",
    "description": "📆 Estimate the Age of Web Resources",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-cyber"
    ],
    "readme": "\n# carbondater\n\nEstimate the Age of Web Resources\n\n## Description\n\nMethods are provided to read ‘URL’ metadata and scan web archives/use\nweb archive ‘APIs’ to determine the approxate age of the resource.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `carbondate`: Carbon date a URL/URI\n  - `get_date_from_url`: Locate and retrieve publication date from a URL\n  - `get_earliest_mementos`: Retrieve the earliest mementos for a\n    URL/URI\n  - `get_earliest_pubdate`: Retrieve the earliest “pubdate” for a URL\n  - `get_last_modified`: Retrieve the last-modified header\n\n## TODO\n\n  - \\[X\\] A `carbondate()` function to wrap up everything\n  - \\[ \\] More error checking\n  - \\[ \\] More resources to check\n  - \\[ \\] More documentation\n  - \\[ \\] Tests\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/carbondater\")\n```\n\n## Usage\n\n``` r\nlibrary(carbondater)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"carbondater\")\n```\n\n    ## [1] '0.1.0'\n\n### Carbon data a few URIs\n\n``` r\n(x <- carbondate(\"https://rud.is/b/2013/12/27/points-polygons-and-power-outages/\"))\n```\n\n    ## Oldest publication record found: 2013-12-27 in url (1592d 8h 50m 23.6s ago)\n\n``` r\nglimpse(x)\n```\n\n    ## Observations: 5\n    ## Variables: 3\n    ## $ method <chr> \"url\", \"memento\", \"memento\", \"pubdate\", \"last_modified\"\n    ## $ date   <dttm> 2013-12-27 00:00:00, 2014-03-30 14:18:34, 2014-01-10 02:26:45, 2013-12-27 15:30:09, NA\n    ## $ uri    <chr> \"https://rud.is/b/2013/12/27/points-polygons-and-power-outages/\", \"http://wayback.vefsafn.is/wayback...\n\n``` r\n(x <- carbondate(\"http://www.cs.odu.edu\"))\n```\n\n    ## Oldest publication record found: 1997-01-02 13:01:37 in memento (7794d 19h 48m 48.7s ago)\n\n``` r\nglimpse(x)\n```\n\n    ## Observations: 8\n    ## Variables: 3\n    ## $ method <chr> \"url\", \"memento\", \"memento\", \"memento\", \"memento\", \"memento\", \"pubdate\", \"last_modified\"\n    ## $ date   <dttm> NA, 1997-06-06 10:50:39, 2009-12-23 04:30:49, 2012-02-08 03:54:08, 2017-03-04 16:34:",
    "url": "https://github.com/hrbrmstr/carbondater",
    "last_updated": "2025-03-22T11:07:15+00:00"
  },
  {
    "full_name": "psf/requests-html",
    "name": "requests-html",
    "description": "Pythonic HTML Parsing for Humans™",
    "language": "Python",
    "topics": [
      "html",
      "scraping",
      "python",
      "requests",
      "http",
      "kennethreitz",
      "lxml",
      "pyquery",
      "css-selectors",
      "beautifulsoup"
    ],
    "readme": "Requests-HTML: HTML Parsing for Humans™\n=======================================\n\n.. image:: https://farm5.staticflickr.com/4695/39152770914_a3ab8af40d_k_d.jpg\n\n.. image:: https://travis-ci.com/psf/requests-html.svg?branch=master\n    :target: https://travis-ci.com/psf/requests-html\n\nThis library intends to make parsing HTML (e.g. scraping the web) as\nsimple and intuitive as possible.\n\nWhen using this library you automatically get:\n\n- **Full JavaScript support**! (Using Chromium, thanks to pyppeteer)\n- *CSS Selectors* (a.k.a jQuery-style, thanks to PyQuery).\n- *XPath Selectors*, for the faint of heart.\n- Mocked user-agent (like a real web browser).\n- Automatic following of redirects.\n- Connection–pooling and cookie persistence.\n- The Requests experience you know and love, with magical parsing abilities.\n- **Async Support**\n\n.. Other nice features include:\n\n    - Markdown export of pages and elements.\n\n\nTutorial & Usage\n================\n\nMake a GET request to 'python.org', using Requests:\n\n.. code-block:: pycon\n\n    >>> from requests_html import HTMLSession\n    >>> session = HTMLSession()\n    >>> r = session.get('https://python.org/')\n\nTry async and get some sites at the same time:\n\n.. code-block:: pycon\n\n    >>> from requests_html import AsyncHTMLSession\n    >>> asession = AsyncHTMLSession()\n    >>> async def get_pythonorg():\n    ...     r = await asession.get('https://python.org/')\n    ...     return r\n    ...\n    >>> async def get_reddit():\n    ...    r = await asession.get('https://reddit.com/')\n    ...    return r\n    ...\n    >>> async def get_google():\n    ...    r = await asession.get('https://google.com/')\n    ...    return r\n    ...\n    >>> results = asession.run(get_pythonorg, get_reddit, get_google)\n    >>> results # check the requests all returned a 200 (success) code\n    [<Response [200]>, <Response [200]>, <Response [200]>]\n    >>> # Each item in the results list is a response object and can be interacted with as such\n    >>> for result in results: \n    .",
    "url": "https://github.com/psf/requests-html",
    "last_updated": "2025-08-31T09:48:07+00:00"
  },
  {
    "full_name": "r-lib/gmailr",
    "name": "gmailr",
    "description": "Access the Gmail RESTful API from R.",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# gmailr\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/gmailr)](https://CRAN.R-project.org/package=gmailr)\n[![R-CMD-check](https://github.com/r-lib/gmailr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/gmailr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/gmailr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-lib/gmailr?branch=main)\n<!-- badges: end -->\n\nExposing the [Gmail API](https://developers.google.com/gmail/api) from\nR.\n\n## Installation\n\nInstall the released version of gmailr from CRAN:\n\n``` r\ninstall.packages(\"gmailr\")\n```\n\nOr install the development version from GitHub with:\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"r-lib/gmailr\")\n```\n\n## Attach gmailr\n\n``` r\nlibrary(gmailr)\n```\n\n## Setup and auth\n\nIn order to use gmailr, you **must** provide your own OAuth client. This\nis documented in the article [Set up an OAuth\nclient](https://gmailr.r-lib.org/dev/articles/oauth-client.html). The\narticle goes deeply into how to create an OAuth client and also how to\nconfigure it for gmailr’s use. If you already have an OAuth client or\nknow how to create one, the help topics for `?gm_auth_configure` and\n`?gm_default_oauth_client` are more concise resources for just the\nclient configuration piece.\n\nConfiguring an OAuth client is step 1 of 2 for getting ready to use\ngmailr. Step 2 is to complete the so-called “OAuth dance”, which is\ntriggered automatically upon first need. You are taken to a web browser,\nwhere you must select or login as the Google user you want to use\n(authenticate yourself) and give your OAuth client permission to do\nGmail stuff on your behalf (authorize). The OAuth dance does not\n(necessarily) need to be repeated in subsequent sessions. See `?gm_auth`\nif these defaults aren’t appropriate for your use case and you’d like to\ntake more control.\n\nYou can call `gm_profile()` to",
    "url": "https://github.com/r-lib/gmailr",
    "last_updated": "2025-05-29T03:06:52+00:00"
  },
  {
    "full_name": "hrbrmstr/crafter",
    "name": "crafter",
    "description": ":microscope: An R package to work with PCAPs",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "pcap",
      "pcap-files",
      "pcap-analyzer",
      "packet-capture",
      "r-cyber"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n    __________________________oooo__oo____________________\n    _ooooo__oo_ooo___ooooo___oo_____oo_____ooooo__oo_ooo__\n    oo___oo_ooo___o_oo___oo_ooooo__oooo___oo____o_ooo___o_\n    oo______oo______oo___oo_oo______oo____ooooooo_oo______\n    oo______oo______oo___oo_oo______oo__o_oo______oo______\n    _ooooo__oo_______oooo_o_oo_______ooo___ooooo__oo______\n    ______________________________________________________\n\n# crafter\n\nTools to Analyze and Visualize Network Packet Capture (PCAP) Files\n\n## Description\n\nLife’s too short to export to CSV/XML. There’s no reason R should not be\nable to read binary PCAP data.\n\n[What is a PCAP?](https://en.wikipedia.org/wiki/Pcap)\n\nYou need the [crafter C++\nlibrary](https://github.com/pellegre/libcrafter) installed and their\nsite lists the other dependencies.\n\nIf there’s any hope for this to run on Windows (`libcrafter` supports\nWindows) it will be due to a Windows + (prbly some infosec) + `#rstats`\nperson tagging along on this project.\n\nYou can find some sample PCAP files:\n\n  - [Netresec](http://www.netresec.com/?page=PcapFiles)\n  - [Wireshark](https://wiki.wireshark.org/SampleCaptures)\n\n## What’s Inside The Tin?\n\nThe following functions are implemented:\n\n  - `read_pcap`: Read in a packet capture file\n  - `seq_in`: Find a (the first) sequence in a vector\n  - `summary.crafter`: Print summary info about a packet capture\n\n(The `pcap` in the functions below is the return value from a call to\n`read_pcap`.)\n\n  - `pcap$get_layer`: return a data.frame with the indicated protocol\n    layer from the pcap packets\n  - `pcap$packet_info`: retrieve a data frame of high level packet info\n  - `pcap$get_payload`: retrieve payload (if any) from a given packet\n    number\n  - `pcap$get_ips`: retrieve a list (with counts) of src/dst/all ips in\n    the capture\n  - `pcap$summary`: summary info about the capture\n\n(There are actually more but they’re inside the pcap object and I just\nneed ",
    "url": "https://github.com/hrbrmstr/crafter",
    "last_updated": "2025-03-22T10:59:02+00:00"
  },
  {
    "full_name": "dgrtwo/StackLite",
    "name": "StackLite",
    "description": "A simple dataset of Stack Overflow questions and tags",
    "language": "R",
    "topics": [],
    "readme": "\n\n\n\n## StackLite: A simple dataset of Stack Overflow questions and tags\n\nThis repository shares a dataset about Stack Overflow questions. For each question, it includes:\n\n* Question ID\n* Creation date\n* Closed date, if applicable\n* Deletion date, if applicable\n* Score\n* Owner user ID\n* Number of answers\n* Tags\n\nThis dataset is ideal for answering questions such as:\n\n* The increase or decrease in questions in each tag over time\n* Correlations among tags on questions\n* Which tags tend to get higher or lower scores\n* Which tags tend to be asked on weekends vs weekdays\n* Rates of question closure or deletion over time\n* The speed at which questions are closed or deleted\n\nThis is all public data within the [Stack Exchange Data Dump](https://archive.org/details/stackexchange), which is much more comprehensive (including question and answer text), but also requires much more computational overhead to download and process. This dataset is designed to be easy to read in and start analyzing. Similarly, this data can be examined within the [Stack Exchange Data Explorer](https://data.stackexchange.com/), but this offers analysts the chance to work with it locally using their tool of choice.\n\n### Status\n\n\n\nThis dataset was extracted from the Stack Overflow database at 2017-04-06 16:39:26 UTC and contains questions up to **2017-04-05**. This includes 13629741 non-deleted questions, and  4133745 deleted ones. (The script for downloading the data can be found in [setup-data.R](setup-data.R), though it can be run only by Stack Overflow employees with database access).\n\n### Examples in R\n\nThe dataset is provided as csv.gz files, which means you can use almost any language or statistical tool to process it. But here I'll share some examples of an analysis in R.\n\nThe question data and the question-tag pairings are stored separately. You can read in the dataset with:\n\n\n```r\nlibrary(readr)\nlibrary(dplyr)\n\nquestions <- read_csv(\"questions.csv.gz\")\nquestion_tags <- read_csv(\"question_tags.",
    "url": "https://github.com/dgrtwo/StackLite",
    "last_updated": "2024-01-04T16:06:15+00:00"
  },
  {
    "full_name": "gojiplus/starclass",
    "name": "starclass",
    "description": "Create Taxonomy and Tags for Liked Github Repositories",
    "language": "Python",
    "topics": [],
    "readme": "# starclass\n\nAutomatically cluster your starred GitHub repositories using semantic similarity analysis. This tool intelligently tracks your starred repos, analyzes their content, and groups them into meaningful clusters with incremental updates to preserve your established organization.\n\n## How it works\n\n**Incremental Pipeline Architecture:**\n1. **Repository Tracking** (`fetch_starred.py`) - Tracks starred repositories and identifies new/updated/removed repos\n2. **Data Collection** (`collect_data.py`) - Incrementally fetches detailed metadata only for changed repositories\n3. **Smart Clustering** (`cluster_repos.py`) - Uses semantic similarity with three modes:\n   - **Auto**: Assigns new repos to existing clusters, reclusters if major changes\n   - **Assign**: Only assigns new repos to existing clusters (preserves organization)\n   - **Full**: Complete reclustering from scratch\n\n## Usage\n\n### GitHub Action\nAdd this action to your workflow:\n\n```yaml\n- name: Cluster starred repositories\n  uses: gojiplus/starclass@main\n  with:\n    github-token: ${{ secrets.GITHUB_TOKEN }}\n    cluster-mode: 'auto'  # or 'assign' or 'full'\n```\n\n### Self-hosted Demo\nThis repository includes a demo workflow. Fork it and:\n\n1. Set repository secrets:\n   - `GH_PAT`: Personal access token with `user` scope  \n2. Go to Actions tab → \"Demo StarClass Action\" → \"Run workflow\"\n\n### Local Usage\n```bash\n# Step 1: Track starred repositories\nGH_TOKEN=<your-token> GH_USER=<your-username> python scripts/fetch_starred.py\n\n# Step 2: Collect detailed data (incremental)\nGH_TOKEN=<your-token> GH_USER=<your-username> python scripts/collect_data.py\n\n# Step 3: Generate clusters\nCLUSTER_MODE=auto python scripts/cluster_repos.py\n```\n\n## Clustering Modes\n\n- **auto** (default): Smart mode that assigns new repos to existing clusters, but reclusters everything if there are major changes\n- **assign**: Only assigns new repositories to existing clusters, preserving your current organization\n- **full**: Performs complete recl",
    "url": "https://github.com/gojiplus/starclass",
    "last_updated": "2025-09-02T09:29:03+00:00"
  },
  {
    "full_name": "amontalenti/elements-of-python-style",
    "name": "elements-of-python-style",
    "description": "Goes beyond PEP8 to discuss what makes Python code feel great. A Strunk & White for Python.",
    "language": "",
    "topics": [
      "python",
      "styleguide",
      "python-style",
      "pep8",
      "documentation",
      "flake8",
      "code-style",
      "codestyle",
      "python3",
      "readability",
      "style-guide"
    ],
    "readme": "# The Elements of Python Style\n\nThis document goes beyond PEP8 to cover the core of what I think of as great Python style. It is opinionated, but not too opinionated. It goes beyond mere issues of syntax and module layout, and into areas of paradigm, organization, and architecture. I hope it can be a kind of condensed [\"Strunk & White\"][strunk-white] for Python code.\n\n[strunk-white]: https://en.wikipedia.org/wiki/The_Elements_of_Style\n\n# Table of Contents\n\n  * [The Elements of Python Style](#the-elements-of-python-style)\n    * [Follow Most PEP8 Guidelines](#follow-most-pep8-guidelines)\n    * [Flexibility on Line Length](#flexibility-on-line-length)\n    * [Consistent Naming](#consistent-naming)\n    * [Nitpicks That Aren't Worth It](#nitpicks-that-arent-worth-it)\n    * [Writing Good Docstrings](#writing-good-docstrings)\n    * [Paradigms and Patterns](#paradigms-and-patterns)\n    * [A Little Zen for Your Code Style](#a-little-zen-for-your-code-style)\n    * [Six of One, Half a Dozen of the Other](#six-of-one-half-a-dozen-of-the-other)\n    * [Standard Tools and Project Structure](#standard-tools-and-project-structure)\n    * [Some Inspiration](#some-inspiration)\n    * [Contributors](#contributors)\n\n## Follow Most [PEP8 Guidelines][pep8]\n\n... but, be flexible on naming and line length.\n\nPEP8 covers lots of mundane stuff like whitespace, line breaks between functions/classes/methods, imports, and warning against use of deprecated functionality. Pretty much everything in there is good.\n\nThe best tool to enforce these rules, while also helping you catch silly Python syntax errors, is [flake8][flake8].\n\nPEP8 is meant as a set of guidelines, not rules to be strictly, or religiously, followed. Make sure to read the section of PEP8 that is titled: \"A Foolish Consistency is the Hobgoblin of Little Minds.\" Also see Raymond Hettinger's excellent talk, [\"Beyond PEP8\"](https://www.youtube.com/watch?v=wf-BqAjZb8M) for more on this.\n\nThe only set of rules that seem to cause a disproport",
    "url": "https://github.com/amontalenti/elements-of-python-style",
    "last_updated": "2025-08-31T17:36:36+00:00"
  },
  {
    "full_name": "trinker/textshape",
    "name": "textshape",
    "description": "Tools for reshaping text data",
    "language": "R",
    "topics": [
      "text-data",
      "data-reshaping",
      "tidy",
      "text-formating",
      "manipulation",
      "r",
      "sentence-boundary-detection"
    ],
    "readme": "textshape\n=========\n\n![](tools/textshape_logo/r_textshape.png)\n\n[![Project Status: Inactive – The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](http://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/)\n\n\n\n**textshape** is small suite of text reshaping and restructuring\nfunctions. Many of these functions are descended from tools in the\n[**qdapTools**](https://github.com/trinker/qdapTools) package. This\nbrings reshaping tools under one roof with specific functionality of the\npackage limited to text reshaping.\n\nOther R packages provide some of the same functionality. **textshape**\ndiffers from these packages in that it is designed to help the user take\nunstructured data (or implicitly structured), extract it into a\nstructured format, and then restructure into common text analysis\nformats for use in the next stage of the text analysis pipeline. The\nimplicit structure of seemingly unstructured data is often\ndetectable/expressible by the researcher. **textshape** provides tools\n(e.g., `split_match`) to enable the researcher to convert this tacit\nknowledge into a form that can be used to reformat data into more\nstructured formats. This package is meant to be used jointly with the\n[**textclean**](https://github.com/trinker/textclean) package, which\nprovides cleaning and text normalization functionality. \n\nTable of Contents\n=================\n\n-   [Functions](#functions)\n-   [Installation](#installation)\n-   [Contact](#contact)\n-   [Contributing](#contributing)\n-   [Examples](#examples)\n-   [Loading Dependencies](#loading-dependencies)\n    -   [Tidying](#tidying)\n    -   [Combining](#combining)\n    -   [Tabulating](#tabulating)\n    -   [Flattening](#flattening)\n    -   [Spanning](#spanning)\n    -   [Splitting](#splitting)\n        -   [Indices](#indices)\n        -   [Matches](#matches)\n        -   [Portions](#portions)\n        -   [Runs](#runs)\n        -   [S",
    "url": "https://github.com/trinker/textshape",
    "last_updated": "2025-07-25T13:54:13+00:00"
  },
  {
    "full_name": "trinker/sentimentr",
    "name": "sentimentr",
    "description": "Dictionary based sentiment analysis that considers valence shifters",
    "language": "R",
    "topics": [
      "r",
      "sentiment-analysis",
      "polarity",
      "sentiment",
      "valence-shifter",
      "amplifier"
    ],
    "readme": "sentimentr   \n============\n\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/0.1.0/active.svg)](https://www.repostatus.org/#active)\n[![Build\nStatus](https://travis-ci.org/trinker/sentimentr.svg?branch=master)](https://travis-ci.org/trinker/sentimentr)\n[![Coverage\nStatus](https://coveralls.io/repos/trinker/sentimentr/badge.svg?branch=master)](https://coveralls.io/github/trinker/sentimentr)\n[![DOI](https://zenodo.org/badge/5398/trinker/sentimentr.svg)](https://zenodo.org/badge/latestdoi/5398/trinker/sentimentr)\n[![](https://cranlogs.r-pkg.org/badges/sentimentr)](https://cran.r-project.org/package=sentimentr)\n\n![](tools/sentimentr_logo/r_sentimentr.png)\n\n**sentimentr** is designed to quickly calculate text polarity sentiment in the \nEnglish language at the sentence level and optionally aggregate by rows or \ngrouping variable(s).\n\n**sentimentr** is a response to my own needs with sentiment detection\nthat were not addressed by the current **R** tools. My own `polarity`\nfunction in the **qdap** package is slower on larger data sets. It is a\ndictionary lookup approach that tries to incorporate weighting for\nvalence shifters (negation and amplifiers/deamplifiers). Matthew Jockers\ncreated the\n[**syuzhet**](https://www.matthewjockers.net/2015/02/02/syuzhet/) package\nthat utilizes dictionary lookups for the Bing, NRC, and Afinn methods as\nwell as a custom dictionary. He also utilizes a wrapper for the\n[Stanford coreNLP](http://nlp.stanford.edu/software/corenlp.shtml) which\nuses much more sophisticated analysis. Jocker's dictionary methods are\nfast but are more prone to error in the case of valence shifters.\nJocker's [addressed these\ncritiques](https://www.matthewjockers.net/2015/03/04/some-thoughts-on-annies-thoughts-about-syuzhet/)\nexplaining that the method is good with regard to analyzing general\nsentiment in a piece of literature. He points to the accuracy of the\nStanford detecti",
    "url": "https://github.com/trinker/sentimentr",
    "last_updated": "2025-07-25T13:54:16+00:00"
  },
  {
    "full_name": "evancz/airplane-mode",
    "name": "airplane-mode",
    "description": "Airplanes are programming heaven. Airplane Mode turns off the bad internet. Yes docs, no facebook!",
    "language": "Python",
    "topics": [],
    "readme": "# Airplane Mode\n\nAirplanes are programming heaven. Going on a 10+ hour flight is a recipe for extraordinary focus and productivity. I always get off these flights feeling good and having done some cool shit. I wanted that without the plane though.\n\nAirplane Mode turns off the distracting parts of the internet, so you can read docs but not get on Facebook, Twitter, Gmail, etc.\n\n![Picture](picture.png)\n\nJust click it to toggle between airplane mode and normal.\n\n\n## Install\n\nAll you need is the `Airplane\\ Mode.app` part (452kb). So just [download the `.zip`](https://github.com/evancz/airplane-mode/archive/master.zip) and grab it out of there.\n\nFrom there just click the icon to toggle things on and off.\n\n\n## Customize\n\nThis is just a wrapper around [a Python file](Airplane%20Mode.app/Contents/Resources/script) that messes with your `/etc/hosts` file. The [`on`](Airplane%20Mode.app/Contents/Resources/on) and [`off`](Airplane%20Mode.app/Contents/Resources/off) files determine how things work. You can just edit them. Add or remove websites to the list depending on your bad habits.\n\nYou can get to these files pretty easy too. Just right click the icon and go to \"Show Package Contents\". That will let you get at all the relevant files in Finder.\n\n![Customize](customize.png)\n\n\n## How do I make something like this?\n\nI made it with this [Platypus](http://www.sveinbjorn.org/platypus) thing for Mac. It was super easy and fun. Just wrote a simple Python script and was able to bundle it all up with a custom icon. Had a great experience, trying to think of more things I can automate with a simple button like this!\n\n\n## Feature Request\n\nCan someone figure out how to make the icon turn on and off depending on the status? Is that even possible? It would be sick though, so open a PR if you figure it out!",
    "url": "https://github.com/evancz/airplane-mode",
    "last_updated": "2025-07-19T05:25:05+00:00"
  },
  {
    "full_name": "seankross/cosmofont",
    "name": "cosmofont",
    "description": ":rocket: Add Fonts to Your Projects Programmatically",
    "language": "R",
    "topics": [],
    "readme": "# cosmofont\n\n### Programmaticaly generate @font-faces for CSS stylesheets in R\n\ncosmofont creates CSS stylesheets so you can use custom fonts in your projects. Use your own font files or use cosmofont to get some of the most popular typography on the web from [Google Fonts](http://google.com/fonts). cosmofont supports `.eot`, `.tff`, and `.woff` file types. Use [Data URIs](https://en.wikipedia.org/wiki/Data_URI_scheme) to embed fonts inline so you can use your webfonts offline!\n\n## To install:\n\n```ruby\nlibrary(devtools)\ninstall_github(\"cosmofont\", \"seankross\")\n```\n\n## Getting started:\n```ruby\n# Get some fonts!\nroboto <- google_font_options(\"Roboto\")\nopen_sans <- google_font_options(\"Open Sans\")\ncosmofont(roboto, open_sans)\n# stylesheet.css should appear in your working directory.\n```\n\n## Using cosmofont with [shiny](https://github.com/rstudio/shiny)\n\nThe structure of your shiny app should look like the following:\n```\n.\n├── ui.R\n├── server.R\n└── stylesheet.css\n```\nA sample `ui.R`:\n```ruby\nlibrary(shiny)\n\n# Define UI for application that plots random distributions \nshinyUI(bootstrapPage(\n\n  # Use custom CSS\n  includeCSS(\"stylesheet.css\"),\n\n  # Application title\n  headerPanel(\"Hello Shiny!\"),\n\n  # Sidebar with a slider input for number of observations\n  sidebarPanel(\n    sliderInput(\"obs\", \n                \"Number of observations:\", \n                min = 1,\n                max = 1000, \n                value = 500)\n  ),\n\n  # Show a plot of the generated distribution\n  mainPanel(\n    plotOutput(\"distPlot\")\n  )\n))\n```\nA sample `server.R`:\n```ruby\nlibrary(shiny)\n\n# Define server logic required to generate and plot a random distribution\nshinyServer(function(input, output) {\n\n  # Expression that generates a plot of the distribution. The expression\n  # is wrapped in a call to renderPlot to indicate that:\n  #\n  #  1) It is \"reactive\" and therefore should be automatically \n  #     re-executed when inputs change\n  #  2) Its output type is a plot \n  #\n  output$distPlot <- render",
    "url": "https://github.com/seankross/cosmofont",
    "last_updated": "2025-03-22T11:10:25+00:00"
  },
  {
    "full_name": "tabulapdf/tabula",
    "name": "tabula",
    "description": "Tabula is a tool for liberating data tables trapped inside PDF files",
    "language": "CSS",
    "topics": [
      "pdf",
      "csv",
      "excel",
      "tables",
      "scraping"
    ],
    "readme": "**Is `tabula` an active project?**\n\nTabula is, and always has been, a volunteer-run project. We've occasionally had funding for specific features, but it's never been a commercial undertaking. At the moment, none of the original authors have the time to actively work on the project. The end-user application, hosted on this repo, is unlikely to see updates from us in the near future. [`tabula-java`](https://github.com/tabulapdf/tabula-java) sees updates and occasional bug-fix releases from time to time.\n\n--\n\n**Repo Note**: The `master` branch is an *in development* version of Tabula. This may be substantially different from the latest [releases of Tabula](https://github.com/tabulapdf/tabula/releases).\n\n---\n\n\n\n# Tabula\n\n[tabula `master`](https://github.com/tabulapdf/tabula/tree/master)\n[![Build Status](https://travis-ci.org/tabulapdf/tabula.svg?branch=master)](https://travis-ci.org/tabulapdf/tabula)  \n\nTabula helps you liberate data tables trapped inside PDF files.\n\n* [Download from the official site](http://tabula.technology/)\n* [Read more about Tabula on OpenNews Source](https://source.opennews.org/en-US/articles/introducing-tabula/)\n* Interested in using Tabula on the command-line? Check out [tabula-java](https://github.com/tabulapdf/tabula-java), a Java library and command-line interface for Tabula. (This is the extraction library that powers Tabula.)\n\n© 2012-2020 Manuel Aristarán. Available under MIT License. See\n[`AUTHORS.md`](AUTHORS.md) and [`LICENSE.md`](LICENSE.md).\n\n-   [Why Tabula?](#why-tabula)\n-   [Using Tabula](#using-tabula)\n-   [Known issues](#known-issues)\n-   [Incorporating Tabula into your own\n    project](#incorporating-tabula-into-your-own-project)\n-   [Running Tabula from source\n    (for developers)](#running-tabula-from-source-for-developers)\n    -   [Building a packaged application\n        version](#building-a-packaged-application-version)\n-   [Contributing](#contributing)\n    -   [Backers](#backers)\n\n## Why Tabula?\n\nIf you’ve ever tried to do",
    "url": "https://github.com/tabulapdf/tabula",
    "last_updated": "2025-09-02T06:10:26+00:00"
  },
  {
    "full_name": "opencivicdata/scrapers-us-municipal",
    "name": "scrapers-us-municipal",
    "description": "Scrapers for US municipal governments.",
    "language": "Python",
    "topics": [],
    "readme": "scrapers-us-municipal\n=====================\n\nSource for municipal scrapers\n\nTo find out more about the ins and outs of these scrapers, as well as how to create your own, head on over to [docs.opencivicdata.org's scraping page](http://docs.opencivicdata.org/en/latest/scrape/index.html).\n\nIssues?\n-------\n\nIssues with the data coming from these scrapers should be filed [in this repository](https://github.com/opencivicdata/scrapers-us-municipal/issues).\n\n## Development\n\n### With Docker\n\nRequires Docker and Docker Compose\n\n#### Initialization\n\n```bash\ndocker-compose run --rm scrapers pupa init YOUR_CITY_SCRAPER\n```\n\n### Without Docker\n\nRequires Python 3, PostGIS\n\n#### Initialization\nAssuming that you want to have your database be called `opencivicdata` on your local machine\n\n```bash\npip install -r requirements.txt\ncreatedb opencivicdata\nexport DATABASE_URL=postgresql:///opencivicdata\npupa dbinit us\npupa init YOUR_CITY_SCRAPER\n```\n\nAt times, the release of ocd-django on PyPI differs from that of Github. This may cause problems if you need to create and run migrations. Specifically, you might encounter an `ImproperlyConfigured` error that instructs you to do the following:\n\n```bash\nYou must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.\n```\n\nFix the problem by running:\n\n```bash\nexport DJANGO_SETTINGS_MODULE=pupa.settings\n```\n\nThen, you should be able to successfully run:\n\n```bash\ndjango-admin makemigrations\ndjango-admin migrate\n```\n\n## Testing\n\nBefore submitting a PR, please run your scraper.\n\n### With Docker\n\n```bash\ndocker-compose run --rm scrapers pupa update YOUR_CITY_SCRAPER\n```\n\n### Without Docker\n\n```bash\nexport DATABASE_URL=postgresql:///opencivicdata\npupa update YOUR_CITY_SCRAPER\n```\n",
    "url": "https://github.com/opencivicdata/scrapers-us-municipal",
    "last_updated": "2025-08-29T07:48:41+00:00"
  },
  {
    "full_name": "notnews/face_of_crime",
    "name": "face_of_crime",
    "description": "Race and Gender of Criminals and Victims in Law and Order",
    "language": "TeX",
    "topics": [
      "race",
      "crime",
      "gender",
      "tv-shows",
      "victim"
    ],
    "readme": "### The Face of Crime During Prime Time: Evidence from Law and Order\n\n#### Data\n\nWe coded the race and gender of criminals and victims in shows from three popular Law & Order series---Original Law & Order, Law & Order: Special Victims Unit, and Law & Order: Criminal Intent. \n\n* [Criminal Intent](data/ci/law_and_order_ci.csv)\n* [Original](data/lo/law_and_order_lo.csv)\n* [Special Victims Unit](data/svu/)\n\nTo contextualize the race and gender distribution in the shows, we assembled relevant data for the years the shows were on air from the census, FBI's Uniform Crime Reports, the National Crime Victimization Survey, and NYC enforcement data:\n\n* [Census](data/census/)\n* [UCR](data/ucr/)\n* [NCVS](data/ncvs/)\n* [NYC Enforcement data](data/ny_enforcement/)\n\n#### Analysis and Write-up\n\n* [Scripts](scripts/)\n* [Manuscript (pdf)](ms/face_of_crime.pdf) \n\n#### Authors\n\nGaurav Sood and Daniel Trielli\n\n#### Contribute to the project\n\nIf you see an inconsistency in the data, or have a suggestion, or some data that you would like to contribute to the project, please create a pull request or open an issue. \n\n#### License\n\nReleased under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/). \n",
    "url": "https://github.com/notnews/face_of_crime",
    "last_updated": "2025-06-12T12:11:27+00:00"
  },
  {
    "full_name": "hrbrmstr/wayback",
    "name": "wayback",
    "description": ":rewind: Tools to Work with the Various Internet Archive Wayback Machine APIs",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "wayback-machine",
      "internet-archive",
      "web-scraping",
      "r-cyber",
      "memento",
      "wayback"
    ],
    "readme": "\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/wayback.svg?branch=master)](https://travis-ci.org/hrbrmstr/wayback)\n[![codecov](https://codecov.io/gh/hrbrmstr/wayback/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/wayback)\n[![Appveyor\nStatus](https://ci.appveyor.com/api/projects/status/w9rwdf8a16t0amht/branch/master?svg=true)](https://ci.appveyor.com/project/hrbrmstr/wayback/branch/master)\n\n# wayback\n\nTools to Work with Internet Archive Wayback Machine APIs\n\n## Description\n\nThe ‘Internet Archive’ provides access to millions of cached sites.\nMethods are provided to access these cached resources through the ‘APIs’\nprovided by the ‘Internet Archive’ and also content from ‘MementoWeb’.\n\n## What’s Inside the Tin?\n\nThe following functions are implemented:\n\n**Memento-ish API**:\n\n  - `archive_available`: Does the Internet Archive have a URL cached?\n  - `cdx_basic_query`: Perform a basic/limited Internet Archive CDX\n    resource query for a URL\n  - `get_mementos`: Retrieve site mementos from the Internet Archive\n  - `get_timemap`: Retrieve a timemap for a URL\n  - `read_memento`: Read a resource directly from the Time Travel\n    MementoWeb\n  - `is_memento`: Various memento-type testers (useful in `purrr` or\n    `dplyr` contexts)\n  - `is_first_memento`: Various memento-type testers (useful in `purrr`\n    or `dplyr` contexts)\n  - `is_next_memento`: Various memento-type testers (useful in `purrr`\n    or `dplyr` contexts)\n  - `is_prev_memento`: Various memento-type testers (useful in `purrr`\n    or `dplyr` contexts)\n  - `is_last_memento`: Various memento-type testers (useful in `purrr`\n    or `dplyr` contexts)\n  - `is_original`: Various memento-type testers (useful in `purrr` or\n    `dplyr` contexts)\n  - `is_timemap`: Various memento-type testers (useful in `purrr` or\n    `dplyr` contexts)\n  - `is_timegate`: Various memento-type testers (useful in `purrr` or\n    `dplyr` contexts)\n\n**Scrape API**\n\n  - `ia_retrieve:` Retrieve directory listings for Intern",
    "url": "https://github.com/hrbrmstr/wayback",
    "last_updated": "2025-03-22T11:18:18+00:00"
  },
  {
    "full_name": "DavidSonoda/clenv",
    "name": "clenv",
    "description": "ClearML configuration profile manager",
    "language": "Python",
    "topics": [],
    "readme": "# clenv - Unofficial ClearML CLI helper\n\n\n\n## Pre-requisites\n\n- `clearml` installed, please refer to [ClearML installation guide](https://clear.ml/docs/latest/docs/getting_started/ds/ds_first_steps) for more details.\n- Run `clearml-init` and initialize your first ever config file.\n\n\n\n## Installation\n\n```bash\npip install clenv\n```\n\n\n## Usage\n\n### Subcommand `config`\nNote: All config files must be in the format of `clearml-<profile_name>.conf`\n\n#### List all config profiles\n```bash\nclenv config list\n```\n\n#### Create a new config profile\n```bash\nclenv config create <profile_name>\n```\n\n#### Delete a config profile\n```bash\nclenv config del <profile_name>\n```\n\n#### Switch to a config profile\n```bash\nclenv config checkout <profile_name>\n```\n\n#### Reinitialize the `api` section of a config\n```bash\nclenv config reinit <profile_name>\n# Please paste your multi-line configuration and press Enter:\n```\nThen paste your multi-line configuration generated through clearML server.\n\n### Subcommand `user`\n\n#### Generate user/password hocon config\n```bash\nclenv user genpass <user_name>\n```\n\n### Subcommand `task`\n\n> Note: This command only support git repos for now. The project name of the task created on the ClearML server will be the same as the git repo name. So please make sure you have a meaningful, easy to read git repo name.\n\n#### Execute a task remotely on ClearML server\n\n```bash\nclenv task exec\n```\n\nIt will prompt you to select an available queue, correct task type, your entrypoint script path and input the task name.\n\n![clenv-task-exec-1](./static/clenv-task-exec-1.png)\n\nAfter inputting all the required configs, it will ask you whether to save the configs. By typing 'y', the config will be saved. When you execute `clenv task exec` next time in the same repo, it will load the saved configs and skip the config input process. However, it will still ask you for confirmation before submitting the task.\n\n#### Ignore the saved run configs when starting a new execution\n\nIf you want to i",
    "url": "https://github.com/DavidSonoda/clenv",
    "last_updated": "2025-01-15T15:30:00+00:00"
  },
  {
    "full_name": "getAsterisk/opcode",
    "name": "opcode",
    "description": "A powerful GUI app and Toolkit for Claude Code - Create custom agents, manage interactive Claude Code sessions, run secure background agents, and more.",
    "language": "TypeScript",
    "topics": [
      "anthropic",
      "anthropic-claude",
      "claude",
      "claude-4",
      "claude-4-sonnet",
      "claude-ai",
      "claude-code",
      "claude-code-sdk",
      "cursor",
      "ide",
      "llm",
      "llm-code",
      "claude-4-opus",
      "rust",
      "tauri"
    ],
    "readme": "<div align=\"center\">\n  <img src=\"src-tauri/icons/icon.png\" alt=\"opcode Logo\" width=\"120\" height=\"120\">\n\n  <h1>opcode</h1>\n  \n  <p>\n    <strong>A powerful GUI app and Toolkit for Claude Code</strong>\n  </p>\n  <p>\n    <strong>Create custom agents, manage interactive Claude Code sessions, run secure background agents, and more.</strong>\n  </p>\n  \n  <p>\n    <a href=\"#features\"><img src=\"https://img.shields.io/badge/Features-✨-blue?style=for-the-badge\" alt=\"Features\"></a>\n    <a href=\"#installation\"><img src=\"https://img.shields.io/badge/Install-🚀-green?style=for-the-badge\" alt=\"Installation\"></a>\n    <a href=\"#usage\"><img src=\"https://img.shields.io/badge/Usage-📖-purple?style=for-the-badge\" alt=\"Usage\"></a>\n    <a href=\"#development\"><img src=\"https://img.shields.io/badge/Develop-🛠️-orange?style=for-the-badge\" alt=\"Development\"></a>\n    <a href=\"https://discord.gg/G9g25nj9\"><img src=\"https://img.shields.io/badge/Discord-Join-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  </p>\n</div>\n\n![457013521-6133a738-d0cb-4d3e-8746-c6768c82672c](https://github.com/user-attachments/assets/a028de9e-d881-44d8-bae5-7326ab3558b9)\n\nhttps://github.com/user-attachments/assets/bf0bdf9d-ba91-45af-9ac4-7274f57075cf\n\n> [!TIP]\n> **⭐ Star the repo and follow [@getAsterisk](https://x.com/getAsterisk) on X for early access to `asteria-swe-v0`**.\n\n> [!NOTE]\n> This project is not affiliated with, endorsed by, or sponsored by Anthropic. Claude is a trademark of Anthropic, PBC. This is an independent developer project using Claude.\n\n## 🌟 Overview\n\n**opcode** is a powerful desktop application that transforms how you interact with Claude Code. Built with Tauri 2, it provides a beautiful GUI for managing your Claude Code sessions, creating custom agents, tracking usage, and much more.\n\nThink of opcode as your command center for Claude Code - bridging the gap between the command-line tool and a visual experience that makes AI-assisted development more intuitive and productive",
    "url": "https://github.com/getAsterisk/opcode",
    "last_updated": "2025-09-02T09:51:15+00:00"
  },
  {
    "full_name": "hrbrmstr/pubcrawl",
    "name": "pubcrawl",
    "description": "🍺📖 Convert 'epub' Files to Text (Use https://github.com/ropensci/epubr instead)",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "epub",
      "tidytext"
    ],
    "readme": "#  ***  IMPORTANT ***\n\nNo further development will occur in this package as it has been supeseded by the actively maintained and quite spiffy! [`epubr`](https://github.com/ropensci/epubr) package.\n\n--------------\n\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/pubcrawl.svg?branch=master)](https://travis-ci.org/hrbrmstr/pubcrawl)\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/hrbrmstr/pubcrawl?branch=master&svg=true)](https://ci.appveyor.com/project/hrbrmstr/pubcrawl)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/hrbrmstr/pubcrawl/master.svg)](https://codecov.io/github/hrbrmstr/pubcrawl?branch=master)\n\n# pubcrawl\n\nConvert ‘epub’ Files to Text\n\n## Description\n\nConvert ‘epub’ Files to Text\n\nThe ‘epub’ file format is really just a structured ‘ZIP’ archive with\nmetadata, graphics and (usually) ‘HTML’ text. Tools are provided to turn\nan ‘epub’ file into a tidy data frame.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `epub_to_text`: Convert an epub file into a data frame of plaintext\n    chapters\n\n## NOTE\n\nThere are edge cases I’ve totally not covered yet. Feel free to jump in\nand make this a real, useful package\\!\n\n## TODO\n\n  - \\[X\\] Refactor so there aren’t so many heavy dependencies\n  - <strike> \\[ \\] Try to get `hgr` on CRAN so it’s not a GH\n    dep</strike> Moved the cleaner code into here\n  - \\[ \\] Better docs\n  - \\[X\\] Embed some epubs for examples and tests\n  - \\[X\\] Setup Travis, Appveyor, code coverage\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/pubcrawl\")\n```\n\n## Usage\n\n``` r\nlibrary(pubcrawl)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"pubcrawl\")\n```\n\n    ## [1] '0.1.0'\n\n### An O’Reilly epub\n\n``` r\nepub_to_text(\"~/Data/R Packages.epub\")\n```\n\n    ## # A tibble: 26 x 4\n    ##    path                         size date                content                                                       \n    ##    <chr>                       <dbl> <dttm>              ",
    "url": "https://github.com/hrbrmstr/pubcrawl",
    "last_updated": "2025-03-22T11:08:33+00:00"
  },
  {
    "full_name": "npetraco/x3pr",
    "name": "x3pr",
    "description": "Basic read/write capability for the x3p surface metrology format in R",
    "language": "R",
    "topics": [],
    "readme": "# x3pr \n\nBasic read/write utilities for the x3p surface metrology format in R. The package also supports the DigitalSurf .sur () format. X3pr is now failrly stable, but still under some development and hence not up on CRAN yet. \nThe best way to install x3pr is with Hadley Wickham's devtools (cf. https://github.com/hadley/devtools and http://cran.r-project.org/web/packages/devtools/index.html). \n\n* If you haven't already done so, first install R (http://www.r-project.org/) and then RStudio (http://www.rstudio.com/) for your operating system. Note that both are open-source (and thus peer-reviewed, ...many times over if that is a concern) and free. \n\n* Install devtools in your R distribution. In the RStudio Console window type:\n  * install.packages(\"devtools\")\n\n* Windows users: Install Rtools 3.1 as recommended by the devtools package: http://cran.r-project.org/bin/windows/Rtools/ \n\n* Mac users: Make sure to install XQuartz (http://xquartz.macosforge.org/trac) which is required by rgl.\n\n* Linux users: Being singularly DIY, you are probably ok. But, ... just incase ... make sure gcc is installed and running. Any toolchain should suffice however. For shinyRGL, try to install from git directly: http://trestletech.github.io/shinyRGL/\n  * There is no shinyRGL for Linux up on CRAN\n\n* Install the required support packages for x3pr. In the RStudio Console window, execute:\n  * install.packages(\"XML\")\n  * install.packages(\"rgl\")\n  * install.packages(\"shiny\")\n  * install.packages(\"shinyRGL\") (Windows and Mac only...)\n\n* Finally, in the RStudio Console window execute: devtools()::install_github(\"x3pr\", username=\"npetraco\").\n\nExamples are provided in the help pages. If you are new to R, below is an example R script which:\n\n* Loads the x3pr package\n* Reads in the example X3P file shipped with the package (glock.x3p)\n* Prints out the header information in the file\n* Plots the surface (a primer shear) in interactive 3D\n\n`library(x3pr)`\n\n`file.path <- system.file(\"extdata\", \"glock.x3p\"",
    "url": "https://github.com/npetraco/x3pr",
    "last_updated": "2017-01-31T03:29:43+00:00"
  },
  {
    "full_name": "ryanflorence/react-training",
    "name": "react-training",
    "description": "Mini React Training Course",
    "language": "JavaScript",
    "topics": [],
    "readme": "React Training\n==============\n\nFor immersive React.js training check out my new job at http://reactjs-training.com\n\nThis is material I used for instructure's react training sessions when I worked there.\n\nThe main branch is gh-pages, so all of the examples in `code/` should\nrun at http://ryanflorence.github.io/react-training/code (but you'll have\nto punch in the URL for now).\n\n\n## Running the examples locally\n\n```bash\nnpm install\nnpm start\n```\n\nThen interact with the examples at:\n\n```\nlocalhost:8080\n```\n",
    "url": "https://github.com/ryanflorence/react-training",
    "last_updated": "2025-08-22T11:38:07+00:00"
  },
  {
    "full_name": "all-contributors/all-contributors",
    "name": "all-contributors",
    "description": "✨ Recognize all contributors, not just the ones who push code ✨",
    "language": "HTML",
    "topics": [
      "contributors",
      "all-contributors",
      "opensource",
      "opensource-management",
      "open-source-tooling",
      "acknowledgements",
      "recognition"
    ],
    "readme": "> Call for translators! [We're looking for translators](https://github.com/all-contributors/all-contributors/issues/143) to help translate this spec for everyone!\n\n<div align=\"center\">\n    <a href=\"https://allcontributors.org\">\n        <img src=\"docs/assets/logo-full.svg\" alt=\"✨ All Contributors ✨\" width=\"800px\" />\n    </a>\n</div>\n\n<table>\n    <caption>Read this documentation in the following languages</caption>\n    <tbody>\n        <tr>\n            <td><a href=\"https://allcontributors.org/docs/ko/overview\">한국어</a></td>\n            <td><a href=\"https://allcontributors.org/docs/zh-CN/overview\">中文</a></td>\n            <td><a href=\"https://allcontributors.org/docs/id/overview\">Bahasa Indonesia</a></td>\n            <td><a href=\"https://allcontributors.org/docs/de/overview\">Deutsch</a></td>\n            <td><a href=\"https://allcontributors.org/docs/pl/overview\">Polski</a></td>\n        </tr>\n        <tr>\n            <td><a href=\"https://allcontributors.org/docs/en/overview\">English</a></td>\n            <td><a href=\"https://allcontributors.org/docs/pt-BR/overview\">Português do Brasil</a></td>\n            <td><a href=\"https://allcontributors.org/docs/es-ES/overview\">Español</a></td>\n            <td><a href=\"https://allcontributors.org/docs/fr/overview\">Français</a></td>\n            <td><a href=\"https://allcontributors.org/docs/ru/overview\">Русский</a></td>\n            <td><a href=\"https://allcontributors.org/docs/ja/overview\">日本語</a></td>\n        </tr>\n    </tbody>\n</table>\n\n[![Build Status](https://img.shields.io/circleci/project/all-contributors/all-contributors/master.svg)](https://circleci.com/gh/all-contributors/workflows/all-contributors/tree/master)\n[![Crowdin](https://d322cqt584bo4o.cloudfront.net/all-contributors/localized.svg)](https://crowdin.com/project/all-contributors)\n[![All Contributors](https://img.shields.io/github/all-contributors/all-contributors/all-contributors?color=ee8449&style=flat-square)](#contributors-)\n[![Netlify Status](https://api.netlify.com/ap",
    "url": "https://github.com/all-contributors/all-contributors",
    "last_updated": "2025-09-02T07:32:14+00:00"
  },
  {
    "full_name": "google/robotstxt",
    "name": "robotstxt",
    "description": "The repository contains Google's robots.txt parser and matcher as a C++ library (compliant to C++11).",
    "language": "C++",
    "topics": [],
    "readme": "\n# Google Robots.txt Parser and Matcher Library\n\nThe repository contains Google's robots.txt parser and matcher as a C++ library\n(compliant to C++14).\n\n## About the library\n\nThe Robots Exclusion Protocol (REP) is a standard that enables website owners to\ncontrol which URLs may be accessed by automated clients (i.e. crawlers) through\na simple text file with a specific syntax. It's one of the basic building blocks\nof the internet as we know it and what allows search engines to operate.\n\nBecause the REP was only a de-facto standard for the past 25 years, different\nimplementers implement parsing of robots.txt slightly differently, leading to\nconfusion. This project aims to fix that by releasing the parser that Google\nuses.\n\nThe library is slightly modified (i.e. some internal headers and equivalent\nsymbols) production code used by Googlebot, Google's crawler, to determine which\nURLs it may access based on rules provided by webmasters in robots.txt files.\nThe library is released open-source to help developers build tools that better\nreflect Google's robots.txt parsing and matching.\n\nFor webmasters, we included a small binary in the project that allows testing a\nsingle URL and user-agent against a robots.txt.\n\n## Building the library\n\n### Quickstart\n\nWe included with the library a small binary to test a local robots.txt against a\nuser-agent and URL. Running the included binary requires:\n\n*   A compatible platform (e.g. Windows, macOS, Linux, etc.). Most platforms are\n    fully supported.\n*   A compatible C++ compiler supporting at least C++14. Most major compilers\n    are supported.\n*   [Git](https://git-scm.com/) for interacting with the source code repository.\n    To install Git, consult the\n    [Set Up Git](https://help.github.com/articles/set-up-git/) guide on\n    [GitHub](https://github.com/).\n*   Although you are free to use your own build system, most of the\n    documentation within this guide will assume you are using\n    [Bazel](https://bazel.build/). To download",
    "url": "https://github.com/google/robotstxt",
    "last_updated": "2025-09-01T14:14:27+00:00"
  },
  {
    "full_name": "jonathan-bower/DataScienceResources",
    "name": "DataScienceResources",
    "description": "Open Source Data Science Resources.",
    "language": "",
    "topics": [],
    "readme": "## Data Science Resources\n\nHello and welcome to the Data Science Resources repo.  I originally built this repo so that I could have a location to host resources that are helpful to me.  Through building the repo I realized that other people might be also be interested. I have tried to curate content on data science topics, high quality resources to learn from, and relevant blog posts.\n\nThe intended goal was to cover more than just the technical component of data science.  I have tried to find topics that cover building data science teams, business practices, use-cases, product metrics and data science career paths.  Hope this is helpful\n\n# Table Of Contents\n### 1. [Data Science Getting Started](#data-science-getting-started)\n  * [Start](#start)\n  * [Data Science Courses](#data-science-courses)\n\n### 2. [Data Pipeline & Tools](#data-pipeline--tools)\n  * [Python](#python)\n  * [Data Structures & CS topics](#data-structures--cs-topics)\n  * [Statistics](#statistics)\n  * [Stats/Engineering Libraries](#statsengineering-libraries)\n  * [Databases/Frameworks](#databasesframeworks)\n  * [Data Acquisition](#data-acquisition)\n  * [Processing & EDA](#processing--exploratory-data-analysis)\n  * [Machine Learning](#machine-learning)\n    * [Machine Learning Theory](#machine-learning-theory)\n    * [Deep Learning](#deep-learning)\n    * [Model Selection](#model-selection)\n    * [Model Evaluation](#model-evaluation)\n    * [Feature Engineering](#feature-engineering)\n  * [Additional Tools or Processes](#additional-tools-or-processes)\n  * [Data Visualization](#data-visualization)\n  * [ipython Notebook Tutorials](#ipython-notebook-tutorials)\n  * [Data Sources](#data-sources)\n  * [New Data Tools](#new-data-tools)\n\n### 3. [Product](#product)\n  * [Product Metrics](#product-metrics)\n  * [Team Communication & Business Tools](#team-communication--business-tools)\n  * [Best Practices](#best-practices)\n\n### 4. [Career Resources](#career-resources)\n  * [Data Science Career Path](#data-science-career-pat",
    "url": "https://github.com/jonathan-bower/DataScienceResources",
    "last_updated": "2025-09-02T05:48:59+00:00"
  },
  {
    "full_name": "datamade/million-dollar-blocks",
    "name": "million-dollar-blocks",
    "description": " :moneybag: An interactive visualization of incarceration spending in Chicago",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Chicago Million Dollar Blocks\n\nThis repo contains code for the Chicago Million Dollar Blocks [website](https://chicagosmilliondollarblocks.com/). This project relies on analysis of incarceration spending in Chicago, which you can find in the [million-dollar-blocks-analysis](https://github.com/datamade/million-dollar-blocks-analysis) repo.\n\n## Setup\nFork the repository, and then clone it to your local machine.\n\n``` bash\ngit clone git@github.com:datamade/million-dollar-blocks.git\ncd million-dollar-blocks\n```\n\nWe built this site with [Jekyll](https://jekyllrb.com/). Install it:\n\n```bash\ngem install jekyll bundler\n```\nTo run the site locally: \n```bash\njekyll serve -w\n```\n\nThen, navigate to http://localhost:5000/.\n\n## Dependencies\nWe used the following open source tools:\n\n* [Bootstrap](https://getbootstrap.com/) - Responsive HTML, CSS and Javascript framework\n* [Maplibre-GL](https://maplibre.org/maplibre-gl-js/docs/) - javascript library interactive maps\n* [pako](https://nodeca.github.io/pako/) - for unzipping our GeoJSON map files\n* [jQuery Address](https://github.com/asual/jquery-address) - javascript library creating RESTful URLs\n* [GitHub pages](https://pages.github.com/) - free static website hosting\n\n## Team\n\n* Forest Gregg, DataMade - developer, data cruncher\n* Cathy Deng, DataMade - designer, developer\n* Eric van Zanten, DataMade - developer, GIS data\n* Derek Eder, DataMade - developer, content\n* Dr. Dan Cooper, Adler University - writer and strategist\n* Dr. Ryan Lugalia-Hollon - writer and strategist\n\nThanks to the folks who helped review this, including: the INN Nerds, Hannah Chung, Pat Sier, Christopher Kelly, Yuanqi Wang, Brian Goggin, Sarah Bump, Joel Inwood\n\n## Credits\n\nThe original idea and concept for [Million Dollar Blocks](https://www.spatialinformationdesignlab.org/projects.php%3Fid%3D16) came from Laura Kurgan at the [Spatial Information Design Lab](https://www.spatialinformationdesignlab.org/) in collaboration with the [Justice Mapping Center](http",
    "url": "https://github.com/datamade/million-dollar-blocks",
    "last_updated": "2025-08-28T20:39:29+00:00"
  },
  {
    "full_name": "dqbd/tiktokenizer",
    "name": "tiktokenizer",
    "description": "Online playground for OpenAPI tokenizers",
    "language": "TypeScript",
    "topics": [
      "chatgpt",
      "nextjs",
      "openai",
      "t3-stack",
      "tiktoken",
      "tokenizer"
    ],
    "readme": "![Tiktokenizer](https://user-images.githubusercontent.com/1443449/222597674-287aefdc-f0e1-491b-9bf9-16431b1b8054.svg)\n\n***\n\n# Tiktokenizer\n\nOnline playground for `openai/tiktoken`, calculating the correct number of tokens for a given prompt.\n\nSpecial thanks to [Diagram](https://diagram.com/) for sponsorship and guidance.\n\nhttps://user-images.githubusercontent.com/1443449/222598119-0a5a536e-6785-44ad-ba28-e26e04f15163.mp4\n\n## Acknowledgments\n\n- [T3 Stack](https://create.t3.gg/)\n- [shadcn/ui](https://github.com/shadcn/ui)\n- [openai/tiktoken](https://github.com/openai/tiktoken)\n",
    "url": "https://github.com/dqbd/tiktokenizer",
    "last_updated": "2025-09-02T07:42:40+00:00"
  },
  {
    "full_name": "fitnr/SublimeDataConverter",
    "name": "SublimeDataConverter",
    "description": "Sublime Text package for converting CSV data to other formats",
    "language": "Python",
    "topics": [],
    "readme": "# Sublime DataConverter\n\nThis [Sublime Text](http://www.sublimetext.com/) package converts csv files to various other formats. It's been adapted from the wonderful [Mr. Data Converter](https://shancarter.github.io/mr-data-converter/).\n\nAfter installing, you'll find commands look like __DataConverter: to *foo*__ in the Command Palette. DataConverter will convert a selection or multiple selections. If nothing is selected, the entire document is converted.\n\n### Examples\n\nTurn this:\n\n    name,value,fruit,date\n    Alice,10,Apple,\"Sep. 12, 2016\"\n    Bob,11,Blueberry,\"Sep. 13, 2016\"\n    Chris,12,Orange,\"Sep. 14, 2016\"\n\ninto this (Ruby):\n\n```ruby\n[{\"name\"=>\"Alice\", \"value\"=>10, \"fruit\"=>\"Apple\", \"date\"=>\"Sep. 12, 2016\"},\n{\"name\"=>\"Bob\", \"value\"=>11, \"fruit\"=>\"Blueberry\", \"date\"=>\"Sep. 13, 2016\"},\n{\"name\"=>\"Chris\", \"value\"=>12, \"fruit\"=>\"Orange\", \"date\"=>\"Sep. 14, 2016\"}];\n```\n\nor this (JSON):\n\n```javascript\n[\n  {\"fruit\": \"Apple\", \"name\": \"Alice\", \"value\": \"10\", \"date\": \"Sep. 12, 2016\"},\n  {\"fruit\": \"Blueberry\", \"name\": \"Bob\", \"value\": \"11\", \"date\": \"Sep. 13, 2016\"},\n  {\"fruit\": \"Orange\", \"name\": \"Chris\", \"value\": \"12\", \"date\": \"Sep. 14, 2016\"}\n]\n```\n\n### Formats supported\n\n* ActionScript\n* ASP\n* HTML tables\n* Gherkin\n* JIRA (Atlassian Confluence)\n* JSON\n* JSON (array of columns)\n* JSON (array of rows)\n* JSON (object, first column is key)\n* Javascript object\n* Markdown (Github-flavored)\n* Perl\n* PHP (two formats)\n* Python (list of dicts)\n* Python (list of lists)\n* Ruby\n* SQL (Postgres, MySQL and SQLite)\n* text table\n* Wiki markup\n* XML\n* XML (property list)\n* XML for data-driven Adobe Illustrator\n* YAML\n\nAdditionally, DataConverter can convert between delimiters. By default, this includes commands to convert to CSV and TSV, and it's possible to add your own delimiter (create a `User.sublime-commands` file following the pattern in [`DataConverter.sublime-commands`](DataConverter.sublime-commands)).\n\n## Installation\n\n### With Package Control\nIf you have [Package Control](http:",
    "url": "https://github.com/fitnr/SublimeDataConverter",
    "last_updated": "2025-07-27T15:42:35+00:00"
  },
  {
    "full_name": "jt14den/acms-lab-stats",
    "name": "acms-lab-stats",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "# ACMS - Library Lab Stats\n",
    "url": "https://github.com/jt14den/acms-lab-stats",
    "last_updated": "2015-07-24T19:38:25+00:00"
  },
  {
    "full_name": "unitedstates/images",
    "name": "images",
    "description": "Public domain photos of Members of the United States Congress",
    "language": "Python",
    "topics": [
      "open-data",
      "us-congress",
      "congress",
      "photos",
      "public-domain"
    ],
    "readme": "# Images of Congress\n\n[![DOI](https://zenodo.org/badge/16810726.svg)](https://zenodo.org/badge/latestdoi/16810726)\n\nPublic domain images of members of the US Congress.\n\n## Using the photos\n\nPhotos are available at predictable URLs, by size and Bioguide ID. Photos are served\nusing GitHub Pages.\n\n```\nhttps://unitedstates.github.io/images/congress/[size]/[bioguide].jpg\n```\n\n`[size]` can be one of:\n\n- `original` - As originally downloaded. Typically, `675x825`, but\n  [it can vary](https://github.com/unitedstates/images/issues/1#issuecomment-35070231).\n- `450x550`\n- `225x275`\n\n`[bioguide]` must be a Bioguide ID. These are unique IDs for members of Congress, as\ndefined by the [Congressional Bioguide](http://bioguide.congress.gov). They can be found\nand connected to many other Congressional datasets, including the partner dataset over\nat\n[unitedstates/congress-legislators](https://github.com/unitedstates/congress-legislators).\n\n**Note:** Our HTTPS permalinks are provided through CloudFlare's\n[Universal SSL](https://blog.cloudflare.com/introducing-universal-ssl/), which also uses\n\"Flexible SSL\" to talk to GitHub Pages' unencrypted endpoints. So, you should know that\nit's not an end-to-end encrypted channel, but is encrypted between your client use and\nCloudFlare's servers (which at least should dissociate your requests from client IP\naddresses).\n\n## Downloading all images of a particular size\n\nIf you want to quickly grab all images of a particular size without cloning the entire\nrepo (and have `svn` installed), you can just do something like this:\n\n`svn checkout https://github.com/unitedstates/images/trunk/congress/225x275`\n\n## Gathering more photos\n\n[![GitHub Actions status](https://github.com/unitedstates/images/workflows/Test/badge.svg)](https://github.com/unitedstates/images/actions)\n[![codecov](https://codecov.io/gh/unitedstates/images/branch/gh-pages/graph/badge.svg)](https://codecov.io/gh/unitedstates/images)\n\nThis project uses a Python script that scrapes the\n[Gover",
    "url": "https://github.com/unitedstates/images",
    "last_updated": "2025-07-14T03:27:03+00:00"
  },
  {
    "full_name": "jupyter/nbdime",
    "name": "nbdime",
    "description": "Tools for diffing and merging of Jupyter notebooks.",
    "language": "TypeScript",
    "topics": [
      "jupyterlab-extension",
      "jupyter",
      "jupyter-notebook",
      "diff",
      "diffing",
      "merge",
      "git",
      "hg",
      "mercurial",
      "mergetool",
      "merge-driver",
      "vcs",
      "version-control"
    ],
    "readme": "**[Installation](#installation)** |\n**[Documentation](#documentation)** |\n**[Contributing](#contributing)** |\n**[Development Install](#development-install)** |\n**[Testing](#testing)** |\n**[License](#license)** |\n**[Getting help](#getting-help)**\n\n# [nbdime](https://github.com/jupyter/nbdime) Jupyter Notebook Diff and Merge tools\n\n[![Test](https://github.com/jupyter/nbdime/actions/workflows/tests.yml/badge.svg)](https://github.com/jupyter/nbdime/actions/workflows/tests.yml)\n[![codecov.io](https://codecov.io/github/jupyter/nbdime/coverage.svg?branch=master)](https://codecov.io/github/jupyter/nbdime?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/nbdime/badge/?version=latest)](http://nbdime.readthedocs.io/en/latest/?badge=latest)\n[![Google Group](https://img.shields.io/badge/-Google%20Group-lightgrey.svg)](https://groups.google.com/forum/#!forum/jupyter)\n\n`nbdime` provides tools for diffing and merging of [Jupyter Notebooks](https://jupyter-notebook.readthedocs.io).\n\n- `nbdiff` compare notebooks in a terminal-friendly way\n- `nbmerge` three-way merge of notebooks with automatic conflict resolution\n- `nbdiff-web` shows you a rich rendered diff of notebooks\n- `nbmerge-web` gives you a web-based three-way merge tool for notebooks\n- `nbshow` present a single notebook in a terminal-friendly way\n\nDiffing notebooks in the terminal:\n\n![terminal-diff](docs/source/images/nbdiff-terminal.png)\n\nMerging notebooks in a browser:\n\n![web-merge](docs/source/images/nbmerge-web.png)\n\n## Installation\n\nInstall nbdime with pip:\n\n    pip install nbdime\n\nSee [the installation docs](https://nbdime.readthedocs.io/en/latest/installing.html) for more installation details and development installation instructions.\n\n## Documentation\n\nSee the latest documentation at https://nbdime.readthedocs.io.\n\nSee also description and discussion in the [Jupyter Enhancement Proposal](https://github.com/jupyter/enhancement-proposals/pull/8).\n\n## Contributing\n\nIf you would like to contribute ",
    "url": "https://github.com/jupyter/nbdime",
    "last_updated": "2025-08-30T07:16:43+00:00"
  },
  {
    "full_name": "beechung/Latent-Factor-Models",
    "name": "Latent-Factor-Models",
    "description": "R functions for fitting latent factor models with internal computation in C/C++",
    "language": "R",
    "topics": [],
    "readme": "\n########################################################\n     Research Code for Fitting Latent Factor Models\n########################################################\n\nAuthors: Bee-Chung Chen, Deepak Agarwal and Liang Zhang\n         Yahoo! Labs\n\n\nI. Introduction\n\n   This code base consists of algorithms for fitting factor models written in\n   R and C/C++.  The entry point of any fitting algorithm is in R.  The\n   computationally intensive parts are written in C/C++. The models and \n   algorithms have been described in the following papers.\n   \n   [1] Bee-Chung Chen, Jian Guo, Belle Tseng, Jie Yang. User reputation in a\n       comment rating environment. KDD 2011.\n   [2] Deepak Agarwal, Bee-Chung Chen. Regression-based latent factor models.\n       KDD 2009.\n   [3] Deepak Agarwal, Bee-Chung Chen, Bo Long. Localized factor models for \n       multi-context recommendation. KDD 2011.\n   [4] Deepak Agarwal, Bee-Chung Chen. Latent OLAP: Data cubes over latent \n       variables. SIGMOD Conference 2011.\n   [5] Deepak Agarwal, Bee-Chung Chen. fLDA: Matrix factorization through \n       latent Dirichlet allocation. WSDM 2010.\n       \nII. Tutorial\n\n   See doc/tutorial.pdf for a tutorial on how to use this package to fit\n   the latent factor models described in [1,2].\n\nIII. Compilation\n\n   You need to have R installed before compiling the code.\n   To install R, see: http://www.r-project.org/\n   You have to install R from source on a linux machine.\n   It is recommended to use R version >= 2.10.1.\n   \n   The following R packages also need to be installed.\n      Matrix\n      glmnet\n   \n   To compile the C/C++ code, just type make.\n\nIV. Examples\n\n   Localized factor model (multi-context, multi-application factorization) [2]:\n        src/multi-app/R/example/fitting.R\n\n   fLDA model (LDA topic modeling + Matrix factorization) [5]:\n        src/LDA-RLFM/R/model/examples.R\n   \n",
    "url": "https://github.com/beechung/Latent-Factor-Models",
    "last_updated": "2025-04-30T01:05:39+00:00"
  },
  {
    "full_name": "SasheO/CS3_Project",
    "name": "CS3_Project",
    "description": "CS3 Project, Spring 2023",
    "language": "Python",
    "topics": [],
    "readme": "### Description\r\nValidator_and_token_generator is a package created for a class project to validate certain strings such as credit/debit card numbers, generate alphanumeric tokens, and do other string processing like converting a datetime object into the relative time from now.\r\n\r\n### Install validator_and_token_generator from PyPi.\r\n```bash\r\npip install validator_and_token_generator\r\n```\r\n\r\n#### Example\r\n```python\r\n  # Import library\r\n  import validator_and_token_generator\r\n  # Generate token\r\n  _token = validator_and_token_generator.token_generator(length=10)\r\n```\r\n-------\r\n\r\n",
    "url": "https://github.com/SasheO/CS3_Project",
    "last_updated": "2025-01-15T15:29:51+00:00"
  },
  {
    "full_name": "irmakcakmak/pygender",
    "name": "pygender",
    "description": "Python wrapper for gender detection library, gender.c",
    "language": "C",
    "topics": [],
    "readme": "A Python wrapper for gender.c\nPackage using command:\npython setup.py sdist\nInstall using either:\npip install pygender\nor If you have the tar.gz file:\npython setup.py install\nNeed sudo as data file is put under /var/lib/gender\n\nUSAGE\nimport pygender\n\nAVAILABLE METHODS\nget_gender\n\n",
    "url": "https://github.com/irmakcakmak/pygender",
    "last_updated": "2020-02-17T22:33:51+00:00"
  },
  {
    "full_name": "fecgov/openFEC",
    "name": "openFEC",
    "description": "The first RESTful API for the Federal Election Commission. We're aiming to make campaign finance more accessible for journalists, academics, developers, and other transparency seekers.",
    "language": "Python",
    "topics": [],
    "readme": "**Develop**\n[![CircleCI](https://circleci.com/gh/fecgov/openFEC.svg?style=svg)](https://circleci.com/gh/fecgov/openFEC)\n[![Test Coverage](https://img.shields.io/codecov/c/github/fecgov/openFEC/develop.svg)](https://codecov.io/github/fecgov/openFEC)\n\n**Master**\n[![Test Coverage](https://img.shields.io/codecov/c/github/fecgov/openFEC/master.svg)](https://codecov.io/github/fecgov/openFEC)\n\n**package.json**\n[![Known Vulnerabilities](https://snyk.io/test/github/fecgov/openFEC/badge.svg?targetFile=package.json)](https://snyk.io/test/github/fecgov/openFEC?targetFile=package.json)\n**requirements.txt**\n[![Known Vulnerabilities](https://snyk.io/test/github/fecgov/openFEC/badge.svg?targetFile=requirements.txt)](https://snyk.io/test/github/fecgov/openFEC?targetFile=requirements.txt)\n**flyway**\n[![Known Vulnerabilities](https://snyk.io/test/github/fecgov/openFEC/badge.svg?targetFile=data/flyway/build.gradle)](https://snyk.io/test/github/fecgov/openfec?targetFile=data/flyway/build.gradle)\n\n## About this project\n\nThe Federal Election Commission (FEC) releases information to the public about money that's raised and spent in federal elections — that's elections for US President, Senate, and House of Representatives.\n\nAre you interested in seeing how much money a candidate raised? Or spent? How much debt they took on? Who contributed to their campaign? The FEC is the authoritative source for that information.\n\nThe new FEC.gov aims to make campaign finance information more accessible (and understandable) to all users.\n\n## This repository, [openFEC](https://github.com/fecgov/openfec), is home to the FEC’s API\n\nAll FEC repositories:\n\n- [FEC](https://github.com/fecgov/fec): a general discussion forum. We [compile feedback](https://github.com/fecgiv/fec/issues) from the FEC.gov feedback widget here, and this is the best place to submit general feedback.\n- [openFEC](https://github.com/fecgov/openfec): the first RESTful API for the Federal Election Commission\n- [fec-cms](https://github.com/",
    "url": "https://github.com/fecgov/openFEC",
    "last_updated": "2025-08-30T15:11:09+00:00"
  },
  {
    "full_name": "uber/prototool",
    "name": "prototool",
    "description": "Your Swiss Army Knife for Protocol Buffers",
    "language": "Go",
    "topics": [
      "protobuf",
      "protocol-buffers",
      "protoc",
      "grpc",
      "grpc-go",
      "proto3"
    ],
    "readme": "# Prototool\n\n[![MIT License][mit-img]][mit] [![GitHub Release][release-img]][release] [![Build Status][ci-img]][ci] [![Coverage Status][cov-img]][cov] [![Docker Image][docker-img]][docker] [![Homebrew Package][homebrew-img]][homebrew] [![AUR Package][aur-img]][aur]\n\n**Update:  We recommend checking out [Buf](https://github.com/bufbuild/buf), which is under active development.  There are a ton of docs for getting started, including for [migration from Prototool](https://buf.build/docs/migration-prototool).**\n\n\n[Protobuf](https://developers.google.com/protocol-buffers) is one of the best interface description\nlanguages out there - it's widely adopted, and after over 15 years of use, it's practically\nbulletproof. However, working with Protobuf and maintaining consistency across your Protobuf files\ncan be a pain - `protoc`, while being a tool that has stood the test of time, is non-trivial to\nuse, and the Protobuf community has not developed common standards with regards to stub generation.\nPrototool aims to solve this by making working with Protobuf much simpler.\n\nPrototool lets you:\n\n- Handle installation of `protoc` and the import of all of the Well-Known Types behind the scenes\n  in a platform-independent manner.\n- Standardize building of your Protobuf files with a common [configuration](#configuration).\n- [Lint](#prototool-lint) your Protobuf files with common linting rules according to\n  [Google' Style Guide](https://developers.google.com/protocol-buffers/docs/style),\n  [Uber's V1 Style Guide](../etc/style/uber1/uber1.proto),\n  [Uber's V2 Style Guide](../style/README.md), or your own set of configured lint rules.\n- [Format](#prototool-format) your Protobuf files in a consistent manner.\n- [Create](#prototool-create) Protobuf files from a template that passes lint, taking care of\n  package naming for you.\n- [Generate](#prototool-generate) stubs using any plugin based on a simple configuration file,\n  including handling imports of all the Well-Known Types.\n- Call [gR",
    "url": "https://github.com/uber/prototool",
    "last_updated": "2025-08-29T04:42:12+00:00"
  },
  {
    "full_name": "stanfordnlp/CoreNLP",
    "name": "CoreNLP",
    "description": "CoreNLP: A Java suite of core NLP tools for tokenization, sentence segmentation, NER, parsing, coreference, sentiment analysis, etc.",
    "language": "Java",
    "topics": [
      "natural-language-processing",
      "nlp",
      "nlp-parsing",
      "named-entity-recognition",
      "stanford-nlp"
    ],
    "readme": "# Stanford CoreNLP\n\n[![Run Tests](https://github.com/stanfordnlp/CoreNLP/actions/workflows/run-tests.yaml/badge.svg)](https://github.com/stanfordnlp/CoreNLP/actions/workflows/run-tests.yaml)\n[![Maven Central](https://img.shields.io/maven-central/v/edu.stanford.nlp/stanford-corenlp.svg)](https://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp)\n[![Twitter](https://img.shields.io/twitter/follow/stanfordnlp.svg?style=social&label=Follow)](https://twitter.com/stanfordnlp/)\n\n[Stanford CoreNLP](http://stanfordnlp.github.io/CoreNLP/) provides a set of natural language analysis tools written in Java. It can take raw human language text input and give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize and interpret dates, times, and numeric quantities, mark up the structure of sentences in terms of syntactic phrases or dependencies, and indicate which noun phrases refer to the same entities. It was originally developed for English, but now also provides varying levels of support for (Modern Standard) Arabic, (mainland) Chinese, French, German, Hungarian, Italian, and Spanish. Stanford CoreNLP is an integrated framework, which makes it very easy to apply a bunch of language analysis tools to a piece of text. Starting from plain text, you can run all the tools with just two lines of code. Its analyses provide the foundational building blocks for higher-level and domain-specific text understanding applications. Stanford CoreNLP is a set of stable and well-tested natural language processing tools, widely used by various groups in academia, industry, and government. The tools variously use rule-based, probabilistic machine learning, and deep learning components.\n\nThe Stanford CoreNLP code is written in Java and licensed under the GNU General Public License (v2 or later). Note that this is the full GPL, which allows many free uses, but not its use in proprietary software that you distribute to others.\n\n### Build",
    "url": "https://github.com/stanfordnlp/CoreNLP",
    "last_updated": "2025-09-01T14:50:22+00:00"
  },
  {
    "full_name": "juba/questionr",
    "name": "questionr",
    "description": "R package to make surveys processing easier",
    "language": "R",
    "topics": [
      "r",
      "cran"
    ],
    "readme": "# questionr\n\nR package to make surveys processing easier\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version-ago/questionr)](https://cran.r-project.org/package=questionr)\n[![DOI](https://zenodo.org/badge/8097494.svg)](https://zenodo.org/badge/latestdoi/8097494)\n![CRAN Downloads](https://cranlogs.r-pkg.org/badges/last-month/questionr)\n[![R build status](https://github.com/juba/questionr/workflows/R-CMD-check/badge.svg)](https://github.com/juba/questionr/actions?query=workflow%3AR-CMD-check)\n\n## Installation\n\nFrom CRAN :\n\n```r\ninstall.packages(\"questionr\")\n```\n\nLatest version from GitHub :\n\n```r\nremotes::install_github(\"juba/questionr\")\n```\n\n## Documentation\n\nQuite sparse for the moment, but please refer to the `pkgdown` generated website :\n\n<https://juba.github.io/questionr/>\n\nYou will find there a [function reference](https://juba.github.io/questionr/reference/index.html) organized by topics, and a [vignette on recoding addins](https://juba.github.io/questionr/articles/recoding_addins.html).\n",
    "url": "https://github.com/juba/questionr",
    "last_updated": "2025-06-10T14:01:21+00:00"
  },
  {
    "full_name": "hu8813/tester_push_swap",
    "name": "tester_push_swap",
    "description": "A simple Push_swap tester for testing memory leaks/errors and error handling for the 42 school project pushswap",
    "language": "Python",
    "topics": [
      "pushswap",
      "push-swap",
      "push-swap42",
      "pushswaptester",
      "push-swap-tester",
      "push-swap-42",
      "tester-push-swap",
      "pushswap-42",
      "pushswap-tester",
      "pushswap42"
    ],
    "readme": "# Push Swap Tester for 42 School Project\n\n## About\nA simple Push_swap tester for testing memory leaks, errors, and error handling in the 42 school project push_swap. This Python script is designed for Linux and macOS systems.\n\n## Usage\nTo use the Push Swap Tester script, follow these steps:\n\n1. Ensure that you have the push_swap and checker programs (for Linux or macOS) in the same directory.\n2. Run the following command in the terminal:\n\n    ```sh\n    curl https://raw.githubusercontent.com/hu8813/tester_push_swap/main/pstester.py | python3 -\n    ```\n\n    This command will download and execute the `pstester.py` script from the GitHub repository.\n\n## Screenshot of a Test Result\n\n![Push_swap tester screenshot](screenshot.png)\n\n## Topics\npush-swap, pushswap, push-swap-tester, pushswap-tester, push-swap42, pushswap-42, pushswaptester, push-swap-42, tester-push-swap, pushswap42\n\n## Keywords\npush-swap, pushswap, push_swap, 42 school project, memory leak tester, error handling tester, push-swap-tester, pushswap-tester, pushswap42, pushswap-42, Linux, macOS, Python script, tester-push-swap, pushswap tester\n",
    "url": "https://github.com/hu8813/tester_push_swap",
    "last_updated": "2025-09-02T02:31:29+00:00"
  },
  {
    "full_name": "rsheets/rexcel",
    "name": "rexcel",
    "description": "Extracts spreadsheet data from Excel workbooks and puts into linen format",
    "language": "R",
    "topics": [],
    "readme": "# rexcel\n[![Build Status](https://travis-ci.org/rsheets/rexcel.svg?branch=master)](https://travis-ci.org/rsheets/rexcel)\n\n**Warning: This project is an experiment; do not use for anything other than amusement/frustration purposes.**\n\n## Design\n\nThis package implements a **very slow**, but thorough, Excel (xlsx) reader.  If you have a rectangular region of cells to read you will be better off with another Excel reading package such as: [`readxl`](http://cran.r-project.org/package=readxl),\n[`openxlsx`](http://cran.r-project.org/package=openxlsx),\n[`XLConnect`](http://cran.r-project.org/package=XLConnect),\n[`xlsx`](http://cran.r-project.org/package=xlsx),\n[`gdata`](http://cran.r-project.org/package=gdata), [`RODBC`](http://cran.r-project.org/package=RODBC), or possibly even\n[`excel.link`](http://cran.r-project.org/package=excel.link), [`WriteXLS`](http://cran.r-project.org/package=WriteXLS), [`table1xlsx`](http://cran.r-project.org/package=table1xlsx), [`tablaxlsx`](http://cran.r-project.org/package=tablaxlsx) (not clear how current these last 4 are). Mango Solutions has a nice review article, [R: the Excel Connection](http://www.mango-solutions.com/wp/2015/05/r-the-excel-connection/), in which they compare several of the above packages, with a special emphasis on those that can both read and write Excel files (XLConnect, xlsx, openxlsx, excel.link).\n\nCompared with the above packages, `rexcel` tries to read all the data from an Excel sheet using [`linen`](https://github.com/rsheets/linen) as an intermediate representation in R. The eventual goal is to provide a common receptacle for detailed spreadsheet information from both Excel and Google Sheets.  Rather than trying to create a single data.frame in one shot, it allows access to data, formulae and formatting information.  Excel type information is preserved, especially for heterogeneous columns.  It has no non-R dependencies (e.g. on Perl or Java) and should run on any platform regardless of whether Excel is installe",
    "url": "https://github.com/rsheets/rexcel",
    "last_updated": "2025-03-21T14:39:29+00:00"
  },
  {
    "full_name": "wireservice/proof",
    "name": "proof",
    "description": "A Python library for creating fast, repeatable and self-documenting data analysis pipelines.",
    "language": "Python",
    "topics": [],
    "readme": ".. image:: https://github.com/wireservice/proof/workflows/CI/badge.svg\n    :target: https://github.com/wireservice/proof/actions\n    :alt: Build status\n\n.. image:: https://coveralls.io/repos/wireservice/proof/badge.svg?branch=master\n    :target: https://coveralls.io/r/wireservice/proof\n    :alt: Coverage status\n\n.. image:: https://img.shields.io/pypi/dw/proof.svg\n    :target: https://pypi.python.org/pypi/proof\n    :alt: PyPI downloads\n\n.. image:: https://img.shields.io/pypi/v/proof.svg\n    :target: https://pypi.python.org/pypi/proof\n    :alt: Version\n\n.. image:: https://img.shields.io/pypi/l/proof.svg\n    :target: https://pypi.python.org/pypi/proof\n    :alt: License\n\n.. image:: https://img.shields.io/pypi/pyversions/proof.svg\n    :target: https://pypi.python.org/pypi/proof\n    :alt: Support Python versions\n\nproof is a Python library for creating optimized, repeatable and self-documenting data analysis pipelines.\n\nproof was designed to be used with the agate data analysis library, but can be used with numpy, pandas or any other method of processing data.\n\nImportant links:\n\n* Documentation:    https://proof.rtfd.io\n* Repository:       https://github.com/wireservice/proof\n* Issues:           https://github.com/wireservice/proof/issues\n",
    "url": "https://github.com/wireservice/proof",
    "last_updated": "2025-09-01T15:37:54+00:00"
  },
  {
    "full_name": "newsdev/nyt-clerk",
    "name": "nyt-clerk",
    "description": "A set of Python modules for downloading, parsing, and outputting data related to the Supreme Court.",
    "language": "Python",
    "topics": [
      "scotus-data"
    ],
    "readme": ".. figure:: https://cloud.githubusercontent.com/assets/109988/9503675/7a4bdfee-4c06-11e5-8619-e8f85ccb49f2.png\n   :alt:\n\nUsage\n=====\n\n::\n\n    pip install -e git@github.com:newsdev/nyt-clerk.git#egg=clerk\n    clerk justices\n\nData\n====\n\n-  ``clerk cases``: SCDB fields about decided merits\n   cases, excluding the justice and vote details.\n-  ``clerk votes``: SCDB fields about a single justice's\n   vote in a single case.\n-  ``clerk courts``: Data about the ideology for\n   each Court term.\n-  ``clerk justices``: Data about the ideology and\n   qualifications for each Justice before they were confirmed.\n-  ``clerk terms``: Data about the ideology of\n   each individual Justice during each Court term.\n\nSCDB Data\n---------\n\nSCDB data includes cases from 1791 term to the 2015 term. Many fields\nneed to be mapped to their full values. The SCDB `maintains an online\ncodebook <http://scdb.wustl.edu/documentation.php>`__ with these maps.\n\n\nIdeology / Qualification Scores\n-------------------------------\n\nMartin-Quinn scores measure the relative ideology of a Justice or a\nSupreme Court term to the median Justice. Andrew Martin and Kevin Quinn\n`wrote an excellent\npaper <http://mqscores.berkeley.edu/media/pa02.pdf>`__ about the method.\n\nSegal-Cover scores measure the ideology and qualification of an\nindividual Justice *before* their appointment to the Court. Jeffrey\nSegal `wrote a\nsummary <http://www.stonybrook.edu/commcms/polisci/jsegal/QualTable.pdf>`__\nand shows the raw data as a table (warning: PDF).\n",
    "url": "https://github.com/newsdev/nyt-clerk",
    "last_updated": "2024-12-31T17:24:03+00:00"
  },
  {
    "full_name": "scoopr/wtf8",
    "name": "wtf8",
    "description": "Small utility functions for handling utf8",
    "language": "C++",
    "topics": [],
    "readme": "\n  wtf8\n  Utility functions for utf8 handling.\n  \n  \n  \n  wtf8_decode - decode a codepoint from utf8 bytes\n  wtf8_encode - encode a codepoint to utf8 bytes\n  wtf8_strlen - count the number of codepoints in a utf8 string\n  wtf8_strnlen - count the number of codepoints in a utf8 string\n  wtf8_is_initial_byte - is byte a valid first byte of a sequence\n  wtf8_is_continuation_byte - is byte a valid continuation byte of a sequence\n\n  The used utf8 decoder is based on the work of Bjoern Hoehrmann.\n  Copyright (c) 2008-2010 Bjoern Hoehrmann <bjoern@hoehrmann.de>\n  See http://bjoern.hoehrmann.de/utf-8/decoder/dfa/ for details.\n\n  All other code is public domain.\n\n\n\n",
    "url": "https://github.com/scoopr/wtf8",
    "last_updated": "2025-05-16T15:32:36+00:00"
  },
  {
    "full_name": "ftherrien/VaspGibbs",
    "name": "VaspGibbs",
    "description": "A simple way to calculate Gibbs free energy from Vasp calculations",
    "language": "Python",
    "topics": [
      "abinitio-simulations",
      "vasp",
      "gibbs-energy",
      "thermodynamics"
    ],
    "readme": "<h1 align=\"center\">\n<img src=\"https://raw.githubusercontent.com/ftherrien/VaspGibbs/3e20b2fadb4c5cdf328a1a3194374cf6318bf84a/docs/VGlogo.svg\" height=\"130\">\n\nVaspGibbs\n</h1>\n<br>\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7874413.svg)](https://doi.org/10.5281/zenodo.7874413)\n[![PyPI](https://img.shields.io/pypi/v/vaspgibbs)](https://pypi.org/project/VaspGibbs/)\n\n## Installation\n\n```\npip install VaspGibbs\n```\n## Usage\n\nIn a folder with a finished vasp calculation, run\n```\nvasp_gibbs\n```\n\n`vasp_gibbs` will rerun Vasp to obtain vibration modes and output the gibbs free energy of your system.\n\nUse `-o` (only) or `-t`(top) to specify a set of atoms for which to calculate vibration modes. Examples:\n\n * `-o C O` would only compute vibration modes associated with C and O keeping all other atoms fixed.\n * `-o 0 2 5` would compute vibration modes associated with the first, third and sixth atoms in the POSCAR keeping all other atoms fixed. (Counts from 0)\n * `-t 10` would compute vibration modes associated with the first 10 atoms starting from the top of the unit cell along the c axis.\n\nThis can be useful when computing free energy differences between systems where one part of the system does not change, e.g. adsorption free energies.\n\nTo run vasp in parallel call:\n```\nvasp_gibbs -n [number of proc] -m [mpi executable] -v [vasp executable]\n```\n\nBy default `srun` and `vasp_std` are used.\n\nVaspGibbs will automatically compute the moment of inertia and symmetry of your molecule and compute rotational and translational contributions if you specify that the system is a molecule with the `-m` flag.\n\nThe temperature and pressure can be set using the `-T` and `-P` flags.\n\n### Output\n\nAll outputs can be found in the VaspGibbs.md file. It contains the following information:\n\n#### Rotational properties\n|     Property     |          Value          |\n| :--------------: | :---------------------: |\n|      Sigma       |            x            |\n|   **P. axes**    |                  ",
    "url": "https://github.com/ftherrien/VaspGibbs",
    "last_updated": "2025-06-23T10:44:58+00:00"
  },
  {
    "full_name": "google-research/fixmatch",
    "name": "fixmatch",
    "description": "A simple method to perform semi-supervised learning with limited data.",
    "language": "Python",
    "topics": [],
    "readme": "# FixMatch\n\nCode for the paper: \"[FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://arxiv.org/abs/2001.07685)\" by \nKihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.\n\nThis is not an officially supported Google product.\n\n![FixMatch diagram](media/FixMatch%20diagram.png)\n\n## Setup\n\n**Important**: `ML_DATA` is a shell environment variable that should point to the location where the datasets are installed. See the *Install datasets* section for more details.\n\n### Install dependencies\n\n```bash\nsudo apt install python3-dev python3-virtualenv python3-tk imagemagick\nvirtualenv -p python3 --system-site-packages env3\n. env3/bin/activate\npip install -r requirements.txt\n```\n\n### Install datasets\n\n```bash\nexport ML_DATA=\"path to where you want the datasets saved\"\nexport PYTHONPATH=$PYTHONPATH:\"path to the FixMatch\"\n\n# Download datasets\nCUDA_VISIBLE_DEVICES= ./scripts/create_datasets.py\ncp $ML_DATA/svhn-test.tfrecord $ML_DATA/svhn_noextra-test.tfrecord\n\n# Create unlabeled datasets\nCUDA_VISIBLE_DEVICES= scripts/create_unlabeled.py $ML_DATA/SSL2/svhn $ML_DATA/svhn-train.tfrecord $ML_DATA/svhn-extra.tfrecord &\nCUDA_VISIBLE_DEVICES= scripts/create_unlabeled.py $ML_DATA/SSL2/svhn_noextra $ML_DATA/svhn-train.tfrecord &\nCUDA_VISIBLE_DEVICES= scripts/create_unlabeled.py $ML_DATA/SSL2/cifar10 $ML_DATA/cifar10-train.tfrecord &\nCUDA_VISIBLE_DEVICES= scripts/create_unlabeled.py $ML_DATA/SSL2/cifar100 $ML_DATA/cifar100-train.tfrecord &\nCUDA_VISIBLE_DEVICES= scripts/create_unlabeled.py $ML_DATA/SSL2/stl10 $ML_DATA/stl10-train.tfrecord $ML_DATA/stl10-unlabeled.tfrecord &\nwait\n\n# Create semi-supervised subsets\nfor seed in 0 1 2 3 4 5; do\n    for size in 10 20 30 40 100 250 1000 4000; do\n        CUDA_VISIBLE_DEVICES= scripts/create_split.py --seed=$seed --size=$size $ML_DATA/SSL2/svhn $ML_DATA/svhn-train.tfrecord $ML_DATA/svhn-extra.tfrecord &\n        CUDA_VISIBLE_DEVICES= scr",
    "url": "https://github.com/google-research/fixmatch",
    "last_updated": "2025-08-25T06:41:18+00:00"
  },
  {
    "full_name": "binux/pyspider",
    "name": "pyspider",
    "description": "A Powerful Spider(Web Crawler) System in Python.",
    "language": "Python",
    "topics": [
      "python",
      "crawler"
    ],
    "readme": "pyspider [![Build Status]][Travis CI] [![Coverage Status]][Coverage]\n========\n\nA Powerful Spider(Web Crawler) System in Python.\n\n- Write script in Python\n- Powerful WebUI with script editor, task monitor, project manager and result viewer\n- [MySQL](https://www.mysql.com/), [MongoDB](https://www.mongodb.org/), [Redis](http://redis.io/), [SQLite](https://www.sqlite.org/), [Elasticsearch](https://www.elastic.co/products/elasticsearch); [PostgreSQL](http://www.postgresql.org/) with [SQLAlchemy](http://www.sqlalchemy.org/) as database backend\n- [RabbitMQ](http://www.rabbitmq.com/), [Redis](http://redis.io/) and [Kombu](http://kombu.readthedocs.org/) as message queue\n- Task priority, retry, periodical, recrawl by age, etc...\n- Distributed architecture, Crawl Javascript pages, Python 2.{6,7}, 3.{3,4,5,6} support, etc...\n\nTutorial: [http://docs.pyspider.org/en/latest/tutorial/](http://docs.pyspider.org/en/latest/tutorial/)  \nDocumentation: [http://docs.pyspider.org/](http://docs.pyspider.org/)  \nRelease notes: [https://github.com/binux/pyspider/releases](https://github.com/binux/pyspider/releases)  \n\nSample Code \n-----------\n\n```python\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl('http://scrapy.org/', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc('a[href^=\"http\"]').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc('title').text(),\n        }\n```\n\n\nInstallation\n------------\n\n* `pip install pyspider`\n* run command `pyspider`, visit [http://localhost:5000/](http://localhost:5000/)\n\n**WARNING:** WebUI is open to the public by default, it can be used to execute any command which may harm your system. Please use it in an internal ",
    "url": "https://github.com/binux/pyspider",
    "last_updated": "2025-09-02T05:19:32+00:00"
  },
  {
    "full_name": "dannyneil/public_plstm",
    "name": "public_plstm",
    "description": "Phased LSTM",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Phased LSTM\nThis is the official repository of \"Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences,\" presented as an oral presentation at NIPS 2016, by Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu.\n\n## Rule of Thumb\nIn general, **if you are using ~1000 timesteps or more in your input sequence, you can benefit from PLSTM.**\n\nIf you're only answering bAbI tasks or doing negative log-likelihood on some paragraph of text, you're unlikely to see improvement from this model.  However, for long sequences (e.g., whole-text summarization), or sequences which are fusing input from multiple sensors with different timing (e.g., one going at 3 Hz and the other at 25 Hz), this model is both natural and efficient.\n\nMaking it work well for speech and NLP is still experimental and ongoing work.  If this is of interest to you, let me know and I can give you an update.\n\n## Now available in TensorFlow and Keras!\n * **Up-to-date PyTorch** implementation available: [https://github.com/dannyneil/pytorch_plstm](https://github.com/dannyneil/pytorch_plstm)\n * Official Tensorflow implementation now available from Andrea Gesmundo at Google [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1720-L1861](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1720-L1861)\n * Tensorflow implementation of the original experiment by Enea Ceolini can be found here: [https://github.com/Enny1991/PLSTM](https://github.com/Enny1991/PLSTM)\n * Tensorflow implementation with MNIST example from Philippe Remy: [https://github.com/philipperemy/tensorflow-phased-lstm](https://github.com/philipperemy/tensorflow-phased-lstm)\n * Keras implementation by Francesco Ferroni: [https://github.com/fferroni/PhasedLSTM-Keras](https://github.com/fferroni/PhasedLSTM-Keras)\n\n# Freq Task 1\nTo run the first task, run the shell script [a_freq_task.sh](/a_freq_task.sh).  It should load the first t",
    "url": "https://github.com/dannyneil/public_plstm",
    "last_updated": "2025-04-23T16:03:02+00:00"
  },
  {
    "full_name": "datamade/usaddress",
    "name": "usaddress",
    "description": ":us: a python library for parsing unstructured United States address strings into address components",
    "language": "Python",
    "topics": [
      "python-library",
      "address",
      "nlp",
      "parserator",
      "python",
      "address-parser",
      "natural-language-processing",
      "machine-learning",
      "conditional-random-fields",
      "crf"
    ],
    "readme": "usaddress\n=================\nusaddress is a Python library for parsing unstructured United States address strings into address components, using advanced NLP methods.\n\n**What this can do:** Using a probabilistic model, it makes (very educated) guesses in identifying address components, even in tricky cases where rule-based parsers typically break down.\n\n**What this cannot do:** It cannot identify address components with perfect accuracy, nor can it verify that a given address is correct/valid.\n\nIt also does not normalize the address. However, [this library built on top of usaddress does](https://github.com/GreenBuildingRegistry/usaddress-scourgify).\n\n\n## Tools built with usaddress\n\n### [Parserator API](https://parserator.datamade.us/)\nA RESTful API built on top of usaddress for programmers who don't use python. Requires an API key and the first 1,000 parses are free.\n\n### [Parserator Google Sheets App](https://workspace.google.com/u/0/marketplace/app/parserator_parse_and_split_addresses/945974620840)\nParserator: Parse and Split Addresses allows you to easily split addresses into separate columns by street, city, state, zipcode and more right in Google Sheets.\n\n## How to use the usaddress python library\n\n1. Install usaddress with [pip](https://pip.readthedocs.io/en/latest/quickstart.html), a tool for installing and managing python packages ([beginner's guide here](http://www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/)).\n\n  In the terminal,\n  \n  ```bash\n  pip install usaddress\n  ```\n2. Parse some addresses!\n\n  ![usaddress](https://cloud.githubusercontent.com/assets/1406537/7869001/65c6ae62-0545-11e5-8b65-5d9e71dface5.gif)\n\n  Note that `parse` and `tag` are different methods:\n  ```python\n  import usaddress\n  addr='123 Main St. Suite 100 Chicago, IL'\n  \n  # The parse method will split your address string into components, and label each component.\n  # expected output: [(u'123', 'AddressNumber'), (u'Main', 'StreetName'), (u'St.', 'StreetNamePostType'), (u'",
    "url": "https://github.com/datamade/usaddress",
    "last_updated": "2025-08-30T19:02:06+00:00"
  },
  {
    "full_name": "soodoku/anes_inc_pid_cor",
    "name": "anes_inc_pid_cor",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "### Cor(Income, PID) Over Time\n\n<img src = \"income_quintile_prop_rep_over_time_white_anes.png\" width = 500px>\n\n<img src = \"income_quintile_prop_rep_over_time_white_south_anes.png\" width = 500px>\n",
    "url": "https://github.com/soodoku/anes_inc_pid_cor",
    "last_updated": "2024-01-26T07:01:12+00:00"
  },
  {
    "full_name": "eytan/www-15-tutorial",
    "name": "www-15-tutorial",
    "description": "Materials for the WWW 2015 tutorial on online experiments for computational social science",
    "language": "HTML",
    "topics": [],
    "readme": "WWW 2015 Tutorial\n=================\n\nWelcome to the github repository for the WWW 2015 tutorial on online experiments for computational social science!\n\n[You can view all slides and materials on the web.](https://eytan.github.io/www-15-tutorial)\n\n\n### Repository contents\n\n- `0-estimation-and-power.ipynb`: IPython notebook discussing inference for experiments and showing how to do a power analysis via simulation.\n\n- `1-planout-intro.ipynb`: IPython notebook containing an introduction to PlanOut and examples of experimental designs in PlanOut.\n\n- `2-making-your-own-data.ipynb`: How to extract a data frame from logs generated through PlanOut.\n\n- `4-analyzing-experiments.ipynb`: How to analyze the experimental data we produced, including scaling analysis to big data.\n\n- `webapp/`: Demo experiment which will work with MTurk.\n\n- `data/`: Data extracted from logfiles used in selective exposure experiment.\n\n- `css_stats.R`: Functions for working with weighted data and multi-way bootstrapping\n\n### Description of tutorial\nThis tutorial teaches attendees how to design, plan, implement, and analyze online experiments. First, we review basic concepts in causal inference and motivate the need for experiments. Then we will discuss basic statistical tools to help plan experiments: exploratory analysis, power calculations, and the use of simulation in R. We then discuss statistical methods to estimate causal quantities of interest and construct appropriate confidence intervals. Particular attention will be given to scalable methods suitable for “big data”, including working with weighted data and clustered bootstrapping. We then discuss how to design and implement online experiments using PlanOut, an open-source toolkit for advanced online experimentation used at Facebook. We will show how basic “A/B tests”, within-subjects designs, as well as more sophisticated experiments can be implemented. We demonstrate how experimental designs from social computing literature can be implemente",
    "url": "https://github.com/eytan/www-15-tutorial",
    "last_updated": "2025-06-11T06:32:31+00:00"
  },
  {
    "full_name": "TaddyLab/deepir",
    "name": "deepir",
    "description": "deep inverse regression",
    "language": "TeX",
    "topics": [],
    "readme": "# deep inverse regression\n\n### or: Document Classification by Inversion of Distributed Language Representations [(ACL 2015)](http://arxiv.org/pdf/1504.07295v3.pdf)\n\nUsing unsupervised deep learning within sub-groups as the input for Bayesian discrimination. \n\nEverything in here is built around the [gensim](https://radimrehurek.com/gensim/) library for python.  See the demo at [deepir.ipynb](https://github.com/TaddyLab/gensim/blob/deepir/docs/notebooks/deepir.ipynb).\n",
    "url": "https://github.com/TaddyLab/deepir",
    "last_updated": "2023-12-10T04:31:26+00:00"
  },
  {
    "full_name": "Robinlovelace/Creating-maps-in-R",
    "name": "Creating-maps-in-R",
    "description": "Introductory tutorial on graphical display of geographical information in R.",
    "language": "TeX",
    "topics": [],
    "readme": "Introduction to visualising spatial data in R\n================\n\nPreface\n-------\n\nThis tutorial is an introduction to visualising and analysing spatial data in R based on the **sp** class system. For a guide to the more recent **sf** package check out [Chapter 2](http://robinlovelace.net/geocompr/spatial-class.html) of the in-development book [Geocomputation with R](https://github.com/Robinlovelace/geocompr), the source code of which can be found at [github.com/Robinlovelace/geocompr](https://github.com/Robinlovelace/geocompr).\n\nAlthough **sf** supersedes **sp** in many ways, there is still merit in learning the content in this tutorial, which teaches principles that will be useful regardless of software. Specifically this tutorial focusses on map-making with R's 'base' graphics and various dedicated map-making packages for R including **tmap** and **leaflet**. It aims to teach the basics of using R as a fast, user-friendly and extremely powerful command-line Geographic Information System (GIS).\n\nBy the end of the tutorial you should have the confidence and skills needed to convert a diverse range of geographical and non-geographical datasets into meaningful analyses and visualisations. Using data and code provided in this repository all of the results are reproducible, culminating in publication-quality maps such as the faceted map of London's population below:\n\n![image-lnd](https://raw.githubusercontent.com/Robinlovelace/Creating-maps-in-R/master/figure/facet_london.png)\n\nThe course will even show you how to make your maps animated:\n\n![](https://raw.githubusercontent.com/Robinlovelace/Creating-maps-in-R/master/figure/lnd-animated.gif)\n\nAn up-to-date pdf version of this tutorial is maintained for teaching purposes in the file [intro-spatial-rl.pdf](https://github.com/Robinlovelace/Creating-maps-in-R/blob/master/intro-spatial-rl.pdf).\n\nIf you have any feedback on this tutorial please let us know via email or via this repository. Contibutions to the `.Rmd` file ([READ",
    "url": "https://github.com/Robinlovelace/Creating-maps-in-R",
    "last_updated": "2025-09-02T09:39:11+00:00"
  },
  {
    "full_name": "cran/quantable",
    "name": "quantable",
    "description": ":exclamation: This is a read-only mirror of the CRAN R package repository.  quantable — Streamline Descriptive Analysis of Quantitative Data Matrices. Homepage: https://github.com/protViz/quantable  Report bugs for this package: https://github.com/protViz/quantable/issues",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/cran/quantable",
    "last_updated": "2021-03-11T01:06:10+00:00"
  },
  {
    "full_name": "sctyner/ggnet-paper",
    "name": "ggnet-paper",
    "description": "",
    "language": "TeX",
    "topics": [],
    "readme": "# ggnet-paper\n\nThis is a paper by Heike Hofmann ([@heike](https://github.com/heike)), Sam Tyner ([@sctyner](https://github.com/sctyner)) and François Briatte ([@briatte](https://github.com/briatte)). Corresponding repos are [sctyner/geomnet](https://github.com/sctyner/geomnet), [briatte/ggnet](https://github.com/briatte/ggnet) and [briatte/ggnetwork](https://github.com/briatte/ggnetwork).\nCreate the paper by compiling `RJwrapper.Rnw` to PDF; `tyner-briatte-hofmann.Rnw` will be compiled as a child.\n\nThe `runtimes` folder contains a script to test the average runtime of each network plotting function against random networks of varying sizes, as well as the results of this script on a mid-2012 MacBook Air (1.8 GHz Intel Core i5, 8 GB 1600 MHz DDR3 SDRAM).\n",
    "url": "https://github.com/sctyner/ggnet-paper",
    "last_updated": "2016-10-24T15:46:01+00:00"
  },
  {
    "full_name": "pablobarbera/data-science-workshop",
    "name": "data-science-workshop",
    "description": "NYU Shortcourse -- \"Data Science and Social Science\" materials",
    "language": "HTML",
    "topics": [],
    "readme": "\n# New York University Shortcourse: Data Science and Social Science\n\n## Co-sponsored by \n* [Institute for Education Sciences-funded Predoctoral Interdisciplinary Research Training (IES-PIRT) program](http://steinhardt.nyu.edu/ihdsc/iespirt)\n* [Center for the Promotion of Research Involving Innovative Statistical Methodology (PRIISM)](http://steinhardt.nyu.edu/priism/)\n\n## January 20-22, 2016\n \n## Instructors\n\n* [Pablo Barber&aacute;](http://pablobarbera.com/)\n* [Dan Cervone](http://dcervone.com/)\n\n(with materials prepared by [Alex Hanna](http://alex-hanna.com))\n\n## Teaching Assistants\n\n* Denis Stukal\n* Kevin Munger\n* Peter Crosta\n* Varun D N\n\n## Description\n\nThis is a three-day short course covering key topics at the intersection of Data Science and Social Science. Each day is structured as a series of modules that will combine instruction on data science methods with implementation using real data in R.  his course covers an introduction to the R programming and statistical language, modeling and visualization, automated textual analysis, social network analysis, and web scraping & APIs. \n\n## Setup and Preparation\n\nYou will need to bring a laptop to all sessions of the workshop. You will need [R](https://cran.r-project.org/) and [RStudio](https://www.rstudio.com/) installed. [Follow the instructions here to install both](https://github.com/pablobarbera/data-science-workshop/blob/master/installing_RStudio.pdf).\n\n### Instructions for using course materials on GitHub ###\n\nYou have three options for downloading the course material found on this page:  \n\n1.  You can download the materials by clicking on each link.  \n\n2.  You can \"clone\" repository, using the buttons found to the right side of your browser window as you view this repository.  This is the button labelled \"Clone in Desktop\".  If you do not have a git client installed on your system, you will need to [get one here](https://git-scm.com/download/gui) and also to make sure that [git is installed](https://git-s",
    "url": "https://github.com/pablobarbera/data-science-workshop",
    "last_updated": "2025-07-01T15:14:04+00:00"
  },
  {
    "full_name": "soodoku/extreme_recall",
    "name": "extreme_recall",
    "description": "Extreme Recall: Which Politicians Come to Mind?",
    "language": "Stata",
    "topics": [],
    "readme": "# Extreme Recall: Which Politicians Come to Mind?\n\nUsing data from MTurk, we find that people are likelier to recall prominent politicians. On average, the people recall politicians who are no more extreme than the median member of Congress. For instance, recalled politicians are more extreme than the Senate mean just 55.6\\% of the times. And the average difference to the Senate means is a mere .017. (For comparison, the standard deviation of CF-scores within the parties in the Senate is about .3.) People do recall more extreme politicians earlier but again the bias is substantively not particularly large. However, people tend to *think* that these relatively moderate politicians are ideologically extreme. Thus, the bias may be less in who is covered in the media (except for the bias related to prominence---executive branch, presidency, especially), and more about how they are covered.\n\n* [Manuscript](ms/)\n* [Tables](tabs/)\n* [Figures](figs/)\n",
    "url": "https://github.com/soodoku/extreme_recall",
    "last_updated": "2023-10-13T19:41:04+00:00"
  },
  {
    "full_name": "soodoku/misinformation",
    "name": "misinformation",
    "description": "",
    "language": "Portugol",
    "topics": [],
    "readme": "## Misinformation About Misinformation: Of Headlines and Survey Design\n\nBy some accounts, the American public is awash in misinformation. Polarization, selective exposure, biased processing, and politicians and media outlets that seem ever readier to promote convenient fictions and deny inconvenient facts make it plausible. Yet there is also reason, in the underlying survey items, to suspect the headlines of being considerably overdrawn. Among other things, the questions on which they rest are frequently phrased as matters of opinion (\"As far as you know ...,\" \"Would you say that...”), and frequently lack explicit DK options and more generally invite guessing. We assemble a novel corpus of media polls to estimate the frequency of such “inflationary” features in misinformation items. We then use a series of survey experiments to estimate the effects of such question design features on the proportion of incorrect responses. We show that common features of misinformation items considerably overstate misinformation levels. \n\n### Data\n\n* [Data](data/)\n\n### Scripts\n\n* [Scripts](scripts/)\n\n\n### Manuscript\n\n* [MS](ms/)\n\n### Authors\n\nRobert C. Luskin, Gaurav Sood, Yul Min Park, and Joshua Blank\n",
    "url": "https://github.com/soodoku/misinformation",
    "last_updated": "2023-06-17T23:53:37+00:00"
  },
  {
    "full_name": "rougier/scientific-visualization-book",
    "name": "scientific-visualization-book",
    "description": "An open access book on scientific visualization using python and matplotlib",
    "language": "Python",
    "topics": [
      "python",
      "matplotlib",
      "dataviz",
      "scientific-publications",
      "book",
      "open-access",
      "numpy",
      "plotting"
    ],
    "readme": "## Scientific Visualization: Python + Matplotlib\n**Nicolas P. Rougier, Bordeaux, November 2021.**  \n\n<img src=\"images/book.png\" width=\"25%\" alt=\"Front cover\" align=\"left\"/>\n\nThe Python scientific visualisation landscape is huge. It is composed of a myriad of tools, ranging from the most versatile and widely used down to the more specialised and confidential. Some of these tools are community based while others are developed by companies. Some are made specifically for the web, others are for the desktop only, some deal with 3D and large data, while others target flawless 2D rendering. In this landscape, Matplotlib has a very special place. It is a versatile and powerful library that allows you to design very high quality figures, suitable for scientific publishing. It also offers a simple and intuitive interface as well as an object oriented architecture that allows you to tweak anything within a figure. Finally, it can be used as a regular graphic library in order to design non‐scientific figures. This book is organized into four parts. The first part considers the fundamental principles of the Matplotlib library. This includes reviewing the different parts that constitute a figure, the different coordinate systems, the available scales and projections, and we’ll also introduce a few concepts related to typography and colors. The second part is dedicated to the actual design of a figure. After introducing some simple rules for generating better figures, we’ll then go on to explain the Matplotlib defaults and styling system before diving on into figure layout organization. We’ll then explore the different types of plot available and see how a figure can be ornamented with different elements. The third part is dedicated to more advanced concepts, namely 3D figures, optimization & animation.  The fourth and final part is a collection of showcases.\n\n### Read the book\n\nYou can read the book **[PDF](https://hal.inria.fr/hal-03427242/document)** (95Mo, preferred site) tha",
    "url": "https://github.com/rougier/scientific-visualization-book",
    "last_updated": "2025-09-02T03:41:23+00:00"
  },
  {
    "full_name": "espnet/espnet",
    "name": "espnet",
    "description": "End-to-End Speech Processing Toolkit",
    "language": "Python",
    "topics": [
      "deep-learning",
      "end-to-end",
      "chainer",
      "pytorch",
      "kaldi",
      "speech-recognition",
      "speech-synthesis",
      "speech-translation",
      "machine-translation",
      "voice-conversion",
      "speech-enhancement",
      "speech-separation",
      "singing-voice-synthesis",
      "speaker-diarization",
      "spoken-language-understanding",
      "text-to-speech"
    ],
    "readme": "<div align=\"left\"><img src=\"doc/image/espnet_logo1.png\" width=\"550\"/></div>\n\n# ESPnet: end-to-end speech processing toolkit\n\n|system/pytorch ver.|2.5.1|2.6.0|2.7.1|2.8.0|\n| :---- | :---: | :---: | :---: | :---: |\n|ubuntu/python3.10/pip|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|\n|ubuntu/python3.11/pip|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|\n|ubuntu/python3.10/conda||[![ci on debian12](https://github.com/espnet/espnet/actions/workflows/ci_on_debian12.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_debian12.yml?query=branch%3Amaster)||\n|debian12/pyt",
    "url": "https://github.com/espnet/espnet",
    "last_updated": "2025-09-02T07:55:44+00:00"
  },
  {
    "full_name": "alexa/alexa-skills-kit-sdk-for-python",
    "name": "alexa-skills-kit-sdk-for-python",
    "description": "The Alexa Skills Kit SDK for Python helps you get a skill up and running quickly, letting you focus on skill logic instead of boilerplate code. ",
    "language": "Python",
    "topics": [
      "skills",
      "sdk",
      "alexa-skills-kit",
      "python",
      "alexa-sdk",
      "alexa",
      "software-development-kit",
      "alexa-skill",
      "alexa-skill-development",
      "sdk-python"
    ],
    "readme": ".. raw:: html\n\n    <embed>\n        <p align=\"center\">\n          <img src=\"https://m.media-amazon.com/images/G/01/mobile-apps/dex/avs/docs/ux/branding/mark1._TTH_.png\">\n          <br/>\n          <h1 align=\"center\">Alexa Skills Kit SDK for Python</h1>\n          <p align=\"center\">\n            <a href=\"https://github.com/alexa/alexa-skills-kit-sdk-for-python/actions?query=workflow%3A%22Tox+tests+on+SDK+packages%22\"><img src=\"https://github.com/alexa/alexa-skills-kit-sdk-for-python/workflows/Tox%20tests%20on%20SDK%20packages/badge.svg?style=flat\"></a>\n            <a href=\"https://github.com/alexa/alexa-skills-kit-sdk-for-python/blob/master/LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\"></a>\n            <a href=\"https://pypi.python.org/pypi/ask-sdk/\"><img src=\"http://img.shields.io/pypi/v/ask-sdk.svg?style=flat\"></a>\n            <a href=\"https://pypi.org/project/ask-sdk-core/\"><img src=\"https://pepy.tech/badge/ask-sdk-core\"></a>\n            <a href=\"https://pypi.python.org/pypi/ask-sdk/\"><img src=\"https://img.shields.io/pypi/pyversions/ask-sdk.svg?style=flat\"></a>\n          </p>\n        </p>\n    </embed>\n\n`English <README.rst>`_ |  `日本語 <README.ja.rst>`_\n\nThe **ASK SDK for Python** makes it easier for you to build highly engaging skills,\nby allowing you to spend more time on implementing features and less on writing\nboiler-plate code.\n\nThe **ASK SMAPI SDK for Python** provides developers a library for easily interacting\nwith all Skill Management APIs (SMAPI), including interaction model, intent request\nhistory, and in-skill purchasing APIs.\n\n.. |Build Status| image:: https://github.com/alexa/alexa-skills-kit-sdk-for-python/workflows/Tox%20tests%20on%20SDK%20packages/badge.svg?style=flat\n    :target: https://github.com/alexa/alexa-skills-kit-sdk-for-python/actions?query=workflow%3A%22Tox+tests+on+SDK+packages%22\n    :alt: Build Status\n.. |Runtime Version| image:: http://img.shields.io/pypi/v/ask-sdk-runtime.svg?style=flat\n    :target: https:",
    "url": "https://github.com/alexa/alexa-skills-kit-sdk-for-python",
    "last_updated": "2025-08-17T16:32:05+00:00"
  },
  {
    "full_name": "kremso/dmoz-parser",
    "name": "dmoz-parser",
    "description": "Dmoz RDF parser",
    "language": "Python",
    "topics": [],
    "readme": "Dmoz\n====\n[Dmoz](http://www.dmoz.org) is an open directory which lists and groups web pages into categories (directories). Their data is publicly available, but provided as an RDF file - a huge, funny XML file.\n\nDmoz Parser\n========\n\nThis is a really simple python implementation of the Dmoz RDF parser. It does not try to be smart and process the parsed XML for you, you have to provide a handler implementation where YOU decide what to do with the data (store it in file, database, print, etc.).\n\nThis parser makes the assumption is the last entity in each dmoz page is _topic_:\n\n     <ExternalPage about=\"http://www.awn.com/\">\n       <d:Title>Animation World Network</d:Title>\n       <d:Description>Provides information resources to the international animation community. Features include searchable database archives, monthly magazine, web animation guide, the Animation Village, discussion forums and other useful resources.</d:Description>\n       <priority>1</priority>\n       <topic>Top/Arts/Animation</topic>\n     </ExternalPage>\n\nThis assumption is strictly checked, and processing will abort if it is violated.\n\nThe RDF file needs to be downloaded, but can stay packed. You can [download the RDF](http://rdf.dmoz.org/rdf/content.rdf.u8.gz) from Dmoz site.\n\nThe RDF is pretty large, over 2G unpacked and parsing it takes some time, so there is a progress indicator.\n\nWarnings\n--------\n\nThis parser does not check for links between topics in the hierarchy, or any sophisticated parsing of the hierarchy.\n\nThe same URL might appear in multiple locations in the hierarchy.\n\nDependencied\n------------\nYou need to install dependencies from the requirements.txt file, for example by `pip install -r requirements.txt`\n\nUsage\n-----\nInstantiate the parser, provide the handler and run.\n\n    #!/usr/bin/env python\n\n    from parser import DmozParser\n    from handlers import JSONWriter\n\n    parser = DmozParser()\n    parser.add_handler(JSONWriter('output.json'))\n    parser.run()\n\nJSONWriter is the bui",
    "url": "https://github.com/kremso/dmoz-parser",
    "last_updated": "2023-01-31T11:28:52+00:00"
  },
  {
    "full_name": "andrie/cran-network-structure",
    "name": "cran-network-structure",
    "description": "Scripts used for my UseR!2015 presentation on the network structure of CRAN",
    "language": "HTML",
    "topics": [
      "r",
      "cran",
      "graph-algorithms"
    ],
    "readme": "# cran-network-structure\nScripts used for my UseR!2015 presentation on the network structure of CRAN\n\nThe slides are at [SlideShare](http://www.slideshare.net/RevolutionAnalytics/the-network-structure-of-cran-2015-0702-final)\n",
    "url": "https://github.com/andrie/cran-network-structure",
    "last_updated": "2018-12-04T14:00:49+00:00"
  },
  {
    "full_name": "cline/cline",
    "name": "cline",
    "description": "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.",
    "language": "TypeScript",
    "topics": [],
    "readme": "<div align=\"center\"><sub>\nEnglish | <a href=\"https://github.com/cline/cline/blob/main/locales/es/README.md\" target=\"_blank\">Español</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/de/README.md\" target=\"_blank\">Deutsch</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/ja/README.md\" target=\"_blank\">日本語</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/zh-cn/README.md\" target=\"_blank\">简体中文</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/zh-tw/README.md\" target=\"_blank\">繁體中文</a> | <a href=\"https://github.com/cline/cline/blob/main/locales/ko/README.md\" target=\"_blank\">한국어</a>\n</sub></div>\n\n# Cline – \\#1 on OpenRouter\n\n<p align=\"center\">\n  <img src=\"https://media.githubusercontent.com/media/cline/cline/main/assets/docs/demo.gif\" width=\"100%\" />\n</p>\n\n<div align=\"center\">\n<table>\n<tbody>\n<td align=\"center\">\n<a href=\"https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev\" target=\"_blank\"><strong>Download on VS Marketplace</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://discord.gg/cline\" target=\"_blank\"><strong>Discord</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://www.reddit.com/r/cline/\" target=\"_blank\"><strong>r/cline</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://github.com/cline/cline/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop\" target=\"_blank\"><strong>Feature Requests</strong></a>\n</td>\n<td align=\"center\">\n<a href=\"https://docs.cline.bot/getting-started/for-new-coders\" target=\"_blank\"><strong>Getting Started</strong></a>\n</td>\n</tbody>\n</table>\n</div>\n\nMeet Cline, an AI assistant that can use your **CLI** a**N**d **E**ditor.\n\nThanks to [Claude Sonnet's agentic coding capabilities](https://www.anthropic.com/claude/sonnet), Cline can handle complex software development tasks step-by-step. With tools that let him create & edit files, explore large projects, use the browser, and execute terminal commands ",
    "url": "https://github.com/cline/cline",
    "last_updated": "2025-09-02T09:56:59+00:00"
  },
  {
    "full_name": "mitsuhiko/pipsi",
    "name": "pipsi",
    "description": "pip script installer",
    "language": "Python",
    "topics": [],
    "readme": "# pipsi\n\n**⚠️ pipsi is no longer maintained. See [pipx](https://github.com/pipxproject/pipx) for an actively maintained alternative. https://github.com/pipxproject/pipx**\n\n---\n\npipsi = **pip** **s**cript **i**nstaller\n\n## What does it do?\n**pipsi** makes installing python packages with global entry points painless. These are Python packages that expose an entry point through the command line such as [Pygments](https://pypi.org/project/Pygments/).\n\nIf you are installing Python packages globally for cli access, you almost certainly want to use pipsi instead of running `sudo pip ...`. so that you get\n* Isolated dependencies to guarantee no version conflicts\n* The ability to install packages globally without using sudo\n* The ability to uninstall a package and its dependencies without affecting other globally installed Python programs\n\npipsi is not meant for installing libraries that will be imported by other Python modules.\n\n## How do I get it?\n\n```bash\ncurl https://raw.githubusercontent.com/mitsuhiko/pipsi/master/get-pipsi.py | python\n```\n\nto see installation options, including not automatically modifying the PATH environment variable\n\n```bash\ncurl https://raw.githubusercontent.com/mitsuhiko/pipsi/master/get-pipsi.py | python - --help\n```\n\n## How does it work?\n\npipsi is a wrapper around virtualenv and pip which installs scripts provided by python packages into isolated virtualenvs so they do not pollute your system's Python packages.\n\npipsi installs each package into `~/.local/venvs/PKGNAME` and then symlinks all new scripts into `~/.local/bin` (these can be changed by `PIPSI_HOME` and `PIPSI_BIN_DIR` environment variables respectively).\n\nHere is a tree view into the directory structure created by pipsi after installing pipsi and running `pipsi install Pygments`.\n\n```\n/Users/user/.local\n├── bin\n│   ├── pipsi -> /Users/user/.local/venvs/pipsi/bin/pipsi\n│   └── pygmentize -> /Users/user/.local/venvs/pygments/bin/pygmentize\n├── share\n│   └── virtualenvs\n└── venvs\n    ├── ",
    "url": "https://github.com/mitsuhiko/pipsi",
    "last_updated": "2025-08-13T01:19:02+00:00"
  },
  {
    "full_name": "open-retractions/open-retractions",
    "name": "open-retractions",
    "description": ":bangbang: :page_facing_up: :mag: an API and web interface to check if a paper has been retracted",
    "language": "",
    "topics": [
      "retractions",
      "science",
      "publishing",
      "open-data",
      "open-datasets"
    ],
    "readme": "----\n\n<div align=\"center\">\n  <h1>:bangbang: :page_facing_up: :mag_right: Open retractions :mag: :page_facing_up: :bangbang:</h1>\n  <h2>An API and web interface to check whether a paper has been retracted</h2>\n</div>\n\n<div align=\"center\">\n    <img src=\"https://travis-ci.org/fathomlabs/open-retractions.svg?branch=master\" alt=\"Travis CI build\" />\n  <a href=\"https://github.com/fathomlabs/crossref-cli/blob/master/LICENSE\" alt=\"MIT license\">\n    <img src=\"https://img.shields.io/badge/code_license-MIT-green.svg?style=flat-square\" />\n  </a>&nbsp;\n  <a href=\"https://github.com/fathomlabs/crossref-cli/blob/master/api/LICENSE\" alt=\"MIT license\">\n    <img src=\"https://img.shields.io/badge/data-public_domain_(CC0_v1.0)-yellow.svg?style=flat-square\" />\n  </a>&nbsp;\n  <a href=\"http://fathomlabs.io\" target=\"_blank\">\n    <img src=\"https://img.shields.io/badge/made_with-❤️💛💚💙💜💖-e6e6e6.svg?style=flat-square\" />\n  </a>\n</div>\n\n----\n\nUse the [web-tool](http://openretractions.com) to check whether a paper has been retracted.\n\n## API\n\nTo query the API, simply `GET` `http://openretractions.com/api/doi/${doi}/data.json`.\n\ne.g.:\n\n```bash\ncurl http://openretractions.com/api/doi/10.1002/job.1787/data.json\n```\n\n## openretraction response format\n\nIf the paper has no recorded update in the database, the API will return a 404 (not found) error.\n\nIf there's a recorded update, the API will return a JSON object.\n\nIf the paper was retracted, the `retracted` field will be `true`. If the field is false, it means a non-retraction update to the paper was recorded by the publisher. This could be:\n\n- a correction or erratum\n- an expression of concern\n- a withdrawal\n\nIn some cases, publishers mis-label retractions as other updates. It is therefore best to follow the update link to check the full statement about any update.\n\nHere's an example JSON response that shows the full format:\n\n```js\n{\n  \"retracted\": false,  // whether or not the paper has been retracted\n  \"timestamp\": 1361836800000, // the UNIXtime wh",
    "url": "https://github.com/open-retractions/open-retractions",
    "last_updated": "2025-03-19T08:38:21+00:00"
  },
  {
    "full_name": "walkerke/tidycensus",
    "name": "tidycensus",
    "description": "Load US Census boundary and attribute data as 'tidyverse' and 'sf'-ready data frames in R",
    "language": "R",
    "topics": [],
    "readme": "## tidycensus\n\n<img src=tools/readme/tidycensus_sticker.png width = \"250px\">\n\n[![R build status](https://github.com/walkerke/tidycensus/workflows/R-CMD-check/badge.svg)](https://github.com/walkerke/tidycensus/actions) ![CRAN Badge](http://www.r-pkg.org/badges/version/tidycensus)  ![CRAN Downloads](http://cranlogs.r-pkg.org/badges/tidycensus)\n\n__tidycensus__ is an R package that allows users to interface with the US Census Bureau's decennial Census and five-year American Community APIs and return tidyverse-ready data frames, optionally with simple feature geometry included.  Install from CRAN with the following command: \n\n```r\ninstall.packages(\"tidycensus\")\n```\n\nFor the latest updates, install from GitHub: \n\n```r\nremotes::install_github(\"walkerke/tidycensus\")\n```\n\nTo learn more about the package, please visit the package documentation at https://walker-data.com/tidycensus/ or read the book _Analyzing US Census Data: Methods, Maps, and Models in R_ at https://walker-data.com/census-r/.  \n\nNote: This product uses the Census Bureau Data API but is not endorsed or certified by the Census Bureau.",
    "url": "https://github.com/walkerke/tidycensus",
    "last_updated": "2025-09-01T00:11:16+00:00"
  },
  {
    "full_name": "orbstack/orbstack",
    "name": "orbstack",
    "description": "Fast, light, simple Docker containers & Linux machines",
    "language": "Shell",
    "topics": [
      "mac",
      "docker",
      "linux",
      "macos",
      "utm",
      "virtual-machine",
      "colima",
      "docker-desktop",
      "lima"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://orbstack.dev\">\n    <img src=\"https://orbstack.dev/img/icon256.png\" height=\"128\">\n    <h1 align=\"center\">OrbStack</h1>\n  </a>\n</p>\n\n[OrbStack](https://orbstack.dev) is the fast, light, and easy way to run Docker containers and Linux machines. It's a supercharged WSL and Docker Desktop alternative, all in one easy-to-use app.\n\n**Say goodbye to slow, clunky containers and VMs.** Develop at lightspeed with our Docker Desktop alternative. **[Get started](https://docs.orbstack.dev/quick-start)**\n\n## Why OrbStack?\n\n- ⚡️ **Lightning Fast.** Starts in 2 seconds, optimized [network](https://docs.orbstack.dev/docker/network) and file system, Rosetta emulation.\n- 💨 **Feather Light.** Low CPU and disk usage, [battery-friendly](https://docs.orbstack.dev/benchmarks), works with less memory, native Swift app.\n- 🍰 **Effortlessly Simple.** Automatic [domain names](https://docs.orbstack.dev/docker/domains) and [migration](https://docs.orbstack.dev/install#docker-migration), CLI & file system integration, VPN and SSH support.\n- ⚙️ **Powerful.** Run [Docker containers](https://docs.orbstack.dev/docker/), [Kubernetes](https://docs.orbstack.dev/kubernetes/), and [Linux distros](https://docs.orbstack.dev/machines/). Manage containers quickly from your menu bar. Explore volume and image files.\n\nCheck the [website](https://orbstack.dev) for demos, or see what we're up to in the [changelog](https://docs.orbstack.dev/release-notes).\n\n## Learn more\n\n- [**Demos**](https://orbstack.dev)\n- [**Benchmarks**](https://orbstack.dev/#benchmarks)\n- [**Testimonials**](https://orbstack.dev/#testimonials)\n\n## Links\n\n- [Website](https://orbstack.dev)\n- [Documentation](https://docs.orbstack.dev)\n- [Frequently asked questions](https://docs.orbstack.dev/faq)\n- [Issue tracker](https://github.com/orbstack/orbstack/issues)\n- [Support](mailto:support@orbstack.dev)\n",
    "url": "https://github.com/orbstack/orbstack",
    "last_updated": "2025-09-02T07:58:32+00:00"
  },
  {
    "full_name": "newsdev/nyt-pyfec",
    "name": "nyt-pyfec",
    "description": "A Python library for downloading, parsing and cleaning Federal Election Commission filings.",
    "language": "Python",
    "topics": [
      "fec"
    ],
    "readme": "# archived in January, 2024\n\n![](https://cloud.githubusercontent.com/assets/109988/9589471/97a005a8-4ffc-11e5-9b8b-3da984d183b3.png)\n\n## NOTE: Refactor underway\nI'm working on a significant refactor in the branch `flat-files`. The goals of this refactor are as follows:\n\n1. be smarter about the python object that comes out of pyfec. Right now it's a little haphazard where fields are located, and it has that terrible flat_filing key. I'll try to give things better names and put them in more logical places. These changes will be breaking whenever merged back in\n2. Create a way to dump the transactions from a filing into csvs for easy import to a database via file copying. This part should be irrelevant to current users.\n\nI don't believe that pyfec has a mailing list. If you're a user who wants to be informed about potentially breaking changes, please contact rachel.shorey@nytimes.com. When I merge in the new branch, I'm going to upgrade to v1 and will be good about version tagging from there on out.\n\n## This is alpha\nReally. On a given day, we make no promises that it works. We make no promises that it's not wrong. We make no promises that we won't push breaking hotfixes. If you're going to use it, we strongly recommend a fork.\n\n## Getting started\n```\npip install -e git+git@github.com:newsdev/nyt-pyfec.git#egg=nyt-pyfec\npython -m pyfec.demo\n```\n\n\n## how to update fec-csv-sources...\n\n...if you, like me, are other than awesome at git.\n\n* Make the updates in the fec-csv-sources repo, commit and push to github\n* Go your local nyt-pyfec repo, cd into pyfec\n* Make sure everything is backed up and committeed and stuff\n* If you've done this before, ```git submodule update --remote fec-csv-sources```\n* If you haven't done it before, ```rm -r fec-csv-sources &&  git submodule update --init --remote fec-csv-sources```\n* Commit and push\n* Voila?\n",
    "url": "https://github.com/newsdev/nyt-pyfec",
    "last_updated": "2024-01-30T17:39:02+00:00"
  },
  {
    "full_name": "kevinushey/nwu-completions-diagnostics-2015",
    "name": "nwu-completions-diagnostics-2015",
    "description": "Slides for talk at NWU about autocompletions, diagnostics in RStudio",
    "language": "HTML",
    "topics": [],
    "readme": "",
    "url": "https://github.com/kevinushey/nwu-completions-diagnostics-2015",
    "last_updated": "2015-10-01T17:29:22+00:00"
  },
  {
    "full_name": "notnews/ipso_facto",
    "name": "ipso_facto",
    "description": "Analysis of IPSO (Independent Press Standard Organization) complaints: https://www.ipso.co.uk/IPSO/index.html",
    "language": "R",
    "topics": [
      "ipso",
      "complaints",
      "telegraph"
    ],
    "readme": "## Analysis of IPSO Complaints\n\n[Independent Press Standards Agency (IPSO)](https://www.ipso.co.uk/IPSO/index.html) handles complaints about accuracy etc. in the media. Here, I analyze the complaints received by IPSO. \n\nIPSO has received 371 complaints as of May 20th, 2016. Of the 371 complaints, [The Telegraph](http://www.telegraph.co.uk/) alone received 35 complaints, or about 9.4% of the total complaints. It was followed closely by [The Mail](http://www.dailymail.co.uk/), with 31 complaints. [The Times](http://www.thetimes.co.uk/) had 25 complaints filed against it, [The Mirror](http://www.mirror.co.uk/) and [The Express](http://www.express.co.uk/) 22 each and [The Sun](http://www.thesun.co.uk/sol/homepage/), 19 complaints. \n\n<img src=\"figs/total_complaints-1.png\" width=\"500\">\n\nHowever, generally less than half the number of complaints were completely or partly upheld. Topping the list was [The Express](http://www.express.co.uk/) and [The Telegraph](http://www.telegraph.co.uk/) with 10 upheld complaints each. And following close behind was [The Times](http://www.thetimes.co.uk/) with 8 complaints, [The Mail](http://www.dailymail.co.uk/) with 6, and the [The Sun](http://www.thesun.co.uk/sol/homepage/) and the Daily Star with 4 each. \n\n<img src=\"figs/total_upheld-1.png\" width=\"500\">\n\nSee also [plot (png)](figure/batting_av-1.png) of batting average of media organizations with most complaints against them.\n\n---------------\n\n**Table of Contents:**\n\n* [Data](data/ipso_complaints.csv)  \n* [Script](ipso_facto.R)  \n* Figures:  \n  * [Total Complaints (pdf)](figs/ipso_n_complaints.pdf)  \n  * [Total Complaints Upheld (pdf)](figs/ipso_n_upheld.pdf)  \n  * [Batting Average (pdf)](figs/ipso_p_upheld.pdf)  \n\n",
    "url": "https://github.com/notnews/ipso_facto",
    "last_updated": "2020-08-13T05:35:35+00:00"
  },
  {
    "full_name": "jupyterhub/binderhub",
    "name": "binderhub",
    "description": "Run your code in the cloud, with technology so advanced, it feels like magic!",
    "language": "Python",
    "topics": [
      "jupyterhub",
      "binder",
      "jupyter-notebook"
    ],
    "readme": "# [BinderHub](https://github.com/jupyterhub/binderhub)\n\n[![Documentation Status](https://img.shields.io/readthedocs/binderhub?logo=read-the-docs)](https://binderhub.readthedocs.io/en/latest/)\n[![GitHub Workflow Status - Test](https://img.shields.io/github/actions/workflow/status/jupyterhub/binderhub/test.yml?logo=github&label=tests)](https://github.com/jupyterhub/binderhub/actions)\n[![Latest chart development release](https://img.shields.io/badge/dynamic/json.svg?label=latest&url=https://jupyterhub.github.io/helm-chart/info.json&query=$.binderhub.latest&colorB=orange)](https://jupyterhub.github.io/helm-chart/)\n[![GitHub](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/jupyterhub/binderhub/issues)\n[![Discourse](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.jupyter.org/c/binder/binderhub)\n[![Gitter](https://img.shields.io/badge/social_chat-gitter-blue.svg)](https://gitter.im/jupyterhub/binder)\n[![Contribute](https://img.shields.io/badge/I_want_to_contribute!-grey?logo=jupyter)](https://binderhub.readthedocs.io/en/latest/contribute.html)\n\n## What is BinderHub?\n\n**BinderHub** allows you to `BUILD` and `REGISTER` a Docker image from a\nGit repository, then `CONNECT` with JupyterHub, allowing you to create a\npublic IP address that allows users to interact with the code and\nenvironment within a live JupyterHub instance. You can select a specific\nbranch name, commit, or tag to serve.\n\nBinderHub ties together:\n\n- [JupyterHub](https://github.com/jupyterhub/jupyterhub) to provide a scalable\n  system for authenticating users and spawning single user Jupyter Notebook\n  servers, and\n- [Repo2Docker](https://github.com/jupyter/repo2docker) which generates a Docker\n  image using a Git repository hosted online.\n\nBinderHub is built with Python, kubernetes, tornado, npm, webpack, and\nsphinx.\n\n## Documentation\n\nFor more information about the architecture, use, and setup of\nBinderHub, see [the BinderHub\ndocumentation](https",
    "url": "https://github.com/jupyterhub/binderhub",
    "last_updated": "2025-09-01T07:22:08+00:00"
  },
  {
    "full_name": "waymo-research/waymo-open-dataset",
    "name": "waymo-open-dataset",
    "description": "Waymo Open Dataset",
    "language": "Python",
    "topics": [
      "autonomous-driving",
      "dataset"
    ],
    "readme": "# Waymo Open Dataset\n\nThe Waymo Open Dataset is a collection of datasets and evaluation code that we\nhave released publicly to aid the research community in making advancements in\nmachine perception and autonomous driving technology.\n\nThe Waymo Open Dataset includes three datasets:\n\n- The Perception dataset, with high resolution sensor data and labels for various tasks.\n- The Motion dataset, with object trajectories and corresponding 3D maps for 103,354 scenes.\n- The End-To-End Driving dataset, with camera data and high-level commands.\n\nIn this codebase, we provide evaluation code to support several tasks based on\nthese three dataset.\n\nSee [release history](src/waymo_open_dataset/LOG.md)\nfor a detailed list of changes.\n\nMore information can be found on the [Waymo Open Dataset website](https://waymo.com/open/).\n\n## License\n\nThis code repository (excluding\n[`src/waymo_open_dataset/wdl_limited`](src/waymo_open_dataset/wdl_limited)\nfolder) is licensed under the Apache License, Version 2.0. The code appearing in\n[`src/waymo_open_dataset/wdl_limited`](src/waymo_open_dataset/wdl_limited) is\nlicensed under terms appearing therein. The Waymo Open Dataset itself is\nlicensed under separate terms. Please\nvisit [https://waymo.com/open/terms/](https://waymo.com/open/terms/) for\ndetails.  Code located in each of the subfolders located at\n[`src/waymo_open_dataset/wdl_limited`](src/waymo_open_dataset/wdl_limited) is\nlicensed under (a) a BSD 3-clause copyright license and (b) an additional\nlimited patent license. Each limited\npatent license is applicable only to code under the respective `wdl_limited`\nsubfolder, and is licensed for use only with the use case laid out in such\nlicense in connection with the Waymo Open Dataset, as authorized by and in\ncompliance with the Waymo Dataset License Agreement for Non-Commercial Use. See\n[wdl_limited/camera/](src/waymo_open_dataset/wdl_limited/camera),\n[wdl_limited/camera_segmentation/](src/waymo_open_dataset/wdl_limited/camera_segmentation),\n[",
    "url": "https://github.com/waymo-research/waymo-open-dataset",
    "last_updated": "2025-09-02T09:51:19+00:00"
  },
  {
    "full_name": "timkpaine/perspective-parquet",
    "name": "perspective-parquet",
    "description": "Parquet file reader and editor in Jupyterlab, built with `perspective` for pivoting, filtering, aggregating, etc",
    "language": "JavaScript",
    "topics": [
      "datavisualization",
      "dataviz",
      "jupyter",
      "jupyterlab",
      "jupyterlab-extension",
      "jupyterlab-extensions",
      "parquet",
      "parquet-viewer",
      "perspective",
      "data-science",
      "data-visualization",
      "pivot-tables"
    ],
    "readme": "# perspective-parquet\nParquet file editor in Jupyterlab, built with [Perspective](https://github.com/finos/perspective)\n\n[![Build Status](https://github.com/timkpaine/perspective-parquet/workflows/Build%20Status/badge.svg?branch=main)](https://github.com/timkpaine/perspective-parquet/actions?query=workflow%3A%22Build+Status%22)\n[![PyPI](https://img.shields.io/pypi/l/perspective-parquet.svg)](https://pypi.python.org/pypi/perspective-parquet)\n[![PyPI](https://img.shields.io/pypi/v/perspective-parquet.svg)](https://pypi.python.org/pypi/perspective-parquet)\n[![npm](https://img.shields.io/npm/v/perspective-parquet.svg)](https://www.npmjs.com/package/perspective-parquet)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/timkpaine/perspective-parquet/main?urlpath=lab)\n\n![](https://raw.githubusercontent.com/timkpaine/perspective-parquet/main/docs/img/demo.gif)\n",
    "url": "https://github.com/timkpaine/perspective-parquet",
    "last_updated": "2025-09-01T00:12:57+00:00"
  },
  {
    "full_name": "jvalhondo/spanish-names-surnames",
    "name": "spanish-names-surnames",
    "description": "Data set with Spanish names and surnames",
    "language": "",
    "topics": [
      "dataset",
      "demographics",
      "spain",
      "names",
      "surnames"
    ],
    "readme": "# Dataset of Spanish names & surnames\n\nThe source can be found at INE (Instituto Nacional de Estadística): \n[Demografía y población / Padrón Población por municipios / Apellidos y nombres más frecuentes] (http://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177009&menu=resultados&secc=1254736195454&idp=1254734710990)\n\nLatest data: 01/01/2015\nPublished : 19/05/2016\n\n# Files Description:\n\n* [female_names.csv] (https://github.com/jvalhondo/spanish-names-surnames/blob/master/female_names.csv): \n\n| name | name frequency | mean name age |\n|:----:|:--------------:|:-------------:|\n| MARIA CARMEN | 668639 | 54.5 |\n\n* [male_names.csv] (https://github.com/jvalhondo/spanish-names-surnames/blob/master/male_names.csv): \n\n| name | name frequency | mean name age |\n|:----:|:--------------:|:-------------:|\n| ANTONIO | 715215 | 54.6 |\n\n* [surnames_freq_ge_100.csv] (https://github.com/jvalhondo/spanish-names-surnames/blob/master/surnames_freq_ge_100.csv): those qith frequency greater or equal to 100\n\n| surname | first surname frequency | second surname frequency | both (first & second) surnames frequency |\n|:-------:|:-----------------------:|:------------------------:|:----------------------------------------:|\n| GARCIA | 1473189 | 1489086 | 80494 |\n\n* [surnames_freq_ge_20_le_99.csv] (https://github.com/jvalhondo/spanish-names-surnames/blob/master/surnames_freq_ge_20_le_99.csv): those with frequency greater or equal to 20 and less or equal to 99\n\n| surname | first surname frequency | second surname frequency | both (first & second) surnames frequency |\n|:-------:|:-----------------------:|:------------------------:|:----------------------------------------:|\n| DE LOPEZ |99 | 427 | .. |\n \n**NOTE: The following value '..' means there is no data for that particular case.**\n  \n  \n  \n",
    "url": "https://github.com/jvalhondo/spanish-names-surnames",
    "last_updated": "2025-02-06T13:53:42+00:00"
  },
  {
    "full_name": "paul-buerkner/brms",
    "name": "brms",
    "description": "brms R package for Bayesian generalized multivariate non-linear multilevel models using Stan",
    "language": "R",
    "topics": [
      "brms",
      "stan",
      "bayesian-inference",
      "multilevel-models",
      "statistical-models",
      "r-package"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<img src=\"man/figures/brms.png\" width = 120 alt=\"brms Logo\"/>[<img src=\"https://raw.githubusercontent.com/stan-dev/logos/master/logo_tm.png\" align=\"right\" width=120 alt=\"Stan Logo\"/>](https://mc-stan.org/)\n\n# brms\n\n[![R-CMD-check](https://github.com/paul-buerkner/brms/workflows/R-CMD-check/badge.svg)](https://github.com/paul-buerkner/brms/actions)\n[![Coverage\nStatus](https://codecov.io/github/paul-buerkner/brms/coverage.svg?branch=master)](https://app.codecov.io/github/paul-buerkner/brms?branch=master)\n[![CRAN\nVersion](https://www.r-pkg.org/badges/version/brms)](https://cran.r-project.org/package=brms)\n[![Downloads](https://cranlogs.r-pkg.org/badges/brms?color=brightgreen)](https://CRAN.R-project.org/package=brms)\n\n## Overview\n\nThe **brms** package provides an interface to fit Bayesian generalized\n(non-)linear multivariate multilevel models using Stan, which is a C++\npackage for performing full Bayesian inference (see\n<https://mc-stan.org/>). The formula syntax is very similar to that of\nthe package lme4 to provide a familiar and simple interface for\nperforming regression analyses. A wide range of response distributions\nare supported, allowing users to fit – among others – linear, robust\nlinear, count data, survival, response times, ordinal, zero-inflated,\nand even self-defined mixture models all in a multilevel context.\nFurther modeling options include non-linear and smooth terms,\nauto-correlation structures, censored data, missing value imputation,\nand quite a few more. In addition, all parameters of the response\ndistribution can be predicted in order to perform distributional\nregression. Multivariate models (i.e., models with multiple response\nvariables) can be fit, as well. Prior specifications are flexible and\nexplicitly encourage users to apply prior distributions that actually\nreflect their beliefs. Model fit can easily be assessed and compared\nwith posterior predictive checks, cross-vali",
    "url": "https://github.com/paul-buerkner/brms",
    "last_updated": "2025-09-01T16:34:29+00:00"
  },
  {
    "full_name": "sckott/cowsay",
    "name": "cowsay",
    "description": "cowsay w/ more animals, in R",
    "language": "HTML",
    "topics": [
      "ascii-art",
      "rstats",
      "r",
      "r-stats"
    ],
    "readme": "cowsay\n======\n\n\n\n![cran checks](https://badges.cranchecks.info/worst/cowsay.svg)\n[![R-check](https://github.com/sckott/cowsay/workflows/R-check/badge.svg)](https://github.com/sckott/cowsay/actions?query=workflow%3AR-check)\n[![codecov](https://codecov.io/gh/sckott/cowsay/graph/badge.svg?token=fewxwGHXfv)](https://app.codecov.io/gh/sckott/cowsay)\n[![rstudio mirror downloads](https://cranlogs.r-pkg.org/badges/cowsay)](https://github.com/r-hub/cranlogs.app)\n[![cran version](https://www.r-pkg.org/badges/version/cowsay)](https://cran.r-project.org/package=cowsay)\n\n### What is this?\n\nIf you are familiar with `cowsay` on the cli, then you know what this is, but for R.  If not, read below.  Why?  Why not?\n\n### History\n\ncowsay was originally written by Tony Monroe, with the first code released in 1999. The version of cowsay you get via homebrew is a fork by Andrew Janke at <https://cowsay.diamonds/> ([code repo](https://github.com/cowsay-org/cowsay)). The code for the original can be found at [tnalpgge/rank-amateur-cowsay](https://github.com/tnalpgge/rank-amateur-cowsay) and the original website can be seen [on the Wayback Machine](https://web.archive.org/web/20120225123719/http://www.nog.net/~tony/warez/cowsay.shtml). Both the original and version by Janke are both written in Perl. The cowsay you get with `apt` on Linux machines is the last version Monroe released in 2016 (v3.0.4 or possibly v3.0.3). \n\nThis R package has some additional features the orginal cowsay doesn't have, but also lacks some features the original has. This package doesn't yet support custom eyes and tongue for the cow or any other animals.\n\n### Contributors (alphabetical)\n\n\n\n\n * Amanda Dobbyn\n * Andreas Brandmaier\n * Andy Teucher\n * Carson Sievert\n * David Schoch\n * Franz-Sebastian Krah\n * Guangchuang Yu\n * Karl Broman\n * Kiyoko Gotanda\n * Lucy D'Agostino McGowan\n * Marion Louveaux\n * Noam Ross\n * Paolo Sonego\n * Philipp Boersch-Supan\n * Rich FitzJohn\n * Scott Chamberlain\n * Thomas Leeper\n * Tyler Rink",
    "url": "https://github.com/sckott/cowsay",
    "last_updated": "2025-08-15T13:24:58+00:00"
  },
  {
    "full_name": "ajschumacher/dc_voter_reg",
    "name": "dc_voter_reg",
    "description": "snapshot of DC voter registration data",
    "language": "",
    "topics": [],
    "readme": "# DC Voter Registration Data\n\n * [20141218-dc_voters.csv.zip](20141218-dc_voters.csv.zip): zipped data file with header row and 469,610 rows of data\n * [20141218-dc_voters.txt](20141218-dc_voters.txt): original documentation, received as `Read Me.txt`\n\nOn Thursday, December 18, 2014, I took a printed copy of a [PDF form](http://www.dcboee.org/pdf_files/Data_Request_Form.pdf) that I found on the [DC Board of Elections web site](http://www.dcboee.org/) to 441 4th Street NW, suite 250 north, Washington, DC, 20001. I had to show ID, have my bag x-rayed, and go through a metal detector to get into the building.\n\nI brought my checkbook so I could pay $2 for a CD-ROM of voter registration data. The clerk informed me that the data is updated daily. She burned my CD while I waited. She told me there are no rules on how the data can be used. I take it to be public domain.\n\nOn the CD were two files:\n\n * The data was in a Microsoft Access database file called `DC VH EXPORT.MDB` (154 MB). I bought a PC laptop running Windows 8, purchased and installed Microsoft Access, opened the file and exported the data as text. It comes out as 130 megabytes of uncompressed text. I was able to get the column headers out and stick them on top of the file so that it can be read as nice CSV. Zipped (using the Windows built-in) the result is the 20 MB [20141218-dc_voters.csv.zip](20141218-dc_voters.csv.zip).\n * The provided documentation file was a plain text `Read Me.txt`. I renamed this to [20141218-dc_voters.txt](20141218-dc_voters.txt). It seems slightly out of date with respect to the data in that the later columns corresponding to elections are not the same in the documentation and in the data. These column names are interpretable as MMYYYY-T, I believe, where MM is numeric month, YYYY is four-digit year, and T is a letter corresponding to election type. I think G is General and P is primary.\n\n*Slightly more\n[on my blog](http://planspace.org/20141220-dc_voter_registration_data/).*\n",
    "url": "https://github.com/ajschumacher/dc_voter_reg",
    "last_updated": "2024-06-15T04:07:43+00:00"
  },
  {
    "full_name": "jadianes/spark-py-notebooks",
    "name": "spark-py-notebooks",
    "description": "Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning as IPython / Jupyter notebooks",
    "language": "Jupyter Notebook",
    "topics": [
      "spark",
      "python",
      "pyspark",
      "data-analysis",
      "mllib",
      "ipython-notebook",
      "notebook",
      "ipython",
      "data-science",
      "machine-learning",
      "big-data",
      "bigdata"
    ],
    "readme": "# Spark Python Notebooks  \n\n[![Join the chat at https://gitter.im/jadianes/spark-py-notebooks](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jadianes/spark-py-notebooks?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nThis is a collection of [IPython notebook](http://ipython.org/notebook.html)/[Jupyter](https://jupyter.org/) \nnotebooks intended to train the reader on different [Apache Spark](http://spark.apache.org/) concepts, from \nbasic to advanced, by using the **Python** language.  \n\nIf Python is not your language, and it is R, you may want to have a look at our [R on Apache Spark (SparkR) notebooks](https://github.com/jadianes/spark-r-notebooks) instead. Additionally, if your are interested in being introduced to some basic Data Science\nEngineering, you might find [these series of tutorials](https://github.com/jadianes/data-science-your-way)\ninteresting. There we explain different concepts and applications \nusing Python and R.  \n\n## Instructions  \n\nA good way of using these notebooks is by first cloning the repo, and then \nstarting your own [IPython notebook](http://ipython.org/notebook.html)/[Jupyter](https://jupyter.org/) in \n**pySpark mode**. For example, if we have a *standalone* Spark installation\nrunning in our `localhost` with a maximum of 6Gb per node assigned to IPython:  \n\n    MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.5.0-bin-hadoop2.6/bin/pyspark\n\nNotice that the path to the `pyspark` command will depend on your specific \ninstallation. So as requirement, you need to have\n[Spark installed](https://spark.apache.org/docs/latest/index.html) in \nthe same machine you are going to start the `IPython notebook` server.     \n\nFor more Spark options see [here](https://spark.apache.org/docs/latest/spark-standalone.html). In general it works the rule of passing options\ndescribed in the form `spark.executor.memory` as `SPARK_EXECUTOR_MEMORY` when\ncalling IPy",
    "url": "https://github.com/jadianes/spark-py-notebooks",
    "last_updated": "2025-08-27T14:32:57+00:00"
  },
  {
    "full_name": "tdunning/t-digest",
    "name": "t-digest",
    "description": "A new data structure for accurate on-line accumulation of rank-based statistics such as quantiles and trimmed means",
    "language": "Java",
    "topics": [
      "quantile",
      "accuracy",
      "online-algorithms",
      "t-digest"
    ],
    "readme": "t-digest  &middot;  [![Java CI](https://github.com/tdunning/t-digest/actions/workflows/maven.yml/badge.svg?branch=main)](https://github.com/tdunning/t-digest/actions/workflows/maven.yml)\n========\n\nA new data structure for accurate online accumulation of rank-based statistics such as quantiles\nand trimmed means.  The t-digest algorithm is also very friendly to parallel programs making it \nuseful in map-reduce and parallel streaming applications implemented using, say, Apache Spark.\n\nThe t-digest construction algorithm uses a variant of 1-dimensional k-means clustering to produce a\nvery compact data structure that allows accurate estimation of quantiles.  This t-digest data \nstructure can be used to estimate quantiles, compute other rank statistics or even to estimate\nrelated measures like trimmed means.  The advantage of the t-digest over previous digests for \nthis purpose is that the _t_-digest handles data with full floating point resolution.  With small\nchanges, the _t_-digest can handle values from any ordered set for which we can compute something akin to a mean.\nThe accuracy of quantile estimates produced by t-digests can be orders of magnitude more accurate than\nthose produced by alternative digest algorithms in spite of the fact that t-digests are much more \ncompact, particularly when serialized.\n\nIn summary, the particularly interesting characteristics of the t-digest are that it\n\n* has smaller summaries when serialized\n* works on double precision floating point as well as integers.\n* provides part per million accuracy for extreme quantiles and typically <1000 ppm accuracy for middle quantiles\n* is very fast (~ 140 ns per add)\n* is very simple (~ 5000 lines of code total, <1000 for the most advanced implementation alone)\n* has a reference implementation that has > 90% test coverage\n* can be used with map-reduce very easily because digests can be merged\n* requires no dynamic allocation after initial creation (`MergingDigest` only)\n* has no runtime dependencie",
    "url": "https://github.com/tdunning/t-digest",
    "last_updated": "2025-08-31T12:02:03+00:00"
  },
  {
    "full_name": "OpenMathLib/OpenBLAS",
    "name": "OpenBLAS",
    "description": "OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. ",
    "language": "C",
    "topics": [
      "blas",
      "lapack",
      "lapacke"
    ],
    "readme": "# OpenBLAS\n\n[![Join the chat at https://gitter.im/xianyi/OpenBLAS](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/xianyi/OpenBLAS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nCirrus CI: [![Build Status](https://api.cirrus-ci.com/github/xianyi/OpenBLAS.svg?branch=develop)](https://cirrus-ci.com/github/xianyi/OpenBLAS)\n\n\n\n[![Build Status](https://dev.azure.com/xianyi/OpenBLAS/_apis/build/status/xianyi.OpenBLAS?branchName=develop)](https://dev.azure.com/xianyi/OpenBLAS/_build/latest?definitionId=1&branchName=develop)\n\nOSUOSL POWERCI [![Build Status](https://powerci.osuosl.org/buildStatus/icon?job=OpenBLAS_gh%2Fdevelop)](http://powerci.osuosl.org/job/OpenBLAS_gh/job/develop/)\n\nOSUOSL IBMZ-CI [![Build Status](http://ibmz-ci.osuosl.org/buildStatus/icon?job=OpenBLAS-Z%2Fdevelop)](http://ibmz-ci.osuosl.org/job/OpenBLAS-Z/job/develop/)\n## Introduction\n\nOpenBLAS is an optimized BLAS (Basic Linear Algebra Subprograms) library based on GotoBLAS2 1.13 BSD version.\n\nFor more information about OpenBLAS, please see:\n\n- The documentation at [openmathlib.org/OpenBLAS/docs/](http://www.openmathlib.org/OpenBLAS/docs),\n- The home page at [openmathlib.org/OpenBLAS/](http://www.openmathlib.org/OpenBLAS).\n\nFor a general introduction to the BLAS routines, please refer to the extensive documentation of their reference implementation hosted at netlib:\n<https://www.netlib.org/blas>. On that site you will likewise find documentation for the reference implementation of the higher-level library LAPACK - the **L**inear **A**lgebra **Pack**age that comes included with OpenBLAS. If you are looking for a general primer or refresher on Linear Algebra, the set of six\n20-minute lecture videos by Prof. Gilbert Strang on either MIT OpenCourseWare [here](https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/) or YouTube [here](https://www.youtube.com/playlist?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek) may be helpful.\n\n## Binary Packa",
    "url": "https://github.com/OpenMathLib/OpenBLAS",
    "last_updated": "2025-09-01T11:34:40+00:00"
  },
  {
    "full_name": "aws-samples/amazon-textract-textractor",
    "name": "amazon-textract-textractor",
    "description": "Analyze documents with Amazon Textract and generate output in multiple formats.",
    "language": "Jupyter Notebook",
    "topics": [
      "amazon-textract"
    ],
    "readme": "![Textractor](https://raw.githubusercontent.com/aws-samples/amazon-textract-textractor/5716c52e8a39c063f43e058e1637e4984a4b2da4/docs/source/textractor_cropped.png)\n\n[![Tests](https://github.com/aws-samples/amazon-textract-textractor/actions/workflows/tests.yml/badge.svg)](https://github.com/aws-samples/amazon-textract-textractor/actions/workflows/tests.yml) [![Documentation](https://github.com/aws-samples/amazon-textract-textractor/actions/workflows/documentation.yml/badge.svg)](https://aws-samples.github.io/amazon-textract-textractor/) [![PyPI version](https://badge.fury.io/py/amazon-textract-textractor.svg)](https://pypi.org/project/amazon-textract-textractor/) [![Downloads](https://static.pepy.tech/badge/amazon-textract-textractor/month)](https://pepy.tech/project/amazon-textract-textractor) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n**Textractor** is a python package created to seamlessly work with [Amazon Textract](https://docs.aws.amazon.com/textract/latest/dg/what-is.html) a document intelligence service offering text recognition, table extraction, form processing, and much more. Whether you are making a one-off script or a complex distributed document processing pipeline, Textractor makes it easy to use Textract.\n\nIf you are looking for the other amazon-textract-* packages, you can find them using the links below:\n\n- [amazon-textract-caller](https://github.com/aws-samples/amazon-textract-textractor/tree/master/caller) (to simplify calling Amazon Textract without additional dependencies)\n- [amazon-textract-response-parser](https://pypi.org/project/amazon-textract-response-parser/) (to parse the JSON response returned by Textract APIs)\n- [amazon-textract-overlayer](https://github.com/aws-samples/amazon-textract-textractor/tree/master/overlayer) (to draw bounding boxes around the document entities on the document image)\n- [amazon-textract-prettyprinter](https://github.com/aws-samples/amazon-",
    "url": "https://github.com/aws-samples/amazon-textract-textractor",
    "last_updated": "2025-08-26T23:34:27+00:00"
  },
  {
    "full_name": "ghgr/BetFair_Arbitrer",
    "name": "BetFair_Arbitrer",
    "description": "Automatic arbitrer in Betfair Exchange under the hypothesis of Poisson Process",
    "language": "Python",
    "topics": [],
    "readme": "# BetFair_Arbitrer\nAutomatic arbitrer for Betfair Exchange under the hypothesis that a soccer match is a Poisson Process.\n\nDISCLAIMER: This whole posts is about an untested, non-guaranteed method of detecting inefficiencies on the sports betting market. It is considered as an interesting mathematical and programming exercise, and I encourage the reader to view it strictly as it. I do not encourage anybody to use this system with real money, and if you decide to do so, do it at your own risk.\n\nPlease <a href=\"http://www.linkedin.com/pulse/arbitraging-betfair-fun-profit-eduardo-pena-vina?trk=prof-post\">read the full blog post</a>\n",
    "url": "https://github.com/ghgr/BetFair_Arbitrer",
    "last_updated": "2025-08-14T05:54:45+00:00"
  },
  {
    "full_name": "JasperSnoek/spearmint",
    "name": "spearmint",
    "description": "Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper:  Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams.  Advances in Neural Information Processing Systems, 2012 ",
    "language": "Python",
    "topics": [],
    "readme": "New code repository!\nAs you might have noticed, the development of this repository has been limited to maintenance and bug fixes for some time now.  The reason is that there has been a collaborative effort to overhaul Spearmint into a new codebase.  This includes both algorithmic/theoretical and engineering improvements.  Check it out at https://github.com/HIPS/Spearmint.  Note that the new repository is under a non-commercial license with a contributor license agreement.  If you prefer not to agree to the license, you can freely use code here (though it is a bit older).\n\nSpearmint\n---------\n\nSpearmint is a package to perform Bayesian optimization according to the\nalgorithms outlined in the paper:  \n\n**Practical Bayesian Optimization of Machine Learning Algorithms**  \nJasper Snoek, Hugo Larochelle and Ryan P. Adams  \n*Advances in Neural Information Processing Systems*, 2012  \n\nThis code is designed to automatically run experiments (thus the code\nname 'spearmint') in a manner that iteratively adjusts a number of\nparameters so as to minimize some objective in as few runs as\npossible.\n\nSpearmint is the result of a collaboration primarily between machine learning researchers at [Harvard University](https://hips.seas.harvard.edu/) and the [University of Toronto](http://learning.cs.toronto.edu/).\n\nDependencies\n------------\nThis package requires:\n\n* Python 2.7\n\n* [Numpy](http://www.numpy.org/) version 1.6.1+\nOn Ubuntu linux you can install this package using the command:\n\n\t\tapt-get install python-numpy\n\n* [Scipy](http://www.scipy.org/) version 0.9.0+\nOn Ubuntu linux you can install this package using the command:\n\n\t\tapt-get install python-scipy\n\n* [Google Protocol Buffers](https://developers.google.com/protocol-buffers/) (for the fully automated code).\nNote that you should be able to install protocol-buffers from source without requiring administrator privileges.  Otherwise, on Ubuntu linux you can install this package using the command:\n\n\t\tapt-get install python-protobuf\n",
    "url": "https://github.com/JasperSnoek/spearmint",
    "last_updated": "2025-08-14T23:53:57+00:00"
  },
  {
    "full_name": "MarkEdmondson1234/searchConsoleR",
    "name": "searchConsoleR",
    "description": "R interface with Google Search Console API v3, including Search Analytics.",
    "language": "R",
    "topics": [
      "search-analytics",
      "googleauthr",
      "r",
      "google",
      "api"
    ],
    "readme": "# searchConsoleR\n\n[![CRAN](http://www.r-pkg.org/badges/version/searchConsoleR)](https://CRAN.R-project.org/package=searchConsoleR)\n[![Travis-CI Build Status](https://travis-ci.org/MarkEdmondson1234/searchConsoleR.svg?branch=master)](https://travis-ci.org/MarkEdmondson1234/searchConsoleR)\n\nR interface with Google Search Console (formally Google Webmaster Tools) API v3.\n\n## Setup Guide\n\nInstall dependency `googleAuthR` from CRAN:\n```r\ninstall.packages(\"googleAuthR\")\nlibrary(googleAuthR)\n```\n\nInstall `searchConsoleR` 0.3.0 from CRAN:\n```r\ninstall.packages(\"searchConsoleR\")\nlibrary(searchConsoleR)\n```\n\nIf you want the development version of `searchConsoleR` on Github:\n\n```r\ndevtools::install_github(\"MarkEdmondson1234/searchConsoleR\")\nlibrary(searchConsoleR)\n```\n\n## Shiny Compatible\nAuthentication can be done locally or within a Shiny app. See a very bare bones example here: https://mark.shinyapps.io/searchConsoleRDemo/\n\n## Info Links\n\n[Google Search Console](http://www.google.com/webmasters/tools/)\n\n[Search Console v3 API docs](https://developers.google.com/webmaster-tools/)\n\n## Function Quick Guide\n\n### Search analytics\n* `search_analytics()` - download Google SEO data into an R dataframe.\n\n### Website admin\n* `list_websites()` - list websites in your Google Search Console.\n* `add_website()` - add a website to your Google Search Console.\n* `delete_website()` - delete a website from your Google Search Console.\n\n### Sitemaps\n* `list_sitemaps()` - list sitemaps recognised in Google Search Console.\n* `add_sitemap()` - add sitemap URL location to Google Search Console.\n* `delete_sitemap()` - remove sitemap URL location in Google Search Console.\n\n### Error listings\n* `crawl_errors()` - list various types of crawl errors googlebot has found.\n* `list_crawl_error_samples()` - get a list of example URLs with errors.\n* `error_sample_url()` - show details about an example URL error (for example, links to a 404 URL)\n* `fix_sample_url()` - mark a URL as fixed.\n\n### Authentication fu",
    "url": "https://github.com/MarkEdmondson1234/searchConsoleR",
    "last_updated": "2025-08-27T12:07:39+00:00"
  },
  {
    "full_name": "dslp/dslp",
    "name": "dslp",
    "description": "The Data Science Lifecycle Process is a process for taking data science teams from Idea to Value repeatedly and sustainably. The process is documented in this repo. ",
    "language": "",
    "topics": [],
    "readme": "# The Data Science Lifecycle Process\n\nThe Data Science Lifecycle Process is a set of prescriptive steps and best practices to enable data science teams to consistently deliver value. It includes issue templates for common data science work types, a branching strategy that fits the data science development flow, and prescriptive guidance on how to piece together all the various tools and workflows required to make data science work.\n\n## Getting Started\n\nTo get started, you can read about the steps [here](steps.md).  \nA template repository for projects following this process can be found [here](https://github.com/dslp/dslp-repo-template).  \n\nFor more background on what this is and why you should consider adopting it, continue reading below.\n\n## Key Topics\n\n- [Branching Strategy](branching/branch-types.md)\n- [Issue Templates](issue-types/0-overview-issue-types.md)\n- [Workflows](steps.md)\n- [Semantic Version of Data Science Assets](semantic-versioning.md)\n- [Labels](labels.md)\n- [Standard Repo Structure](https://github.com/dslp/dslp-repo-template)\n\n## Why did we create this and why should you adopt it?\n\nData Science is hard. Enterprise Data Science is harder. Figuring out how to consistently go from asking a question to delivering a solution is something very few teams and even fewer enterprises have figured out. There have been major steps forward in this space. The proliferation of DataOps and MLOps are standardizing practices for building robust data pipelines and model deployments and tools like AzureML make parts of the data science process significantly simpler.\n\nHowever, even with these important evolutions, we were still seeing huge gaps in data science teams across the board. Simply put, while many of the solutions solve key problems data science teams face, no one was answering the question, **How do we put this all together into a single workflow?**\n\nThis meant everyone was solving these problems from scratch. The Data Science Lifecyle Process is designed to ",
    "url": "https://github.com/dslp/dslp",
    "last_updated": "2025-09-01T00:56:09+00:00"
  },
  {
    "full_name": "jknowles/merTools",
    "name": "merTools",
    "description": "Convenience functions for working with merMod objects from lme4",
    "language": "R",
    "topics": [],
    "readme": "[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/merTools)](https://cran.r-project.org/package=merTools)\r\n[![Downloads](http://cranlogs.r-pkg.org/badges/merTools)](https://cran.r-project.org/package=merTools)\r\n[![Downloads](http://cranlogs.r-pkg.org/badges/grand-total/merTools)](https://cran.r-project.org/package=merTools)\r\n\r\n<!-- README.md is generated from README.Rmd. Please edit that file -->\r\n\r\n# merTools\r\n\r\nA package for getting the most out of large multilevel models in R\r\n\r\nby Jared E. Knowles and Carl Frederick\r\n\r\nWorking with generalized linear mixed models (GLMM) and linear mixed\r\nmodels (LMM) has become increasingly easy with advances in the `lme4`\r\npackage. As we have found ourselves using these models more and more\r\nwithin our work, we, the authors, have developed a set of tools for\r\nsimplifying and speeding up common tasks for interacting with `merMod`\r\nobjects from `lme4`. This package provides those tools.\r\n\r\n## Installation\r\n\r\n``` r\r\n# development version\r\nlibrary(devtools)\r\ninstall_github(\"jknowles/merTools\")\r\n\r\n# CRAN version\r\ninstall.packages(\"merTools\")\r\n```\r\n\r\n## Recent Updates\r\n\r\n## merTools 0.6.2 (Early 2024)\r\n\r\n- Maintenance release to fix minor issues with function documentation\r\n- Fix \\#130 by avoiding conflict with `vcov` in the `merDeriv` package\r\n- Upgrade package test infrastructure to 3e testthat specification\r\n\r\n## merTools 0.6.1 (Spring 2023)\r\n\r\n- Maintenance release to keep package listed on CRAN\r\n- Fix a small bug where parallel code path is run twice (#126)\r\n- Update plotting functions to avoid deprecated `aes_string()` calls\r\n  (#127)\r\n- Fix (#115) in description\r\n- Speed up PI using @bbolker pull request (#120)\r\n- Updated package maintainer contact information\r\n\r\n### merTools 0.5.0\r\n\r\n#### New Features\r\n\r\n- `subBoot` now works with `glmerMod` objects as well\r\n- `reMargins` a new function that allows the user to marginalize the\r\n  prediction over breaks in the distribution of random effect\r\n  distributions, see `?reMarg",
    "url": "https://github.com/jknowles/merTools",
    "last_updated": "2025-03-22T08:14:40+00:00"
  },
  {
    "full_name": "IRS-Public/direct-file",
    "name": "direct-file",
    "description": "Direct File",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Direct File\n[Direct File](https://directfile.irs.gov) is a service from the United States Government that provides taxpayers the option to electronically file their federal tax return for free, directly with the Internal Revenue Service (IRS). Direct File is an interview-based service that is intended to work as well on a mobile phone as it does on a laptop, tablet, or desktop computer. It is available in English and Spanish and is designed to be accessible to taxpayers who have a variety of attitudes, aptitudes, abilities, and access needs.\n\nDirect File interprets the United States' [Internal Revenue Code (26 USC)](https://www.irs.gov/privacy-disclosure/tax-code-regulations-and-official-guidance) as plain language questions, the answers to which should be known to taxpayers without need of external instructions or publications. Taxpayers' answers are then translated into standard tax forms and transmitted to the IRS's [Modernized e-File (MeF)](https://www.irs.gov/e-file-providers/modernized-e-file-program-information) API, which is available for authorized public use. These questions and logic, developed in close collaboration with the IRS [Office of Chief Counsel](https://www.irs.gov/about-irs/office-of-chief-counsel-at-a-glance), as well as the associated test cases and scenarios, may be useful for others working on products that need to accurately interpret United States tax law as of Tax Year 2024.\n\nDirect File also incorporates the Fact Graph, a declarative, XML-based knowledge graph data structure that is designed to reason about incomplete information, such as a partially completed tax return. The Fact Graph is written in the Scala programming language; it runs on the JVM on the backend and is transpiled via [Scala.js](https://www.scala-js.org) to run on the client as well. Direct File's Fact Graph is not domain-specific, and it may be useful to revenue agencies and as a reference for business rules engine implementations.\n\nAlthough Direct File only files ",
    "url": "https://github.com/IRS-Public/direct-file",
    "last_updated": "2025-09-02T07:02:36+00:00"
  },
  {
    "full_name": "markhuberty/twitter_election2012",
    "name": "twitter_election2012",
    "description": "Repository for the predicton algorithms, code, and data for real-time prediction of the 2012 Congressional elections",
    "language": "JavaScript",
    "topics": [],
    "readme": "Voting with your tweet: 2012 edition\n====================\n\nThis repo holds the prediction algorithms, code, and supporting data\nfor real-time prediction of the 2012 Congressional elections using the\nTwitter feed. The project is based on a [working\npaper](http://markhuberty.berkeley.edu/files/twitter_paper.pdf.zip) by\n[Mark Huberty](http://markhuberty.berkeley.edu/),\nentitled \"Voting with your tweet: forecasting elections with social\nmedia data\". \n\nThanks to the support of the UC Berkeley Graduate School of Journalism, we published real-time predictions throughout the 2012 election cycle. See [Voting with your Tweet](http://californianewsservice.org/category/tweet-vote/) for more detail.\n\nBackground\n-----------------------\nA range of very interesting papers have attempted to predict elections\nbased on the content of election-related messages in the Twitter\nfeed. These include papers on the U.S. Presidential Election ([O'Connor\net al 2010](http://brenocon.com/oconnor_balasubramanyan_routledge_smith.icwsm2010.tweets_to_polls.pdf)), the\nGerman Bundestag ([Tumasjan et al 2010](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM10/paper/view/1441/1852)), and the British\nParliament ([Tweetminster 2010](http://www.scribd.com/doc/29154537/Tweetminster-Predicts)). More recently, [Daniel Gayo-Avello](http://arxiv.org/abs/1204.6441v1) pointed out various\nproblems with most of these papers. One of the more significant\nproblems he identifies is the lack of /a priori/ prediction of future\nelections. This project is an experiment doing just that.\n\nThe algorithms in the </algorithms> folder were trained on the results\nof the 2010 United States Congressional elections. They use a bigram\nbag-of-words language model and the SuperLearner machine learning\nalgorithm proposed by [van der Laan et al (2007)](http://biostats.bepress.com/ucbbiostat/paper222/). Both binary (win/loss) and\ncontinuous (vote share) algorithms are provided. More detail can be found in the\nworking paper cited above. \n\nTec",
    "url": "https://github.com/markhuberty/twitter_election2012",
    "last_updated": "2018-09-25T02:30:12+00:00"
  },
  {
    "full_name": "linkedin/photon-ml",
    "name": "photon-ml",
    "description": "A scalable machine learning library on Apache Spark",
    "language": "Terra",
    "topics": [],
    "readme": "# Photon Machine Learning (Photon ML)\n\n[![Build Status](https://travis-ci.org/linkedin/photon-ml.svg?branch=master)](https://travis-ci.org/linkedin/photon-ml)\n\n**Check out our [hands-on tutorial](https://github.com/linkedin/photon-ml/wiki/Photon-ML-Tutorial).**\n\nPhoton ML is a machine learning library based on Apache Spark. It was originally developed by the LinkedIn Machine Learning Algorithms Team. Currently, Photon ML supports training different types of [Generalized Linear Models](https://en.wikipedia.org/wiki/Generalized_linear_model)(GLMs) and [Generalized Linear Mixed Models](https://en.wikipedia.org/wiki/Generalized_linear_mixed_model)(GLMMs/GLMix model): logistic, linear, and Poisson.\n\n- [Features](#features)\n  - [Generalized Linear Models](#generalized-linear-models)\n  - [GAME - Generalized Additive Mixed Effects](#game---generalized-additive-mixed-effects)\n  - [Configurable Optimizers](#configurable-optimizers)\n  - [Regularization](#regularization)\n  - [Feature scaling and normalization](#feature-scaling-and-normalization)\n  - [Offset training](#offset-training)\n  - [Feature summarization](#feature-summarization)\n  - [Model validation](#model-validation)\n  - [Warm-start training](#warm-start-training)\n  - [Partial re-training](#partial-re-training)\n- [Experimental Features](#experimental-features)\n  - [Smoothed Hinge Loss Linear SVM](#smoothed-hinge-loss-linear-svm)\n  - [Hyperparameter Auto-Tuning](#hyperparameter-auto-tuning)\n  - [Regularize by Previous Model During Warm-Start Training](#regularize-by-previous-model-during-warm-start-training)\n- [How to Build](#how-to-build)\n- [How to Use](#how-to-use)\n  - [Drivers](#drivers)\n  - [API](#api)\n  - [Avro Schemas](#avro-schemas)\n  - [What about other formats?](#what-about-other-formats)\n  - [Input Data Format](#input-data-format)\n  - [Models](#models)\n  - [Shaded Jar](#shaded-jar)\n  - [Try It Out!](#try-it-out)\n    - [Install Spark](#install-spark)\n    - [Get and Build the Code](#get-and-build-the-code)\n    ",
    "url": "https://github.com/linkedin/photon-ml",
    "last_updated": "2025-08-28T16:11:32+00:00"
  },
  {
    "full_name": "lgatto/SC-ICS-Proposal",
    "name": "SC-ICS-Proposal",
    "description": "Software Carpentry ICS proposal ",
    "language": "HTML",
    "topics": [],
    "readme": "# Software Carpentry ICS proposal\n\nThe R consortium invites the community to submit\n[ISC Proposals](https://www.r-consortium.org/projects/call-for-proposals):\n\n> The goal of the Infrastructure Steering Committee (the ISC) is to\n> support projects that broadly help the R community. This might be\n> software development, developing new teaching materials, documenting\n> best practices, standardising APIs or doing research. Currently, the\n> ISC chiefly provides financial support for projects proposed by\n> individuals or teams who have the skills to carry out the work, but\n> we can also provide administrative support, promotion and some\n> collaboration tools for groups who would like to study more\n> ambitious projects.\n\n> The proposal was\n> [submitted](https://github.com/lgatto/SC-ICS-Proposal/tree/submitted)\n> on Friday 8 January.\n\n**Good news, we [got the grant](http://software-carpentry.org/blog/2016/03/r-consortium-training.html)!**\n\n### Outputs \n\n- The\n  [course](https://swcarpentry.github.io/2016-09-19-ttt-cambridge/)\n  was run in Cambridge, on the 19-20 September 2016, and was lead by\n  Karin Lagesen and Steve Crouch.\n  \n-  Here's a\n   [blog post](https://www.software.ac.uk/blog/2016-10-18-cambridge-instructor-training-19-20-september)\n   about the course.\n  \n- Some [outputs/exercises](./exercises.md) from the course.\n\n- [Slides](https://lgatto.github.io/SC-ICS-Proposal/slides/2017_01_31_Rconsortium.html)\n  for the ISC Project Status Webinar.\n\n### Who\n\n* Laurent Gatto\n* John Blishak\n* Greg Wilson\n* David LeBauer\n* Jonah Duckles\n\nIf you have questions or comments, please open an\n[issue](https://github.com/lgatto/SC-ICS-Proposal/issues). \n\n",
    "url": "https://github.com/lgatto/SC-ICS-Proposal",
    "last_updated": "2019-03-21T03:07:25+00:00"
  },
  {
    "full_name": "probot/probot",
    "name": "probot",
    "description": "🤖 A framework for building GitHub Apps to automate and improve your workflow",
    "language": "TypeScript",
    "topics": [
      "probot",
      "github-apps"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://probot.github.io\"><img src=\"/static/robot.svg\" width=\"160\" alt=\"Probot's logo, a cartoon robot\" /></a>\n</p>\n<h3 align=\"center\"><a href=\"https://probot.github.io\">Probot</a></h3>\n<p align=\"center\">A framework for building GitHub Apps to automate and improve your workflow</p>\n<p align=\"center\"><a href=\"https://npmjs.com/package/probot\"><img src=\"https://badgen.net/npm/v/probot\" alt=\"npm\"></a> <a href=\"https://github.com/probot/probot/actions\"><img src=\"https://github.com/probot/probot/actions/workflows/test.yml/badge.svg\" alt=\"Build Status\"></a> <a href=\"https://twitter.com/ProbotTheRobot\"><img src=\"https://img.shields.io/twitter/follow/ProbotTheRobot\" alt=\"@ProbotTheRobot on Twitter\"></a>\n</p>\n\n---\n\nIf you've ever thought, \"wouldn't it be cool if GitHub could…\"; I'm going to stop you right there. Most features can actually be added via [GitHub Apps](https://docs.github.com/en/developers/apps), which extend GitHub and can be installed directly on organizations and user accounts and granted access to specific repositories. They come with granular permissions and built-in webhooks. Apps are first class actors within GitHub.\n\n## How it works\n\n**Probot is a framework for building [GitHub Apps](https://docs.github.com/en/developers/apps) in [Node.js](https://nodejs.org/)**, written in [TypeScript](https://www.typescriptlang.org/). GitHub Apps can listen to webhook events sent by a repository or organization. Probot uses its internal event emitter to perform actions based on those events. A simple Probot App might look like this:\n\n```js\nexport default (app) => {\n  app.on(\"issues.opened\", async (context) => {\n    const issueComment = context.issue({\n      body: \"Thanks for opening this issue!\",\n    });\n    return context.octokit.issues.createComment(issueComment);\n  });\n\n  app.onAny(async (context) => {\n    context.log.info({ event: context.name, action: context.payload.action });\n  });\n\n  app.onError(async (error) => {\n    app.log.error",
    "url": "https://github.com/probot/probot",
    "last_updated": "2025-08-30T11:17:57+00:00"
  },
  {
    "full_name": "ropensci-archive/wishlist",
    "name": "wishlist",
    "description": ":no_entry: ARCHIVED :no_entry: ",
    "language": "",
    "topics": [],
    "readme": "# wishlist\n\n[![Project Status: Abandoned](https://www.repostatus.org/badges/latest/abandoned.svg)](https://www.repostatus.org/#abandoned)\n\nThis repository has been archived. The former README is now in [README-NOT.md](README-NOT.md).\n",
    "url": "https://github.com/ropensci-archive/wishlist",
    "last_updated": "2023-01-28T00:55:49+00:00"
  },
  {
    "full_name": "jennybc/ggplot2-tutorial",
    "name": "ggplot2-tutorial",
    "description": "Teaching materials for the R package ggplot2",
    "language": "R",
    "topics": [],
    "readme": "ggplot2-tutorial\n================\n\nTeaching materials for the R package ggplot2\n\nOffered\n\n  * Thursday May 15, 2014 [under the auspices of the Vancouver R Users Group and hosted at the BC Centres for Disease Control](http://www.meetup.com/Vancouver-R-Users-Group-data-analysis-statistics/events/164489182/)\n  * Thursday May 14, 2015 [as part of the Workshop on Big Data in Environmental Science at UBC](http://www.pims.math.ca/scientific-event/150511-bdes)\n  * Tuesday July 19, 2016 [Data Carpentry at the University of Zurich](https://markrobinsonuzh.github.io/2016-07-18-zurich/)\n  \nThere are some slides.\n\n  * Directory for everything re: the slides: [ggplot2-tutorial-slides](../master/ggplot2-tutorial-slides/)\n    - Keynote file\n    - PDF\n    - Individual slides as PNGs (scroll through README to see 'em all)\n  * [Slides on Speakerdeck](https://speakerdeck.com/jennybc/ggplot2-tutorial)\n  \n  \nWe do live coding together. Indicative content:\n\n  * Scatterplots: [demo](../master/gapminder-ggplot2-scatterplot.md) | [R source](../master/gapminder-ggplot2-scatterplot.r)\n  * Stripplots: [demo](../master/gapminder-ggplot2-stripplot.md) | [R source](../master/gapminder-ggplot2-stripplot.r)\n  * Exploring distribution of a quantitative variable: [demo](../master/gapminder-ggplot2-univariate-quantitative.md) | [R source](../master/gapminder-ggplot2-univariate-quantitative.r)\n  * Drawing bars: [demo](../master/gapminder-ggplot2-univariate-factor.md) | [R source](../master/gapminder-ggplot2-univariate-factor.r)\n  * Change overall look and feel via themes: [demo](../master/gapminder-ggplot2-themes.md) | [R source](../master/gapminder-ggplot2-themes.r)\n  * Take control of a qualitative color scheme: [demo](../master/gapminder-ggplot2-colors.md) | [R source](../master/gapminder-ggplot2-colors.r)\n  * Bubble and line plots, lots of customization: [demo](../master/gapminder-ggplot2-shock-and-awe.md) | [R source](../master/gapminder-ggplot2-shock-and-awe.r)\n  \n  \nLinks and references\n\n  * All ",
    "url": "https://github.com/jennybc/ggplot2-tutorial",
    "last_updated": "2025-09-01T01:15:27+00:00"
  },
  {
    "full_name": "scrapy/scrapely",
    "name": "scrapely",
    "description": "A pure-python HTML screen-scraping library",
    "language": "HTML",
    "topics": [],
    "readme": "========\nScrapely\n========\n\n.. image:: https://api.travis-ci.org/scrapy/scrapely.svg?branch=master\n    :target: https://travis-ci.org/scrapy/scrapely\n\nScrapely is a library for extracting structured data from HTML pages. Given\nsome example web pages and the data to be extracted, scrapely constructs a\nparser for all similar pages.\n\nOverview\n========\n\nScrapinghub wrote a nice `blog post`_ explaining how scrapely works and how it's used in Portia_.\n\n.. _blog post: https://blog.scrapinghub.com/2016/07/07/scrapely-the-brains-behind-portia-spiders/\n.. _Portia: http://portia.readthedocs.io/\n\nInstallation\n============\n\nScrapely works in Python 2.7 or 3.3+.\nIt requires numpy and w3lib Python packages.\n\nTo install scrapely on any platform use::\n\n    pip install scrapely\n\nIf you're using Ubuntu (9.10 or above), you can install scrapely from the\nScrapy Ubuntu repos. Just add the Ubuntu repos as described here:\nhttp://doc.scrapy.org/en/latest/topics/ubuntu.html\n\nAnd then install scrapely with::\n\n    aptitude install python-scrapely\n\nUsage (API)\n===========\n\nScrapely has a powerful API, including a template format that can be edited\nexternally, that you can use to build very capable scrapers.\n\nWhat follows is a quick example of the simplest possible usage, that you can\nrun in a Python shell.\n\nStart by importing and instantiating the Scraper class::\n\n    >>> from scrapely import Scraper\n    >>> s = Scraper()\n\nThen, proceed to train the scraper by adding some page and the data you expect\nto scrape from there (note that all keys and values in the data you pass must\nbe strings)::\n\n    >>> url1 = 'http://pypi.python.org/pypi/w3lib/1.1'\n    >>> data = {'name': 'w3lib 1.1', 'author': 'Scrapy project', 'description': 'Library of web-related functions'}\n    >>> s.train(url1, data)\n\nFinally, tell the scraper to scrape any other similar page and it will return\nthe results::\n\n    >>> url2 = 'http://pypi.python.org/pypi/Django/1.3'\n    >>> s.scrape(url2)\n    [{u'author': [u'Django Software Fo",
    "url": "https://github.com/scrapy/scrapely",
    "last_updated": "2025-08-25T00:06:07+00:00"
  },
  {
    "full_name": "ropensci/bib2df",
    "name": "bib2df",
    "description": "Parse a BibTeX file to a tibble",
    "language": "R",
    "topics": [
      "bibtex",
      "r",
      "rstats",
      "r-package",
      "peer-reviewed"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/bib2df)](https://cran.r-project.org/package=bib2df) [![Travis-CI Build Status](https://travis-ci.org/ropensci/bib2df.svg?branch=master)](https://travis-ci.org/ropensci/bib2df) [![Build status](https://ci.appveyor.com/api/projects/status/6k3q7272ddnjh20o?svg=true)](https://ci.appveyor.com/project/ottlngr/bib2df) [![](http://cranlogs.r-pkg.org/badges/bib2df)](http://cran.rstudio.com/web/packages/bib2df/index.html) [![codecov](https://codecov.io/gh/ropensci/bib2df/branch/master/graph/badge.svg)](https://codecov.io/gh/ropensci/bib2df) [![](https://badges.ropensci.org/124_status.svg)](https://github.com/ropensci/onboarding/issues/124)\n\n`bib2df` - Parse a BibTeX file to a tibble\n------------------------------------------\n\nEveryone writing reports and articles with LaTeX has probably used BibTeX before. BibTeX is the de facto standard for reference management and grounds its functionality on a list of references stored in local text file. Depending on the reference type, several fields are necessary to define a reference properly. An exemplary BibTeX entry looks as follows:\n\n    @Article{Binmore2008,\n      Title = {Do Conventions Need to Be Common Knowledge?},\n      Author = {Binmore, Ken},\n      Journal = {Topoi},\n      Year = {2008},\n      Number = {1},\n      Pages = {17--27},\n      Volume = {27}\n    }\n\nParse the BibTeX file to a tibble\n---------------------------------\n\nThe BibTeX format is not convenient for any kind of analysis or visualization. Many R applications require a `data.frame` (or `tibble`) and `bib2df` offers a straightforward framework to parse a BibTeX file to a `tibble`.\n\n``` r\nlibrary(bib2df)\n\npath <- system.file(\"extdata\", \"LiteratureOnCommonKnowledgeInGameTheory.bib\", package = \"bib2df\")\n\ndf <- bib2df(path)\ndf\n#> # A tibble: 37 x 27\n#>    CATEGORY BIBTEXKEY ADDRESS ANNOTE AUTHOR BOOKTITLE CHAPTER CROSSREF\n#>    <chr>    ",
    "url": "https://github.com/ropensci/bib2df",
    "last_updated": "2025-07-30T17:01:38+00:00"
  },
  {
    "full_name": "jstray/media-attention",
    "name": "media-attention",
    "description": "Analysis of the relationship between media mentions and standing in the polls for the 2016 primary election.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# media-attention\nAnalysis of the relationship between media mentions and standing in the polls for the 2016 primary election.\nFrom the story [Do media mentions make the candidate? Digging into the data on attention and popularity](https://medium.com/@jonathanstray/does-media-attention-make-the-candidate-digging-into-the-data-bb4ba839e3c9)\n\nFiles\n* Generate Media Time Series.ipynb - Generates pollsMentionsQ4.csv, mentionsWeek.csv, pollsWeek.csv\n* GetMediaCloudMentions.py - use the MediaCloud.org API to get mentions time series\n* 2016-national-gop-primary.csv, 2016-national-democratic-primary.csv - downloaded from Huffinton Post Pollster\n",
    "url": "https://github.com/jstray/media-attention",
    "last_updated": "2023-01-21T02:40:52+00:00"
  },
  {
    "full_name": "wrathematics/ngram",
    "name": "ngram",
    "description": "Fast n-Gram Tokenization",
    "language": "C",
    "topics": [
      "r",
      "ngram",
      "text",
      "text-mining"
    ],
    "readme": "# ngram\n\n* **Version:** 3.2.2\n* **License:** [BSD 2-Clause](https://opensource.org/license/bsd-2-clause/)\n* **Project home**: https://github.com/wrathematics/ngram\n* **Bug reports**: https://github.com/wrathematics/ngram/issues\n* **Author:** Drew Schmidt and Christian Heckendorf\n\n\n**ngram** is an R package for constructing n-grams (\"tokenizing\"), as well as generating new text based on the n-gram structure of a given text input (\"babbling\").  The package can be used for serious analysis or for creating \"bots\" that say amusing things.  See details section below for more information.\n\nThe package is designed to be extremely fast at tokenizing, summarizing, and babbling tokenized corpora.  Because of the architectural design, we are also able to handle very large volumes of text, with performance scaling very nicely.  Benchmarks and example usage can be found in the package vignette.\n\n\n\n## Package Details\n\nThe original purpose for the package was to combine the book \"Modern Applied Statistics in S\" with the collected works of H. P. Lovecraft and generate amusing nonsense.  This resulted in the post [Modern Applied Statistics in R'lyeh](https://librestats.com/2014/07/01/modern-applied-statistics-in-rlyeh/).  I had originally tried several other available R packages to do this, but they were taking hours on a subset of the full combined corpus to preprocess the data into a somewhat inconvenient format.  However, the the ngram package can do the preprocessing into the desired format in well under a second (with about half of the preprocessing time spent on copying data for R coherency).\n\nThe package is mostly C, with the returned object (to R) being an external pointer.  In fact, the underlying C code can be compiled as a standalone library.  There is some minimal compatibility with exporting the data to proper R data structures, but it is incomplete at this time.\n\nFor more information, see the package vignette.\n\n\n\n## Installation\n\nYou can install the stable version from ",
    "url": "https://github.com/wrathematics/ngram",
    "last_updated": "2025-08-11T14:05:35+00:00"
  },
  {
    "full_name": "cisagov/dotgov-data",
    "name": "dotgov-data",
    "description": "Official list of .gov domains",
    "language": "Shell",
    "topics": [
      "gov",
      "dotgov"
    ],
    "readme": "# .gov data\n<img width=\"70\" alt=\"favicon\" src=\"https://github.com/cisagov/dotgov-data/assets/603901/7cf1f2e8-ed6d-4f3d-a5fd-813a1ec2b566\">\n\n\n\nThe [.gov top-level domain](https://get.gov) exists so that the online services of U.S.-based government organizations are easy to identify on the internet. In support of that goal, the .gov registry publishes information about our domains.\n\nThis repository contains the official, full list of registered domains in the .gov zone. The U.S. Government's executive, legislative, and judicial branches are represented, as are many state, territory, tribal, city, and county governments in the United States.\n\nThree files are updated daily (when there is activity):\n* [gov.txt](https://github.com/cisagov/dotgov-data/blob/main/gov.txt) – a copy of the .gov zone file\n* [current-full.csv](https://github.com/cisagov/dotgov-data/blob/main/current-full.csv) – a CSV of all domains in the zone, including federal domains\n* [current-federal.csv](https://github.com/cisagov/dotgov-data/blob/main/current-federal.csv) – a CSV of only federal domains in the zone\n\ncurrent-full.csv and current-federal.csv contain the same registered domains as the zone file, but instead of name server records they detail the registrant organization. They include all domains in the \"Ready\" and \"On hold\" states of our [registrar](https://github.com/cisagov/manage.get.gov/). \n\nEach file lists the \"second-level domains\" (e.g., get.gov) that are registered in the .gov zone; they do not list every _hostname_ (e.g., manage.get.gov) in use in the .gov namespace. This repo hosts several other files that include .gov hostnames, though they are not complete.\n\nNote that not all registered domains offer an online service (e.g., a website, an email server) at the domain. \n\n## Spot an issue?\n\n**This repo doesn't accept pull requests on the zone file or current-{full,federal}.csv**. If you manage a domain in these files and you notice that metadata about it is incorrect, log in to the [",
    "url": "https://github.com/cisagov/dotgov-data",
    "last_updated": "2025-09-02T07:12:09+00:00"
  },
  {
    "full_name": "datameet/maps",
    "name": "maps",
    "description": "Repository for all spatial data.",
    "language": "HTML",
    "topics": [
      "india",
      "opendata",
      "geospatial",
      "datameet"
    ],
    "readme": "Maps\n====\n\nRepository for all spatial data.\n\nFor details go to our project site at [http://projects.datameet.org/maps/](http://projects.datameet.org/maps/)\n\n----\n### Note on Format ###\n\nThis Repository contains Geospatial Data in [Shapefile Format](https://en.wikipedia.org/wiki/Shapefile), which is the defacto format for most GIS software.\n\nIn case you want this data in GeoJSON or KML, or any one of the myrid Vector formats, you could use Gdal's [ogr2ogr](http://www.gdal.org/ogr2ogr.html) tool to convert these shapefiles to your prefered format. This can be done by the following commands:\n\n`ogr2ogr -f GeoJSON <output name>.geojson <input name>.shp` &\n\n`ogr2ogr -f KML <output name>.kml <input name>.shp`  &\n\n`ogr2ogr -f LIBKML <output name>.kmz <input name>.shp`  \n\nIf you wish to convert these files using an Online too, you could look at: [MapSharper](http://www.mapshaper.org/); Do remeber to Zip up all the Files (*.shp, *.dbf, *.shx, *.prj) and import that into MapSharper. That way, you will get all attributes in the converted file.\n\n\n## Data License\n\nUnless explicitly stated, all datasets in this repository is shared under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license. Please mention and link to relevant dataset in the attribution, eg. *[India boundaries](https://github.com/datameet/maps/blob/master/Country/india-composite.geojson) by [DataMeet India community](http://datameet.org/) ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))*\n",
    "url": "https://github.com/datameet/maps",
    "last_updated": "2025-08-27T11:12:16+00:00"
  },
  {
    "full_name": "noamross/gams-in-r-course",
    "name": "gams-in-r-course",
    "description": "Generalized Additive Models in R: A Free Interactive Course",
    "language": "CSS",
    "topics": [],
    "readme": "<p align=\"center\">\n<img src=\"static/logo.svg\" align=\"center\" width=\"450px\"/>\n</p>\n\nWelcome! This is the source repository for **Generalized Additive Models in R: A Free Interactive Course**, hosted at: <https://noamross.github.io/gams-in-r-course>.\n\nThis course is based on material developed collaboratively over years of teaching workshops with <a href='http://converged.yt/'>David Miller</a>, <a href='https://www.fromthebottomoftheheap.net/'>Gavin L. Simpson</a>, <a href='https://ericpedersen3.wixsite.com/research'>Eric J. Pedersen</a>.\n\nThe site itself is built on an amazing framework created by <a href='https://ines.io/'>Ines Montani</a>, originally created for her [spaCy course](https://course.spacy.io).  The front-end is powered by\n[Gatsby](http://gatsbyjs.org/) and [Reveal.js](https://revealjs.com) and the\nback-end code execution uses [Binder](https://mybinder.org). <a href='https://florencia.netlify.com/'>Florencia D'Andrea</a> helped port the course materials and made the lovely logo.</p>\n\nIf you are interested in building a course on this framework, see Ines's starter repos for making courses in [Python](https://github.com/ines/course-starter-python) and [R](https://github.com/ines/course-starter-r), and her behind-the-scenes explanation of how the framework works at [the original course repo](https://github.com/ines/spacy-course#-faq).\n\nThe course material in this course is licensed [CC-BY](https://creativecommons.org/licenses/by/4.0/), meaning you are free to use it, change it, and remix it as long as you attribute me as the original course.  The _code_ is [MIT](https://opensource.org/licenses/MIT)-licensed.\n",
    "url": "https://github.com/noamross/gams-in-r-course",
    "last_updated": "2025-08-06T12:59:26+00:00"
  },
  {
    "full_name": "jlsutherland/doc2text",
    "name": "doc2text",
    "description": "Detect text blocks and OCR poorly scanned PDFs in bulk. Python module available via pip.",
    "language": "Python",
    "topics": [],
    "readme": "doc2text\n========\n\n.. image:: https://badge.fury.io/py/doc2text.svg\n    :target: https://badge.fury.io/py/doc2text\n\n|\n\n.. image:: docs/assets/images/news-button.png\n   :alt: Signup for Announcements\n   :target: http://eepurl.com/celDRz\n   :width: 500px\n\n.. image:: https://peachtree.ai/images/doc2text.png\n   :alt: doc2text Example\n   :target: https://peachtree.ai/images/doc2text.png\n   :width: 250px\n\n`doc2text` extracts higher quality text by fixing common scan errors\n--------------------------------------------------------------------\nDeveloping text corpora can be a massive pain in the butt. Much of the text data we are interested in as scientists are locked away in pdfs that are poorly scanned. These scans can be off kilter, poor resolution, have a hand in them... and if you OCR these scans without fixing these errors, the OCR doesn't turn out so well. `doc2text` was created to help researchers fix these errors and extract the highest quality text from\ntheir pdfs as possible.\n\n\n`doc2text` is super duper alpha atm\n-----------------------------------\n`doc2text` is developed and tested on Ubuntu 16.04 LTS Xenial Xerus. We do not pretend to serve all operating systems at the moment because that would be irresponsible. Please use this software with a huge grain of salt. We are currently working on:\n\n- Increasing the responsiveness of the text block identifier.\n- Optimizing the binarization for tesseract detection.\n- Identifying text in multiple columns (right now, treats as one big column).\n- Handling tables.\n- Many other optimizations.\n\nSupport and Contributions\n-------------------------\nIf you have feedback or would like to contribute, *please, please* submit a pull request or contact me at `joseph dot sutherland at columbia dot edu`.\n\n\nInstallation\n------------\nTo install the `doc2text` package, simply:\n\n.. code-block:: python\n\n   pip install doc2text\n\n`doc2text` relies on the `OpenCV <http://github.com/opencv/opencv>`_, `tesseract <http://github.com/tesseract-ocr/t",
    "url": "https://github.com/jlsutherland/doc2text",
    "last_updated": "2025-09-01T17:58:29+00:00"
  },
  {
    "full_name": "kevinushey/RcppParallel-user2016-talk",
    "name": "RcppParallel-user2016-talk",
    "description": "Slides + other resources for my talk on RcppParallel at useR! 2016.",
    "language": "HTML",
    "topics": [],
    "readme": "# RcppParallel\n\nThis repository houses the materials used for my talk at [useR! 2016](http://user2016.org/).\n\n[Click here](https://rawgit.com/kevinushey/RcppParallel-user2016-talk/master/slides.html) to quickly preview the slides online.\n\nVisit the [RcppParallel webpage](https://rcppcore.github.io/RcppParallel/) to learn more!\n\n",
    "url": "https://github.com/kevinushey/RcppParallel-user2016-talk",
    "last_updated": "2017-05-03T02:53:30+00:00"
  },
  {
    "full_name": "github/innovationgraph",
    "name": "innovationgraph",
    "description": "GitHub Innovation Graph",
    "language": "Python",
    "topics": [
      "data",
      "github",
      "open-data"
    ],
    "readme": "## GitHub Innovation Graph\n\nThis repo contains structured data files of public activity on GitHub, aggregated by economy on a quarterly basis from 2020 onward.\n\nThrough offerings such as the GitHub Innovation Graph, we hope to inform research and public policy that could benefit from data on software development activity globally. We welcome developers, data analysts, researchers, policymakers, and all other interested stakeholders to explore the data, discover insights, and create visualizations, among much more.\n\nThe GitHub Innovation Graph provides data on the following areas:\n\n- [Git Pushes](./data/git_pushes.csv)\n- [Developers](./data/developers.csv)\n- [Organizations](./data/organizations.csv)\n- [Repositories](./data/repositories.csv)\n- [Languages](./data/languages.csv)\n- [Licenses](./data/licenses.csv)\n- [Topics](./data/topics.csv)\n- [Economy Collaborators](./data/economy_collaborators.csv)\n\nSee the [datasheet](./docs/datasheet.md) for more information.\n\n### Exploring Innovation Graph data\nFor an overview of the dataset, check out the charts and tables at the [GitHub Innovation Graph website](https://innovationgraph.github.com/).\n\nTo dive deeper into the data and run your own analyses, feel free to fork this repo, explore the [structured data files](./data) using the exploratory data analysis tool of your choice, and share your findings in our [Discussions](#holder) page.\n\n### Limitations\n\nThe GitHub Innovation Graph dataset contains data on (1) public activity (2) on GitHub (3) aggregated by economy (4) on a quarterly basis. As such, this dataset would not be useful for understanding:\n\n1. private activity;\n2. outside of GitHub;\n3. at a more granular geographic level than economy; or\n4. at a more granular temporal level than quarterly.\n\nAdditionally, economies that have fewer developers on GitHub (which generally correlates with the population of an economy) will have less data associated with them in this dataset.\n\nSee the [datasheet](./docs/datasheet.md#inte",
    "url": "https://github.com/github/innovationgraph",
    "last_updated": "2025-08-31T10:38:58+00:00"
  },
  {
    "full_name": "gsbDBI/news_crowdsourcer",
    "name": "news_crowdsourcer",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "\n#### To give user-mode access to port 80:\n\n iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080\n\n### Exclusions\n\nEach HIT may specify a set of exclusions, a whitespace separated list of other HIT IDs such that if a worker has completed any of the HITs listed as exclusions she may not complete the HIT that listed those exlclusions. Perhaps an example would be most informative. Consider the following HITs:\n\n```xml\n  <hits>\n    <hit>\n      <hitid>1</hitid>\n      <exclusions>2</exclusions>\n      <tasks>\n\t1\n\t2\n      </tasks>\n    </hit>\n    <hit>\n      <hitid>2</hitid>\n      <tasks>\n\t1\n      </tasks>\n    </hit> \n  </hits>\n```\n\nIn this case, a worker who first completes HIT 2 may not then complete HIT 1, since HIT 1 excludes HIT 2. However, since HIT 2 lists no exclusions, a worker who first completes HIT 1 would then be permitted to complete HIT 2. There is no enforcement that exclusions be symmetric.\n\n\n### Scale Questions\n\nAfter the scale question type was dropped from a previous release, scale questions must now be added as categorical types. All questions may have an options tag, and the nested layout tag specifies whether the radio buttons are rendered horizontally or vertically (the latter being the default). Several other options are available, as in the following example:\n\n```xml\n<options>\n  <layout>horizontal</layout>\n  <lowLabel>Conservative</lowLabel>\n  <highLabel>Liberal</highLabel>\n  <outsideCategories>N/A</outsideCategories>\n  <outsideCategories>Unsure</outsideCategories>\n</options>\n```\n\nThis will yield the following layout:\n\n![new project](https://github.com/sgrondahl/news_crowdsourcer/raw/master/markdown/ScaleQuestion.PNG)\n\n\n### Nested Categorical Questions\n\nSee src/tests/test_xml_cat_expand_1.xml for example usage. To control nesting, encode nests in text tags, i.e. \n\n```xml\n<text> Hard News | Science and Tech | Computers </text>.\n```\n\nThe <value> will be sent along if the rightmost category is selected. It is possible to have asymm",
    "url": "https://github.com/gsbDBI/news_crowdsourcer",
    "last_updated": "2018-12-02T21:51:46+00:00"
  },
  {
    "full_name": "drsimonj/pipelearner",
    "name": "pipelearner",
    "description": "Tidy machine learning pipelines",
    "language": "R",
    "topics": [],
    "readme": "pipelearner\n================\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\npipelearner makes it easy to create machine learning pipelines in R.\n\nInstallation and background\n---------------------------\n\npipelearner is currently available from github as a development package only. It can be installed by running:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"drsimonj/pipelearner\")\n```\n\npipelearner is built on top of [tidyverse](https://github.com/tidyverse/tidyverse) packages like [modelr](https://github.com/hadley/modelr). To harness the full power of pipelearner, it will help to possess some technical knowledge of tidyverse tools such as:\n\n-   `%>%` the pipe operator from [magrittr](https://github.com/tidyverse/magrittr) package.\n-   tibbles from [tibble](https://github.com/tidyverse/tibble) package.\n-   `map()` and other iteration functions from [purrr](https://github.com/hadley/purrr) package.\n-   `resample` objects from [modelr](https://github.com/hadley/modelr) package.\n\nAn excellent resource to get started with these is [R for Data Science](http://r4ds.had.co.nz/), by Garrett Grolemund and Hadley Wickham.\n\nAPI\n---\n\nSimilar to the way ggplot2 elements are layered with `+`, you initialize and customize a pipelearner object, which is a list, with functions that can be piped into eachother with `%>%`. Rather than plotting, however, a pipelearner then learns.\n\n**Initialize** a pipelearner object with:\n\n-   `pipelearner()`\n\n**Customize** a pipelearner with:\n\n-   `learn_cvpairs()` to customize the cross-validation pairs.\n-   `learn_curves()` to customize the learning curves using incremental proportions of training data.\n-   `learn_models()` to add new learning models.\n\n**Learn** (fit) everything and obtain a tibble of results with:\n\n-   `learn()`\n\n### Initialization\n\nThe following initializes a pipelearner object that will use the `iris` data set and linear regression (`lm`) to learn how to predict `Sepal.Length` with all ot",
    "url": "https://github.com/drsimonj/pipelearner",
    "last_updated": "2025-07-24T07:57:07+00:00"
  },
  {
    "full_name": "anordal/shellharden",
    "name": "shellharden",
    "description": "The corrective bash syntax highlighter",
    "language": "Rust",
    "topics": [
      "lint",
      "syntax-highlighter",
      "policy"
    ],
    "readme": "<img src=\"img/logo.png\" align=\"right\"/>\n\n[![Build and test status](https://github.com/anordal/shellharden/workflows/build-and-tests/badge.svg?branch=master)](https://github.com/anordal/shellharden/actions)\n\nShellharden\n===========\n\nShellharden is a syntax highlighter and a tool to semi-automate the rewriting\nof scripts to ShellCheck conformance, mainly focused on quoting.\n\nThe default mode of operation is like `cat`, but with syntax highlighting in\nforeground colors and suggestive changes in background colors:\n\n![real-world example](img/ex-realworld.png)\n\nAbove: Selected portions of `xdg-desktop-menu` as highlighted by Shellharden.\nThe foreground colors are syntax highlighting, whereas the background colors\n(green and red) show characters that Shellharden would have added or removed\nif let loose with the `--transform` option.\nBelow: An artificial example that shows more tricky cases and special features.\n\n![artificial example](img/ex-artificial.png)\n\nWhy\n---\n\nA variable in bash is like a hand grenade – take off its quotes, and it starts ticking. Hence, rule zero of [bash pitfalls][1]: Always use quotes.\n\nName\n----\n\nShellharden can do what Shellcheck can't: Apply the suggested changes.\n\nIn other words, harden vulnerable shellscripts.\nThe builtin assumption is that the script does not *depend* on the vulnerable behavior –\nthe user is responsible for the code review.\n\nShellharden was previously known as \"Naziquote\".\nIn the right jargon, that was the best name ever,\nbut oh so misleading and unspeakable to outsiders.\n\nI couldn't call it \"bash cleaner\" either, as that means \"poo smearer\" in Norwegian.\n\nPrior art\n---------\n\n* [Shellcheck][2] is a wonderful tool to *detect*, and give general advice, about vulnerable bash code. The only thing missing is something to say yes with, and *apply* those advice (assuming proper review of course).\n\n* I asked [this SO question][3], for a tool that could rewrite bash scripts with proper quoting. One answerer beat me to it. But if it w",
    "url": "https://github.com/anordal/shellharden",
    "last_updated": "2025-08-31T12:07:03+00:00"
  },
  {
    "full_name": "dennybritz/deeplearning-papernotes",
    "name": "deeplearning-papernotes",
    "description": "Summaries and notes on Deep Learning research papers",
    "language": "",
    "topics": [],
    "readme": "#### 2018-02\n\n- The Matrix Calculus You Need For Deep Learning [[arXiv](https://arxiv.org/abs/1802.01528v2)]\n- Regularized Evolution for Image Classifier Architecture Search [[arXiv](https://arxiv.org/abs/1802.01548)]\n- Online Learning: A Comprehensive Survey [[arXiv](https://arxiv.org/abs/1802.02871)]\n- Visual Interpretability for Deep Learning: a Survey [[arXiv](https://arxiv.org/abs/1802.00614)]\n- Behavior is Everything – Towards Representing Concepts with Sensorimotor Contingencies [[paper](https://www.vicarious.com/wp-content/uploads/2018/01/AAAI18-pixelworld.pdf)] [[article](https://www.vicarious.com/2018/02/07/learning-concepts-through-sensorimotor-interactions/)] [[code](https://github.com/vicariousinc/pixelworld)]\n- IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures [[arXiv](https://arxiv.org/abs/1802.01561)] [[article](https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/)] [[code](https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30)]\n- DeepType: Multilingual Entity Linking by Neural Type System Evolution [[arXiv](https://arxiv.org/abs/1802.01021)] [[article](https://blog.openai.com/discovering-types-for-entity-disambiguation/)] [[code](https://github.com/openai/deeptype)]\n- DensePose: Dense Human Pose Estimation In The Wild [[arXiv](https://arxiv.org/abs/1802.00434)] [[article](http://densepose.org/)]\n\n#### 2018-01\n\n- Nested LSTMs [[arXiv](https://arxiv.org/abs/1801.10308)]\n- Generating Wikipedia by Summarizing Long Sequences [[arXiv](https://arxiv.org/abs/1801.10198)]\n- Scalable and accurate deep learning for electronic health records [[arXiv](https://arxiv.org/abs/1801.07860)]\n- Kernel Feature Selection via Conditional Covariance Minimization [[NIPS paper](https://papers.nips.cc/paper/7270-kernel-feature-selection-via-conditional-covariance-minimization.pdf)] [[article](http://bair.berkeley.edu/blog/2018/01/23/kernels/)] [[code](https://github.com/Jianbo-Lab/CCM)]\n- ",
    "url": "https://github.com/dennybritz/deeplearning-papernotes",
    "last_updated": "2025-08-30T10:41:45+00:00"
  },
  {
    "full_name": "thepracticaldev/orly-full-res",
    "name": "orly-full-res",
    "description": "Full resolution images of the O RLY book covers made by The Practical Dev",
    "language": "",
    "topics": [],
    "readme": "# Full Resolution O Rly Book Covers\nFull resolution images of the O RLY book covers made by [The Practical Dev](http://twitter.com/thepracticaldev)\n\nThese are parody images and are not associated with the actual [O'Reilly Media](http://www.oreilly.com/) company or brand.\n\nHere is the [O RLY Cover Generator](http://dev.to/rly).\n\nThese images can be used for non-commercial purposes under the [Attribution-NonCommercial 2.0 Generic](https://creativecommons.org/licenses/by-nc/2.0/)\n",
    "url": "https://github.com/thepracticaldev/orly-full-res",
    "last_updated": "2025-08-29T01:19:13+00:00"
  },
  {
    "full_name": "probml/sts-jax",
    "name": "sts-jax",
    "description": "Structural Time Series in JAX",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# sts-jax\nStructural Time Series (STS) in JAX\n\nThis library has a similar to design to [tfp.sts](https://www.tensorflow.org/probability/api_docs/python/tfp/sts),\nbut is built entirely in JAX,\nand uses the [Dynamax](https://github.com/probml/dynamax/tree/main/dynamax) library\nfor state-space models.\nWe also include an implementation of the\n[causal impact](https://google.github.io/CausalImpact/) method.\nThis has a similar to design to [tfcausalimpact](https://github.com/WillianFuks/tfcausalimpact),\nbut is built entirely in JAX.\n\n## Installation\n\nTo install the latest development branch:\n\n``` {.console}\npip install git+https://github.com/probml/sts-jax\n```\nor use\n``` {.console}\ngit clone git@github.com:probml/sts-jax.git\ncd sts-jax\npip install -e .\n```\n\n## What are structural time series (STS) models?\n\nThe STS model is a linear state space model with a specific structure. In particular,\nthe latent state $z_t$ is a composition of states of all latent components:\n\n$$z_t = [c_{1, t}, c_{2, t}, ...]$$\n\nwhere $c_{i,t}$ is the state of latent component $c_i$ at time step $t$.\n\nThe STS model (with scalar Gaussian observations) takes the form:\n\n$$y_t = H_t z_t + u_t + \\epsilon_t, \\qquad  \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2_t)$$\n\n$$z_{t+1} = F_t z_t + R_t \\eta_t, \\qquad \\eta_t \\sim \\mathcal{N}(0, Q_t)$$\n\nwhere\n\n* $y_t$: observation (emission) at time $t$.\n* $\\sigma^2_t$: variance of the observation noise.\n* $H_t$: emission matrix, which sums up the contributions of all latent components.\n* $u_t = x_t^T \\beta$: regression component from external inputs.\n* $F_t$: fixed transition matrix of the latent dynamics.\n* $R_t$: the selection matrix, which is a subset of columns of base vector $e_i$, converting\n    the non-singular covariance matrix into the (possibly singular) covariance matrix of\n    the latent state $z_t$.\n* $Q_t$: non-singular covariance matrix of the latent state, so the dimension of $Q_t$\n        can be smaller than the dimension of $z_t$.\n\nThe covariance matrix ",
    "url": "https://github.com/probml/sts-jax",
    "last_updated": "2025-08-03T07:26:52+00:00"
  },
  {
    "full_name": "soodoku/pcomp",
    "name": "pcomp",
    "description": "Replication Materials for \"The Parties in our Heads: Misperceptions About Party Composition and Their Consequences\"",
    "language": "Stata",
    "topics": [],
    "readme": "### Replication Materials for \"The Parties in our Heads: Misperceptions About Party Composition and Their Consequences\"\n\nAhler and Sood, 2017, The Journal of Politics\n\nSummary:\n---------\nThis folder contains data and scripts needed for replicating the results in \"The Parties in our Heads: Misperceptions About Party Composition and Their Consequences\" by Doug Ahler and Gaurav Sood, to be published in The Journal of Politics (doi:). For more information, please contact Doug Ahler (doug.ahler@gmail.com) or Gaurav Sood (gsood07@gmail.com).\n\nData\n-----\n* [pcomp_yougov_data.dta](data/pcomp_yougov_data.dta) --- data from YouGov Study\n* [pcomp_igspoll.dta](data/pcomp_igspoll.dta)--- data from IGS Poll\n* [affect_exp_data.dta](data/affect_exp_data.dta) --- data from MTurk Experiment to Assess Impact on Partisan Affect\n* [alt_exps_data.dta](data/alt_exps_data.dta)  --- data from MTurk Experiment to Assess Alternate Explanations\n* [extremity_exp_data.dta](data/extremity_exp_data.dta) --- data from MTurk Experiment to Assess Impact on Perceived Extremity\n* [anes_timeseries_2012.dta](data/anes_timeseries_2012.dta) --- data from the 2012 American National Election Studies\n* [Pew_Relig_P1.pdf](data/Pew_Relig_P1.pdf) and [Pew_Relig_P5.pdf](data/Pew_Relig_P5.pdf) --- sources for proportion of Atheists and Agnostics in the population and the Democratic Party.\n\nOther data files included in the folder are generated when the scripts are run. \n\nCodebook\n----------\n\n* [codebook.txt](codebook.txt)\n\nScripts\n--------\n\nRun pcomp_replication_main.do first. And then the R files for producing the figures.\n\n* [01_pcomp_replication_main.do](scripts/01_pcomp_replication_main.do) --- Code for reproducing all the tables and miscellaneous numbers cited in the main text of the paper and for producing the numbers that underlie some of the figures.\n* [02_fig1.R](scripts/02_fig1.R) --- Code for reproducing figure 1 in the main text\n* [03_fig2.R](scripts/03_fig2.R) --- Code for reproducing figure 2 in the m",
    "url": "https://github.com/soodoku/pcomp",
    "last_updated": "2025-09-01T10:31:26+00:00"
  },
  {
    "full_name": "open-machine-learning/cite-software",
    "name": "cite-software",
    "description": "Citing software and how to measure it.",
    "language": "Python",
    "topics": [],
    "readme": "# cite-software\nCiting software and how to measure it.\n\nMLOSS 2015 ICML workshop mini-hackathon\n---------------------------------------\n\nOur aim is to study software citations using ICML proceedings as an example.\n\nGoals / potential questions:\n\n1. Develop a tool for extracting software citations from papers.\n2. Analysing citation patterns: styles, counts, ...\n3. Detecting shared code links in papers.\n\nFor more details, please see the issues.\n",
    "url": "https://github.com/open-machine-learning/cite-software",
    "last_updated": "2015-07-12T22:54:59+00:00"
  },
  {
    "full_name": "rungalileo/bulk-labeling",
    "name": "bulk-labeling",
    "description": "A tool for quickly adding labels to unlabeled datasets",
    "language": "Python",
    "topics": [],
    "readme": "# bulk-labeling\nA tool for quickly adding labels to unlabeled datasets\n\nRunning on [streamlit!](https://rungalileo-bulk-labeling-app-0l2mzc.streamlitapp.com/)\n\n## How to use\nWe can walk through a simple example of going from an unlabeled dataset to some usable labels in just a few minutes\n\nFirst, go to the streamlit app above, or you can [run it locally](#run-locally)\n\nThen upload a csv file with your text. The only requirement of the file is that it must have a `text` column. Any other columns added can be used for coloring the embedding plot. If you don't have one, you can use the [conv-intent](https://github.com/rungalileo/bulk-labeling/blob/main/conv_intent.csv) dataset from this repo!\n\n![image](https://user-images.githubusercontent.com/22605641/212553133-1cb5342c-5636-4b8b-bae6-e811b6186614.png)\n\n\nOnce the embeddings have processed, you'll see your dataframe on the left and embeddings on the right. The dataframe view comes with an extra `text_length` column that you can sort by, or color the embeddings plot with (in case text length is useful to you).\n\nYou can filter with the text search (regex coming soon!) or, by lasso selecting embedding clusters from the chart. You can also color the chart and resize the points using the menu on the left\n\n![image](https://user-images.githubusercontent.com/22605641/193920464-bee6c734-6ad9-45cc-83e0-00dc5a27f4e4.png)\n\n\nSince we see some clear clusters already, let's start by investigating them. We can see one cluster with a lot of references to weather.\nLet's select this cluster\n\nhttps://user-images.githubusercontent.com/22605641/193921160-b024c2f4-3057-41e6-a200-b73bf258e6e9.mov\n\n\nConfirming that this is about weather, we can register a new label \"weather\" and assign our samples\n\nhttps://user-images.githubusercontent.com/22605641/193921485-a052dfdb-e905-4860-b01b-202dce04486a.mov\n\n\nThe UI will reset automatically. Let's look at another one. This cluster has a lot of references to bookings and reservations. Let's select that ",
    "url": "https://github.com/rungalileo/bulk-labeling",
    "last_updated": "2025-05-16T16:31:13+00:00"
  },
  {
    "full_name": "yishn/tikzcd-editor",
    "name": "tikzcd-editor",
    "description": "A simple visual editor for creating commutative diagrams.",
    "language": "JavaScript",
    "topics": [
      "category-theory",
      "tikzcd",
      "tikz",
      "diagram",
      "latex",
      "editor",
      "hacktoberfest"
    ],
    "readme": "# tikzcd-editor [![CI](https://github.com/yishn/tikzcd-editor/workflows/CI/badge.svg)](https://github.com/yishn/tikzcd-editor/actions)\n\nA simple visual editor for creating commutative diagrams.\n\nYou can\n[download the latest release](https://github.com/yishn/tikzcd-editor/releases)\nto host it on your own or [try it out here](https://tikzcd.yichuanshen.de/).\n\n![Screenshot](./screenshot.png)\n\n## Building\n\nMake sure you have [Node.js](https://nodejs.org/) and npm installed. First,\nclone this repository:\n\n```\n$ git clone https://github.com/yishn/tikzcd-editor\n$ cd tikzcd-editor\n```\n\nInstall dependencies with npm:\n\n```\n$ npm install\n```\n\nYou can build by using the `build` command:\n\n```\n$ npm run build\n```\n\nThis will create a minified bundle `bundle.js` and its source map. To launch,\nsimply open `index.html` in your favorite modern browser.\n\nUse the `watch` command for development:\n\n```\n$ npm run watch\n```\n\nTo create a self-contained archive file ready for distribution, run the\nfollowing command:\n\n```\n$ npm run dist\n```\n\nThis will create a folder and a `zip` file in the `dist` folder.\n\n## Contributing\n\nBug reports and pull requests are always welcome! Please consult the\n[issues list](https://github.com/yishn/tikzcd-editor/issues) for existing issues\nbeforehand.\n\nYou can also support this project by [donating](https://paypal.me/yishn/4).\n\n## Donators\n\nA big thanks to these lovely people:\n\n- Jeremy Rouse\n- Marko Rodriguez\n- Steve Heim\n- Max New\n- Bingyu Zhang\n- Ariella Lee\n\n## Related\n\n- [jsx-tikzcd](https://github.com/yishn/jsx-tikzcd) - Render tikzcd diagrams\n  with JSX.\n",
    "url": "https://github.com/yishn/tikzcd-editor",
    "last_updated": "2025-09-02T03:14:25+00:00"
  },
  {
    "full_name": "dgrtwo/ebbr",
    "name": "ebbr",
    "description": "Empirical Bayes binomial estimation",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\nebbr: Empirical Bayes on the Binomial in R\n----------\n\n**License**: [MIT](https://opensource.org/licenses/MIT)\n\n[![Travis-CI Build Status](https://travis-ci.org/dgrtwo/ebbr.svg?branch=master)](https://travis-ci.org/dgrtwo/ebbr)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/dgrtwo/ebbr?branch=master&svg=true)](https://ci.appveyor.com/project/dgrtwo/ebbr)\n[![Coverage Status](https://img.shields.io/codecov/c/github/dgrtwo/ebbr/master.svg)](https://codecov.io/github/dgrtwo/ebbr?branch=master)\n\nMethods for empirical Bayes shrinkage and estimation on data with many observations of success/total counts. These methods are described in [this series of blog posts on baseball batting averages](varianceexplained.org/r/ebbr-package), but can be applied to a variety of data types.\n\n### Installation\n\nYou can install the package from GitHub using [devtools](https://CRAN.R-project.org/package=devtools):\n\n\n```r\ndevtools::install_github(\"dgrtwo/ebbr\")\n```\n\n### Functions\n\n`ebbr` provides two types of functions: ones that fit particular models, and ones that add columns to data:\n\n* `ebb_fit_prior` fits a beta to a dataset of success/total counts using maximum likelihood estimation. It includes `tidy` to retrieve the alpha/beta parameters and `augment` to update observations with the prior.\n* `add_ebb_estimate` is a shortcut for performing `ebb_fit_prior` to fit a prior, then updating each observation to create a posterior.\n* `add_ebb_prop_test` performs an empirical Bayesian version of a one-sample or two-sample proportion test, comparing each observation in the data to either a fixed threshold or to another beta posterior.\n* `ebb_fit_mixture` fits a mixture of beta distributions as the prior.\n\n### Example\n\nSuppose we simulated some data from a beta-binomial model. Each observation has a true probability drawn from a beta distribution (with $$\\alpha=10;\\beta=40$$, and a mean of 20%).",
    "url": "https://github.com/dgrtwo/ebbr",
    "last_updated": "2023-11-21T17:12:42+00:00"
  },
  {
    "full_name": "airbnb/knowledge-repo",
    "name": "knowledge-repo",
    "description": "A next-generation curated knowledge sharing platform for data scientists and other technical professions.",
    "language": "Python",
    "topics": [
      "data",
      "data-science",
      "knowledge",
      "data-analysis"
    ],
    "readme": "# Knowledge Repo\n\n[![PyPI](https://img.shields.io/pypi/status/knowledge-repo.svg)]()\n[![license](https://img.shields.io/github/license/airbnb/knowledge-repo.svg)]()\n[![PyPI version](https://badge.fury.io/py/knowledge-repo.svg)](https://badge.fury.io/py/knowledge-repo)\n[![Python](https://img.shields.io/pypi/pyversions/knowledge-repo.svg?maxAge=2592000)](https://pypi.python.org/pypi/knowledge-repo)\n[![Build Status](https://github.com/airbnb/knowledge-repo/actions/workflows/main.yml/badge.svg)](https://github.com/airbnb/knowledge-repo/actions)\n[![PyPI downloads](https://static.pepy.tech/personalized-badge/knowledge-repo?period=total&units=international_system&left_color=black&right_color=brightgreen&left_text=total%20downloads)](https://pepy.tech/project/knowledge-repo)\n[![PyPI monthly downloads](https://static.pepy.tech/personalized-badge/knowledge-repo?period=month&units=international_system&left_color=black&right_color=brightgreen&left_text=downloads/month)](https://pepy.tech/project/knowledge-repo)\n\n## About the Knowledge Repo\nThe Knowledge Repo project aims to streamline the sharing of knowledge among data scientists and other technical roles \nby utilizing data formats and tools that are commonly used in these professions. Our platform offers various options for \nstoring and managing \"knowledge posts\", with a focus on utilizing notebooks (such as R Markdown and Jupyter/IPython \nNotebook) to better promote reproducible research.\n\n## Content Submission Options\n### 1. Github Integration: \nEasily submit your posts in markdown format directly through Github. Our platform will automatically \ndetect and publish your new content in a timely manner.\n\n### 2. Built-in Editor: \nUtilize our user-friendly editor to compose and upload your posts in various formats including Jupyter Notebook, R \nmarkdown, and Google document link. We securely store your content on our internal storage for easy access and management. \nUsers can make updates, delete, share, and add comments to thei",
    "url": "https://github.com/airbnb/knowledge-repo",
    "last_updated": "2025-08-31T20:01:57+00:00"
  },
  {
    "full_name": "GovLab/LegisLetters",
    "name": "LegisLetters",
    "description": "Coding space for the LegisLetters project.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# LegisLetters\n\nCoding space for the LegisLetters project.\n\n### Installation\n\nDocker preferred.\n\n    ./build.sh\n    ./run.sh\n    ./exec.sh\n\nYou will then be able to run all relevant scripts\n\n### Usage\n\nInside the docker container, this will add documents to the Elasticsearch\ndatabase:\n\n    python legisletters/scraper.py\n\nThis will try to parse them into separate fields:\n\n    python legisletters/parser.py\n\n### Rebuilding\n\nYou'll need `npm`, `bower`, and `grunt` to build.\n\n    npm install -g grunt-cli bower\n    npm install\n    bower install\n    grunt\n\nThis will place updated HTML & JS in the `dist` folder, which is served by the\ncontainer `nginx`.\n\n### Contributions\n\nPlease make sure all files pass `pylint` and `pyflakes`.\n",
    "url": "https://github.com/GovLab/LegisLetters",
    "last_updated": "2018-01-29T05:05:47+00:00"
  },
  {
    "full_name": "janfreyberg/superintendent",
    "name": "superintendent",
    "description": "Practical active learning in python",
    "language": "Python",
    "topics": [],
    "readme": "# Superintendent\n\n[![PyPI version](https://badge.fury.io/py/superintendent.svg)](https://badge.fury.io/py/superintendent)\n[![Tests](https://github.com/janfreyberg/superintendent/actions/workflows/test.yml/badge.svg)](https://github.com/janfreyberg/superintendent/actions/workflows/test.yml)\n[![Documentation Status](https://readthedocs.org/projects/superintendent/badge/?version=latest)](https://superintendent.readthedocs.io/en/latest/?badge=latest)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/janfreyberg/superintendent/master)\n[![Coverage Status](https://coveralls.io/repos/github/janfreyberg/superintendent/badge.svg)](https://coveralls.io/github/janfreyberg/superintendent)\n\n\n---\n\n![](docs/img/logo.png)\n\n**`superintendent`** provides an `ipywidget`-based interactive labelling tool\nfor your data. It allows you to flexibly label all kinds of data. It also allows\nyou to combine your data-labelling task with a statistical or machine learning\nmodel to enable quick and practical active learning.\n\n## Getting started\n\nTake a look at the documentation: http://www.janfreyberg.com/superintendent/\n\nIt has some explanations of how the library works, and it also has many\nexamples.\n\nIf you'd like to try the library without installing it, check out the\n[repository on binder](https://mybinder.org/v2/gh/janfreyberg/superintendent/master?filepath=examples.ipynb).\n\n## Installation\n\n```\npip install superintendent\n```\n\nIf you want to also use the keyboard shortcuts for labelling faster, you will\nalso have to enable the ipyevents jupyter extension:\n\n```\njupyter nbextension enable --py --sys-prefix ipyevents\n```\n\nIf you also want to run the examples, you need three additional packages:\n`requests`, `bs4` and `wordcloud`. You can install them via pip by running:\n\n```\npip install superintendent[examples]\n```\n\nIf you want to contribute to `superintendent`, you will",
    "url": "https://github.com/janfreyberg/superintendent",
    "last_updated": "2025-07-17T14:57:25+00:00"
  },
  {
    "full_name": "vipul-sharma20/document-scanner",
    "name": "document-scanner",
    "description": "An OpenCV based document scanner",
    "language": "Python",
    "topics": [],
    "readme": "document-scanner\n================\n\nA document scanner built using OpenCV + Python.\nI highly recommend to see my blog post for better understanding: [http://vipulsharma20.blogspot.on](http://vipulsharma20.blogspot.in)\n\nMy sincere thanks to the article and the author here: [http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/](http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/) which has some really good set of articles on OpenCV and way more informative.\n\n* Original Image\n![Alt](http://1.bp.blogspot.com/-gbFBHQKFU7w/VpGzzVfxmLI/AAAAAAAAEks/jtX2fikHs5o/s1600/Original.jpg \"original\")\n\n* Grayscaled Image\n![Alt](http://2.bp.blogspot.com/--LzOU44-dhM/VpG2B6DJbxI/AAAAAAAAEk4/4BGXnfhvsk4/s1600/Original%2BGray.jpg \"gray\")\n\n* Gaussian Blur\n![Alt](http://2.bp.blogspot.com/-KfEWWzIXxBg/VpG2RY0upjI/AAAAAAAAElA/psuYvv1rnm0/s1600/Original%2BBlurred.jpg \"blurred\")\n\n* Edge Detection (Canny Edge Detection)\n![Alt](http://3.bp.blogspot.com/-5TVP2UFeGXk/VpG5-bIYNqI/AAAAAAAAElM/zmyNrbvnh8Q/s1600/Original%2BEdged.jpg \"edge\")\n\n* Contour Detection\n![Alt](http://1.bp.blogspot.com/-Es0PkMvJJxU/VpHcQEXzXaI/AAAAAAAAElc/NuCZmuW1K6o/s1600/Outline_all.jpg \"contour\")\n\n* Approximated Contour\n![Alt](http://4.bp.blogspot.com/-DL7XWsLvWg8/VpHeN6bA3gI/AAAAAAAAElo/1TMug5_tCeQ/s1600/Outline.jpg \"approx\")\n\n* Perspective Transform\n![Alt](http://4.bp.blogspot.com/-1dhSo9PrD6o/VpHjhgH0viI/AAAAAAAAEl4/AzYqjzLiNbI/s1600/dst.jpg \"transform\")\n\n",
    "url": "https://github.com/vipul-sharma20/document-scanner",
    "last_updated": "2025-08-25T12:19:12+00:00"
  },
  {
    "full_name": "cgivre/drillworkshop",
    "name": "drillworkshop",
    "description": "Repository for the Apache Drill Workshop",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Apache Drill Workshop\nWelcome to the Apache Drill workshop by Dark Labs!  This 180 minute workshop will cover:\n* Overview of Apache Drill\n* Querying simple data\n* Querying nested data\n* Connecting external data sources\n* Writing User Defined Functions (UDFs)\n* Using Python to interact with Drill\n\nIf you have any questions, please contact me at: givre_charles@bah.com.  \nDon't forget to check out my blog at http://thedataist.com\n\n## Drill Version\nFor this class we will be using a special \"pre-release\" version of Drill which is available here: https://drive.google.com/open?id=0Bygp_V8Ca-cpZWhYMkxoTlpJYUU.\n",
    "url": "https://github.com/cgivre/drillworkshop",
    "last_updated": "2025-03-22T11:19:55+00:00"
  },
  {
    "full_name": "USEPA/elevatr",
    "name": "elevatr",
    "description": "An R package for accessing elevation data",
    "language": "R",
    "topics": [
      "elevation-data",
      "mapzen-elevation-service",
      "digital-elevation-model",
      "r-language",
      "r-package",
      "rstats",
      "epa",
      "elevatr"
    ],
    "readme": "\n[![R build\nstatus](https://github.com/jhollist/elevatr/workflows/R-CMD-check/badge.svg)](https://github.com/jhollist/elevatr/actions)\n[![](https://www.r-pkg.org/badges/version/elevatr)](https://www.r-pkg.org/pkg/elevatr)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/elevatr)](https://www.r-pkg.org/pkg/elevatr)\n[![Codecov test\ncoverage](https://codecov.io/gh/jhollist/elevatr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/jhollist/elevatr?branch=main)\n[![DOI](https://zenodo.org/badge/65325400.svg)](https://zenodo.org/badge/latestdoi/65325400)\n\n# Key information about version 0.99.0 and upcoming versions of `elevatr`\n\nSeveral major changes have been made to `elevatr` in response to the\nretirement of legacy spatial packages (see\n<https://r-spatial.org/r/2023/05/15/evolution4.html> for details).\nVersion 0.99.0 has switched to using `sf` and `terra` for all data\nhandling; however, in this version a `raster RasterLayer` is still\nreturned from `get_elev_raster()`. Additional changes are planned for\nversion 1+, most notably the return for `get_elev_raster()` will be a\n`terra SpatRaster`. Please plan accordingly for your analyses and/or\npackages account for this change.\n\n# elevatr\n\nAn R package for accessing elevation data from various sources\n\nThe `elevatr` package currently provides access to elevation data from\nAWS Open Data [Terrain\nTiles](https://registry.opendata.aws/terrain-tiles/) and the Open\nTopography [Global datasets\nAPI](https://opentopography.org/developers#API) for raster digital\nelevation models. For point elevation data,the [USGS Elevation Point\nQuery Service](https://apps.nationalmap.gov/epqs/)) may be used or the\npoint elevations may be derived from the AWS Tiles. Additional elevation\ndata sources may be added as they come available.\n\nCurrently this package includes just two primary functions to access\nelevation web services:\n\n- `get_elev_point()`: Get point elevations using the USGS Elevation\n  Point Query Service (for the U",
    "url": "https://github.com/USEPA/elevatr",
    "last_updated": "2025-08-29T01:58:28+00:00"
  },
  {
    "full_name": "stan-dev/loo",
    "name": "loo",
    "description": "loo R package for approximate leave-one-out cross-validation (LOO-CV) and Pareto smoothed importance sampling (PSIS)",
    "language": "R",
    "topics": [
      "cross-validation",
      "stan",
      "r-package",
      "bayesian-data-analysis",
      "information-criterion",
      "bayesian-methods",
      "model-comparison",
      "bayesian",
      "bayesian-inference",
      "bayesian-statistics",
      "bayes"
    ],
    "readme": "# loo <img src=\"man/figures/stanlogo.png\" align=\"right\" width=\"120\" />\n\n<!-- badges: start -->\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/loo?color=blue)](https://cran.r-project.org/web/packages/loo)\n[![RStudio_CRAN_mirror_downloads_badge](https://cranlogs.r-pkg.org/badges/loo?color=blue)](https://cran.r-project.org/web/packages/loo)\n[![codecov](https://codecov.io/gh/stan-dev/loo/branch/master/graph/badge.svg)](https://codecov.io/github/stan-dev/loo?branch=master)\n[![R-CMD-check](https://github.com/stan-dev/loo/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/stan-dev/loo/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n### Efficient approximate leave-one-out cross-validation for fitted Bayesian models\n\n__loo__ is an R package that allows users to compute efficient approximate\nleave-one-out cross-validation for fitted Bayesian models, as well as model\nweights that can be used to average predictive distributions. \nThe __loo__ package package implements the fast and stable computations for \napproximate LOO-CV and WAIC from\n\n* Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model \nevaluation using leave-one-out cross-validation and WAIC. \n_Statistics and Computing_. 27(5), 1413--1432. \ndoi:10.1007/s11222-016-9696-4. [Online](https://link.springer.com/article/10.1007/s11222-016-9696-4), \n[arXiv preprint arXiv:1507.04544](https://arxiv.org/abs/1507.04544).\n\nand computes model weights as described in\n\n* Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018). Using\nstacking to average Bayesian predictive distributions. In Bayesian\nAnalysis, doi:10.1214/17-BA1091. \n[Online](https://projecteuclid.org/euclid.ba/1516093227),\n[arXiv preprint arXiv:1704.02030](https://arxiv.org/abs/1704.02030).\n\nFrom existing posterior simulation draws, we compute approximate LOO-CV using\nPareto smoothed importance sampling (PSIS), a new procedure for regularizing\nimportance weights. As a byproduct of our calculations, we also obtain\n",
    "url": "https://github.com/stan-dev/loo",
    "last_updated": "2025-09-01T16:24:03+00:00"
  },
  {
    "full_name": "rstudio/plumber",
    "name": "plumber",
    "description": "Turn your R code into a web API.",
    "language": "R",
    "topics": [
      "plumber",
      "r",
      "api",
      "api-server"
    ],
    "readme": "# plumber <a href='https://www.rplumber.io/'><img src='man/figures/logo.svg' align=\"right\" height=\"138.5\" style=\"margin:10px;\" /></a>\n\n<!-- badges: start -->\n[![R build status](https://github.com/rstudio/plumber/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/plumber/actions)\n[![CRAN version](https://www.r-pkg.org/badges/version/plumber)](https://www.r-pkg.org/pkg/plumber)\n[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/plumber?color=brightgreen)](https://www.r-pkg.org/pkg/plumber)\n[![codecov](https://app.codecov.io/gh/rstudio/plumber/branch/main/graph/badge.svg)](https://app.codecov.io/gh/rstudio/plumber)\n[![RStudio community](https://img.shields.io/badge/community-plumber-blue?style=social&logo=rstudio&logoColor=75AADB)](https://forum.posit.co/tag/plumber)\n<!-- badges: end -->\n\nPlumber allows you to create a web API by merely decorating your existing R\nsource code with `roxygen2`-like comments. Take a look at an example.\n\n```r\n# plumber.R\n\n#* Echo back the input\n#* @param msg The message to echo\n#* @get /echo\nfunction(msg=\"\") {\n  list(msg = paste0(\"The message is: '\", msg, \"'\"))\n}\n\n#* Plot a histogram\n#* @serializer png\n#* @get /plot\nfunction() {\n  rand <- rnorm(100)\n  hist(rand)\n}\n\n#* Return the sum of two numbers\n#* @param a The first number to add\n#* @param b The second number to add\n#* @post /sum\nfunction(a, b) {\n  as.numeric(a) + as.numeric(b)\n}\n```\n\nThese comments allow `plumber` to make your R functions available as API\nendpoints. You can use either `#*` as the prefix or `#'`, but we recommend the\nformer since `#'` will collide with `roxygen2`.\n\n```r\nlibrary(plumber)\n# 'plumber.R' is the location of the file shown above\npr(\"plumber.R\") %>%\n  pr_run(port=8000)\n```\n\nYou can visit this URL using a browser or a terminal to run your R function and\nget the results. For instance\n`http://localhost:8000/plot` will show you a\nhistogram, and\n`http://localhost:8000/echo?msg=hello`\nwill echo back the 'hello' message you prov",
    "url": "https://github.com/rstudio/plumber",
    "last_updated": "2025-08-25T16:31:06+00:00"
  },
  {
    "full_name": "Uberi/speech_recognition",
    "name": "speech_recognition",
    "description": "Speech recognition module for Python, supporting several engines and APIs, online and offline.",
    "language": "Python",
    "topics": [
      "python",
      "audio",
      "speech-recognition",
      "speech-to-text"
    ],
    "readme": "SpeechRecognition\r\n=================\r\n\r\n.. image:: https://img.shields.io/pypi/v/SpeechRecognition.svg\r\n    :target: https://pypi.python.org/pypi/SpeechRecognition/\r\n    :alt: Latest Version\r\n\r\n.. image:: https://img.shields.io/pypi/status/SpeechRecognition.svg\r\n    :target: https://pypi.python.org/pypi/SpeechRecognition/\r\n    :alt: Development Status\r\n\r\n.. image:: https://img.shields.io/pypi/pyversions/SpeechRecognition.svg\r\n    :target: https://pypi.python.org/pypi/SpeechRecognition/\r\n    :alt: Supported Python Versions\r\n\r\n.. image:: https://img.shields.io/pypi/l/SpeechRecognition.svg\r\n    :target: https://pypi.python.org/pypi/SpeechRecognition/\r\n    :alt: License\r\n\r\n.. image:: https://api.travis-ci.org/Uberi/speech_recognition.svg?branch=master\r\n    :target: https://travis-ci.org/Uberi/speech_recognition\r\n    :alt: Continuous Integration Test Results\r\n\r\nLibrary for performing speech recognition, with support for several engines and APIs, online and offline.\r\n\r\n**UPDATE 2022-02-09**: Hey everyone! This project started as a tech demo, but these days it needs more time than I have to keep up with all the PRs and issues. Therefore, I'd like to put out an **open invite for collaborators** - just reach out at me@anthonyz.ca if you're interested!\r\n\r\nSpeech recognition engine/API support:\r\n\r\n* `CMU Sphinx <http://cmusphinx.sourceforge.net/wiki/>`__ (works offline)\r\n* Google Speech Recognition\r\n* `Google Cloud Speech API <https://cloud.google.com/speech/>`__\r\n* `Wit.ai <https://wit.ai/>`__\r\n* `Microsoft Azure Speech <https://azure.microsoft.com/en-us/services/cognitive-services/speech/>`__\r\n* `Microsoft Bing Voice Recognition (Deprecated) <https://www.microsoft.com/cognitive-services/en-us/speech-api>`__\r\n* `Houndify API <https://houndify.com/>`__\r\n* `IBM Speech to Text <http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/speech-to-text.html>`__\r\n* `Snowboy Hotword Detection <https://snowboy.kitt.ai/>`__ (works offline)\r\n* `Tensorflow <https://www.tensorflow.o",
    "url": "https://github.com/Uberi/speech_recognition",
    "last_updated": "2025-09-01T16:02:26+00:00"
  },
  {
    "full_name": "alteryx/locust-grasshopper",
    "name": "locust-grasshopper",
    "description": "a load testing tool extended from locust",
    "language": "Python",
    "topics": [
      "load-testing",
      "performance",
      "performance-metrics",
      "performance-testing",
      "pytest",
      "pytest-plugin",
      "python"
    ],
    "readme": "<div id=\"top\"></div>\n\n# Grasshopper\n\nA lightweight framework for performing load tests against an environment, primarily \nagainst an API. Grasshopper glues [Locust](https://locust.io/), [Pytest](https://docs.pytest.org/en/7.1.x/#), some plugins (namely [Locust InfluxDBListener](https://github.com/hoodoo-digital/locust-influxdb-listener) ) and some custom code to provide a\npackage that makes authoring load tests simple with very little boilerplate needed.\n\nHere are some key functionalities that this project extends on Locust:\n- [checks](#checks)\n- [custom trends](#custom-trends)\n- [timing thresholds](#thresholds)\n- [streamlined metric reporting/tagging system](#db-reporting)\n  (only influxDB is supported right now)\n\n## Installation\nThis package can be installed via pip: `pip install locust-grasshopper`\n\n## Example Load Test\n- You can refer to the test `test_example.py` in the `example` directory for a basic \n  skeleton of how to get a load test running. In the same directory, there is also an \n  example `conftest.py` that will show you how to get basic parameterization working.\n- This test can be invoked by running `pytest example/test_example.py` in the root of \n  this project.\n- This test can also be invoked via a YAML scenario file:\n```shell\ncd example\npytest example_scenarios.YAML --tags=example1\n```\n In this example scenario file, you can see how grasshopper_args, \n grasshopper_scenario_args, and tags are being set.\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Creating a load test\nWhen creating a new load test, the primary grasshopper function you will be using \nis called `Grasshopper.launch_test`. This function can be imported like so: `from grasshopper.lib.grasshopper import Grasshopper`\n`launch_test` takes in a wide variety of args:\n- `user_classes`: User classes that the runner will run. These user classes must \n  extend BaseJourney, which is a grasshopper class \n  (`from grasshopper.lib.journeys.base_journey import BaseJourney`). This can be a ",
    "url": "https://github.com/alteryx/locust-grasshopper",
    "last_updated": "2025-08-25T18:33:29+00:00"
  },
  {
    "full_name": "hrbrmstr/cc",
    "name": "cc",
    "description": "⛏Extract metadata of a specific target based on the results of \"commoncrawl.org\"",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "common-crawl",
      "domains",
      "urls",
      "reconnaissance",
      "recon",
      "r-cyber"
    ],
    "readme": "# cc\n\nExtract metadata of a specific target based on the results of \"commoncrawl.org\"\n\nAn R clone of [`cc.py`](https://github.com/si9int/cc.py) with some differences:\n\n- The Common Crawl is a free service but it costs real money to run so this doesn't abuse it and forces the caller to be the abuser vs enabling it with bad default behaviour\n- You get all the CDX metadata back in [ndjson](http://ndjson.org/) format which can be easily processed with `jq`, scripts or compiled/interpred languages\n\nThis is a command-line utility vs a package since we don't have enough R-based command-line utility pacakges. Eventually, this will be a package with a command-line utility installer.\n\n## Install\n\n    git clone git@gitlab.com:hrbrmstr/cc\n    cd cc\n\n    # ensure needed pkgs are installed and that cc.R is executable\n    Rscript preflight.R # or ./preflight.R if the git clone didn't mangle the executable status\n\n## Usage\n\nHere are the available options:\n\n    usage: ./cc.R [--] [--help] [--list] [--opts OPTS] [--domain DOMAIN] [--out OUT] [--index INDEX]\n\n    Extract metadata of a specific target based on the results of \"commoncrawl.org\"\n\n    Examples:\n\n    $ ./cc.R --list                                      # list indices\n    $ ./cc.R --domain github.com                         # defaults to most recent index\n    $ ./cc.R --domain github.com --out /tmp/gh.json      # specify an oputput file\n    $ ./cc.R --index CC-MAIN-2018-34 --domain github.com # specify which index\n\n\n    flags:\n      -h, --help      show this help message and exit\n      -l, --list      list all available indexes\n\n    optional arguments:\n      -x, --opts OPTS     RDS file containing argument values\n      -d, --domain DOMAIN     domain which will be crawled\n      -o, --out OUT     specify an output file (default: domain.json)\n      -i, --index INDEX     use a specific index file\n\n## Example\n\n    ./cc.R --domain \"hilton.com\" --out ~/Data/hilton.json \n\nOR\n\n    Rscript cc.R --domain \"hilton.com\" --out ~/Data/hilto",
    "url": "https://github.com/hrbrmstr/cc",
    "last_updated": "2025-03-22T11:03:35+00:00"
  },
  {
    "full_name": "astamm/nloptr",
    "name": "nloptr",
    "description": "nloptr provides an R interface to NLopt, a free/open-source library for nonlinear optimization providing a common interface to a number of different optimization routines which can handle nonlinear constraints and lower and upper bounds for the controls.",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# [**nloptr**](https://astamm.github.io/nloptr/) <img src='man/figures/logo.png' align=\"right\" height=\"139\" />\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/astamm/nloptr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/astamm/nloptr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/astamm/nloptr/graph/badge.svg)](https://app.codecov.io/gh/astamm/nloptr)\n[![pkgdown](https://github.com/astamm/nloptr/workflows/pkgdown/badge.svg)](https://github.com/astamm/nloptr/actions)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/nloptr)](https://CRAN.R-project.org/package=nloptr)\n<!-- badges: end -->\n\n[**nloptr**](https://astamm.github.io/nloptr/) is an R interface to\n[NLopt](https://nlopt.readthedocs.io/en/latest/), a free/open-source\nlibrary for nonlinear optimization started by Steven G. Johnson,\nproviding a common interface for a number of different free optimization\nroutines available online as well as original implementations of various\nother algorithms. It can be used to solve general nonlinear programming\nproblems with nonlinear constraints and lower and upper bounds for the\ncontrols, such as\n\n$$ \\min_{x \\in \\mathbb{R}^n} \\quad f(x), $$\n\ns.t. $g(x) \\le 0$, $h(x) = 0$ and $\\ell \\le x \\le u$.\n\nThe [NLopt](https://nlopt.readthedocs.io/en/latest/) library is\navailable under the GNU Lesser General Public License (LGPL), and the\ncopyrights are owned by a variety of authors. See the\n[website](https://nlopt.readthedocs.io/en/latest/Citing_NLopt/) for\ninformation on how to cite NLopt and the algorithms you use.\n\n## Installation\n\n### Windows\n\nOn Windows, for old versions of R (`R <= 4.1.x`), the *nlopt* `v2.7.1`\nfrom [rwinlib](https://github.com/rwinlib/nlopt) is used. For newer\nversions of R (`R >= 4.2.0`), the *nlopt* version from the corresponding\n`RTools` toolchain is used.\n\n### Linux and macOS\n\nOn Unix-like platforms, we use `pkg-confi",
    "url": "https://github.com/astamm/nloptr",
    "last_updated": "2025-08-19T15:01:44+00:00"
  },
  {
    "full_name": "zeno-ml/zeno-build",
    "name": "zeno-build",
    "description": "Build, evaluate, understand, and fix LLM-based apps",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Zeno Build\n\n[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n[![Discord](https://img.shields.io/discord/1086004954872950834)](https://discord.gg/km62pDKAkE)\n[![Open Zeno](https://img.shields.io/badge/%20-Open_Zeno-612593.svg?labelColor=white&logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzMiIGhlaWdodD0iMzMiIHZpZXdCb3g9IjAgMCAzMyAzMyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTMyIDE1Ljc4NDJMMTYuNDg2MiAxNS43ODQyTDE2LjQ4NjIgMC4yNzA0MDFMMjQuMzAyIDguMDg2MTdMMzIgMTUuNzg0MloiIGZpbGw9IiM2MTI1OTMiLz4KPHBhdGggZD0iTTE1Ljc5MTcgMTUuODMxMUw4LjAzNDc5IDguMDc0MjJMMTUuNzkxNyAwLjMxNzMyOEwxNS43OTE3IDE1LjgzMTFaIiBmaWxsPSIjNjEyNTkzIiBmaWxsLW9wYWNpdHk9IjAuOCIvPgo8cGF0aCBkPSJNMTQuODY1NSAxNS44MzExTDcuNTk0ODUgMTUuODMxMUw3LjU5NDg1IDguNTYwNDJMMTQuODY1NSAxNS44MzExWiIgZmlsbD0iIzYxMjU5MyIgZmlsbC1vcGFjaXR5PSIwLjYiLz4KPHBhdGggZD0iTTYuMTEyOSAxNS44MzExTDMuMjQxNyAxNS44MzExTDMuMjQxNyAxMi44NjcyTDYuMTEyOSAxNS44MzExWiIgZmlsbD0iIzZBMUI5QSIgZmlsbC1vcGFjaXR5PSIwLjQiLz4KPHBhdGggZD0iTTIuNzMyMjggMTUuODMxTDEuNTE1NSAxNC42MTQzTDIuNzQyNzEgMTMuMzg3TDIuNzMyMjggMTUuODMxWiIgZmlsbD0iIzZBMUI5QSIgZmlsbC1vcGFjaXR5PSIwLjMiLz4KPHBhdGggZD0iTTIuMDM3NiAxNS43ODQyTDEuMTU3NzEgMTUuNzg0MkwxLjE1NzcxIDE0Ljk1MDZMMi4wMzc2IDE1Ljc4NDJaIiBmaWxsPSIjNkExQjlBIiBmaWxsLW9wYWNpdHk9IjAuMiIvPgo8cGF0aCBkPSJNMC44MzM1NjggMTUuNzg0MUwwLjUwOTM5OSAxNS40NkwwLjgzMzU2NyAxNS4xMzU4TDAuODMzNTY4IDE1Ljc4NDFaIiBmaWxsPSIjNjEyNTkzIiBmaWxsLW9wYWNpdHk9IjAuMSIvPgo8cGF0aCBkPSJNMC4xMDYxODcgMTUuNzk0NEwwLjMwMTAyNSAxNS41OTk2TDAuNDk1ODYzIDE1Ljc5NDRIMC4xMDYxODdaIiBmaWxsPSIjNjEyNTkzIiBmaWxsLW9wYWNpdHk9IjAuMSIvPgo8cGF0aCBkPSJNNi45NTIxMyAxNS44MjQ4TDMuNjQwOTkgMTIuNTEzN0w2Ljk2OTYzIDkuMTg1MDNMNi45NTIxMyAxNS44MjQ4WiIgZmlsbD0iIzYxMjU5MyIgZmlsbC1vcGFjaXR5PSIwLjUiLz4KPHBhdGggZD0iTTAuMjk0MjM1IDE2LjQ3OTVMMTUuODA4IDE2LjQ3OTVMMTUuODA4IDMxLjk5MzNMNy45OTIyMyAyNC4xNzc1TDAuMjk0MjM1IDE2LjQ3OTVaIiBmaWxsPSIjNjEyNTkzIi8+CjxwYXRoIGQ9Ik0xNi40OTU2IDE3LjI0MzZMMjMuODUwNyAyNC41ODVMMTYuNDk1NiAzMS45NEwxNi40OTU2ID",
    "url": "https://github.com/zeno-ml/zeno-build",
    "last_updated": "2025-07-30T02:52:05+00:00"
  },
  {
    "full_name": "BenLangmead/c-cpp-notes",
    "name": "c-cpp-notes",
    "description": "Lecture notes and example code for teaching C & C++",
    "language": "Python",
    "topics": [],
    "readme": "# C & C++ programming notes\n\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)\n\n* Author: Ben Langmead\n\nThese are lecture notes and example code for teaching C &amp; C++.  The slides themselves are written in a custom \"runnable Markdown\" dialect for C/C++.  These files have extension `.cppmd`.  The script that \"compiles\" `.cppmd` files is `units/cppmd_render.py`.  This script:\n\n* Generates C/C++ source files based on specially-formatted comments in the Markdown (`<!---cppmd-file ... -->`)\n* Runs shell commands based on specially-formatted comments in the Markdown (`<!---cppmd-shell ... -->`), often in order to compile and run the source code\n    * Possibly integrates the output from the shell back into the slides, so that output can be included in final rendered doc\n* Uses `pandoc` & `beamer` to render a PDF\n\nOutputs consist of (a) a PDF of the slides, (b) source files for all the examples.\n\nAll the shell commands are run within a Docker image.  We use this image from Docker Hub:\n\n* [`benlangmead/fedora-cpp-slides`](https://hub.docker.com/r/benlangmead/fedora-cpp-slides/)\n\nAnd the corresponding `Dockerfile` and scripts used to build/pull/push are available in the `docker` subdirectory.  The base image is Fedora 27.  The most relevant software versions are:\n\n```\ngcc-7.2.1\ng++-7.2.1\ngdb: Fedora 8.0.1-33.fc27\nvalgrind-3.13.0\ngit v2.14.3\n```\n\n### Philosophy\n\nThe reality of programming in C and C++ can seem divorced from the conceptual discussions in texts.  In these notes, explaining concepts is always intertwined with showing examples.  When examples are shown, I give exact code and exact command lines, executed in a predictable environment, so that students can recreate the conditions exactly.  I show examples that do work and others that don't.  The idea is never to stray too far from a code example.\n",
    "url": "https://github.com/BenLangmead/c-cpp-notes",
    "last_updated": "2025-08-08T10:52:40+00:00"
  },
  {
    "full_name": "dswah/pyGAM",
    "name": "pyGAM",
    "description": "[CONTRIBUTORS WELCOME] Generalized Additive Models in Python",
    "language": "Python",
    "topics": [
      "machine-learning",
      "data-science",
      "scientific-computing",
      "python",
      "interpretable-machine-learning",
      "gams",
      "explainable-ai",
      "explainable-ml",
      "interpretable-ai",
      "interpretable-ml"
    ],
    "readme": "\n\n# pyGAM\n\n<a href=\"https://pygam.readthedocs.io/en/latest/?badge=latest\"><img src=imgs/pygam_tensor.png width=\"250\" align=\"right\" /></a>\n\nGeneralized Additive Models in Python.\n\n:rocket: **Version 0.10.1 out now!** [See release notes here](https://github.com/dswah/pyGAM/releases).\n\n`pyGAM` is a package for building Generalized Additive Models in Python, with an emphasis on modularity and performance.\n\nThe API is designed for users of `scikit-learn` or `scipy`.\n\n\n|  | **[Documentation](https://pygam.readthedocs.io/en/latest/?badge=latest)** · **[Tutorials](https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html)** · **[Medium article](https://medium.com/just-another-data-scientist/building-interpretable-models-with-generalized-additive-models-in-python-c4404eaf5515)** |\n|---|---|\n| **Open&#160;Source** | [![Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/dswah/pygam/blob/main/LICENSE) [![GC.OS Sponsored](https://img.shields.io/badge/GC.OS-Sponsored%20Project-orange.svg?style=flat&colorA=0eac92&colorB=2077b4)](https://gc-os-ai.github.io/) |\n| **Community** | [![!discord](https://img.shields.io/static/v1?logo=discord&label=discord&message=chat&color=lightgreen)](https://discord.gg/Rt8By5Jj) [![!slack](https://img.shields.io/static/v1?logo=linkedin&label=LinkedIn&message=news&color=lightblue)](https://www.linkedin.com/company/german-center-for-open-source-ai) |\n| **CI/CD** | [![github-actions](https://img.shields.io/github/actions/workflow/status/dswah/pygam/pypi.yml?logo=github)](https://github.com/dswah/pygam/actions/workflows/pypi.yml) [![readthedocs](https://img.shields.io/readthedocs/pygam?logo=readthedocs)](https://pygam.readthedocs.io/en/latest/?badge=latest) |\n| **Code** |  [![!pypi](https://img.shields.io/pypi/v/pygam?color=orange)](https://pypi.org/project/pygam/) [![!conda](https://img.shields.io/conda/vn/conda-forge/pygam)](https://anaconda.org/conda-forge/pygam) [![!python-versions](https://img.shield",
    "url": "https://github.com/dswah/pyGAM",
    "last_updated": "2025-08-27T02:54:46+00:00"
  },
  {
    "full_name": "google/tacotron",
    "name": "tacotron",
    "description": "Audio samples accompanying publications related to Tacotron, an end-to-end speech synthesis model.",
    "language": "HTML",
    "topics": [
      "machine-learning",
      "tts",
      "speech",
      "audio",
      "prosody",
      "tacotron"
    ],
    "readme": "This repository contains audio samples accompanying publications related to\nTacotron, an end-to-end speech synthesis model from the Sound Understanding and\nBrain teams at Google.\n\nThis is not an official Google product.\n",
    "url": "https://github.com/google/tacotron",
    "last_updated": "2025-08-28T15:20:04+00:00"
  },
  {
    "full_name": "sms2015/Manhattan-Real-Estate-Project",
    "name": "Manhattan-Real-Estate-Project",
    "description": "Shiny app and related files to display Manhattan Real Estate data from 2003 to 2014",
    "language": "R",
    "topics": [],
    "readme": "# Manhattan-Real-Estate-Project\nShiny app and related files to display Manhattan Real Estate data from 2003 to 2014\n",
    "url": "https://github.com/sms2015/Manhattan-Real-Estate-Project",
    "last_updated": "2023-01-10T23:10:05+00:00"
  },
  {
    "full_name": "conradbez/hstream",
    "name": "hstream",
    "description": "Hyper Stream",
    "language": "Python",
    "topics": [],
    "readme": "# HStream\n\nConvert your script to interactive python web app like so:\n\n    user_said = hs.text_input(\"What would you like to say:\")\n\nPowered by [Django](https://github.com/django/django) + [htmx](https://github.com/bigskysoftware/htmx) enables easy app ejection to scale/extend once you've outgrown HStream. Inspired by [Streamlit](https://github.com/streamlit/streamlit).\n\n## Usage\n\n`pip install hstream`\n\n`hstream init # populates example.py`\n\n`hstream run example.py`\n\n![hstream demo](./demo/example.png)\n\n## Motivation\n\nWrite beautiful user interfaces that enable quick iteration for Proof-of-Concept (PoC) python scripts, without the need to start over when we go to production.\n\nLove Streamlit but:\n\n- impossible to customise beyond PoC phase\n- hard to reason about when extending and deploying\n- non-standard approach doesn't play nicely with existing ecosystems\n\nH-(html)-Stream is built with semantic html, Django and htmx to provide a fast and simple framework for rapid web app development that follows traditional frontend/server architecture (or at least follow it closer than Streamlit).\n\n## Some features that excite us\n\n- [Eject to a Django (traditional web app)](docs/features/eject.md)\n- Display pandas dataframes, plots, markdown and more! [see supported components](docs/features/components.md)\n\n[Some examples]((./demo))\n\n## Technologies\n\nBig thanks to the following libraries in particular\n\n\n- Streamlit\n- htmx\n- Yattag\n- pico css\n- Django\n\n## Backlog (WIP)\n\n- [x] live server reload on file change (through univorn)\n- [x] semantic html and basic html manipulation from within script\n- [x] basic components - see below\n- [x] swap stylesheet\n- [x] complex html manipulation from within script (setting attributes)\n- [x] plotly plot support\n- [x] select component\n- [x] multi select component\n- [ ] auto ssl certs for easy deployment\n- [ ] example component architecture\n- [ ] reload browser on code change\n",
    "url": "https://github.com/conradbez/hstream",
    "last_updated": "2025-08-04T11:10:55+00:00"
  },
  {
    "full_name": "sc-zhang/bioplotz",
    "name": "bioplotz",
    "description": "A plot package for bioinformatics",
    "language": "Python",
    "topics": [],
    "readme": "# bioplotz: A package for plotting images for bioinformatics\n\n[![Latest PyPI version](https://img.shields.io/pypi/v/bioplotz.svg)](https://pypi.python.org/pypi/bioplotz)\n[![Downloads](https://static.pepy.tech/badge/bioplotz)](https://pepy.tech/project/bioplotz)\n\n## Dependencies\n\nPython modules:\n\n- numpy\n- matplotlib\n- pandas\n\n## Installation\n\n### Install via pip\n\n```bash\npip install bioplotz\n```\n\n### Install from source code\n\n```bash\npip install git+https://github.com/sc-zhang/bioplotz.git --user\n```\n\n## Usage\n\n### Manhattan Plot\n\n```python\nimport bioplotz as bp\n\nfig, ax = bp.manhattan(data, threshold=0, color=['orange', 'green'], threshold_line_color='blue', log_base=0,\n                       reverse=False, xtick_labels=True, ytick_labels=True, ax=None, marker='.', s=1, **kwargs)\n```\n\n| parameter                | value type    | explain                                                                                                                                                                                                                                                                                                                                                              |\n|--------------------------|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **data**                 | dict<br>list  | **dict** key: block name<br>&ensp;&ensp;&ensp;&ensp;value: [[x1,x2,...,xn], [y1,y2,...,yn]]<br>**list** is a list like: [[x1,y1], [x2, y2], ..., [xn, yn]]                                                                                                                                                                                                    ",
    "url": "https://github.com/sc-zhang/bioplotz",
    "last_updated": "2025-08-15T06:57:42+00:00"
  },
  {
    "full_name": "NicolasHug/Surprise",
    "name": "Surprise",
    "description": "A Python scikit for building and analyzing recommender systems",
    "language": "Python",
    "topics": [
      "recommender",
      "systems",
      "recommendation",
      "svd",
      "matrix",
      "factorization",
      "machine-learning"
    ],
    "readme": "[![GitHub version](https://badge.fury.io/gh/nicolashug%2FSurprise.svg)](https://badge.fury.io/gh/nicolashug%2FSurprise)\n[![Documentation Status](https://readthedocs.org/projects/surprise/badge/?version=stable)](https://surprise.readthedocs.io/en/stable/?badge=stable)\n[![python versions](https://img.shields.io/badge/python-3.8+-blue.svg)](https://surpriselib.com)\n[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.02174/status.svg)](https://doi.org/10.21105/joss.02174)\n\n[![logo](./logo_black.svg)](https://surpriselib.com)\n\nOverview\n--------\n\n[Surprise](https://surpriselib.com) is a Python\n[scikit](https://projects.scipy.org/scikits.html) for building and analyzing\nrecommender systems that deal with explicit rating data.\n\n[Surprise](https://surpriselib.com) **was designed with the\nfollowing purposes in mind**:\n\n- Give users perfect control over their experiments. To this end, a strong\n  emphasis is laid on\n  [documentation](https://surprise.readthedocs.io/en/stable/index.html), which we\n  have tried to make as clear and precise as possible by pointing out every\n  detail of the algorithms.\n- Alleviate the pain of [Dataset\n  handling](https://surprise.readthedocs.io/en/stable/getting_started.html#load-a-custom-dataset).\n  Users can use both *built-in* datasets\n  ([Movielens](https://grouplens.org/datasets/movielens/),\n  [Jester](https://eigentaste.berkeley.edu/dataset/)), and their own *custom*\n  datasets.\n- Provide various ready-to-use [prediction\n  algorithms](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)\n  such as [baseline\n  algorithms](https://surprise.readthedocs.io/en/stable/basic_algorithms.html),\n  [neighborhood\n  methods](https://surprise.readthedocs.io/en/stable/knn_inspired.html), matrix\n  factorization-based (\n  [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_a",
    "url": "https://github.com/NicolasHug/Surprise",
    "last_updated": "2025-09-02T06:54:16+00:00"
  },
  {
    "full_name": "tidyverse/forcats",
    "name": "forcats",
    "description": "🐈🐈🐈🐈: tools for working with categorical variables (factors)",
    "language": "R",
    "topics": [
      "r",
      "factor",
      "tidyverse"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# forcats <img src='man/figures/logo.png' align=\"right\" height=\"139\" />\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/forcats)](https://cran.r-project.org/package=forcats)\n[![R-CMD-check](https://github.com/tidyverse/forcats/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/forcats/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/forcats/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/forcats)\n<!-- badges: end -->\n\n## Overview\n\nR uses **factors** to handle categorical variables, variables that have\na fixed and known set of possible values. Factors are also helpful for\nreordering character vectors to improve display. The goal of the forcats\npackage is to provide a suite of tools that solve common problems with\nfactors, including changing the order of levels or the values. Some\nexamples include:\n\n- `fct_reorder()`: Reordering a factor by another variable.\n- `fct_infreq()`: Reordering a factor by the frequency of values.\n- `fct_relevel()`: Changing the order of a factor by hand.\n- `fct_lump()`: Collapsing the least/most frequent values of a factor\n  into “other”.\n\nYou can learn more about each of these in `vignette(\"forcats\")`. If\nyou’re new to factors, the best place to start is the [chapter on\nfactors](https://r4ds.hadley.nz/factors.html) in R for Data Science.\n\n## Installation\n\n    # The easiest way to get forcats is to install the whole tidyverse:\n    install.packages(\"tidyverse\")\n\n    # Alternatively, install just forcats:\n    install.packages(\"forcats\")\n\n    # Or the the development version from GitHub:\n    # install.packages(\"pak\")\n    pak::pak(\"tidyverse/forcats\")\n\n## Cheatsheet\n\n<a href=\"https://raw.githubusercontent.com/rstudio/cheatsheets/main/factors.pdf\"><img src=\"https://github.com/rstudio/cheatsheets/raw/main/pngs/thumbnails/forcats-cheatsheet-thumbs.png\" width=\"320\" height=\"",
    "url": "https://github.com/tidyverse/forcats",
    "last_updated": "2025-09-01T02:03:07+00:00"
  },
  {
    "full_name": "christophergandrud/Rep-Res-Book",
    "name": "Rep-Res-Book",
    "description": "Source code files for the book Reproducible Research with R/RStudio",
    "language": "TeX",
    "topics": [],
    "readme": "\n# Reproducible Research with R and RStudio (Third Edition)\n\n[<img src=\"img/re-res-book-cover-3rd.png\" align=\"right\" />](http://amzn.com/1498715370)\n\nChristopher Gandrud\n\n[CRC Press/Chapman &\nHall](http://www.tandf.net/books/details/9781498715379/)\n\nThe files in this repository comprise the source code for creating\n**Reproducible Research with R and RStudio**.\n\n### File Organization\n\n### Reproduce the Book\n\nThe book can be reproduced by using the R package *bookdown*. To do\nthis:\n\n### Session Info\n\nThe current version of the book manuscript was compiled with\n[RStudio](http://www.rstudio.com/) (v. 1.2.5019 preview build) with the\nfollowing R session:\n\n    ## R version 3.6.2 (2019-12-12)\n    ## Platform: x86_64-apple-darwin15.6.0 (64-bit)\n    ## Running under: macOS Catalina 10.15.2\n    ## \n    ## Matrix products: default\n    ## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib\n    ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\n    ## \n    ## locale:\n    ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n    ## \n    ## attached base packages:\n    ## [1] stats     graphics  grDevices utils     datasets  methods   base     \n    ## \n    ## loaded via a namespace (and not attached):\n    ##  [1] compiler_3.6.2  magrittr_1.5    tools_3.6.2     htmltools_0.4.0\n    ##  [5] yaml_2.2.0      Rcpp_1.0.3      stringi_1.4.3   rmarkdown_2.0  \n    ##  [9] knitr_1.26      stringr_1.4.0   xfun_0.11       digest_0.6.23  \n    ## [13] rlang_0.4.2     evaluate_0.14\n\n-----\n\n( c ) Christopher Gandrud (2020)\n",
    "url": "https://github.com/christophergandrud/Rep-Res-Book",
    "last_updated": "2025-07-04T15:24:11+00:00"
  },
  {
    "full_name": "spark-mooc/mooc-setup",
    "name": "mooc-setup",
    "description": "Information for setting up for the BerkeleyX Spark Intro MOOC, and lab assignments for the course",
    "language": "Python",
    "topics": [],
    "readme": "# mooc-setup\nInformation for setting up for the Spark MOOC, and lab assignments for the course.\n",
    "url": "https://github.com/spark-mooc/mooc-setup",
    "last_updated": "2025-08-12T15:18:59+00:00"
  },
  {
    "full_name": "kbroman/ProgrammingNotes",
    "name": "ProgrammingNotes",
    "description": "My personal notes on various programming languages and tools",
    "language": "R",
    "topics": [],
    "readme": "## Programming Notes\n\nThis is a set of text files with my personal notes related to various\nprogramming languages and tools.\n\nSome of this may be specific to my own particular setup (for example,\nthat I'm mostly working on a Mac).\n\n---\n\n[![CC0 badge](https://i.creativecommons.org/p/zero/1.0/88x31.png)](https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "url": "https://github.com/kbroman/ProgrammingNotes",
    "last_updated": "2025-09-01T13:49:24+00:00"
  },
  {
    "full_name": "ColinFay/proustr",
    "name": "proustr",
    "description": "Tools for Natural Language Processing in French and texts from Marcel Proust's collection \"A La Recherche Du Temps Perdu\"",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/proustr)](https://cran.r-project.org/package=proustr)\n[![Travis-CI Build\nStatus](https://travis-ci.org/ColinFay/proustr.svg?branch=master)](https://travis-ci.org/ColinFay/proustr)\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/ColinFay/proustr?branch=master&svg=true)](https://ci.appveyor.com/project/ColinFay/proustr)\n[![Coverage\nstatus](https://codecov.io/gh/ColinFay/proustr/branch/master/graph/badge.svg)](https://codecov.io/github/ColinFay/proustr?branch=master)\n[![Rdoc](http://www.rdocumentation.org/badges/version/proustr)](http://www.rdocumentation.org/packages/proustr)\n![Metacran](http://www.r-pkg.org/badges/version-last-release/proustr)\n![Metacran](https://cranlogs.r-pkg.org/badges/proustr)\n\n![lifecycle](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)\n\n`proustr` is now on\n[CRAN](https://CRAN.R-project.org/package=proustr).\n\n## Tools for Natural Language Processing in French and texts from Marcel Proust’s collection “A La Recherche Du Temps Perdu”\n\n<p align=\"center\">\n\n<img src=\"inst/hex-proustr.png\" width = \"250\">\n\n</p>\n\nThis package gives you access to tools designed to do Natural Language\nProcessing in French. You can use these tools with the books from Marcel\nProust “À la recherche du temps perdu”, which are provided in this\npackage. Of course, these tools can be expanded to almost all french\ntexts.\n\nAll the functions from this package are consistent with the tidyverse\nphilosophy.\n\nHere is a list of all the books contained in this pacakage :\n\n  - Du côté de chez Swann (1913): `ducotedechezswann`.\n  - À l’ombre des jeunes filles en fleurs (1919):\n    `alombredesjeunesfillesenfleurs`.\n  - Le Côté de Guermantes (1921): `lecotedeguermantes`.\n  - Sodome et Gomorrhe (1922) : `sodomeetgomorrhe`.\n  - La Prisonnière (1923) :`laprisonniere`.\n  - Albertine disparue (1925, also know as : La Fugit",
    "url": "https://github.com/ColinFay/proustr",
    "last_updated": "2022-08-31T03:47:31+00:00"
  },
  {
    "full_name": "kbodwin/demoR",
    "name": "demoR",
    "description": "(this package has been replaced by \"flair\")",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\ndemoR\n=====\n\nTo get started, please refer to [this vignette](https://web.calpoly.edu/~kbodwin/demoR/articles/demoR.html).\n\n<!-- badges: start\n[![CRAN status](https://www.r-pkg.org/badges/version/demoR)](https://cran.r-project.org/package=demoR)-->\nOverview\n--------\n\ndemoR is a set of tools for formatting `R` code for presentations, teaching demonstrations, and much more:\n\n-   The `demo_*()` functions prepare your source code to be both evaluated and displayed by `knitr::knit()`.\n\n-   The `hlt_*()` functions add text display formatting to printed code.\n\nThese combine naturally within code chunks in R Markdown files via code chunk options. You can learn more about them in `vignette(\"demoR\")`.\n\nInstallation\n------------\n\n<!--\n\n```r\n# The easiest way to get demoR is the basic installation:\ninstall.packages(\"demoR\")\n```\nNot on CRAN yet!\n-->\n### Development version\n\nCurrently, only the development version of demoR is available. You can install this by running\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"kbodwin/demoR\")\n```\n\nUsage\n-----\n\n``` r\nlibrary(demoR)\ndata(iris)\ncodestring <- \"mean(iris$Sepal.Length, na.rm = TRUE)\"\n\nhlt_funs(codestring)\n#> [1] \"<pre class='prettyprint'><code><span style='background-color:#ffff7f'>mean</span>(iris$Sepal.Length, na.rm = TRUE)</code></pre>\"\n\nhlt_args(codestring)\n#> [1] \"<pre class='prettyprint'><code>mean(iris$Sepal.Length, <span style='background-color:#ffff7f'>na.rm</span> = TRUE)</code></pre>\"\n\ndemo_code(codestring) %>% hlt_args()\n```\n\n<pre class='prettyprint'><code>mean(iris$Sepal.Length, <span style='background-color:#ffff7f'>na.rm</span> = TRUE)</code></pre>\n    #> [1] 5.843333\n\nGetting help\n------------\n\nIf you encounter a clear bug, please file a minimal reproducible example on [github](https://github.com/kbodwin/demoR/issues).\n\n<!-- This doesn't apply to us, right?\nFor questions and other discussion, please use [community.rstudio.com](ht",
    "url": "https://github.com/kbodwin/demoR",
    "last_updated": "2024-02-06T13:36:57+00:00"
  },
  {
    "full_name": "newslynx/newslynx-core",
    "name": "newslynx-core",
    "description": "The API and Data Collection tasks that power NewsLynx",
    "language": "Python",
    "topics": [],
    "readme": "[![travis-img](https://travis-ci.org/newslynx/newslynx-core.svg)](https://travis-ci.org/newslynx/newslynx-core)\n# newslynx-core\n\nNewsLynx Core is an expandable open-source toolkit for building modular content analytics workflows. It provides a fully RESTful API as well as a comprehensive `python` client and command line interface.\n\nNewsLynx Core was built to power [`newslynx-app`](http://github.com/newslynx/newslynx-app) but is capable of powering a diverse range of potential applications, as well, including:\n\n* A Mention.net-like pipeline for your personal or company blog.\n* A Flexible timeseries store for content metrics which will automatically summarize and compare your data, as well as enable the additional of custom, computed metrics.\n* A framework for configuring, scheduling, and monitoring arbitrary ``python`` jobs via API.\n* A content-extraction API. \n\n## Installation\n\nFor most applications, refer to our [installation guide](http://newslynx.readthedocs.org/en/latest/install.html). If you'd like to setup a development environment, following the instructions below for MacOS X.  If you'd like to spin up a Virtual Machine, check out our [automation guide](https://github.com/newslynx/automation).\n\n### Dependencies\n\n#### Postgres\n\n**NOTE** We recommend using [Postgres APP](http://postgresapp.com/). However, if you prefer the `brew` distribution, make sure to install it with plpythonu.\n\n```\n$ brew install postgresql --build-from-source --with-python\n```\n\n(Re)create a `postgresql` database\n\n```shell\n# If you already have a database called `newslynx`, delete it first\n$ dropdb newslynx \n$ createdb newslynx\n````\n\n#### Redis\n\nInstall `redis`:\n\n```shell\n$ brew install redis\n```\n\nOpen another tab in your shell and run:\n\n```\n$ redis-server\n```\n\n### Installation / Initialization \n\n**NOTE** we recommend that you install `newslynx` in a [virtual environment](http://docs.python-guide.org/en/latest/dev/virtualenvs/).\n\nFirst clone this repository and move into it's root directo",
    "url": "https://github.com/newslynx/newslynx-core",
    "last_updated": "2024-06-10T03:02:46+00:00"
  },
  {
    "full_name": "Zettlr/Zettlr",
    "name": "Zettlr",
    "description": "Your One-Stop Publication Workbench",
    "language": "TypeScript",
    "topics": [
      "office",
      "productivity",
      "electron",
      "offline",
      "desktop",
      "markdown",
      "pdf",
      "html",
      "libreoffice",
      "docx",
      "zettlr",
      "macos",
      "pandoc",
      "languages",
      "linux",
      "windows",
      "nodejs"
    ],
    "readme": "<h1 align=\"center\">\n  <a href=\"https://github.com/Zettlr/Zettlr\">\n    <img src=\"https://raw.githubusercontent.com/Zettlr/Zettlr/master/resources/icons/png/256x256.png\" alt=\"Zettlr\"/>\n  </a>\n  <br/>\n  Zettlr [<em>ˈset·lər</em>]\n</h1>\n\n<p align=\"center\"><strong>Your One-Stop Publication Workbench</strong>.</p>\n\n<p align=\"center\">\n  <a href=\"https://doi.org/10.5281/zenodo.2580173\">\n    <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.2580173.svg\" alt=\"DOI\">\n  </a>\n  <a href=\"https://www.gnu.org/licenses/gpl-3.0\">\n    <img src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" alt=\"License: GNU GPL v3\">\n  </a>\n  <a href=\"https://www.zettlr.com/download\">\n    <img alt=\"GitHub tag (latest by date)\" src=\"https://img.shields.io/github/tag-date/Zettlr/Zettlr.svg?label=latest\">\n  </a>\n  <img alt=\"GitHub All Releases\" src=\"https://img.shields.io/github/downloads/Zettlr/Zettlr/total.svg\">\n  <img alt=\"Unit Tests / Lint\" src=\"https://github.com/Zettlr/Zettlr/actions/workflows/check.yml/badge.svg\">\n  <img alt=\"Build\" src=\"https://github.com/Zettlr/Zettlr/workflows/Build/badge.svg\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.zettlr.com/\" target=\"_blank\">Homepage</a> |\n  <a href=\"https://www.zettlr.com/download\">Download</a> |\n  <a href=\"https://docs.zettlr.com/\" target=\"_blank\">Documentation</a> |\n  <a href=\"https://forum.zettlr.com/\" target=\"_blank\">Community Forum</a> |\n  <a href=\"https://go.zettlr.com/discord\" target=\"_blank\">Discord</a> |\n  <a href=\"#contributing\">Contributing</a> |\n  <a href=\"https://zettlr.com/supporters\" target=\"_blank\">Support Us</a> |\n  <a rel=\"me\" href=\"https://fosstodon.org/@zettlr\" target=\"_blank\">Mastodon</a>\n</p>\n\n![screenshot](/resources/screenshots/zettlr_view.png)\n\nZettlr brings simplicity back to your texts. Open-minded writing that adapts to your style. Fast information retrieval that finds what matters to you. Versatile exporting that enables you to adapt to whatever publication pipeline your employer or school uses.\n\nFocus on what ",
    "url": "https://github.com/Zettlr/Zettlr",
    "last_updated": "2025-09-02T09:48:53+00:00"
  },
  {
    "full_name": "tjmahr/GelmanHill",
    "name": "GelmanHill",
    "description": "Examples and data from Gelman & Hill 2007",
    "language": "PostScript",
    "topics": [],
    "readme": "# Gelman and Hill (2006)\n\nExamples and data for the classic textbook [Gelman and Hill (2006)][book-link].\n\n## Notebooks\n\nA notebook file contains the code, output and formatted comments for an R\nscript. As I work through the examples, I save notebooks.\n\n- Chapter 7\n  - [07-01 Simulating Probability Models](examples/Ch07/07-01_SimulationOfProbabilityModels.md)\n  - [07-02 Summarizing Linear Regression Using Simulation](examples/Ch07/07-02_SummarizingLinearRegressionUsingSimulation.md)\n  - [07-03 Simulation for Nonlinear Predictions](examples/Ch07/07-03_SimulationForNonLinearPredictions.md)\n\n\n## BUGS vs. Stan\n\nI created this repository so that I could work through the book's examples \nas an RStudio project. Things have become slightly more complicated:\nThe BUGS code in the book is effectively unsupported (in favor of Stan).\n\nI originally downloaded the materials from [Gelman's page][arm-page] and \nstored them in this repository. These files used BUGS for model fitting, \nso they represent the canonical computing materials for the first \nedition of the book (2006). \n\nGelman's [instructions page for BUGS][bugsR] is now titled \"Use Stan \ninstead\", so I'll be using [Stan](http://mc-stan.org/)/RStan instead. \nFortunately, Stan materials for the book were \n[available][examples-commit], and I've replaced the BUGS examples with \nStan examples. These Stan examples are from circa 2015.\n\nI'm taking the liberty to modify examples and rename files as I work \nthrough them. I've tagged the commits with unmodified versions of the \n[original BUGS examples][pure-bugs] and [original Stan \nexamples][pure-stan] so that those are readily available. \n\n\n[book-link]: http://amzn.to/1Mjudi0\n[arm-page]: http://www.stat.columbia.edu/~gelman/arm/software/\n[bugsR]: http://www.stat.columbia.edu/~gelman/bugsR/\n[examples-commit]: https://github.com/stan-dev/example-models/tree/57f9cbcb0d6355e663679f1088adb21261da73bf\n[pure-bugs]: https://github.com/tjmahr/GelmanHill/releases/tag/v0.0.1\n[pure-stan]:http",
    "url": "https://github.com/tjmahr/GelmanHill",
    "last_updated": "2022-06-05T12:56:34+00:00"
  },
  {
    "full_name": "anthonydb/CDC-flu-scraper",
    "name": "CDC-flu-scraper",
    "description": "Python scraper to get weekly CDC flu surveillance data",
    "language": "Python",
    "topics": [],
    "readme": "Scrapes the weekly \"National and Regional Summary of Select \nSurveillance Components\" table from the Centers for Disease\nControl's flu summary site at http://www.cdc.gov/flu/weekly/\n\nExports data to comma-delimited text and json file.\n    \nExample usage:\n\n    >>> import fluscrape.py\n    >>> fluscrape.run()\n        \n    $ python fluscrape.py\n",
    "url": "https://github.com/anthonydb/CDC-flu-scraper",
    "last_updated": "2020-02-27T21:14:56+00:00"
  },
  {
    "full_name": "rapidsai/cudf",
    "name": "cudf",
    "description": "cuDF - GPU DataFrame Library ",
    "language": "C++",
    "topics": [
      "gpu",
      "rapids",
      "cudf",
      "arrow",
      "cuda",
      "pandas",
      "dataframe",
      "dask",
      "data-analysis",
      "data-science",
      "pydata",
      "cpp",
      "python"
    ],
    "readme": "# <div align=\"left\"><img src=\"img/rapids_logo.png\" width=\"90px\"/>&nbsp;cuDF - GPU DataFrames</div>\n\n## 📢 cuDF can now be used as a no-code-change accelerator for pandas! To learn more, see [here](https://rapids.ai/cudf-pandas/)!\n\ncuDF (pronounced \"KOO-dee-eff\") is a GPU DataFrame library\nfor loading, joining, aggregating, filtering, and otherwise\nmanipulating data. cuDF leverages\n[libcudf](https://docs.rapids.ai/api/libcudf/stable/), a\nblazing-fast C++/CUDA dataframe library and the [Apache\nArrow](https://arrow.apache.org/) columnar format to provide a\nGPU-accelerated pandas API.\n\nYou can import `cudf` directly and use it like `pandas`:\n\n```python\nimport cudf\n\ntips_df = cudf.read_csv(\"https://github.com/plotly/datasets/raw/master/tips.csv\")\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())\n```\n\nOr, you can use cuDF as a no-code-change accelerator for pandas, using\n[`cudf.pandas`](https://docs.rapids.ai/api/cudf/stable/cudf_pandas).\n`cudf.pandas` supports 100% of the pandas API, utilizing cuDF for\nsupported operations and falling back to pandas when needed:\n\n```python\n%load_ext cudf.pandas  # pandas operations now use the GPU!\n\nimport pandas as pd\n\ntips_df = pd.read_csv(\"https://github.com/plotly/datasets/raw/master/tips.csv\")\ntips_df[\"tip_percentage\"] = tips_df[\"tip\"] / tips_df[\"total_bill\"] * 100\n\n# display average tip by dining party size\nprint(tips_df.groupby(\"size\").tip_percentage.mean())\n```\n\n## Resources\n\n- [Try cudf.pandas now](https://nvda.ws/rapids-cudf): Explore `cudf.pandas` on a free GPU enabled instance on Google Colab!\n- [Install](https://docs.rapids.ai/install): Instructions for installing cuDF and other [RAPIDS](https://rapids.ai) libraries.\n- [cudf (Python) documentation](https://docs.rapids.ai/api/cudf/stable/)\n- [libcudf (C++/CUDA) documentation](https://docs.rapids.ai/api/libcudf/stable/)\n- [RAPIDS Community](https://rapids.ai/l",
    "url": "https://github.com/rapidsai/cudf",
    "last_updated": "2025-09-02T03:56:04+00:00"
  },
  {
    "full_name": "tonyfischetti/assertr",
    "name": "assertr",
    "description": "Assertive programming for R analysis pipelines",
    "language": "R",
    "topics": [
      "predicate-functions",
      "analysis-pipeline",
      "assertions",
      "assertion-methods",
      "assertion-library",
      "r",
      "rstats",
      "r-package",
      "peer-reviewed"
    ],
    "readme": "assertr\n===\n\n![assertr logo](https://thepolygram.com/assertrlogo.png)\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/ropensci/assertr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ropensci/assertr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test coverage](https://codecov.io/gh/ropensci/assertr/branch/master/graph/badge.svg)](https://app.codecov.io/gh/ropensci/assertr?branch=master)\n[![CRAN status](https://www.r-pkg.org/badges/version/assertr)](https://CRAN.R-project.org/package=assertr)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/assertr)](https://cran.r-project.org/package=assertr)\n[![rOpenSci software peer-review](https://badges.ropensci.org/23_status.svg)](https://github.com/ropensci/software-review/issues/23)\n<!-- badges: end -->\n\n### What is it?\nThe assertr package supplies a suite of functions designed to verify\nassumptions about data early in an analysis pipeline so that\ndata errors are spotted early and can be addressed quickly.\n\nThis package does not need to be used with the magrittr/dplyr piping\nmechanism but the examples in this README use them for clarity.\n\n### Installation\n\nYou can install the latest version on CRAN like this\n```r\n    install.packages(\"assertr\")\n```\n\nor you can install the bleeding-edge development version like this:\n```r\n    install.packages(\"devtools\")\n    devtools::install_github(\"ropensci/assertr\")\n```\n### What does it look like?\nThis package offers five assertion functions, `assert`, `verify`,\n`insist`, `assert_rows`, and `insist_rows`, that are designed to be used\nshortly after data-loading in an analysis pipeline...\n\nLet’s say, for example, that the R’s built-in car dataset, `mtcars`, was not \nbuilt-in but rather procured from an external source that was known for making\nerrors in data entry or coding. Pretend we wanted to find the average\nmiles per gallon for each number of engine cylinders. We might want to first,\nconfirm\n- that it has the columns \"mpg\", \"vs\", and \"am\"\n-",
    "url": "https://github.com/tonyfischetti/assertr",
    "last_updated": "2025-07-25T21:18:32+00:00"
  },
  {
    "full_name": "jonschlinkert/markdown-toc",
    "name": "markdown-toc",
    "description": "API and CLI for generating a markdown TOC (table of contents) for a README or any markdown files. Uses Remarkable to parse markdown. Used by NASA/openmct, Prisma, Joi, Mocha, Sass, Prettier, Orbit DB, FormatJS, Raneto, hapijs/code, webpack-flow, docusaurus, release-it, ts-loader, json-server, reactfire, bunyan, husky, react-easy-state, react-snap, chakra-ui, carbon, alfresco, repolinter, Assemble, Verb, and thousands of other projects.",
    "language": "JavaScript",
    "topics": [
      "toc-generator",
      "table-of-contents",
      "toc",
      "markdown",
      "navigation",
      "project",
      "readme",
      "md",
      "markdown-toc",
      "node",
      "nodejs",
      "javascript",
      "jonschlinkert",
      "remarkable"
    ],
    "readme": "# markdown-toc [![NPM version](https://img.shields.io/npm/v/markdown-toc.svg?style=flat)](https://www.npmjs.com/package/markdown-toc) [![NPM monthly downloads](https://img.shields.io/npm/dm/markdown-toc.svg?style=flat)](https://npmjs.org/package/markdown-toc) [![NPM total downloads](https://img.shields.io/npm/dt/markdown-toc.svg?style=flat)](https://npmjs.org/package/markdown-toc)\n\n> Generate a markdown TOC (table of contents) with Remarkable.\n\nPlease consider following this project's author, [Jon Schlinkert](https://github.com/jonschlinkert), and consider starring the project to show your :heart: and support.\n\n- [Highlights](#highlights)\n- [Usage](#usage)\n- [API](#api)\n  * [toc.plugin](#tocplugin)\n  * [toc.json](#tocjson)\n  * [toc.insert](#tocinsert)\n  * [Utility functions](#utility-functions)\n- [Options](#options)\n  * [options.append](#optionsappend)\n  * [options.filter](#optionsfilter)\n  * [options.slugify](#optionsslugify)\n  * [options.bullets](#optionsbullets)\n  * [options.maxdepth](#optionsmaxdepth)\n  * [options.firsth1](#optionsfirsth1)\n  * [options.stripHeadingTags](#optionsstripheadingtags)\n- [About](#about)\n\n_(TOC generated by [verb](https://github.com/verbose/verb) using [markdown-toc](https://github.com/jonschlinkert/markdown-toc))_\n\n## Install\n\nInstall with [npm](https://www.npmjs.com/):\n\n```sh\n$ npm install --save markdown-toc\n```\n\n# Sponsors\n\nThanks to the following companies, organizations, and individuals for supporting the ongoing maintenance and development of markdown-toc! [Become a Sponsor](https://github.com/sponsors/jonschlinkert) to add your logo to this README, or any of [my other projects](https://github.com/jonschlinkert?tab=repositories&q=&type=&language=&sort=stargazers)\n\n## Gold Sponsors\n\n| [<img src=\"https://github.com/jonschlinkert/clone-deep/assets/383994/98036489-2cae-48a2-8d29-7dec58ea05c4\" alt=\"https://jaake.tech/\" width=\"100\"/>](https://jaake.tech/) |\n|:---:|\n| [https://jaake.tech/](https://jaake.tech/) |\n\n<br />\n\n## Quick Start\n",
    "url": "https://github.com/jonschlinkert/markdown-toc",
    "last_updated": "2025-09-01T17:58:40+00:00"
  },
  {
    "full_name": "cvxpy/cvxpy",
    "name": "cvxpy",
    "description": "A Python-embedded modeling language for convex optimization problems.",
    "language": "C++",
    "topics": [
      "python",
      "cvxpy",
      "optimization",
      "modeling-language",
      "convex-optimization",
      "mathematical-optimization",
      "optimization-modeling",
      "numerical-optimization"
    ],
    "readme": "CVXPY\n=====================\n[![Build Status](http://github.com/cvxpy/cvxpy/workflows/build/badge.svg?event=push)](https://github.com/cvxpy/cvxpy/actions/workflows/build.yml)\n![PyPI - downloads](https://img.shields.io/pypi/dm/cvxpy.svg?label=Pypi%20downloads)\n![Conda - downloads](https://img.shields.io/conda/dn/conda-forge/cvxpy.svg?label=Conda%20downloads)\n[![Discord](https://img.shields.io/badge/Chat-Discord-Blue?color=5865f2)](https://discord.gg/4urRQeGBCr)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=cvxpy_cvxpy&metric=coverage)](https://sonarcloud.io/summary/new_code?id=cvxpy_cvxpy)\n[![Benchmarks](http://img.shields.io/badge/benchmarked%20by-asv-blue.svg?style=flat)](https://cvxpy.github.io/benchmarks/)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/cvxpy/cvxpy/badge)](https://api.securityscorecards.dev/projects/github.com/cvxpy/cvxpy)\n\n**The CVXPY documentation is at [cvxpy.org](https://www.cvxpy.org/).**\n\n*We are building a CVXPY community on [Discord](https://discord.gg/4urRQeGBCr). Join the conversation! For issues and long-form discussions, use [Github Issues](https://github.com/cvxpy/cvxpy/issues) and [Github Discussions](https://github.com/cvxpy/cvxpy/discussions).*\n\n**Contents**\n- [Installation](#installation)\n- [Getting started](#getting-started)\n- [Issues](#issues)\n- [Community](#community)\n- [Contributing](#contributing)\n- [Team](#team)\n- [Citing](#citing)\n\n\nCVXPY is a Python-embedded modeling language for convex optimization problems. It allows you to express your problem in a natural way that follows the math, rather than in the restrictive standard form required by solvers.\n\nFor example, the following code solves a least-squares problem where the variable is constrained by lower and upper bounds:\n\n```python3\nimport cvxpy as cp\nimport numpy\n\n# Problem data.\nm = 30\nn = 20\nnumpy.random.seed(1)\nA = numpy.random.randn(m, n)\nb = numpy.random.randn(m)\n\n# Construct the problem.\nx = cp.Variable(n)\nobjec",
    "url": "https://github.com/cvxpy/cvxpy",
    "last_updated": "2025-09-02T06:01:56+00:00"
  },
  {
    "full_name": "jellyfangs/messenger-bot-tutorial",
    "name": "messenger-bot-tutorial",
    "description": "Facebook Messenger bot 15 minute tutorial",
    "language": "JavaScript",
    "topics": [],
    "readme": "# 🤖 Creating your own Facebook Messenger bot\n\n![Alt text](/demo/Demo.gif)\n\nFacebook recently opened up their Messenger platform to enable bots to converse with users through Facebook Apps and on Facebook Pages. \n\nYou can read the  [documentation](https://developers.facebook.com/docs/messenger-platform/quickstart) the Messenger team prepared but it's not very clear for beginners and intermediate hackers. \n\nSo instead here is how to create your own messenger bot in 15 minutes.\n\n## 🙌 Get set\n\nMessenger bots uses a web server to process messages it receives or to figure out what messages to send. You also need to have the bot be authenticated to speak with the web server and the bot approved by Facebook to speak with the public.\n\nYou can also skip the whole thing by git cloning this repository, running npm install, and run a server somewhere.\n\n### *Build the server*\n\n1. Install the Heroku toolbelt from here https://toolbelt.heroku.com to launch, stop and monitor instances. Sign up for free at https://www.heroku.com if you don't have an account yet.\n\n2. Install Node from here https://nodejs.org, this will be the server environment. Then open up Terminal or Command Line Prompt and make sure you've got the very most recent version of npm by installing it again:\n\n    ```\n    sudo npm install npm -g\n    ```\n\n3. Create a new folder somewhere and let's create a new Node project. Hit Enter to accept the defaults.\n\n    ```\n    npm init\n    ```\n\n4. Install the additional Node dependencies. Express is for the server, request is for sending out messages and body-parser is to process messages.\n\n    ```\n    npm install express request body-parser --save\n    ```\n\n5. Create an index.js file in the folder and copy this into it. We will start by authenticating the bot.\n\n    ```javascript\n    'use strict'\n    \n    const express = require('express')\n    const bodyParser = require('body-parser')\n    const request = require('request')\n    const app = express()\n\n    app.set('port', (process.e",
    "url": "https://github.com/jellyfangs/messenger-bot-tutorial",
    "last_updated": "2025-08-31T06:18:23+00:00"
  },
  {
    "full_name": "vahidk/EffectiveTensorflow",
    "name": "EffectiveTensorflow",
    "description": "TensorFlow tutorials and best practices.",
    "language": "",
    "topics": [
      "tensorflow",
      "neural-network",
      "deep-learning",
      "machine-learning",
      "ebook"
    ],
    "readme": "# Effective TensorFlow 2\n\nTable of Contents\n=================\n## Part I: TensorFlow 2 Fundamentals\n1.  [TensorFlow 2 Basics](#basics)\n2.  [Broadcasting the good and the ugly](#broadcast)\n3.  [Take advantage of the overloaded operators](#overloaded_ops)\n4.  [Control flow operations: conditionals and loops](#control_flow)\n5.  [Prototyping kernels and advanced visualization with Python ops](#python_ops)\n6.  [Numerical stability in TensorFlow](#stable)\n---\n\n_We updated the guide to follow the newly released TensorFlow 2.x API. If you want the original guide for TensorFlow 1.x see the [v1 branch](https://github.com/vahidk/EffectiveTensorflow/tree/v1)._\n\n_To install TensorFlow 2.0 (alpha) follow the [instructions on the official website](https://www.tensorflow.org/install/pip):_\n```\npip install tensorflow==2.0.0-alpha0\n```\n\n_We aim to gradually expand this series by adding new articles and keep the content up to date with the latest releases of TensorFlow API. If you have suggestions on how to improve this series or find the explanations ambiguous, feel free to create an issue, send patches, or reach out by email._\n\n# Part I: TensorFlow 2.0 Fundamentals\n<a name=\"fundamentals\"></a>\n\n## TensorFlow Basics\n<a name=\"basics\"></a>\nTensorFlow 2 went under a massive redesign to make the API more accessible and easier to use. If you are familiar with numpy you will find yourself right at home when using TensorFlow 2. Unlike TensorFlow 1 which was purely symbolic, TensorFlow 2 hides its symbolic nature behind the hood to look like any other imperative library like NumPy. It's important to note the change is mostly an interface change, and TensorFlow 2 is still able to take advantage of its symbolic machinery to do everything that TensorFlow 1.x can do (e.g. automatic-differentiation and massively parallel computation on TPUs/GPUs).\n\nLet's start with a simple example, we want to multiply two random matrices. First we look at an implementation done in NumPy:\n```python\nimport numpy as ",
    "url": "https://github.com/vahidk/EffectiveTensorflow",
    "last_updated": "2025-08-27T19:13:51+00:00"
  },
  {
    "full_name": "frappe/charts",
    "name": "charts",
    "description": "Simple, responsive, modern SVG Charts with zero dependencies",
    "language": "JavaScript",
    "topics": [
      "chart",
      "javascript",
      "svg",
      "svg-chart",
      "graph",
      "zero-dependency",
      "hacktoberfest"
    ],
    "readme": "<div align=\"center\" markdown=\"1\">\n    \n<img width=\"80\" alt=\"charts-logo\" src=\"https://github.com/user-attachments/assets/37b7ffaf-8354-48f2-8b9c-fa04fae0135b\" />\n    \n# Frappe Charts\n**GitHub-inspired modern, intuitive and responsive charts with zero dependencies**\n\n<p align=\"center\">\n    <a href=\"https://bundlephobia.com/result?p=frappe-charts\">\n        <img src=\"https://img.shields.io/bundlephobia/minzip/frappe-charts\">\n    </a>\n</p>\n\n<img src=\".github/example.gif\">\n\n<div>\n\n[Explore Demos](https://frappe.io/charts) - [Edit at CodeSandbox](https://codesandbox.io/s/frappe-charts-demo-viqud) - [Documentation](https://frappe.io/charts/docs)  \n\n</div>\n\n</div>\n\n## Frappe Charts\nFrappe Charts is a simple charting library with a focus on a simple API. The design is inspired by various charts you see on GitHub.\n\n### Motivation\n\nERPNext needed a simple sales history graph for its user company master to help users track sales. While using c3.js for reports, the library didn’t align well with our product’s classic design. Existing JS libraries were either too complex or rigid in their structure and behavior. To address this, I decided to create a library for translating value pairs into relative shapes or positions, focusing on simplicity.\n\n### Key Features\n\n- **Variety of chart types**: Frappe Charts supports various chart types, including Axis Charts, Area and Trends, Bar, Line, Pie, Percentage, Mixed Axis, and Heatmap.\n- **Annotations and tooltips**: Charts can be annotated with x and y markers, regions, and tooltips for enhanced data context and clarity.\n- **Dynamic data handling**: Add, remove, or update individual data points in place, or refresh the entire dataset to reflect changes.\n- **Customizable configurations**: Flexible options like colors, animations, and custom titles allow for a highly personalized chart experience.\n\n## Usage\n\n```sh\nnpm install frappe-charts\n```\n\nImport in your project:\n```js\nimport { Chart } from 'frappe-charts'\n// or esm import\nimport { Cha",
    "url": "https://github.com/frappe/charts",
    "last_updated": "2025-09-01T02:02:44+00:00"
  },
  {
    "full_name": "mwouts/jupytext",
    "name": "jupytext",
    "description": "Jupyter Notebooks as Markdown Documents, Julia, Python or R scripts",
    "language": "Python",
    "topics": [
      "jupyter-notebook",
      "jupyterlab-extension",
      "version-control",
      "knitr",
      "rstudio",
      "markdown",
      "rmarkdown",
      "python",
      "hydrogen",
      "notebooks",
      "jupyterlab"
    ],
    "readme": "![](https://github.com/mwouts/jupytext/blob/17aea37c612f33a4e27eeee4b81966f1506920fd/docs/images/logo_large.png?raw=true)\n\n<!-- INDEX-START -->\n\n[![CI](https://github.com/mwouts/jupytext/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/mwouts/jupytext/actions)\n[![Documentation Status](https://readthedocs.org/projects/jupytext/badge/?version=latest)](https://jupytext.readthedocs.io/en/latest/?badge=latest)\n[![codecov.io](https://codecov.io/github/mwouts/jupytext/coverage.svg?branch=main)](https://codecov.io/gh/mwouts/jupytext/branch/main)\n[![MIT License](https://img.shields.io/github/license/mwouts/jupytext)](LICENSE)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![GitHub language count](https://img.shields.io/github/languages/count/mwouts/jupytext)](docs/languages.md)\n[![Conda Version](https://anaconda.org/conda-forge/jupytext/badges/version.svg)](https://anaconda.org/conda-forge/jupytext/)\n[![Pypi](https://img.shields.io/pypi/v/jupytext.svg)](https://pypi.python.org/pypi/jupytext)\n[![pyversions](https://img.shields.io/pypi/pyversions/jupytext.svg)](https://pypi.python.org/pypi/jupytext)\n[![Binder:lab](https://img.shields.io/badge/binder-jupyterlab-0172B2.svg)](https://mybinder.org/v2/gh/mwouts/jupytext/main?urlpath=lab/tree/demo/get_started.ipynb)\n[![Binder:notebook](https://img.shields.io/badge/binder-notebook-0172B2.svg)](https://mybinder.org/v2/gh/mwouts/jupytext/main?filepath=demo)\n[![launch - renku](https://renkulab.io/renku-badge.svg)](https://renkulab.io/projects/best-practices/jupytext/sessions/new?autostart=1)\n[![Jupyter Con 2020](https://img.shields.io/badge/YouTube-JupyterCon%202020-red.svg)](https://www.youtube.com/watch?v=SDYdeVfMh48)\n\n# Jupytext\n\nHave you always wished Jupyter notebooks were plain text documents? Wished you could edit them in your favorite IDE? And get clear and meaningful diffs when doing version control? Then, Jupytext may well be the tool you're ",
    "url": "https://github.com/mwouts/jupytext",
    "last_updated": "2025-09-02T08:11:32+00:00"
  },
  {
    "full_name": "sailthru/tidyjson",
    "name": "tidyjson",
    "description": "Tools for using dplyr with JSON data",
    "language": "R",
    "topics": [],
    "readme": "# tidyjson\n\n[![Build Status](https://travis-ci.org/sailthru/tidyjson.png?branch=master)](https://travis-ci.org/sailthru/tidyjson)\n\ntidyjson is a complementary set of tools to [tidyr](https://github.com/hadley/tidyr)\nfor working with JSON data. It's primary objective is to turn JSON data into \n[tidy](http://vita.had.co.nz/papers/tidy-data.pdf) tables for downstream use by \n[dplyr](http://github.com/hadley/dplyr) or other relational, analytical or \nmachine learning frameworks in R. Behind the scenes, tidyjson uses \n[jsonlite](https://github.com/jeroenooms/jsonlite) to parse the JSON data. \ntidyjson is also designed to be used with the `%>%` operator imported into dplyr\nfrom the [magrittr](https://github.com/smbache/magrittr) package.\n\ntidyjson operates on the following principles:\n\n* Allow for structuring in tidy form arbitrarily nested (arrays or objects) JSON\n* Naturally handle 'ragged' arrays and / or objects (varying lengths by document)\n* Allow for extraction of data in values *or* key names\n* Integrate with pipelines built on `dplyr` and the `%>%` operator\n* Ensure edge cases are handled correctly (especially empty data)\n\nYou can install tidyjson from github directly by running:\n\n```R\ndevtools::install_github(\"sailthru/tidyjson\")\n```\n\ntidyjson comes with several JSON examples:\n\n* `commits`: commit data for the dplyr repo from github API\n* `issues`: issue data for the dplyr repo from github API\n* `worldbank`: world bank funded projects from \n[jsonstudio](http://jsonstudio.com/resources/)\n* `companies`: startup company data from \n[jsonstudio](http://jsonstudio.com/resources/)\n\nNote that the tidyjson package closely follows the definition and semantics of \nthe [JSON standard](http://json.org/).\n\nAn example of how tidyjson works is as follows:\n\n```R\nlibrary(tidyjson)   # this package\nlibrary(dplyr)      # for %>% and other dplyr functions\n\njson <- '[{\"name\": \"bob\", \"age\": 32}, {\"name\": \"susan\", \"age\": 54}]'\n\njson %>%            # Use the %>% pipe operator to pass js",
    "url": "https://github.com/sailthru/tidyjson",
    "last_updated": "2024-08-29T17:40:13+00:00"
  },
  {
    "full_name": "matroid/matroid-python",
    "name": "matroid-python",
    "description": "Python bindings for Matroid API",
    "language": "Python",
    "topics": [],
    "readme": "# Matroid API Python Client\n\nUse our Python client to access the Matroid API for image and video classification.\n\n## Full documentation\n\nNavigate to any detector's page, such as the [Famous Places Detector](https://app.matroid.com/detector/58d010c75bcac50d00ad85ed?tab=api), and click on the \"Overview\" tab. The \"Overview\" section contains the full specifications for each REST endpoint.\n\n## Installation\n\n```\npip install matroid\n```\n\nYou can pass in your API credentials directly to the API client or save them as environment variables where the client will use them automatically. Here is a bash example:\n\n```\nnano .bash_profile\n```\n\nInside your `.bash_profile`, add the following lines, replacing the placeholder with the real values from the [API documentation's](https://app.matroid.com/detector/58d010c75bcac50d00ad85ed?tab=api) \"Account Info\" section\n\n```\nexport MATROID_CLIENT_ID=PLACEHOLDER\nexport MATROID_CLIENT_SECRET=PLACEHOLDER\n```\n\nThen run `source ~/.bash_profile` on the command line to ensure the environment variables are loaded.\n\n## Example API client usage\n\n```\nimport matroid\nfrom matroid.client import Matroid\n\napi = Matroid(client_id = 'abc', client_secret = '123')\n\n# List available detectors\ndetectors_to_use = api.search_detectors()\n\n# Classifying a picture from a URL\nlogo_classification_result = api.classify_image(detectorId = 'test', url = 'https://app.matroid.com/images/logo2.png', num_results = 5)\n\n# Classifying a picture from a file path\nstadium_classification_result = api.classify_image(detectorId = 'test', file = '/Users/matroid/Desktop/stadium.jpg')\n\n# Classifying pictures from multiple file paths\nfamous_people_results = api.classify_image(detectorId = 'test', file = ['/home/matroid/taylor.png', '/home/matroid/kanye.jpeg'])\n\n# Begin video classification\nclassifying_video = api.classify_video(detectorId = 'test', file = '/home/matroid/video.mp4')\n\n# Classify YouTube video\nclassifying_youtube_video = api.classify_video(detectorId = 'test', url = 'https:/",
    "url": "https://github.com/matroid/matroid-python",
    "last_updated": "2025-08-14T23:39:06+00:00"
  },
  {
    "full_name": "jsvine/waybackpack",
    "name": "waybackpack",
    "description": "Download the entire Wayback Machine archive for a given URL.",
    "language": "Python",
    "topics": [],
    "readme": "# waybackpack\n\n[![Version](https://img.shields.io/pypi/v/waybackpack.svg)](https://pypi.python.org/pypi/waybackpack) [![Support Python versions](https://img.shields.io/pypi/pyversions/waybackpack.svg)](https://pypi.python.org/pypi/waybackpack)\n\nWaybackpack is a command-line tool that lets you download the entire Wayback Machine archive for a given URL.\n\nFor instance, to download every copy of the Department of Labor's homepage through 1996 (which happens to be the first year the site was archived), you'd run:\n\n```sh\nwaybackpack http://www.dol.gov/ -d ~/Downloads/dol-wayback --to-date 1996\n```\n\nResult:\n\n```sh\n~/Downloads/dol-wayback/\n├── 19961102145216\n│   └── www.dol.gov\n│       └── index.html\n├── 19961103063843\n│   └── www.dol.gov\n│       └── index.html\n├── 19961222171647\n│   └── www.dol.gov\n│       └── index.html\n└── 19961223193614\n    └── www.dol.gov\n        └── index.html\n```\n\nOr, just to print the URLs of all archived snapshots:\n\n```sh\nwaybackpack http://www.dol.gov/ --list\n```\n\n## Installation\n\n```\npip install waybackpack\n```\n\n## Usage\n\n```\nusage: waybackpack [-h] [--version] (-d DIR | --list) [--raw] [--root ROOT]\n                   [--from-date FROM_DATE] [--to-date TO_DATE]\n                   [--user-agent USER_AGENT] [--follow-redirects]\n                   [--uniques-only] [--collapse COLLAPSE] [--ignore-errors]\n                   [--max-retries MAX_RETRIES] [--no-clobber] [--quiet]\n                   [--progress] [--delay DELAY] [--delay-retry DELAY_RETRY]\n                   url\n\npositional arguments:\n  url                   The URL of the resource you want to download.\n\noptions:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  -d DIR, --dir DIR     Directory to save the files. Will create this\n                        directory if it doesn't already exist.\n  --list                Instead of downloading the files, only print the list\n                        of snapshots.\n  --raw       ",
    "url": "https://github.com/jsvine/waybackpack",
    "last_updated": "2025-09-01T03:16:38+00:00"
  },
  {
    "full_name": "CSX460/CSX460",
    "name": "CSX460",
    "description": "CSX460 Entire Class",
    "language": "HTML",
    "topics": [],
    "readme": "# CSX460 \r\n\r\nThis is the repository for *Practical Machine Learning with R* (CSX460) at the University of California, Berkeley. The most recent class is/was `Spring 2017`. \r\n\r\n## Course Description\r\n\r\nThis course provides an introduction to machine learning using R, the open source, statistical programming language. Once a niche set of tools for statisticians, programmers and quants, machine learning (sometimes also called statistical learning or data mining) has spread in popularity to become an indispensable tool to a wide variety of applications and disciplines. This course teaches the fundamentals of machine learning without delving into too much mathematics or code.  The course will teach practical aspects of machine learning. Upon completion of the course students will be able to apply lessons to solve problems using machine learning in their own work.\r\n \r\n\r\n## Course Learning Objectives\r\n\r\nStudents of this class will learn:\r\n\r\n- Fundamental concepts in ML\r\n- The differnece between supervised, unsupervised, semi-supervised, adaptive/reinforcement learning\r\n- The three prerequisites of ML algorithms/models\r\n  - Loss function\r\n  - Restricted class of functions\r\n  - Search methodology for training\r\n- How to evaluate and compare ML model performance\r\n- How to pre-process data and build features\r\n- How to train ML models for prediction, categorization and recommendations\r\n- How to apply ML models on new data\r\n- How to use resampling techniques to calculate model performance\r\n- What the bootstrap is and how it works\r\n- What Bagging is and how and why it improves model performance\r\n- What Boosting is and how and why it improves model performance \r\n- How to implement/deploy ML models for use by a wider audience\r\n- How to frame questions to be answered using ML techniques\r\n- Collaborate in a group using tools for collaborative/social programming\r\n- Generate high quality, graphical and textual results \r\n\r\n\r\n\r\n## Intended Audience\r\n\r\n- Anyone who wishes to learn the fund",
    "url": "https://github.com/CSX460/CSX460",
    "last_updated": "2024-01-15T20:49:28+00:00"
  },
  {
    "full_name": "OCA/purchase-workflow",
    "name": "purchase-workflow",
    "description": "Odoo Purchases, Workflow and Organization",
    "language": "HTML",
    "topics": [
      "erp",
      "hacktoberfest",
      "odoo",
      "python"
    ],
    "readme": "\n[![Runboat](https://img.shields.io/badge/runboat-Try%20me-875A7B.png)](https://runboat.odoo-community.org/builds?repo=OCA/purchase-workflow&target_branch=18.0)\n[![Pre-commit Status](https://github.com/OCA/purchase-workflow/actions/workflows/pre-commit.yml/badge.svg?branch=18.0)](https://github.com/OCA/purchase-workflow/actions/workflows/pre-commit.yml?query=branch%3A18.0)\n[![Build Status](https://github.com/OCA/purchase-workflow/actions/workflows/test.yml/badge.svg?branch=18.0)](https://github.com/OCA/purchase-workflow/actions/workflows/test.yml?query=branch%3A18.0)\n[![codecov](https://codecov.io/gh/OCA/purchase-workflow/branch/18.0/graph/badge.svg)](https://codecov.io/gh/OCA/purchase-workflow)\n[![Translation Status](https://translation.odoo-community.org/widgets/purchase-workflow-18-0/-/svg-badge.svg)](https://translation.odoo-community.org/engage/purchase-workflow-18-0/?utm_source=widget)\n\n<!-- /!\\ do not modify above this line -->\n\n# purchase-workflow\n\npurchase-workflow\n\n<!-- /!\\ do not modify below this line -->\n\n<!-- prettier-ignore-start -->\n\n[//]: # (addons)\n\nAvailable addons\n----------------\naddon | version | maintainers | summary\n--- | --- | --- | ---\n[partner_supplierinfo_smartbutton](partner_supplierinfo_smartbutton/) | 18.0.1.0.0 | <a href='https://github.com/victoralmau'><img src='https://github.com/victoralmau.png' width='32' height='32' style='border-radius:50%;' alt='victoralmau'/></a> | Access supplied products from the vendor\n[procurement_purchase_no_grouping](procurement_purchase_no_grouping/) | 18.0.1.0.0 |  | Procurement Purchase No Grouping\n[product_supplier_code_purchase](product_supplier_code_purchase/) | 18.0.1.0.0 |  | This module adds to the purchase order line the supplier code defined in the product.\n[product_supplierinfo_purchase_contact](product_supplierinfo_purchase_contact/) | 18.0.1.0.0 | <a href='https://github.com/victoralmau'><img src='https://github.com/victoralmau.png' width='32' height='32' style='border-radius:50%;' alt='vic",
    "url": "https://github.com/OCA/purchase-workflow",
    "last_updated": "2025-09-01T07:12:43+00:00"
  },
  {
    "full_name": "bioscan-ml/BIOSCAN-1M",
    "name": "BIOSCAN-1M",
    "description": "A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset",
    "language": "Python",
    "topics": [],
    "readme": "# BIOSCAN-1M\n\n![Alt Text](dataset/bioscan_images/Fig1.png)\n\n###### <h3> Overview\nThis repository houses the codes and data pertaining to the [BIOSCAN-1M-Insect project](https://biodiversitygenomics.net/1M_insects/). \nWithin this project, we introduce the __BIOSCAN-1M Insect dataset__, which can be accessed \nfor download via the provided links. The repository encompasses code for data sampling and splitting, \ndataset statistics analysis, as well as image-based classification experiments centered around \nthe taxonomy classification of insects. \n\nAnyone interested in using BIOSCAN-1M Insect dataset and/or the corresponding code repository, please cite the [Paper](http://arxiv.org/abs/2307.10455):\n\n```\n@inproceedings{gharaee2023step,\n    title={A Step Towards Worldwide Biodiversity Assessment: The {BIOSCAN-1M} Insect Dataset},\n    booktitle={Advances in Neural Information Processing Systems},\n    author={Gharaee, Z. and Gong, Z. and Pellegrino, N. and Zarubiieva, I. and Haurum, J. B. and Lowe, S. C. and McKeown, J. T. A. and Ho, C. Y. and McLeod, J. and Wei, Y. C. and Agda, J. and Ratnasingham, S. and Steinke, D. and Chang, A. X. and Taylor, G. W. and Fieguth, P.},\n    editor={A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},\n    pages={43593--43619},\n    publisher={Curran Associates, Inc.},\n    year={2023},\n    volume={36},\n    url={https://proceedings.neurips.cc/paper_files/paper/2023/file/87dbbdc3a685a97ad28489a1d57c45c1-Paper-Datasets_and_Benchmarks.pdf},\n}\n```\n\n> **ℹ️ Note**  \n> The samples from the **BIOSCAN-1M** dataset are included in the larger **BIOSCAN-5M** dataset.  \n> BIOSCAN-5M enhances the original with additional attributes such as:\n> - Geographic coordinates  \n> - Specimen size information  \n> - Cleaned and pruned taxonomic labels  \n> - A new split strategy optimized for **multimodal learning**\n>\n> For more details kindly visit the [BIOSCAN-5M](https://github.com/bioscan-ml/BIOSCAN-5M) repository.\n\n###### <h3> Dataset Acce",
    "url": "https://github.com/bioscan-ml/BIOSCAN-1M",
    "last_updated": "2025-06-02T00:18:28+00:00"
  },
  {
    "full_name": "meta-llama/llama",
    "name": "llama",
    "description": "Inference code for Llama models",
    "language": "Python",
    "topics": [],
    "readme": "## **Note of deprecation**\n\nThank you for developing with Llama models. As part of the Llama 3.1 release, we’ve consolidated GitHub repos and added some additional repos as we’ve expanded Llama’s functionality into being an e2e Llama Stack. Please use the following repos going forward:\n- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies\n- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations \n- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations\n- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications\n- [llama-cookbook](https://github.com/meta-llama/llama-recipes) - Community driven scripts and integrations\n\nIf you have any questions, please feel free to file an issue on any of the above repos and we will do our best to respond in a timely manner. \n\nThank you!\n\n\n# (Deprecated) Llama 2\n\nWe are unlocking the power of large language models. Llama 2 is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly. \n\nThis release includes model weights and starting code for pre-trained and fine-tuned Llama language models — ranging from 7B to 70B parameters.\n\nThis repository is intended as a minimal example to load [Llama 2](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) models and run inference. For more detailed examples leveraging Hugging Face, see [llama-cookbook](https://github.com/facebookresearch/llama-recipes/).\n\n## Updates post-launch\n\nSee [",
    "url": "https://github.com/meta-llama/llama",
    "last_updated": "2025-09-02T07:18:55+00:00"
  },
  {
    "full_name": "palewire/django-calaccess-campaign-browser",
    "name": "django-calaccess-campaign-browser",
    "description": "A Django app to refine, review and republish campaign finance data drawn from the California Secretary of State’s CAL-ACCESS database",
    "language": "Python",
    "topics": [],
    "readme": "# django-calaccess-campaign-browser\n\nA Django app to refine, review and investigate campaign finance data drawn from the California Secretary of State’s CAL-ACCESS database.\n\n**This is a work in progress. Its analysis should be considered as provisional until it is further tested and debugged.**\n\n[![Docs status](https://readthedocs.org/projects/django-calaccess-campaign-browser/badge/)](http://django-calaccess-campaign-browser.californiacivicdata.org/)\n[![Build Status](https://travis-ci.org/california-civic-data-coalition/django-calaccess-campaign-browser.png?branch=master)](https://travis-ci.org/california-civic-data-coalition/django-calaccess-campaign-browser)\n[![PyPI version](https://badge.fury.io/py/django-calaccess-campaign-browser.png)](http://badge.fury.io/py/django-calaccess-campaign-browser)\n[![Coverage Status](https://coveralls.io/repos/california-civic-data-coalition/django-calaccess-campaign-browser/badge.png?branch=master)](https://coveralls.io/r/california-civic-data-coalition/django-calaccess-campaign-browser?branch=master)\n\n* Documentation: [django-calaccess-campaign-browser.californiacivicdata.org](http://django-calaccess-campaign-browser.californiacivicdata.org//)\n* Issues: [github.com/california-civic-data-coalition/django-calaccess-campaign-browser/issues](https://github.com/california-civic-data-coalition/django-calaccess-campaign-browser/issues)\n* Packaging: [pypi.python.org/pypi/django-calaccess-campaign-browser](https://pypi.python.org/pypi/django-calaccess-campaign-browser)\n* Testing: [travis-ci.org/california-civic-data-coalition/django-calaccess-campaign-browser](https://travis-ci.org/california-civic-data-coalition/django-calaccess-campaign-browser)\n* Coverage: [coveralls.io/r/california-civic-data-coalition/django-calaccess-campaign-browser](https://coveralls.io/r/california-civic-data-coalition/django-calaccess-campaign-browser)\n",
    "url": "https://github.com/palewire/django-calaccess-campaign-browser",
    "last_updated": "2023-01-28T06:01:36+00:00"
  },
  {
    "full_name": "dannguyen/facebook-trending-rss-fetcher",
    "name": "facebook-trending-rss-fetcher",
    "description": "Python code to scrape and collect data from the RSS feeds Facebook uses to augment its Trending Section",
    "language": "Python",
    "topics": [],
    "readme": "# Facebook Trending RSS Feed Fetcher\n\nA quickie Python 3.5 script that parses the [PDF-listing of RSS feeds](data/rss-urls.pdf) that Facebook uses to monitor for breaking news stories to add to its Trending Section.\n\n# Background\n\nOn May 12, 2016, Gizmodo published an article titled, [Facebook Admits Its Trending Section Includes Topics Not Actually Trending on Facebook](http://gizmodo.com/facebook-admits-its-trending-section-includes-topics-no-1776319308), which covered the fallout from Gizmodo's previous reporting that [Facebook's Trending Section is mostly human-curated](http://gizmodo.com/former-facebook-workers-we-routinely-suppressed-conser-1775461006). As part of its response, Facebook released a list of 1,000 RSS feeds ([as a PDF file](https://fbnewsroomus.files.wordpress.com/2016/05/rss-urls.pdf)) that it says it uses to crawl for interesting news stories that may not have yet percolated through its social shares.\n\nThis repo contains code (and the results) to convert that PDF list into a machine-readable CSV ([data/rss-urls.csv](data/rss-urls.csv)) and then to fetch each RSS URL. A few of the URLs 404, but programmers who know how to parse XML can make use of the [retrieved data](data/feeds/) to do their own content analysis.\n\nNote: There appears to be only __929__ lines in Facebook's list of RSS feeds, according to `wc -l data/rss-urls.csv`, not \"1,000\". And when counting uniques --\n\n~~~sh\ncsvcut -c3 data/rss-urls.csv | sort | uniq | wc -l\n~~~\n\nThe result is __888__ lines. \n\nEach URL is given a country and category. Here's the group count of those fields:\n\n| count | country |     topic     |\n|-------|---------|---------------|\n|    11 | AU      | business      |\n|    10 | AU      | entertainment |\n|    20 | AU      | general       |\n|    10 | AU      | health        |\n|    13 | AU      | politics      |\n|     7 | AU      | science       |\n|    11 | AU      | sports        |\n|     7 | AU      | tech          |\n|    20 | CA      | business      |\n|    35 | C",
    "url": "https://github.com/dannguyen/facebook-trending-rss-fetcher",
    "last_updated": "2024-06-20T10:31:19+00:00"
  },
  {
    "full_name": "rapidfuzz/RapidFuzz",
    "name": "RapidFuzz",
    "description": "Rapid fuzzy string matching in Python using various string metrics",
    "language": "Python",
    "topics": [
      "string-matching",
      "string-similarity",
      "string-comparison",
      "levenshtein",
      "python",
      "cpp",
      "levenshtein-distance"
    ],
    "readme": "<h1 align=\"center\">\n<img src=\"https://raw.githubusercontent.com/rapidfuzz/RapidFuzz/main/docs/img/RapidFuzz.svg?sanitize=true\" alt=\"RapidFuzz\" width=\"400\">\n</h1>\n<h4 align=\"center\">Rapid fuzzy string matching in Python and C++ using the Levenshtein Distance</h4>\n\n<p align=\"center\">\n  <a href=\"https://github.com/rapidfuzz/RapidFuzz/actions\">\n    <img src=\"https://github.com/rapidfuzz/RapidFuzz/workflows/Test%20Build/badge.svg\"\n         alt=\"Continuous Integration\">\n  </a>\n  <a href=\"https://pypi.org/project/rapidfuzz/\">\n    <img src=\"https://img.shields.io/pypi/v/rapidfuzz\"\n         alt=\"PyPI package version\">\n  </a>\n  <a href=\"https://anaconda.org/conda-forge/rapidfuzz\">\n    <img src=\"https://img.shields.io/conda/vn/conda-forge/rapidfuzz.svg\"\n         alt=\"Conda Version\">\n  </a>\n  <a href=\"https://www.python.org\">\n    <img src=\"https://img.shields.io/pypi/pyversions/rapidfuzz\"\n         alt=\"Python versions\">\n  </a><br/>\n  <a href=\"https://rapidfuzz.github.io/RapidFuzz\">\n    <img src=\"https://img.shields.io/badge/-documentation-blue\"\n         alt=\"Documentation\">\n  </a>\n  <a href=\"https://codecov.io/gh/rapidfuzz/RapidFuzz\">\n    <img src=\"https://codecov.io/gh/rapidfuzz/RapidFuzz/branch/main/graph/badge.svg?token=1IJLT65K8B\"\n         alt=\"Code Coverage\">\n  </a>\n  <a href=\"https://github.com/rapidfuzz/RapidFuzz/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/github/license/rapidfuzz/rapidfuzz\"\n         alt=\"GitHub license\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"#description\">Description</a> •\n  <a href=\"#installation\">Installation</a> •\n  <a href=\"#usage\">Usage</a> •\n  <a href=\"#license\">License</a>\n</p>\n\n---\n\n## Description\nRapidFuzz is a fast string matching library for Python and C++, which is using the string similarity calculations from [FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy). However there are a couple of aspects that set RapidFuzz apart from FuzzyWuzzy:\n1) It is MIT licensed so it can be used whichever License you might want to choose ",
    "url": "https://github.com/rapidfuzz/RapidFuzz",
    "last_updated": "2025-09-01T17:57:12+00:00"
  },
  {
    "full_name": "cswinter/LocustDB",
    "name": "LocustDB",
    "description": "Blazingly fast analytics database that will rapidly devour all of your data.",
    "language": "Rust",
    "topics": [
      "analytics",
      "database",
      "rust"
    ],
    "readme": "# LocustDB\n\n[![Build Status][bi]][bl] [![Crates.io][ci]][cl] [![Gitter][gi]][gl]\n\n[bi]: https://github.com/cswinter/LocustDB/workflows/Test/badge.svg\n[bl]: https://github.com/cswinter/LocustDB/actions\n\n[ci]: https://img.shields.io/crates/v/locustdb.svg\n[cl]: https://crates.io/crates/locustdb/\n\n[gi]: https://badges.gitter.im/LocustDB/Lobby.svg\n[gl]: https://gitter.im/LocustDB/Lobby\n\nAn experimental analytics database aiming to set a new standard for query performance and storage efficiency on commodity hardware.\nSee [How to Analyze Billions of Records per Second on a Single Desktop PC][blogpost] and [How to Read 100s of Millions of Records per Second from a Single Disk][blogpost-2] for an overview of current capabilities.\n\n## Usage\n\nDownload the [latest binary release][latest-release], which can be run from the command line on most x64 Linux systems, including Windows Subsystem for Linux. For example, to load the file `test_data/nyc-taxi.csv.gz` in this repository and start the repl run:\n\n```Bash\n./locustdb --load test_data/nyc-taxi.csv.gz --trips\n```\n\nWhen loading `.csv` or `.csv.gz` files with `--load`, the first line of each file is assumed to be a header containing the names for all columns. The type of each column will be derived automatically, but this might break for columns that contain a mixture of numbers/strings/empty entries.\n\nTo persist data to disk in LocustDB's internal storage format (which allows fast queries from disk after the initial load), specify the storage location with `--db-path`\nWhen creating/opening a persistent database, LocustDB will open a lot of files and might crash if the limit on the number of open files is too low.\nOn Linux, you can check the current limit with `ulimit -n` and set a new limit with e.g. `ulimit -n 4096`.\n\nThe `--trips` flag will configure the ingestion schema for loading the 1.46 billion taxi ride dataset which can be downloaded [here][nyc-taxi-trips].\n\nFor additional usage info, invoke with `--help`:\n\n```Bash\n$ ./l",
    "url": "https://github.com/cswinter/LocustDB",
    "last_updated": "2025-08-27T21:13:55+00:00"
  },
  {
    "full_name": "chiphuyen/sniffly",
    "name": "sniffly",
    "description": "Claude Code dashboard with usage stats, error analysis, and sharable feature",
    "language": "Python",
    "topics": [
      "agent",
      "ai",
      "analytics",
      "coding"
    ],
    "readme": "# Sniffly - Claude Code Analytics Dashboard\n\nAnalyze Claude Code logs to help you use Claude Code better.\n\n[Website](https://sniffly.dev) | [Quickstart](#-quickstart) | [Features](#-features) | [Sharable dashboard](#-sharing-your-dashboard)\n\n## 📊 Features\n### Understanding your usage patterns\n<center>\n<img src=\"assets/features/stats.png\" width=\"800\" />\n</center>\n\n### Error breakdown\n_See where Claude Code makes mistakes so that you avoid these mistakes._\n\n<center>\n<img src=\"assets/features/error-type.png\" width=\"500\" />\n</center>\n\n### Message history analysis\n_Walk through all your instructions and share them with your coworkers if needed._\n\n<center>\n<img src=\"assets/features/command-walkthrough.png\" width=\"1000\" />\n</center>\n\n\n## 🚀 Quickstart\n- Requirement: Python 3.10+\n\n### With UV (recommended)\nMake sure you have `uv` installed! https://github.com/astral-sh/uv\n\n```bash\n# One-time execution (no installation needed)\nuvx sniffly@latest init\n```\n\n```bash\n# Install the package\nuv tool install sniffly@latest\nsniffly init\n```\n\nAfter running `sniffly init`, access your dashboard on your browser at http://localhost:8081 (or whichever host/port you choose).\n\nIf you run Sniffly on a remote server, use [port forwarding](https://www.reddit.com/r/HomeNetworking/comments/i7ijiz/a_guide_to_port_forwarding/) to open the browser on your local computer.\n\n### With pip\n\n```bash\npip install sniffly\nsniffly init\n```\n\n### From source\n```bash\ngit clone https://github.com/chiphuyen/sniffly.git\ncd sniffly\npip install -e .\nsniffly init\n```\n\n## 🔧 Configuration\n\n### Common Settings\n\n```bash\n# Change port (default: 8081)\nsniffly config set port 8090\n\n# Disable auto-opening browser\nsniffly config set auto_browser false\n\n# Show current configuration\nsniffly config show\n```\n\n### All Configuration Options\n\n| Key | Default | Description |\n|-----|---------|-------------|\n| `port` | 8081 | Server port |\n| `host` | 127.0.0.1 | Server host |\n| `auto_browser` | true | Auto-open browser on start |\n| `cac",
    "url": "https://github.com/chiphuyen/sniffly",
    "last_updated": "2025-09-02T06:38:46+00:00"
  },
  {
    "full_name": "ChristosChristofidis/awesome-deep-learning",
    "name": "awesome-deep-learning",
    "description": "A curated list of awesome Deep Learning tutorials, projects and communities.",
    "language": "",
    "topics": [
      "deep-learning",
      "neural-network",
      "machine-learning",
      "awesome",
      "awesome-list",
      "recurrent-networks",
      "deep-networks",
      "deep-learning-tutorial",
      "face-images"
    ],
    "readme": "﻿# Awesome Deep Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n## Table of Contents\n\n* **[Books](#books)**\n\n* **[Courses](#courses)**  \n\n* **[Videos and Lectures](#videos-and-lectures)**  \n\n* **[Papers](#papers)**  \n\n* **[Tutorials](#tutorials)**  \n\n* **[Researchers](#researchers)**  \n\n* **[Websites](#websites)**  \n\n* **[Datasets](#datasets)**\n\n* **[Conferences](#Conferences)**\n\n* **[Frameworks](#frameworks)**  \n\n* **[Tools](#tools)**  \n\n* **[Miscellaneous](#miscellaneous)**  \n\n* **[Contributing](#contributing)**  \n\n\n### Books\n\n1.  [Deep Learning](http://www.deeplearningbook.org/) by Yoshua Bengio, Ian Goodfellow and Aaron Courville  (05/07/2015)\n2.  [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by  Michael Nielsen (Dec 2014)\n3.  [Deep Learning](http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf) by Microsoft Research (2013)\n4.  [Deep Learning Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) by LISA lab, University of Montreal (Jan 6 2015)\n5.  [neuraltalk](https://github.com/karpathy/neuraltalk) by Andrej Karpathy : numpy-based RNN/LSTM implementation\n6.  [An introduction to genetic algorithms](http://www.boente.eti.br/fuzzy/ebook-fuzzy-mitchell.pdf)\n7.  [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n8.  [Deep Learning in Neural Networks: An Overview](http://arxiv.org/pdf/1404.7828v4.pdf)\n9.  [Artificial intelligence and machine learning: Topic wise explanation](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/)\n10. [Grokking Deep Learning for Computer Vision](https://www.manning.com/books/grokking-deep-learning-for-computer-vision)\n11. [Dive into Deep Learning](https://d2l.ai/) - numpy based interactive Deep Learning book\n12. [Practical Deep Learning for Cloud, Mobile, and Edge](https://www.oreilly.com/library/view/practical-de",
    "url": "https://github.com/ChristosChristofidis/awesome-deep-learning",
    "last_updated": "2025-09-02T09:25:19+00:00"
  },
  {
    "full_name": "rOpenHealth/rpubmed",
    "name": "rpubmed",
    "description": "Tools for extracting and processing Pubmed and Pubmed Central records.",
    "language": "R",
    "topics": [],
    "readme": "rpubmed\n=======\n\n[![Build Status](https://api.travis-ci.org/ropensci/rpubmed.png)](https://travis-ci.org/ropensci/rpubmed)\n\nTools for extracting and processing records from Pubmed and Pubmed Central.\n----------------------------------------------------------------------------\n\nThis project is still very much in development... Please contact me with any questions, suggestions or bug reports.  \n\nI have built in support for searching and processing [MeSH headings](http://www.nlm.nih.gov/bsd/disted/meshtutorial/introduction/index.html), making this package of particular use for biomedical researchers conducting systematic reviews and meta-analyses. Two of these functions (`mesh_assoc_table` and `keyword_assoc_table`) produce association matrices which can be fed into graph packages such as `igraph` to visualise the associations between different search terms.\n\n\n\n\n### Available functions:\n\n* fetch - Tools for bulk downloading of Pubmed records\n    - `fetch_in_chunks(ids, chunk_size = 500, delay = 0, ...)`\n    - `pubmed_fetch(ids, file_format = \"xml\", as_r_object = TRUE, ...)`\n* textsearch  - Tools for text-mining of abstracts and metadata from downloaded records\n    - `get_articles_by_terms(corpus, term_list, where, case_sensitive = FALSE, ...)`\n    - `record_counts_by_year(corpus)`\n* io - saving records to disk and printing summaries of abstract lists to file or sdout\n    - `write_JSON_file(x, file)`\n    - `write_record_list(articles, out_file = \"\", abstract_p = FALSE, markdown_p = FALSE, linestart = \"* \")`\n* locations - Geocoding functionality added for finding the coordinates of departments affiliated with Pubmed Articles.\n    - `geocode_addresses(addresses, sleeper = 0.33, depth = 3)`\n    - `get_article_location_data(abstracts)`\n    - `geocode_address(address, depth = 3)`\n* mesh - Tools for processing and exploring associations between MeSH headings and other keywords\n    - `mesh_assoc_table(corpus)`\n    - `keyword_assoc_table(corpus, keyword_list, keyword_names, ...",
    "url": "https://github.com/rOpenHealth/rpubmed",
    "last_updated": "2025-05-29T04:27:31+00:00"
  },
  {
    "full_name": "BloombergGraphics/2024-h1b-immigration-data",
    "name": "2024-h1b-immigration-data",
    "description": "US H-1B Visa Lottery and Petition Data FY 2021 - FY 2024",
    "language": "",
    "topics": [],
    "readme": "# June 2025 Update\n\nFollowing the publication of our latest story \"[H-1B Middlemen Bring Cheap Labor to Citi, Capital One](https://www.bloomberg.com/graphics/2025-h1b-visa-middlemen-cheap-labor-for-us-banks/)\", we're providing an updated version of the data with an **un-redacted \"DOL_ETA_CASE_NUMBER\" column**. You can now link this data with [Labor Condition Application data](https://www.dol.gov/agencies/eta/foreign-labor/performance) publicly released by the DOL to gain additional information about end clients, prevailing wages and more.\n\n# US H-1B Visa Lottery and Petition Data FY 2021 - FY 2024\n\nBloomberg News obtained data on all H-1B lottery registrations, selections, and petitions for fiscal years 2021 through 2024 after bringing a lawsuit against the Department of Homeland Security under the Freedom of Information Act. \n\nYou can use this repository to reproduce findings featured in our story \"[Thousands of US Work Visas Are Going to Middlemen Gaming the System](https://www.bloomberg.com/graphics/2024-staffing-firms-game-h1b-visa-lottery-system/)\". Our methodology is described at the bottom of the article. \n\n## Background\n\nThis data was produced by the U.S. Citizenship and Immigration Services, an agency under DHS tasked with adjudicating H-1B petitions. Every April, the USCIS conducts a random lottery to determine which H-1B seekers can submit full visa petitions. \n\nEach row in the data represents a lottery registration, which may or may not correspond to a unique individual because each candidate can have multiple registrations submitted by different employers. Once selected, the employer can submit an H-1B petition on behalf of the beneficiary. In case of a visa petition, the data includes more information about the proposed job, including salary, location, etc. \n\nAn approved H-1B petition is necessary for, but does not always result in, an H-1B visa, which is a travel document. After an applicant receives an H-1B approval from the USCIS, they can apply for",
    "url": "https://github.com/BloombergGraphics/2024-h1b-immigration-data",
    "last_updated": "2025-08-20T13:06:58+00:00"
  },
  {
    "full_name": "kjhealy/endlabel-example",
    "name": "endlabel-example",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# Endpoint Label example\n\n## Kieran Healy `@kjhealy`\n\n- An example of a little trick to use `geom_text()` together with a single-column data frame to label the endpoints of multiple trend lines in a space-efficient way.\n\n![Assault death rates by region and state](figures/assault-deaths-us-ts-region.png)\n",
    "url": "https://github.com/kjhealy/endlabel-example",
    "last_updated": "2019-10-08T04:03:46+00:00"
  },
  {
    "full_name": "mzucker/noteshrink",
    "name": "noteshrink",
    "description": "Convert scans of handwritten notes to beautiful, compact PDFs",
    "language": "Python",
    "topics": [],
    "readme": "noteshrink\n==========\n\nConvert scans of handwritten notes to beautiful, compact PDFs -- see full writeup at <https://mzucker.github.io/2016/09/20/noteshrink.html>\n\n## Requirements\n\n - Python 2 or 3\n - NumPy 1.10 or later\n - SciPy\n - ImageMagick\n - Image module from PIL or Pillow\n\n## Usage\n\n```\n./noteshrink.py IMAGE1 [IMAGE2 ...]\n```\n\nBuilding the examples (already in `example_output`):\n\n```\nmake\n```\n\n## Packages\nPackages are available for:\n - [Arch Linux (AUR)](https://aur.archlinux.org/packages/noteshrink/)\n \n## Derived works\n\n*Note:* Projects listed here aren't necessarily tested or endorsed by me -- use with care!\n\n  - [Web-based (Django) front-end](https://github.com/delneg/noteshrinker-django)\n",
    "url": "https://github.com/mzucker/noteshrink",
    "last_updated": "2025-08-28T09:55:51+00:00"
  },
  {
    "full_name": "vadimg/sfzoning",
    "name": "sfzoning",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "# Install\n\n## Prerequisites\n* Python3\n* `brew install geos`\n\n## Steps\n\n* Run `./install.sh`\n* Run `./get_data.sh` (this will take awhile, it needs to download a lot of data)\n\n# Run\n\n* `make`\n\nOr, for a specific city:\n* `make sf`\n* `make mountain_view`\n\n# View Map\n\nRun: `python3 -m http.server`\n\nView the maps at:\n* http://localhost:8000\n* http://localhost:8000/#mountain_view\n\n# Warning\n\nI started this project using python 2, and then started using python 3. Some of the code in this repo might not yet be converted to work with python 3.\n\n# Broken things\n\nCurrently, the mountain view map is outdated and the prop E map doesn't work. They were not being used much anyway, and I don't have time to fix them.\n",
    "url": "https://github.com/vadimg/sfzoning",
    "last_updated": "2024-11-01T16:40:25+00:00"
  },
  {
    "full_name": "betars/Face-Resources",
    "name": "Face-Resources",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "# Face-Resources\nFollowing is a growing list of some of the materials I found on the web for research on face recognition algorithm.\n\n## Papers\n\n1. [DeepFace](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf).A work from Facebook.\n2. [FaceNet](http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf).A work from Google.\n3. [ One Millisecond Face Alignment with an Ensemble of Regression Trees](http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf). Dlib implements the algorithm.\n4. [DeepID](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)\n5. [DeepID2](http://arxiv.org/abs/1406.4773)\n6. [DeepID3](http://arxiv.org/abs/1502.00873)\n7. [Learning Face Representation from Scratch](http://arxiv.org/abs/1411.7923)\n8. [Face Search at Scale: 80 Million Gallery](http://arxiv.org/abs/1507.07242)\n9. [A Discriminative Feature Learning Approach for Deep Face Recognition](http://ydwen.github.io/papers/WenECCV16.pdf)\n\n10. [NormFace: L2 Hypersphere Embedding for Face Verification](https://arxiv.org/abs/1704.06369).* attention: model released !*\n11. [SphereFace: Deep Hypersphere Embedding for Face Recognition](https://arxiv.org/abs/1704.08063)\n\n## Datasets\n\n1. [CASIA WebFace Database](http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html). 10,575 subjects and 494,414 images\n2. [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/).13,000 images and 5749 subjects\n3. [Large-scale CelebFaces Attributes (CelebA) Dataset](http://mmlab.ie.cuhk.edu.hk/projects/) 202,599 images and 10,177 subjects. 5 landmark locations, 40 binary attributes.\n4. [MSRA-CFW](http://research.microsoft.com/en-us/projects/msra-cfw/). 202,792 images and 1,583 subjects.\n5. [MegaFace Dataset](http://megaface.cs.washington.edu/) 1 Million Faces for Recognition at Scale\n690,572 unique people\n6. [FaceScrub](http://vintage.winklerbros.net/facescrub.html). A Dataset With Over 100,000 Face Images of 530 People.\n7. [FDDB](http://vis-www.cs.umass.edu/fddb/).Face Detection and Da",
    "url": "https://github.com/betars/Face-Resources",
    "last_updated": "2025-08-04T18:51:04+00:00"
  },
  {
    "full_name": "marktext/marktext",
    "name": "marktext",
    "description": "📝A simple and elegant markdown editor, available for Linux, macOS and Windows.",
    "language": "JavaScript",
    "topics": [
      "macos",
      "typewriter-mode",
      "electron",
      "vue",
      "element-ui",
      "next-generation",
      "markdown",
      "editor",
      "mac",
      "windows",
      "linux",
      "source-code",
      "focus-mode",
      "latex",
      "emoji",
      "dark-mode",
      "marktext"
    ],
    "readme": "<p align=\"center\"><img src=\"static/logo-small.png\" alt=\"MarkText\" width=\"100\" height=\"100\"></p>\n\n<h1 align=\"center\">MarkText</h1>\n\n<div align=\"center\">\n  <a href=\"https://twitter.com/intent/tweet?via=marktextme&url=https://github.com/marktext/marktext/&text=What%20do%20you%20want%20to%20say%20to%20app?&hashtags=happyMarkText\">\n    <img src=\"https://img.shields.io/twitter/url/https/github.com/marktext/marktext.svg?style=for-the-badge\" alt=\"twitter\">\n  </a>\n</div>\n<div align=\"center\">\n  <strong>:high_brightness: Next generation markdown editor :crescent_moon:</strong><br>\n  A simple and elegant open-source markdown editor that focused on speed and usability.<br>\n  <sub>Available for Linux, macOS and Windows.</sub>\n</div>\n\n<br>\n\n<div align=\"center\">\n  <!-- License -->\n  <a href=\"LICENSE\">\n    <img src=\"https://img.shields.io/github/license/marktext/marktext.svg\" alt=\"LICENSE\">\n  </a>\n  <!-- Build Status -->\n  <a href=\"https://travis-ci.org/marktext/marktext/\">\n    <img src=\"https://travis-ci.org/marktext/marktext.svg?branch=master\" alt=\"build\">\n  </a>\n  <a href=\"https://ci.appveyor.com/project/marktext/marktext/branch/master\">\n    <img src=\"https://ci.appveyor.com/api/projects/status/l4gxgydj0i95hmxg/branch/master?svg=true\" alt=\"build\">\n  </a>\n  <!-- Downloads total -->\n  <a href=\"https://github.com/marktext/marktext/releases\">\n    <img src=\"https://img.shields.io/github/downloads/marktext/marktext/total.svg\" alt=\"total download\">\n  </a>\n  <!-- Downloads latest release -->\n  <a href=\"https://github.com/marktext/marktext/releases/latest\">\n    <img src=\"https://img.shields.io/github/downloads/marktext/marktext/v0.17.1/total.svg\" alt=\"latest download\">\n  </a>\n  <!-- sponsors -->\n  <a href=\"https://opencollective.com/marktext\">\n    <img src=\"https://opencollective.com/marktext/tiers/silver-sponsors/badge.svg?label=SilverSponsors&color=brightgreen\" alt=\"sponsors\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <h3>\n    <a href=\"https://github.com/marktext/marktext\">\n      Website\n  ",
    "url": "https://github.com/marktext/marktext",
    "last_updated": "2025-09-02T09:49:33+00:00"
  },
  {
    "full_name": "microsoft/DMTK",
    "name": "DMTK",
    "description": "Microsoft Distributed Machine Learning Toolkit",
    "language": "",
    "topics": [
      "dmtk",
      "multiverso",
      "lightgbm",
      "microsoft",
      "machine-learning"
    ],
    "readme": "\n# DMTK\n\nDistributed Machine Learning Toolkit [https://www.dmtk.io](https://www.dmtk.io)\nPlease open issues in the project below. For any technical support email to [dmtk@microsoft.com](mailto:dmtk@microsoft.com)\n\nDMTK includes the following projects:\n* [DMTK framework(Multiverso)](https://github.com/Microsoft/multiverso): The parameter server framework for distributed machine learning.\n* [LightLDA](https://github.com/Microsoft/lightlda): Scalable, fast and lightweight system for large-scale topic modeling.\n* [LightGBM](https://github.com/Microsoft/lightGBM): LightGBM is a fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. \n* [Distributed word embedding](https://github.com/Microsoft/multiverso/tree/master/Applications/WordEmbedding): Distributed algorithm for word embedding implemented on multiverso.\n\n\n\n# Updates\n## 2017-02-04\n* A tutorial on the latests updates of Distributed Machine Learning is presented on [AAAI 2017](https://www.aaai.org/Conferences/AAAI/aaai17.php). you can download the slides [here](https://www.dmtk.io/tutorial_on_aaai2017.html).\n\n## 2016-11-21 \n* [Multiverso](https://github.com/Microsoft/multiverso) has been officially used in Microsoft [CNTK](https://github.com/microsoft/cntk) to power its ASGD parallel training.  \n\n## 2016-10-17 \n* [LightGBM](https://github.com/Microsoft/lightGBM) has been released. which is a fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. \n\n## 2016-09-12\n* A talk on the latest updates of DMTK is presented on [GTC China](http://www.gputechconf.cn/page/home.html). We also described the latest research work from our team, including the lightRNN(to be appeared in NIPS2016) and [DC-ASGD](https://arxiv.org/abs/1609.08326). \n\n## 2016-07-05",
    "url": "https://github.com/microsoft/DMTK",
    "last_updated": "2025-08-09T15:37:03+00:00"
  },
  {
    "full_name": "WinVector/zmPDSwR",
    "name": "zmPDSwR",
    "description": "Example R scripts and data for \"Practical Data Science with R\" 1st edition by Nina Zumel and John Mount (Manning Publications)",
    "language": "HTML",
    "topics": [],
    "readme": "# Example code and data for \"Practical Data Science with R 1st Edition\" by Nina Zumel and John Mount, Manning 2014.\n\nPlease consider upgrading to the in-progress [\"Practical Data Science with R 2nd Edition\" by Nina Zumel and John Mount (Manning 2019)](https://www.manning.com/books/practical-data-science-with-r-second-edition) (code/data/examples [here](https://github.com/WinVector/PDSwR2)).\n\n## The book:\n\n * The book: [\"Practical Data Science with R 1st Edition\" by Nina Zumel and John Mount, Manning 2014](https://www.manning.com/books/practical-data-science-with-r) (book copyright Manning Publications Co., all rights reserved)\n * The book at [Amazon.com](http://www.amazon.com/Practical-Data-Science-Nina-Zumel/dp/1617291560/)\n\n## Additional materials\n\n * The support site (code and data): [GitHub WinVector/zmPDSwR](https://github.com/WinVector/zmPDSwR)\n * [Code examples](https://github.com/WinVector/zmPDSwR/tree/master/CodeExamples) (also as R Markdown notebooks: [Notebooks running all examples](RunExamples) )\n * The discussion boards: [Forum: Practical Data Science with R](http://www.manning-sandbox.com/forum.jspa?forumID=863)\n * [Errata](http://winvector.github.io/PDSwR/PracticalDataScienceWithRErrata.html)\n\n## Zip file of code excerpts from the book:\n\n * [CodeExamples.zip](CodeExamples.zip)\n * [CodeExamples directory](CodeExamples)\n * [Notebooks running all examples](RunExamples)\n\n## More from the authors:\n\n * [Win-Vector blog](http://www.win-vector.com/blog/)\n * [WinVectorLLC on Twitter](https://twitter.com/WinVectorLLC)\n * [Nina Zumel homepage](http://ninazumel.com/)\n * [John Mount homepage](http://johnmount.com/)\n * [Win-Vector data science consulting services](http://www.win-vector.com/index.html)\n\n## Example data sets:\n\nIncludes works derived from others (data sets) remain controlled by them.  We are distributing as these parties have allowed and not making any claim or grant of additional rights or license.\n\n * [bioavailability](bioavailability) Synthetic sim",
    "url": "https://github.com/WinVector/zmPDSwR",
    "last_updated": "2025-06-02T05:38:02+00:00"
  },
  {
    "full_name": "cran/shinystan",
    "name": "shinystan",
    "description": ":exclamation: This is a read-only mirror of the CRAN R package repository.  shinystan — Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models. Homepage: https://mc-stan.org/shinystan/, https://discourse.mc-stan.org  Report bugs for this package: https://github.com/stan-dev/shinystan/issues/",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/cran/shinystan",
    "last_updated": "2022-03-03T03:36:33+00:00"
  },
  {
    "full_name": "rstudio/swagger",
    "name": "swagger",
    "description": "Swagger is a collection of HTML, Javascript, and CSS assets that dynamically generate beautiful documentation from a Swagger-compliant API.",
    "language": "HTML",
    "topics": [],
    "readme": "Dynamically Generate Documentation from a 'Swagger' Compliant API\n================\n\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/swagger)](https://cran.r-project.org/package=swagger)\n[![R build status](https://github.com/rstudio/swagger/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/swagger/actions)\n\nSwagger is a collection of HTML, JavaScript, and CSS assets that dynamically generate beautiful documentation from a Swagger-compliant API.\n\nThe main purpose of this package is to enable package authors to create APIs that are compatible with [swagger.io](https://swagger.io/) and [openapis.org](https://www.openapis.org/).\n\nPackage authors providing web interfaces can serve the static files from `swagger_path()` using [httpuv](https://github.com/rstudio/httpuv) or [fiery](https://github.com/thomasp85/fiery). As a start, we can also browse them by running:\n\n``` r\nlibrary(swagger)\nbrowseURL(swagger_index())\n```\n\n<img src=\"man/figures/browse_swagger.png\" width=450 />\n\nTo learn more about Swagger visit: [swagger.io/swagger-ui](https://swagger.io/swagger-ui/)\n",
    "url": "https://github.com/rstudio/swagger",
    "last_updated": "2025-01-29T18:05:41+00:00"
  },
  {
    "full_name": "dbro/csvquote",
    "name": "csvquote",
    "description": "Enables common unix utlities like cut, awk, wc, head to work correctly with csv data containing delimiters and newlines",
    "language": "C",
    "topics": [
      "csv",
      "command-line"
    ],
    "readme": "csvquote\n========\n_smart and simple CSV processing on the command line_\n\nDan Brown, May 2013  \nhttps://github.com/dbro/csvquote\n\nAre you looking for a way to process CSV data with standard UNIX shell commands?\n\nAre you running into problems with embedded commas and newlines that mess\neverything up?\n\nDo you wish there was some way to add some CSV intelligence to these UNIX tools?\n\n* awk, sed\n* cut, join\n* head, tail\n* sort, uniq\n* wc, split\n\nThis program can be used at the start and end of a text processing pipeline\nso that regular unix command line tools can properly handle CSV data that\ncontain commas and newlines inside quoted data fields.\n\nWithout this program, embedded special characters would be incorrectly\ninterpreted as separators when they are inside quoted data fields.\n\nBy using csvquote, you temporarily replace the special characters inside quoted\nfields with harmless nonprinting characters that can be processed as data by\nregular text tools. At the end of processing the text, these nonprinting\ncharacters are restored to their previous values.\n\nIn short, csvquote wraps the pipeline of UNIX commands to let them work on\nclean data that is consistently separated, with no ambiguous special\ncharacters present inside the data fields.\n\nBy default, the program expects to use these as special characters:\n\n    \" quote character  \n    , field delimiter  \n    \\n record separator  \n\nIt is possible to specify different characters for the field and record\nseparators, such as tabs or pipe symbols.\n\nNote that the quote character can be contained inside a quoted field\nby repeating it twice, eg.\n\n    field1,\"field2, has a comma in it\",\"field 3 has a \"\"Quoted String\"\" in it\"\n\nTypical usage of csvquote is as part of a command line pipe, to permit\nthe regular unix text-manipulating commands to avoid misinterpreting\nspecial characters found inside fields. eg.\n\n    csvquote foobar.csv | cut -d ',' -f 5 | sort | uniq -c | csvquote -u\n\nor taking input from stdin,\n\n    cat foobar.cs",
    "url": "https://github.com/dbro/csvquote",
    "last_updated": "2025-07-18T15:35:31+00:00"
  },
  {
    "full_name": "christophergandrud/eu_members",
    "name": "eu_members",
    "description": "A List of European Union Members and their country-years of membership",
    "language": "R",
    "topics": [],
    "readme": "# eu_members\n\nA List of European Union Members and their country-years of membership. Useful for merging with country-year formatted data to create an EU membership variable.\n",
    "url": "https://github.com/christophergandrud/eu_members",
    "last_updated": "2015-10-23T12:50:41+00:00"
  },
  {
    "full_name": "edwindj/daff",
    "name": "daff",
    "description": "Diff, patch and merge for data.frames, see  http://paulfitz.github.io/daff/",
    "language": "R",
    "topics": [
      "daff",
      "data",
      "diff",
      "r"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n<!-- badges: start -->\n\n![version](http://www.r-pkg.org/badges/version/daff)\n[![R-CMD-check](https://github.com/edwindj/daff/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/edwindj/daff/actions/workflows/R-CMD-check.yaml)\n![downloads](http://cranlogs.r-pkg.org/badges/daff)\n\n<!-- badges: end -->\n\n# Daff: diff, patch and merge for data.frames\n\ndaff is an R package that can find difference in values between\n`data.frames`, store this difference, render it and apply this\ndifference to patch a `data.frame`. It can also merge two versions of a\n`data.frame` having a common parent. It wraps the\n[daff.js](http://paulfitz.github.io/daff/) library using the\n[V8](https://github.com/jeroen/v8) package.\n\nThe diff format is described in\n<https://paulfitz.github.io/daff-doc/spec.html>.\n\nFunctions:\n\n- diff: `diff_data`\n- patch: `patch_data`\n- write/read diff: `read_diff` and `write_diff`\n- render to html: `render_diff`\n- merge two tables based on a same version: `merge_data`\n\n## Installation\n\nYou can install the development version of daff from\n[GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"edwindj/daff\")\n```\n\n# Usage\n\n## diff_data\n\nCalculate the difference between a reference and a changed `data.frame`\n\n``` r\nlibrary(daff)\ny <- iris[1:3,]\nx <- y\n\nx <- head(x,2) # remove a row\nx[1,1] <- 10 # change a value\nx$hello <- \"world\"  # add a column\nx$Species <- NULL # remove a column\n\npatch <- diff_data(y, x)\n\n# write a patch to disk\nwrite_diff(patch, \"patch.csv\")\n```\n\n`render_diff(patch)` will generate the following HTML page:\n\n<figure>\n<img src=\"man/figures/render_diff.png\" title=\"render_diff\" alt=\"render_diff\" />\n<figcaption aria-hidden=\"true\">\nrender_diff\n</figcaption>\n</figure>\n\n## patch_data\n\nPatch a `data.frame` using a diff generated with `diff_data`.\n\n``` r\n# read a diff from disk\npatch <- read_diff(\"patch.csv\")\n\n# apply patch\ny_patched <- pa",
    "url": "https://github.com/edwindj/daff",
    "last_updated": "2025-08-23T07:48:10+00:00"
  },
  {
    "full_name": "py-econometrics/duckreg",
    "name": "duckreg",
    "description": "Every big regression is a small regression with weights. ",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# `duckreg` : very fast out-of-memory regressions with `duckdb`\n\npython package to run stratified/saturated regressions out-of-memory with duckdb. R users, check out [Grant McDermott's port of this package](https://github.com/grantmcdermott/duckreg). \n\nThe package is a wrapper around the `duckdb` package and provides a simple interface to run regressions on very large datasets that do not fit in memory by reducing the data to a set of summary statistics and runs weighted least squares with frequency weights. Robust standard errors are computed from sufficient statistics, while clustered standard errors are computed using the cluster bootstrap. Methodological details and benchmarks are provided in [this](https://arxiv.org/abs/2410.09952) paper. See examples in `notebooks/introduction.ipynb`.\n\n<p align=\"center\">\n  <img src=\"https://static.independent.co.uk/s3fs-public/thumbnails/image/2016/02/14/12/duck-rabbit.png\" width=\"350\">\n</p>\n\n- install\n\n```\npip install duckreg\n```\n\n- dev install (preferably in a `venv`) with\n```\n(uv) pip install git+https://github.com/apoorvalal/duckreg.git\n```\n\nor git clone this repository and install in editable mode.\n\n---\n\nCurrently supports the following regression specifications:\n1. `DuckRegression`: general linear regression, which compresses the data to y averages stratified by all unique values of the x variables\n2. `DuckMundlak`: One- or Two-Way Mundlak regression, which compresses the data to the following RHS and avoids the need to incorporate unit (and time FEs)\n\n$$\ny \\sim 1, w, \\bar{w}\\_{i, .}, \\bar{w}\\_{., t}\n$$\n\n3. `DuckDoubleDemeaning`: Double demeaning regression, which compresses the data to y averages by all values of $w$ after demeaning. This also eliminates unit and time FEs\n\n$$\ny \\sim (W\\_{it} - \\bar{w}\\_{i, .} - \\bar{w}\\_{., t} + \\bar{w}\\_{., .})\n$$\n\n4. `DuckMundlakEventStudy`: Two-way mundlak with dynamic treatment effects. This incorporates treatment-cohort FEs ($\\psi\\_i$), time-period FEs ($\\gamma\\_t$) and dynamic tre",
    "url": "https://github.com/py-econometrics/duckreg",
    "last_updated": "2025-08-31T20:47:39+00:00"
  },
  {
    "full_name": "13bzhang/fbsample",
    "name": "fbsample",
    "description": "R Package for Quota Sampling on Facebook",
    "language": "R",
    "topics": [
      "r",
      "facebook-marketing-api",
      "sampling"
    ],
    "readme": "# fbsample\n\nR Package for Quota Sampling on Facebook\n\nThis R package makes it easier for one to conduct quota sampling via Facebook advertisements. `fbsample` serves two main functions. First, it allows one to specify which demographic groups to target. Second, it uses R wrapper functions in [fbRads](https://github.com/cardcorp/fbRads) to create ads in batches and upload them via the [Facebook Marketplace API](https://developers.facebook.com/docs/marketing-apis).\n\n-----------------------------\n\n## Updates\n\n3/18/2017: I added four helpful datasets in the `data` folder that include the ids and keys for ad targets. Now you can easily look up targets for level of education, ethnicity (US), politics (US), and US states/regions.\n\n3/19/2017: Facebook has removed `ethnic_affinity` as an ad target category after [receiving negative press](https://www.nytimes.com/2016/11/12/business/media/facebook-will-stop-some-ads-from-targeting-users-by-race.html?_r=0). But you can still recruit people by ethnic affinity using the `behavior` category. I removed `ethnic_affinity` as a parameter from the function `create_target` to reflect this change. To sample white respondents, you would need to exclude all the minority ethnic affinity groups. I am providing this information for social scientific purposes only. Don't use the functionality for racist purposes! \n\n-----------------------------\n\n## In a nutshell\n\nFacebook allows advertisers to target audiences by demographic groups. Using the [Facebook Marketing API](https://developers.facebook.com/docs/marketing-apis), researchers can create ads that target a large number of strata for quota sampling. Researchers recruit respondents by advertising their online survey using Facebook ads. \n\n## Understanding the structure of Facebook ad campaigns\n\nFacebook ad campaigns have a three-level structure. For the purposes of quota sampling, each sampling project is a **Campaign**. The Campaign objective determines how you pay to recruit respondents. F",
    "url": "https://github.com/13bzhang/fbsample",
    "last_updated": "2025-01-07T20:34:35+00:00"
  },
  {
    "full_name": "5harad/openpolicing",
    "name": "openpolicing",
    "description": "The Stanford Open Policing Project - https://openpolicing.stanford.edu",
    "language": "R",
    "topics": [],
    "readme": "This repository contains the code and accessory files needed to process the state data and reproduce the analysis in the Stanford Open Policing Project. It is organized as follows: \n  \n  * `/data_and_models` stores the state data and fitted models. It is initially empty.\n  * `/resources` contains small accessory files needed for the analysis: for example, mappings from the raw data values to standardized data values. \n  * `/results` contains the tables, figures, and data underlying the figures in the paper. \n  * `/src` contains the code.\n  * `/tutorial` contains R tutorials for analyzing the data.\n\nWhen using this code or data, please cite our [working paper](https://arxiv.org/abs/1706.05678):\n\nE. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.\n\n## Instructions for working with the data and reproducing results in the paper\n\n1. Download the state data from the [Open Policing website](https://openpolicing.stanford.edu/data/) and store it in `data_and_models/data/`. Each state should be stored in its own folder using a two-letter state code in the following format: \"TWO_LETTER_CODE/TWO_LETTER_CODE-clean.csv.gz\". For example, Rhode Island would be stored in \"RI/RI-clean.csv.gz\".\n2. Download the external datasets necessary for the analysis -- the [Census data](https://stacks.stanford.edu/file/druid:py883nd2578/census-clean.csv.gz) and the [PPCS](https://stacks.stanford.edu/file/druid:py883nd2578/ppcs.tsv) data -- from the Open Policing website and store them in `data_and_models/external_datasets/`. \n3. The video tutorial posted on the Open Policing website provides an introduction to performing analyses on the data. \n4. The script used to reproduce all results in the paper is `src/recreate_results_in_paper.R`. All analyses should be run using this script, not by running other scripts directly, since it sources the necessary dependenci",
    "url": "https://github.com/5harad/openpolicing",
    "last_updated": "2024-04-19T06:49:38+00:00"
  },
  {
    "full_name": "maelle/usaqmindia",
    "name": "usaqmindia",
    "description": ":mask: Data from the U.S. Embassy and Consulate air quality monitors in India :mask:",
    "language": "R",
    "topics": [],
    "readme": "-   [Introduction to the data and the repo](#introduction-to-the-data-and-the-repo)\n-   [R package](#r-package)\n-   [Data format](#data-format)\n-   [Time series plot](#time-series-plot)\n-   [Calendar plot](#calendar-plot)\n-   [Contributing](#contributing)\n\n[![Build Status](https://travis-ci.org/masalmon/usaqmindia.svg?branch=master)](https://travis-ci.org/masalmon/usaqmindia) [![Build status](https://ci.appveyor.com/api/projects/status/lujc2gn88smyvhrq?svg=true)](https://ci.appveyor.com/project/masalmon/usaqmindia) [![codecov.io](https://codecov.io/github/masalmon/usaqmindia/coverage.svg?branch=master)](https://codecov.io/github/masalmon/usaqmindia?branch=master)\n\nIntroduction to the data and the repo\n=====================================\n\nThe U.S. Embassy and Consulates General in India maintain an air quality monitoring program with on-site measuring instruments and put the corresponding data [on this website](http://newdelhi.usembassy.gov/airqualitydata.html). There are csv files for 2013 and 2014, for 2015 except December which is in a pdf, and various csv/pdf for the months of the beginning of 2016. From August 2016 I chose to use OpenAQ and in particular [ropenaq](https://github.com/ropenscilabs/ropenaq) to get new data.\n\nIn this repository I have made a copy of these files and provide a R code for wrangling them to get a single csv with all measures for Delhi, Mumbai, Kolkata, Hyderabad and Chennai. Refer to original source for licensing questions.\n\n*Useful even for non R users: You will find the raw data [in this folder](inst/extdata) and my wrangling code is [here](inst/pm25_consulate.R). The resulting csv is [here](inst/pm25USA.csv). I will try to update the repository as new data comes in on the embassy website.*\n\nR package\n=========\n\nI have made a R package out of the data so that my fellow R users can easily play with the data. The package imports [`ggTimeSeries`](https://github.com/Ather-Energy/ggTimeSeries) for doing the calendar plot. It can be insta",
    "url": "https://github.com/maelle/usaqmindia",
    "last_updated": "2018-05-23T05:17:35+00:00"
  },
  {
    "full_name": "kjhealy/myriad",
    "name": "myriad",
    "description": "Themes for ggplot, using the Adobe Myriad typeface. http://kjhealy.github.io/myriad/",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# myriad\n\n## About\n\nMyriad Pro-based theme for ggplot, providing `theme_myriad_semi()`,\n`theme_myriad_map()` and `theme_myriad_nymap()`. All based on Myriad Pro\nSemiCondensed face, with Myriad Pro Condensed also available for labels\ninside the plot area.\n\nNote that this repo does not include the Myriad font files (the `.otf`\nfiles), which are owned by Adobe. You may already have them installed on\nyour computer, or they may have come bundled with your copy of Acrobat\nReader or other Adobe software.\n\n## Usage\n\nWhen creating PDFs, use with `showtext`.\n\n``` r\nlibrary(tidyverse)\nlibrary(ggrepel)\n```\n\n``` r\nlibrary(showtext)\nshowtext_opts(dpi = 300)\nshowtext_auto()\n\nlibrary(myriad)\n\n# Semi variant\nimport_myriad_semi()\n\n# Condensed for in-graph text\nimport_myriad_condensed()\n\n# ggplot theme\ntheme_set(theme_myriad_semi())\n```\n\n# Basic Test\n\n``` r\np <- ggplot(mtcars, \n            aes(x = wt, y = mpg)) + \n  geom_point() + \n  labs(x = \"Weight\", y = \"Miles per Gallon\", \n       title = \"This is the title\", subtitle = \"This is the subtitle\", \n       caption = \"This is the caption\")  \n\np\n```\n\n<img src=\"man/figures/README-unnamed-chunk-3-1.png\" width=\"100%\" />\n\n# Check PDF works\n\n``` r\nggsave(\"man/figures/ggfont-test-myrnew-2.pdf\", p, width = 6, height = 4)\n```\n\n# With labels (using Myriad Pro Condensed)\n\n``` r\nout <- mtcars |>\n  mutate(car = rownames(mtcars)) |> \n  as_tibble() |> \n  ggplot(aes(x = wt, y = mpg, label = car)) + \n  geom_point() + \n  geom_text_repel(family = \"Myriad Pro Condensed\") +\n  facet_wrap(~ cyl, ncol = 1) + \n  labs(title = \"Title\", \n       subtitle = \"Subtitle\")\n\nout\n#> Warning: ggrepel: 2 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps\n```\n\n<img src=\"man/figures/README-unnamed-chunk-5-1.png\" width=\"100%\" />\n\n## Check PDF works\n\n``` r\nggsave(\"man/figures/ggfont-test-myrnew-3.pdf\", out, width = 12, height = 8)\n```\n",
    "url": "https://github.com/kjhealy/myriad",
    "last_updated": "2025-07-09T13:27:18+00:00"
  },
  {
    "full_name": "ewenharrison/finalfit",
    "name": "finalfit",
    "description": "Quickly create elegant regression results tables and plots when modelling in R",
    "language": "HTML",
    "topics": [],
    "readme": "[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/finalfit)](https://cran.r-project.org/package=finalfit)\n[![CRAN_Status_Badge](https://cranlogs.r-pkg.org/badges/finalfit)](https://cran.r-project.org/package=finalfit)\n[![CRAN_Status_Badge](https://cranlogs.r-pkg.org/badges/grand-total/finalfit)](https://cranlogs.r-pkg.org/badges/grand-total/finalfit)\n[![TravisCRAN_Status_Badge](https://api.travis-ci.com/ewenharrison/finalfit.svg?branch=master)](https://app.travis-ci.com/github/ewenharrison/finalfit)\n[![Build status](https://ci.appveyor.com/api/projects/status/3wpgw2rs6vn1lsrn?svg=true)](https://ci.appveyor.com/project/ewenharrison/finalfit)\n[![Coverage status](https://codecov.io/gh/ewenharrison/finalfit/branch/master/graph/badge.svg)](https://app.codecov.io/github/ewenharrison/finalfit?branch=master)\n\nfinalfit <img src=\"man/figures/finalfit_hex.png\" align=\"right\" />\n==============================\n\n\nThe `finalfit` package provides functions that help you quickly create elegant final results tables and plots when modelling in R. These can easily be exported as Word documents, PDFs, or html files. \n\nIts design follows Hadley Wickham's [tidy tool manifesto](https://tidyverse.tidyverse.org/articles/manifesto.html).\n\nIn addition, it provides functions for identifying and handling missing data, together with a number of functions to bootstrap simulate regression model results. \n\n## Installation\n\nYou can install `finalfit` from CRAN:\n\n``` r\ninstall.packages(\"finalfit\")\n```\n\nIt is recommended that this package is used together with `dplyr` which can be installed via:\n\n``` r\ninstall.packages(\"dplyr\")\n```\n\n## Documentation\n\nThe package documentation is maintained independently at [finalfit.org](https://finalfit.org/). \n\n## Examples\n\nSee [Get started](https://finalfit.org/articles/finalfit.html) and the [All tables](https://finalfit.org/articles/all_tables_examples.html) vignettes for extensive examples.  \n\n### Crosstable / table 1\n\n``` r\n# Crosstable \nexplanatory = c(",
    "url": "https://github.com/ewenharrison/finalfit",
    "last_updated": "2025-03-21T22:41:29+00:00"
  },
  {
    "full_name": "mattmakai/choose-your-own-adventure-presentations",
    "name": "choose-your-own-adventure-presentations",
    "description": "\"Choose Your Own Adventure\" live voting presentations with Reveal.js, Flask, WebSockets and SMS.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Choose Your Own Adventure Presentations\nUse this code to create real-time branching presentations so \nyour audience can choose the path through your next technical talk.\n\nThis project also is the default WSGI application for the\n[Full Stack Python Guide to Deployments](http://www.deploypython.com/) book.\n\nHere's a screenshot of the vanilla choices screen. You can of course modify\nthis initial screen with your own styles and visualizations.\n\n<img src=\"./cyoa/static/img/cyoa-choices.jpg\">\n\nThe screenshot may not look like much at first. That's where your story and\ncontent come in. To see the potential of audience voting during a presentation\nto determine the path of the story, check out the \"see it in action\" section\nbelow.\n\n## Code walkthrough\nDetailed installation instructions and a walkthrough of the code base\ncan be found in \n[this blog post I wrote for Twilio](https://www.twilio.com/blog/2014/11/choose-your-own-adventure-presentations-with-reveal-js-python-and-websockets.html)\nalong with the follow-up three part tutorial:\n\n1. [Wizards Only](https://www.twilio.com/blog/2015/03/choose-your-own-adventures-presentations-wizard-mode-part-1-of-3.html)\n1. [Even Wizards Need Web Forms](https://www.twilio.com/blog/2015/05/choose-your-own-adventure-presentations-wizard-mode-part-2-of-3.html)\n1. [Voting with a Wand (or Smartphone)](https://www.twilio.com/blog/2015/07/choose-your-own-adventure-presentations-flask-reveal-js-websockets.html)\n\n\n## See it in action\nYou can check out how\nthese branching presentations work if you watch the DjangoCon 2014 video \n\"[Choose Your Own Django Deployment Adventure](https://www.youtube.com/watch?v=QrFEKghISEI)\" \nor \n\"[Choose Your Own WSGI Deployment Adventure](https://www.youtube.com/watch?v=R-YvQ_YvzA4)\"\nwith [Matt Makai](https://twitter.com/mattmakai) and \n[Kate Heddleston](https://twitter.com/heddle317). \n\nHere's how the screen looks initially.\n<img src=\"./cyoa/static/img/cyoa-no-votes.jpg\">\n\nAfter votes start coming in...\n<img src=\".",
    "url": "https://github.com/mattmakai/choose-your-own-adventure-presentations",
    "last_updated": "2024-11-15T04:23:13+00:00"
  },
  {
    "full_name": "floydhub/dl-setup",
    "name": "dl-setup",
    "description": "Instructions for setting up the software on your deep learning machine",
    "language": "",
    "topics": [],
    "readme": "[Website](https://www.floydhub.com) • [Docs](https://docs.floydhub.com) • [Forum](https://forum.floydhub.com) • [Twitter](https://twitter.com/floydhub_) • [We're Hiring](https://angel.co/floydhub)\n\n[![FloydHub Logo](https://github.com/floydhub/static/blob/master/Group.png)](https://www.floydhub.com)\n\n## Update: I've built a quick tool based on this repo. Start running your Tensorflow project on AWS in <30seconds using Floyd. See [www.floydhub.com](https://www.floydhub.com). It's free to try out. \n### Happy to take feature requests/feedback and answer questions - mail me sai@floydhub.com.\n\n## Setting up a Deep Learning Machine from Scratch (Software)\nA detailed guide to setting up your machine for deep learning research. Includes instructions to install drivers, tools and various deep learning frameworks. This was tested on a 64 bit machine with Nvidia Titan X, running Ubuntu 14.04\n\nThere are several great guides with a similar goal. Some are limited in scope, while others are not up to date. This guide is based on (with some portions copied verbatim from):\n* [Caffe Installation for Ubuntu](https://github.com/tiangolo/caffe/blob/ubuntu-tutorial-b/docs/install_apt2.md)\n* [Running a Deep Learning Dream Machine](http://graphific.github.io/posts/running-a-deep-learning-dream-machine/)\n\n### Table of Contents\n* [Basics](#basics)\n* [Nvidia Drivers](#nvidia-drivers)\n* [CUDA](#cuda)\n* [cuDNN](#cudnn)\n* [Python Packages](#python-packages)\n* [Tensorflow](#tensorflow)\n* [OpenBLAS](#openblas)\n* [Common Tools](#common-tools)\n* [Caffe](#caffe)\n* [Theano](#theano)\n* [Keras](#keras)\n* [Torch](#torch)\n* [X2Go](#x2go)\n\n### Basics\n* First, open a terminal and run the following commands to make sure your OS is up-to-date\n\n        sudo apt-get update  \n        sudo apt-get upgrade  \n        sudo apt-get install build-essential cmake g++ gfortran git pkg-config python-dev software-properties-common wget\n        sudo apt-get autoremove \n        sudo rm -rf /var/lib/apt/lists/*\n\n### Nvidia D",
    "url": "https://github.com/floydhub/dl-setup",
    "last_updated": "2025-08-31T20:18:31+00:00"
  },
  {
    "full_name": "unitedstates/contact-congress",
    "name": "contact-congress",
    "description": "Sending electronic written messages to members of Congress by reverse engineering their contact forms.",
    "language": "Python",
    "topics": [],
    "readme": "## Contacting Congress\n\n\n\nThis project defines an open data format to describe the contact forms of members of Congress, and we're currently working on creating a definitive, updated list of active legislators' contact forms according to that format. Once completed, the files in this project can be used to power any system that allows US citizens to send messages to their elected members of Congress.\n\n\n\n### Current Status\n\n\n\n- [✓] Create a standardized YAML data format for describing legislator contact forms.\n\n- [✓] Create a bookmarklet to make it easy to generate YAML files.\n\n- [✓] Make a testing suite to error-check generated files.\n\n- [✓] Create YAML files for almost all 100 members of the Senate.\n\n- [✓] Create YAML files for almost all 439 members of the House of Representatives.\n\n- [ - ] Tested files.\n\n**Note:** As of January 1, 2017, YAML files for members of the House of Representatives are no longer actively being maintained, as we are using Congress's CWC API to send messages instead of contact forms.\n\nYou can read about the CWC API here: http://www.house.gov/content/vendors/cwc/\n\nOur application for reading these YAMLs and submitting messages, Phantom of the Capitol, has been updated to use the CWC API, in the CWC branch here: https://github.com/EFForg/phantom-of-the-capitol/tree/cwc\n\nYou are still welcome to submit PRs for YAMLs for the House of Representatives, we will be happy to continue merging them if these are files you still use!\n\n\n\n\n\n### Why is this important?\n\n\n\nThis dataset will allow open government and advocacy organizations to create tools that allow citizens to email their elected representatives.\n\n\n\nAt present, the only way to send electronic messages to members of Congress is to visit each legislator's contact form, or to use a proprietary third-party vendor on an advocacy organization's website. The contact forms are cumbersome and difficult to use, and third party vendors are expensive. A common, up-to-date dataset will allow for simpler",
    "url": "https://github.com/unitedstates/contact-congress",
    "last_updated": "2025-08-17T13:39:06+00:00"
  },
  {
    "full_name": "gojiplus/generator",
    "name": "generator",
    "description": "Autogenerate list of repositories in the org., summarize via OpenAI, and publish online",
    "language": "Python",
    "topics": [],
    "readme": "## 🏭 Generator: AutoCreate and Update GitHub Website With Summary of All the Public Repositories\n\nTired of manually updating your organization's website with repository details? Say hello to the ultimate GitHub repository summary generator! \n\n### ✨ Features\n\n- **Automated Discovery**: Crawls through all public repositories in an organization\n- **AI-Powered Summaries**: Generates crisp, compelling two-sentence descriptions using OpenAI\n- **Flexible Output**: Produces a clean CSV ready for website integration\n- **GitHub Actions Ready**: Seamlessly runs as a workflow\n\n## 🚦 Roadmap\n\n### Immediate Future\n- 🌐 Create a Jekyll-ready repository for easy website deployment\n\n### Upcoming Features\n- 📈 Commit-based change tracking\n- 🎛 Granular repository filtering:\n  - Limit to public repositories\n  - Filter by star count\n  - Custom inclusion/exclusion rules\n\n## 🔧 Configuration\n\n### Prerequisites\n- GitHub Token\n- OpenAI API Key\n\n### GitHub Actions Setup\n```yaml\n- name: Generate Repo Summaries\n  uses: yourusername/repo-summarizer@v1\n  with:\n    org_name: 'your-org-name'\n```\n\n## 💡 Why Use This?\n\nKeeping your organization's digital presence up-to-date shouldn't be a chore. This tool automates the mundane, letting you focus on what matters - building awesome software!\n\n## 📄 License\n\nMIT @ MatmulAI\n",
    "url": "https://github.com/gojiplus/generator",
    "last_updated": "2025-05-15T01:27:11+00:00"
  },
  {
    "full_name": "gadenbuie/countdown",
    "name": "countdown",
    "description": "⏲ countdown timer for slides and HTML docs in Quarto, R Markdown, and Shiny",
    "language": "JavaScript",
    "topics": [
      "rstats",
      "rmarkdown",
      "xaringan",
      "slides",
      "countdown-timer",
      "html",
      "quarto-extension",
      "quarto-slides"
    ],
    "readme": "# countdown\n\n<!-- badges: start -->\n[![CRAN status](https://www.r-pkg.org/badges/version/countdown)](https://CRAN.R-project.org/package=countdown)\n[![countdown on r-universe/gadenbuie](https://gadenbuie.r-universe.dev/badges/countdown)](https://gadenbuie.r-universe.dev)\n[![R-CMD-check](https://github.com/gadenbuie/countdown/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/gadenbuie/countdown/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n**countdown** makes it easy to drop in a simple countdown timer in slides and HTML documents written in [Quarto](https://quarto.org) or [R Markdown](https://rmarkown.rstudio.com).\n\n``` r\nlibrary(countdown)\n\ncountdown(minutes = 0, seconds = 15)\n```\n\n<img src=\"r/man/figures/countdown.gif\" width=\"200px\">\n\n### Want to know more?\n\nCheck out countdown in its native environment in the [countdown presentation](https://pkg.garrickadenbuie.com/countdown/).\n\n## Installation\n\n### Quarto Extension\n\n``` bash\nquarto add gadenbuie/countdown/quarto\n```\n\nThis will install the extension under the `_extensions` subdirectory. If\nyou’re using version control, you will want to check in this directory.\n\nTo use the extension, use the `{{< countdown >}}` shortcode. For example, a countdown clock can be created by writing anywhere:\n\n``` default\n{{< countdown \"5:30\" >}}\n```\n\nLearn more in the [Quarto countdown README](quarto/README.md).\n\n### R Package\n\nYou can install countdown from CRAN\n\n``` r\ninstall.packages(\"countdown\")\n```\n\nor you can install the development version of countdown from [gadenbuie.r-universe.dev](https://gadenbuie.r-universe.dev/ui#package:countdown)\n\n``` r\noptions(repos = c(\n  gadenbuie = 'https://gadenbuie.r-universe.dev',\n  getOption(\"repos\")\n))\n\ninstall.packages('countdown')\n```\n\nor from GitHub\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"gadenbuie/countdown\", subdir = \"r\")\n```\n\nLearn more in the [R countdown README](r/README.md).\n",
    "url": "https://github.com/gadenbuie/countdown",
    "last_updated": "2025-09-01T18:58:36+00:00"
  },
  {
    "full_name": "rOpenGov/rtimes",
    "name": "rtimes",
    "description": "R wrapper for NYTimes API for government data - ABANDONED",
    "language": "R",
    "topics": [
      "api",
      "data",
      "journalism",
      "rstats",
      "r",
      "propublica-api",
      "congress-api",
      "propublica"
    ],
    "readme": "rtimes\n======\n\n[![Project Status: Abandoned – the project has been abandoned and the author(s) do not intend on continuing development.](https://www.repostatus.org/badges/latest/abandoned.svg)](https://www.repostatus.org/#abandoned)\n\n`rtimes` has been abandoned - get in touch if you want to take over maintainance.\n\n## Meta\n\n* Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.\n* Maintainer: Scott Chamberlain\n* License: MIT\n* Report any problems in the [Issues Tracker](https://github.com/ropengov/rtimes/issues), or just fork and submit changes, etc.\n",
    "url": "https://github.com/rOpenGov/rtimes",
    "last_updated": "2025-06-30T18:42:20+00:00"
  },
  {
    "full_name": "dougbrion/pytorch-classification-uncertainty",
    "name": "pytorch-classification-uncertainty",
    "description": "This repo contains a PyTorch implementation of the paper: \"Evidential Deep Learning to Quantify Classification Uncertainty\"",
    "language": "Python",
    "topics": [
      "paper",
      "pytorch",
      "evidential-deep-learning",
      "uncertainty-neural-networks",
      "classification",
      "mnist",
      "mnist-classification",
      "dirichlet-distributions",
      "uncertainty",
      "torchvision"
    ],
    "readme": "# Evidential Deep Learning to Quantify Classification Uncertainty\n\n[![arXiv](http://img.shields.io/badge/arXiv-1806.01768-B31B1B.svg)](https://arxiv.org/abs/1806.01768)\n\nThe purpose of this repository is to provide an easy-to-run demo using PyTorch with low computational requirements for the ideas proposed in the paper *Evidential Deep Learning to Quantify Classification Uncertainty*. The authors of the paper originally used Tensorflow in their implementation.\n\nThe paper can be accessed over at: [http://arxiv.org/abs/1806.01768](http://arxiv.org/abs/1806.01768)\n\nPart of: [Advances in Neural Information Processing Systems 31 (NIPS 2018)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)\n\n## 📝 Table of Contents\n- [About](#about)\n- [Paper Abstract](#abstract)\n- [Authors](#authors)\n- [Demonstration](#demonstration)\n- [Usage](#usage)\n- [License](./LICENSE)\n- [Requirements](./requirements.txt)\n\n## 🧐 About <a name = \"about\"></a>\nThe purpose of this repository is to provide an easy-to-run demo using PyTorch with low computational requirements for the ideas proposed in the paper *Evidential Deep Learning to Quantify Classification Uncertainty*. The authors of the paper originally used Tensorflow in their implementation.\n\nThe paper can be accessed over at: [http://arxiv.org/abs/1806.01768](http://arxiv.org/abs/1806.01768)\n\nPart of: [Advances in Neural Information Processing Systems 31 (NIPS 2018)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)\n\n## 📚 Paper Abstract <a name = \"abstract\"></a>\n\nDeterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of",
    "url": "https://github.com/dougbrion/pytorch-classification-uncertainty",
    "last_updated": "2025-08-27T06:24:26+00:00"
  },
  {
    "full_name": "MilesMcBain/datapasta",
    "name": "datapasta",
    "description": "On top of spaghetti, all covered in cheese....",
    "language": "R",
    "topics": [
      "r",
      "copypaste",
      "excel",
      "addin",
      "clipboard",
      "tibble"
    ],
    "readme": "# datapasta 3.1.1 'Leave to Simmer'\n[![r-universe status badge](https://milesmcbain.r-universe.dev/badges/datapasta)](https://milesmcbain.r-universe.dev)\n[![CRAN status.](http://www.r-pkg.org/badges/version/datapasta)](http://www.r-pkg.org/pkg/datapasta)\n[![Downloads](http://cranlogs.r-pkg.org/badges/datapasta)](https://CRAN.R-project.org/package=datapasta)\n\n<img src=\"https://raw.githubusercontent.com/milesmcbain/datapasta/master/inst/media/hex_web.png\" width=\"200\"/>\n\n# The Goods\n![pow!](https://raw.githubusercontent.com/milesmcbain/datapasta/master/inst/media/tribble_paste.gif)\n\n# Introducing datapasta\n\n`datapasta` is about reducing resistance associated with copying and pasting data to and from R. It is a response to the realisation that I often found myself using intermediate programs like Sublime to munge text into suitable formats. Addins and functions in `datapasta` support a wide variety of input and output situations, so it (probably) \"just works\". Hopefully tools in this package will remove such intermediate steps and associated frustrations from our data slinging workflows.  \n\n# Prerequisites\n* Linux users will need to install either `xsel` or `xclip`. These applications provide an interface to X selections (clipboard-like).\n    - For example: `sudo apt-get install xsel` - it's 72kb...\n* Windows and MacOS have nothing extra to do.\n\n# Installation\n\n## R Universe (preferred)\n\n1. install with R universe repo:\n\n```\ninstall.packages(\n   \"datapasta\", \n   repos = c(mm = \"https://milesmcbain.r-universe.dev\", getOption(\"repos\")))\n```\n\n2. Set the keyboard shortcuts using **Tools** -> **Addins** -> **Browse Addins**, then click **Keyboard Shortcuts...**\n\n\n## CRAN (outdated)\n\nFor now, no further versions of `datapasta` will be going to CRAN. There are some known bugs in the CRAN version that have been fixed in `3.1.1`.\n\n1. `install.packages(\"datapasta\")`\n\n# Usage\n\n## Use with RStudio\n\n### Getting data into source\n\nAt the moment this package contains these RStudio addi",
    "url": "https://github.com/MilesMcBain/datapasta",
    "last_updated": "2025-08-21T15:19:56+00:00"
  },
  {
    "full_name": "StackExchange/stack-blog",
    "name": "stack-blog",
    "description": "Stack Overflow Blog",
    "language": "HTML",
    "topics": [],
    "readme": "# [Stack Overflow Blog](http://blog.stackoverflow.com/)\n\nNOTE: This repository is no longer being actively maintained since the Stack Overflow blog has been moved off of Jekyll. We've kept it available for you to use and explore, but any PRs or issues will no longer be addressed. Thank you to everyone who has contributed!\n\n## Getting Started\nThis blog runs on [Jekyll](http://jekyllrb.com/). Posts are written in [markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).\n\nIf you are actively involved in improving the infrastructure of this project, you should read the documentation for these tools thoroughly (they're pretty short as it is). If you are simply contributing, this guide should be enough to get you going.\n\n## Quick Links\n - [Add yourself as a contributor on the blog](https://github.com/StackExchange/blog/new/master/_people) ([See an example file](https://github.com/StackExchange/blog/edit/master/_people/jonhmchan.md))\n - [Publish a new post](https://github.com/StackExchange/blog/new/master/_posts) ([See an example file](https://github.com/StackExchange/blog/edit/master/_posts/2014-01-28-My-First-Six-Weeks-Working-At-Stack-Overflow.md), [published version](http://stackexchange.github.io/blog/01/28/My-First-Six-Weeks-Working-At-Stack-Overflow/))\n - [Dev blog (where to preview posts before going live; mainly for employees, but not secret)](http://dev.blog.stackexchange.com/)\n - [Internal image uploader (for employees only, must be on VPN or on office network)](http://blogtools.ds.stackexchange.com/)\n - [Company editorial board and calendar (for employees only, post ideas and see schedule)](https://trello.com/b/WYAPaUEC/blog-editorial-and-calendar)\n\n## What is this blog for?\nThis is the official company blog for Stack Overflow. Everything related to new features, announcements, engineering projects, and all things Stack Overflow live on this blog. The contributors on this blog are Stack Overflow employees, but in the future, we may open this",
    "url": "https://github.com/StackExchange/stack-blog",
    "last_updated": "2025-02-07T16:59:31+00:00"
  },
  {
    "full_name": "libindic/indic-trans",
    "name": "indic-trans",
    "description": "The project aims on adding a state-of-the-art transliteration module for cross transliterations among all Indian languages including English.",
    "language": "Python",
    "topics": [],
    "readme": "indic-trans\n===========\n\n|travis| |coverage| |CircleCI| |Documentation Status|\n\n----\n\nThe project aims on adding a state-of-the-art transliteration module for cross transliterations among all Indian languages including English and Urdu.\n\nThe module currently supports the following languages:\n\n  * Hindi       \n  * Bengali\n  * Gujarati\n  * Punjabi\n  * Malayalam\n  * Kannada\n  * Tamil\n  * Telugu\n  * Oriya\n  * Marathi\n  * Assamese\n  * Konkani\n  * Bodo\n  * Nepali\n  * Urdu\n  * English\n\nLinks & References\n------------------\n\n* `Official source code repo <https://github.com/libindic/indic-trans>`_\n* `HTML documentation <http://indic-trans.readthedocs.org>`_\n* `Transliteration Blog <http://irshadbhat.github.io/gsoc>`_\n* Mailing list: silpa-discuss@nongnu.org\n* IRC channel: ``#silpa`` at ``irc.freenode.net``\n\nInstallation\n------------\n\nDependencies\n^^^^^^^^^^^^\n\n`indictrans`_ requires `cython`_, and `SciPy`_.\n\n.. _`indictrans`: https://github.com/libindic/indic-trans\n\n.. _`cython`: http://docs.cython.org/src/quickstart/install.html\n\n.. _`Scipy`: http://www.scipy.org/install.html\n\nClone & Install\n^^^^^^^^^^^^^^^\n\n::\n\n    Clone the repository:\n        git clone https://github.com/libindic/indic-trans.git\n        ------------------------OR--------------------------\n        git clone https://github.com/irshadbhat/indic-trans.git\n\n    Change to the cloned directory:\n        cd indic-trans\n        pip install -r requirements.txt\n        pip install .\n\nExamples\n--------\n\n1. From Console:\n^^^^^^^^^^^^^^^^\n\n.. parsed-literal::\n\n    indictrans --h\n\n    -h, --help          show this help message and exit\n    -v, --version       show program's version number and exit\n    -s, --source        select language (3 letter ISO-639 code) {hin, guj, pan,\n                        ben, mal, kan, tam, tel, ori, eng, mar, nep, bod, kok,\n                        asm, urd}\n    -t, --target        select language (3 letter ISO-639 code) {hin, guj, pan,\n                        ben, mal, kan, tam, tel, ori, ",
    "url": "https://github.com/libindic/indic-trans",
    "last_updated": "2025-07-02T18:26:15+00:00"
  },
  {
    "full_name": "zenml-io/mlstacks",
    "name": "mlstacks",
    "description": "A series of Terraform based recipes to provision popular MLOps stacks on the cloud.",
    "language": "HCL",
    "topics": [
      "infrastructure-as-code",
      "ml",
      "mlops"
    ],
    "readme": "# MLStacks: Deploy your MLOps infrastructure in minutes\n\n## 🌰 In a nutshell: What is MLStacks?\n\n[MLStacks](https://mlstacks.zenml.io) is a\n[Python package](https://pypi.org/project/mlstacks/) that allows you to quickly\nspin up MLOps infrastructure using Terraform. It is designed to be used with\n[ZenML](https://zenml.io), but can be used with any MLOps tool or platform.\n\nSimply write stack and component YAML specification files and deploy them using\nthe MLStacks CLI. MLStacks will take care of the rest. We currently support\nmodular MLOps stacks on AWS, GCP and K3D (for local use).\n\n## 👷 Why We Built MLStacks\n\n[![maintained-by-zenml](https://user-images.githubusercontent.com/3348134/173032050-ad923313-f2ce-4583-b27a-afcaa8b355e2.png)](https://github.com/zenml-io/zenml)\n\nWhen we first created [ZenML](https://zenml.io) as an extensible MLOps framework\nfor creating portable, production-ready MLOps pipelines, we saw many of our\nusers having to deal with the pain of deploying infrastructure from scratch to\nrun these pipelines. The community consistently asked questions like:\n\n- How do I deploy tool X with tool Y?\n- Does a combination of tool X with Y make sense?\n- Isn't there an easy way to just try these stacks out to make an informed\n  decision?\n\nTo address these questions, the ZenML team presents you a series of\nTerraform-based stacks to quickly provision popular combinations of MLOps tools.\nThese stacks will be useful for you if:\n\n- You are at the start of your MLOps journey, and would like to explore\n  different tools.\n- You are looking for guidelines for production-grade deployments.\n- You would like to run your MLOps pipelines on your chosen\n  [ZenML Stack](https://docs.zenml.io/user-guide/starter-guide/understand-stacks).\n\n🔥 **Do you use these tools or do you want to add one to your MLOps stack?** At\nZenML, we are looking for design partnerships and collaboration to implement and\ndevelop these MLOps stacks in a real-world setting.\n\nIf you'd like to learn more, plea",
    "url": "https://github.com/zenml-io/mlstacks",
    "last_updated": "2025-07-02T19:26:25+00:00"
  },
  {
    "full_name": "teamookla/ooklaOpenDataR",
    "name": "ooklaOpenDataR",
    "description": "R package for Ookla's open data",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\nooklaOpenDataR\n==============\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nThe goal of ooklaOpenDataR is to make it easier to access data from\nOokla’s [open data\nprogram](https://registry.opendata.aws/speedtest-global-performance/).\nThis dataset provides global fixed broadband and mobile (cellular)\nnetwork performance metrics in zoom level 16 [web Mercator\ntiles](https://en.wikipedia.org/wiki/Tiled_web_map) (approximately 610.8\nmeters by 610.8 meters at the equator). The tiles are updated quarterly.\n\nInstallation\n------------\n\nYou can install the development version from\n[GitHub](https://github.com/) with:\n\n    # install.packages(\"remotes\")\n    remotes::install_github(\"teamookla/ooklaOpenDataR\")\n\nExamples\n--------\n\nThis is how you can get the global dataset for mobile network data in Q2\n2020:\n\n    library(ooklaOpenDataR)\n\n    mobile_q2 <- get_performance_tiles(service = \"mobile\", quarter = 2, year = 2020)\n\nOr you can get the fixed broadband data for the same quarter as an `sf`\ndata frame with the `sf` argument.\n\n    fixed_q2_sf <- get_performance_tiles(service = \"mobile\", quarter = 2, year = 2020, sf = TRUE)\n\nThe package vignettes demonstrate how to filter the tiles to a\nparticular area of interest using the `filter_by_quadkey()` function.\n\nLicense\n-------\n\nThe dataset is licensed under the [Creative Commons license for\nnon-commerical use](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n\n**Recommended citation**\n\nSpeedtest® by Ookla® Global Fixed and Mobile Network Performance Maps.\nBased on analysis by Ookla of Speedtest Intelligence® data for \\[DATA\nTIME PERIOD\\]. Provided by Ookla and accessed \\[DAY MONTH YEAR\\]. Ookla\ntrademarks used under license and reprinted with permission.\n",
    "url": "https://github.com/teamookla/ooklaOpenDataR",
    "last_updated": "2025-08-14T20:11:25+00:00"
  },
  {
    "full_name": "hrbrmstr/scamtracker",
    "name": "scamtracker",
    "description": "R pacakge interface to the BBB ScamTracker : https://www.bbb.org/scamtracker/us",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-cyber",
      "scamtracker"
    ],
    "readme": "\n`scamtracker` is an R pacakge interface to the BBB ScamTracker <https://www.bbb.org/scamtracker/us>\n\nThe following functions are implemented:\n\n- `bbb_search`: Search the BBB ScamTracker for incidents\n- `scam_details`:\tRetrive details about a BBB spam report\n\n### News\n\n- Version  released\n\n### Installation\n\n\n```r\ndevtools::install_github(\"hrbrmstr/scamtracker\")\n```\n\n\n\n### Usage\n\n\n```r\nlibrary(scamtracker)\n\n# current verison\npackageVersion(\"scamtracker\")\n```\n\n```\n## [1] '0.1.0'\n```\n\n### Test Results\n\n\n```r\nlibrary(scamtracker)\nlibrary(testthat)\n\ndate()\n```\n\n```\n## [1] \"Mon Jan 11 18:49:13 2016\"\n```\n\n```r\ntest_dir(\"tests/\")\n```\n\n```\n## testthat results ========================================================================================================\n## OK: 0 SKIPPED: 0 FAILED: 0\n```\n\n",
    "url": "https://github.com/hrbrmstr/scamtracker",
    "last_updated": "2025-03-22T10:58:57+00:00"
  },
  {
    "full_name": "jeroen/curl",
    "name": "curl",
    "description": "A Modern and Flexible Web Client for R",
    "language": "C",
    "topics": [],
    "readme": "# curl\n\n> A Modern and Flexible Web Client for R\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/curl)](https://cran.r-project.org/package=curl)\n[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/curl)](https://cran.r-project.org/package=curl)\n\nBindings to [libcurl](https://curl.se/libcurl/) for performing fully\ncustomizable HTTP/FTP/SCP requests where responses can be processed either in\nmemory, on disk, or streaming via the callback or connection interfaces. Some \nknowledge of 'libcurl' is recommended; for a more-user-friendly web client see\nthe 'httr2' package which builds on this package with http specific tools and logic\n\n## Installation\n\nThe latest version of the package can be installed from r-universe:\n\n```r\ninstall.packages(\"curl\", repos = \"https://jeroen.r-universe.dev\")\n```\n\nOther resources to get started:\n\n - [Intro vignette](https://cran.r-project.org/web/packages/curl/vignettes/intro.html): *curl - A Modern and Flexible Web Client for R*\n - [R-universe page](https://jeroen.r-universe.dev/curl) latest development information and resources \n - [Libcurl handle options](https://curl.se/libcurl/c/curl_easy_setopt.html): overview of available options in libcurl\n\n## Install from source on Linux / MacOS\n\nInstallation from source on Linux requires [`libcurl`](https://curl.se/libcurl/). On __Debian__ or __Ubuntu__ use [libcurl4-openssl-dev](https://packages.debian.org/testing/libcurl4-openssl-dev):\n\n```\nsudo apt-get install -y libcurl-dev\n```\n\nOn __Fedora__, __CentOS or RHEL__ use [libcurl-devel](https://src.fedoraproject.org/rpms/curl):\n\n```\nsudo yum install libcurl-devel\n````\n\nOn __MacOS__ libcurl is included with the system, so usually nothing extra is needed. However if you want to test using the very most recent version of libcurl you can install [curl from homebrew](https://github.com/Homebrew/homebrew-core/blob/master/Formula/c/curl.rb) and then recompile the R package:\n\n\n```sh\nbrew install curl pkg-config\n```\n\nYou need to set",
    "url": "https://github.com/jeroen/curl",
    "last_updated": "2025-08-19T09:45:53+00:00"
  },
  {
    "full_name": "ilkarman/DemoNeuralNet",
    "name": "DemoNeuralNet",
    "description": "Toy project to create a simple working neural net in R",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# DemoNeuralNet\nToy project to create a simple working neural net in R\n",
    "url": "https://github.com/ilkarman/DemoNeuralNet",
    "last_updated": "2024-11-17T17:50:21+00:00"
  },
  {
    "full_name": "Arun-Kuchibhotla/HulC",
    "name": "HulC",
    "description": "Hull based confidence regions",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# HulC: Hull based confidence regions\n<p align=\"center\">\n  <img src=\"Hulk.png?raw=true\" alt=\"HulC\"/>\n</p>\n\nWe provide an introduction to the general purpose method of construction confidence intervals called HulC (Hull based confidence regions). HulC is developed by [Kuchibhotla, Balakrishnan and Wasserman (2021)](https://arxiv.org/abs/2105.14577). On this page, we provide both R and python implementations of the HulC method as well as reproducible code to obtain the plots and tables in the paper. This page also contains more illustrations of the method beyond those given in the paper. \n\nThe \"R\" folder contains the R code for all the HulC procedures (in HulC.R) and the jupyter notebooks for illustrations using the functions in HulC.R. The python code to the same is in the \"python\" folder.\n### Related papers:\n[1] The \"monotonic_regression_code\" contains all the R code related to [Mallick, Sarkar and Kuchibhotla (2023)](https://arxiv.org/abs/2310.20058), where new asymptotic limit theory for the LSE monotonic regression estimator has been developed, with an improved understanding of its rate of convergence, adaptivity properties, and pointwise asymptotic distribution. HulC is shown to provide better inference in this new asymptotic regime compared to prior methods.\n",
    "url": "https://github.com/Arun-Kuchibhotla/HulC",
    "last_updated": "2025-04-23T16:26:27+00:00"
  },
  {
    "full_name": "gojiplus/ghostsky",
    "name": "ghostsky",
    "description": "Autopost a random (or new) Ghost blog to bsky",
    "language": "Python",
    "topics": [
      "bsky-bot",
      "ghost-blog",
      "github"
    ],
    "readme": "## &#x1F47B;&#x1F98B; Post Random Ghost Blog to Bluesky\n\nFetch a random post from your Ghost blog’s sitemap and publish it to your Bluesky feed on a schedule or on demand.\n\n## Features\n\n- Reads all post URLs from your Ghost sitemap (`sitemap-posts.xml`)\n- Picks one at random each run\n- Authenticates and posts via the Bluesky AT Protocol client\n- Fully configurable via Action inputs\n\n## Usage\n\n### 1. Add the Action to your repo\n\nCreate a folder `.github/actions/post-ghost` and put both:\n- [action.yaml](#action-metadata)  \n- `scripts/post_random_ghost.py` (from earlier)\n\n### 2. Configure Secrets\n\nGo to **Settings → Secrets** and add:\n- `BSKY_HANDLE` – your Bluesky username\n- `BSKY_PASSWORD` – your Bluesky password\n\n### 3. Example workflow\n\n```yaml\nname: 🥝 Post Random Ghost Blog to Bluesky\n\non:\n  schedule:\n    - cron: '0 16 * * *'     # every day at 16:00 UTC\n  workflow_dispatch:         # allow manual trigger\n\njobs:\n  post:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Post random blog link\n        uses: ./.github/actions/post-ghost\n        with:\n          handle: ${{ secrets.BSKY_HANDLE }}\n          password: ${{ secrets.BSKY_PASSWORD }}\n          sitemap-url: https://www.gojiberries.io/sitemap-posts.xml\n```\n\n### Inputs\n\n| Name        | Description                                     | Required | Default                                 |\n|-------------|-------------------------------------------------|:--------:|:----------------------------------------|\n| `handle`      | Your Bluesky handle (username)                  | ✅       |                                          |\n| `password`    | Your Bluesky password                           | ✅       |                                          |\n| `sitemap-url` | Full URL to your Ghost blog’s `sitemap-posts.xml` | ✅       | `https://yourblog.com/sitemap-posts.xml` |\n\n\n\n### How it works\n\n1. Fetch the sitemap (XML) and extract all <loc> URLs\n2. Choose one at random\n3. Login to Blues",
    "url": "https://github.com/gojiplus/ghostsky",
    "last_updated": "2025-05-02T01:42:07+00:00"
  },
  {
    "full_name": "OpenTechSchool/learn.opentechschool.org",
    "name": "learn.opentechschool.org",
    "description": "Portal for all of OTS' learning materials",
    "language": "CSS",
    "topics": [],
    "readme": "# OpenTechSchool Materials Landing\n\nA simple one-page landing for all of OTS' workshop materials.\n\nBuilt using the [Gumby Framework](http://gumbyframework.com).\nAnd jekyll, and compass.\n\nYou can edit / add workshops simply by editing the yaml\n`sections` dictionary found in `_config.yml`.\n\nTo build this site, you'll need Ruby installed, and possibly Node.js.\n\n### Ruby\n\nUse `bundle install`, or install the `compass`, `jekyll` and\n`modular-scale` gems.\n\nTo run the site, use `compass watch` in one terminal and `jekyll serve -w` in\nanother, then visit http://localhost:4000/\n\nCSS adjustments are made in `sass/_custom.scss` and `sass/_fonts.scss`.\n\nJekyll might not automatically rebuild the site if you have changed\n`_config.yml`, so I just `rm -r _site/*` before restarting the server again.\n\n### Node.js\n\nOnly needed to update gumby.\n\nNode requirements: `bower`, `claymate`. These two are designed to be installed\nglobally (`[sudo ]npm install -g bower claymate`).\n\nTo update gumby, use `bower update gumby`. To then rebuild it, use\n`claymate build`.\n",
    "url": "https://github.com/OpenTechSchool/learn.opentechschool.org",
    "last_updated": "2024-02-15T09:42:55+00:00"
  },
  {
    "full_name": "brentonk/github-course-starter",
    "name": "github-course-starter",
    "description": "Starter files for a Jekyll-based course website to host on GitHub, or elsewhere...",
    "language": "CSS",
    "topics": [],
    "readme": "## About\n\nThis is a template for an academic course website generated using\n[Jekyll](http://jekyllrb.com/), designed in\n[Bootstrap](http://getbootstrap.com), and compatible with hosting on\n[Github Pages](https://pages.github.com/).\n\nA sample course page formatted via this template can be viewed at\nhttp://brentonk.github.io/github-course-starter/.\n\n\n## Structure\n\nThe template is in the following directories and files.  Further explanation\nof the directory structure is available in\n[the Jekyll documentation](http://jekyllrb.com/docs/structure/).  These can,\nin theory, be left unchanged (except for the base URL in `_config.yml`, which\nis site-specific).\n\n* `_config.yml`: YAML configuration file for Jekyll\n* `_includes/`: snippets that can be reused throughout the site\n    * `credits_and_license.html`: footer text\n* `_layouts/`: HTML layout templates\n    * `default.html`: default layout\n    * `assignment.md` and `note.md`: layouts for assignments and course\n      notes, respectively\n* `assets/`: CSS stylesheets, fonts, and Javascript code\n\nYour actual course content goes in the following directories and files:\n\n* `index.md`: course home page in Markdown format\n* `syllabus.md`: syllabus in Markdown format\n* `_assignments/`: a [collection](http://jekyllrb.com/docs/collections/)\n  containing assignments in Markdown format\n* `_data/`: data files in [YAML](http://www.yaml.org/) format\n    * `course.yml`: information about the course\n    * `instructor.yml`: information about the instructor\n    * `license.yml`: (optional) information about the license for course\n      content (e.g., Creative Commons)\n* `_notes/`: a [collection](http://jekyllrb.com/docs/collections/) containing\n  course notes in Markdown format\n\n\n## Suggested Workflow\n\n1. Fork this repository\n2. Alter the template at will (e.g., replace the default Bootstrap CSS file\n   with one of the more attractive options from\n   [Bootswatch](http://bootswatch.com/))\n3. Initialize a page for each of your courses by cloning ",
    "url": "https://github.com/brentonk/github-course-starter",
    "last_updated": "2025-08-19T13:57:09+00:00"
  },
  {
    "full_name": "ydataai/ydata-profiling",
    "name": "ydata-profiling",
    "description": "1 Line of code data quality profiling & exploratory data analysis for Pandas and Spark DataFrames. ",
    "language": "Python",
    "topics": [
      "pandas-profiling",
      "pandas-dataframe",
      "statistics",
      "jupyter-notebook",
      "exploration",
      "data-science",
      "python",
      "pandas",
      "machine-learning",
      "deep-learning",
      "exploratory-data-analysis",
      "eda",
      "data-quality",
      "html-report",
      "data-exploration",
      "data-analysis",
      "jupyter",
      "big-data-analytics",
      "data-profiling",
      "hacktoberfest"
    ],
    "readme": "# ydata-profiling\n\n[![Build Status](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml/badge.svg?branch=master)](https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml)\n[![PyPI download month](https://img.shields.io/pypi/dm/ydata-profiling.svg)](https://pypi.python.org/pypi/ydata-profiling/)\n[![](https://pepy.tech/badge/pandas-profiling)](https://pypi.org/project/ydata-profiling/)\n[![Code Coverage](https://codecov.io/gh/ydataai/pandas-profiling/branch/master/graph/badge.svg?token=gMptB4YUnF)](https://codecov.io/gh/ydataai/pandas-profiling)\n[![Release Version](https://img.shields.io/github/release/ydataai/pandas-profiling.svg)](https://github.com/ydataai/pandas-profiling/releases)\n[![Python Version](https://img.shields.io/pypi/pyversions/ydata-profiling)](https://pypi.org/project/ydata-profiling/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=cb7e69df-af81-4352-809a-d4251756affc\" />\n\n<p align=\"center\"><img width=\"300\" src=\"https://assets.ydata.ai/oss/ydata-profiling_black.png\" alt=\"YData Profiling Logo\"></p>\n\n<p align=\"center\">\n  <a href=\"https://ydata-profiling.ydata.ai/docs/master/\">Documentation</a>\n  |\n  <a href=\"https://tiny.ydata.ai/dcai-ydata-profiling\">Discord</a>\n  | \n  <a href=\"https://stackoverflow.com/questions/tagged/pandas-profiling+or+ydata-profiling\">Stack Overflow</a>\n  |\n  <a href=\"https://ydata-profiling.ydata.ai/docs/master/pages/reference/changelog.html#changelog\">Latest changelog</a>\n\n</p>\n\n<p align=\"center\">\n  Do you like this project? Show us your love and <a href=\"https://engage.ydata.ai\">give feedback!</a>\n</p>\n\n`ydata-profiling` primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas `df.describe()` function, that is so handy, ydata-profiling delivers an extended analysis of",
    "url": "https://github.com/ydataai/ydata-profiling",
    "last_updated": "2025-09-02T03:15:01+00:00"
  },
  {
    "full_name": "sparklyr/sparklyr",
    "name": "sparklyr",
    "description": "R interface for Apache Spark",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "apache-spark",
      "machine-learning",
      "dplyr",
      "sparklyr",
      "remote-clusters",
      "livy",
      "ide",
      "distributed",
      "spark"
    ],
    "readme": "sparklyr: R interface for Apache Spark\n================\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/sparklyr/sparklyr/actions/workflows/R-CMD-check.yaml)\n[![Spark-Tests](https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml/badge.svg)](https://github.com/sparklyr/sparklyr/actions/workflows/spark-tests.yaml)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/sparklyr)](https://CRAN.R-project.org/package=sparklyr)\n[![Codecov test\ncoverage](https://codecov.io/gh/sparklyr/sparklyr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/sparklyr/sparklyr?branch=main)\n<!-- badges: end -->\n\n<img src=\"tools/readme/sparklyr-diagram.png\" width=\"320\" align=\"right\" style=\"margin-left: 20px; margin-right: 20px\"/>\n\n- Install and connect to [Spark](https://spark.apache.org/) using YARN,\n  Mesos, Livy or Kubernetes.\n- Use [dplyr](#using-dplyr) to filter and aggregate Spark datasets and\n  [streams](https://spark.posit.co/guides/streaming/) then bring them\n  into R for analysis and visualization.\n- Create interoperable machine learning\n  [pipelines](https://spark.posit.co/guides/pipelines.html)\n- Create [extensions](#extensions) that call the full Spark API or run\n  [distributed R](#distributed-r) code to support new functionality.\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Connecting to Spark](#connecting-to-spark)\n- [Using dplyr](#using-dplyr)\n  - [Window Functions](#window-functions)\n- [Using SQL](#using-sql)\n- [Machine Learning](#machine-learning)\n- [Reading and Writing Data](#reading-and-writing-data)\n- [Distributed R](#distributed-r)\n- [Extensions](#extensions)\n- [Table Utilities](#table-utilities)\n- [Connection Utilities](#connection-utilities)\n- [RStudio IDE](#rstudio-ide)\n- [Using H2O](#using-h2o)\n- [Connecting through Livy](#connecting-through-livy)\n- [Connecting through",
    "url": "https://github.com/sparklyr/sparklyr",
    "last_updated": "2025-09-01T00:09:24+00:00"
  },
  {
    "full_name": "mimno/PyMallet",
    "name": "PyMallet",
    "description": "Python tools for text",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# PyMallet\n\nThis package provides tools for extracting latent semantic representations of text, particularly probabilistic topic models.\n\nThe implementation of LDA uses Gibbs sampling, which is simple but reliable. People often find the resulting models more useful than the stochastic variational algorithm used in Gensim.\n\nTo compile:\n\n    python setup.py build_ext --inplace\n\nAs an example, the `sample_data` directory contains 10000 posts from the stats Stack Exchange forum.\n\nTo run on this sample collection with 50 topics:\n\n    python lda.py sample_data/stats_10k.txt 50\n\nThe script `lda_reference.py` contains a reference implementation in pure Python (no Cython) to compare speed. The Cython version is currently about 100x faster.",
    "url": "https://github.com/mimno/PyMallet",
    "last_updated": "2025-08-07T12:23:40+00:00"
  },
  {
    "full_name": "marimo-team/marimo",
    "name": "marimo",
    "description": "Transform data, train models, and run SQL with marimo — feels like a next-gen reactive notebook, stored as Git-friendly reproducible Python. Deploy as scripts, pipelines, endpoints, and apps. All from an AI-native editor (or your own).",
    "language": "Python",
    "topics": [
      "notebooks",
      "python",
      "data-science",
      "machine-learning",
      "artificial-intelligence",
      "data-visualization",
      "developer-tools",
      "reactive",
      "pipeline",
      "web-app",
      "dag",
      "dataflow",
      "sql"
    ],
    "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg\">\n</p>\n\n<p align=\"center\">\n  <em>A reactive Python notebook that's reproducible, git-friendly, and deployable as scripts or apps.</em>\n\n<p align=\"center\">\n  <a href=\"https://docs.marimo.io\" target=\"_blank\"><strong>Docs</strong></a> ·\n  <a href=\"https://marimo.io/discord?ref=readme\" target=\"_blank\"><strong>Discord</strong></a> ·\n  <a href=\"https://docs.marimo.io/examples/\" target=\"_blank\"><strong>Examples</strong></a> ·\n  <a href=\"https://marimo.io/gallery/\" target=\"_blank\"><strong>Gallery</strong></a> ·\n  <a href=\"https://www.youtube.com/@marimo-team/\" target=\"_blank\"><strong>YouTube</strong></a>\n</p>\n\n<p align=\"center\">\n  <b>English | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Traditional_Chinese.md\" target=\"_blank\"><b>繁體中文</b></a>\n  <b> | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Chinese.md\" target=\"_blank\"><b>简体中文</b></a>\n  <b> | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Japanese.md\" target=\"_blank\"><b>日本語</b></a>\n  <b> | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Spanish.md\" target=\"_blank\"><b>Español</b></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://pypi.org/project/marimo/\"><img src=\"https://img.shields.io/pypi/v/marimo?color=%2334D058&label=pypi\"/></a>\n<a href=\"https://anaconda.org/conda-forge/marimo\"><img src=\"https://img.shields.io/conda/vn/conda-forge/marimo.svg\"/></a>\n<a href=\"https://marimo.io/discord?ref=readme\"><img src=\"https://shields.io/discord/1059888774789730424\" alt=\"discord\" /></a>\n<img alt=\"Pepy Total Downloads\" src=\"https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads\"/>\n<img alt=\"Conda Downloads\" src=\"https://img.shields.io/conda/d/conda-forge/marimo\" />\n<a href=\"https://github.com/marimo-team/marimo/blob/main/LICENSE\"><img src=\"https://img.shields.io/pypi/l/marimo\" /></a>\n</p>\n\n**marimo** i",
    "url": "https://github.com/marimo-team/marimo",
    "last_updated": "2025-09-02T09:06:27+00:00"
  },
  {
    "full_name": "dreamRs/esquisse",
    "name": "esquisse",
    "description": "RStudio add-in to make plots interactively with ggplot2",
    "language": "R",
    "topics": [
      "r",
      "ggplot2",
      "addin",
      "data-visualization",
      "visualization",
      "rstudio-addin"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# esquisse <img src=\"man/figures/logo.png\" width=200 align=\"right\" />\n\n**Try online : <https://dreamrs.shinyapps.io/esquisse/>**\n\n<!-- badges: start -->\n\n[![version](https://www.r-pkg.org/badges/version/esquisse)](https://CRAN.R-project.org/package=esquisse)\n[![cranlogs](https://cranlogs.r-pkg.org/badges/esquisse)](https://CRAN.R-project.org/package=esquisse)\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![R-CMD-check](https://github.com/dreamRs/esquisse/workflows/R-CMD-check/badge.svg)](https://github.com/dreamRs/esquisse/actions)\n<!-- badges: end -->\n\nThis addin allows you to interactively explore your data by visualizing\nit with the [ggplot2](https://github.com/tidyverse/ggplot2) package. It\nallows you to draw bar plots, curves, scatter plots, histograms, boxplot\nand [sf](https://github.com/r-spatial/sf) objects, then export the graph\nor retrieve the code to reproduce the graph.\n\nSee online documentation :\n<https://dreamrs.github.io/esquisse/index.html>\n\nUse esquisse online : <https://dreamrs.shinyapps.io/esquisse/>\n\nIf you find bugs, please open an\n[issue](https://github.com/dreamRs/esquisse/issues)\n\n## Installation\n\nInstall from [CRAN](https://CRAN.R-project.org/package=esquisse) with :\n\n``` r\ninstall.packages(\"esquisse\")\n```\n\nOr install development version from\n[GitHub](https://github.com/dreamRs/esquisse) :\n\n``` r\nremotes::install_github(\"dreamRs/esquisse\")\n```\n\nThen launch the addin via the RStudio menu or with\n`esquisse::esquisser()`.\n\n## esquisse addin\n\n``` r\nesquisse::esquisser()\n# or with your data:\nesquisse::esquisser(palmerpenguins::penguins)\n```\n\n![](man/figures/esquisse.gif)\n\nAbove gif was made with :heart: by [@mfanny](https://github.com/mfanny)\nand cannot be removed, but in the meantime {esquisse} has evolved, the\nlatest version now looks like:\n\n![](man/figures/esquisse.png)\n\n### Interna",
    "url": "https://github.com/dreamRs/esquisse",
    "last_updated": "2025-09-01T15:30:00+00:00"
  },
  {
    "full_name": "ericdunipace/causalOT",
    "name": "causalOT",
    "description": "Weighting for causal estimands using optimal transport",
    "language": "R",
    "topics": [],
    "readme": "<!-- badges: start -->\n[![R-CMD-check](https://github.com/ericdunipace/causalOT/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ericdunipace/causalOT/actions/workflows/R-CMD-check.yaml)\n[![CRAN status](https://www.r-pkg.org/badges/version/causalOT)](https://CRAN.R-project.org/package=causalOT)\n<!-- badges: end -->\n\n# causalOT: Optimal transport methods for causal inference\n\nThis R package implements the methods described in [*Optimal transport methods for causal inference*](https://arxiv.org/abs/2109.01991).\n\n## Installation\n\nThis package can be installed in a few ways.\n\n### 1. devtools\n\nUsing the `remotes` package in `R`, one can install the package with\n\n    remotes::install_github(\"ericdunipace/causalOT\")\n\n### 2. download and install\n\nAfter downloading the git package using `git clone` or by downloading the .zip file from the button above (Code -\\> Download Zip) and unzipping, you can install the package with\n\n    devtools::install(\"path/to/causalOT\")\n\n### 3. CRAN\n\nA stable version of this package is available on [CRAN](https://CRAN.R-project.org/package=causalOT), but usually this GitHub will have the latest version.\n\n## Usage\n\nThe functions in the package are built to construct weights to make distributions more same and estimate causal effects. The primary method we recommend is by using optimal transport weights which balance distributions by design. For more information about using this package, see the vignette \"Using causalOT\".\n\n## Reproducing the paper\n\nIn the folder `inst/Reproduce` you can find code and an RMarkdown file to reproduce the figures present in the [paper](https://arxiv.org/abs/2109.01991).\n\n## Package author\n\n[Eric Dunipace](https://ericdunipace.github.io)\n\n## License\n\nThis package is licensed under GPL 3.0.\n",
    "url": "https://github.com/ericdunipace/causalOT",
    "last_updated": "2025-04-30T17:43:59+00:00"
  },
  {
    "full_name": "ghostery/ghostery-extension",
    "name": "ghostery-extension",
    "description": "Ghostery Browser Extension for Firefox, Chrome, Opera, Edge and Safari",
    "language": "JavaScript",
    "topics": [
      "firefox-extension",
      "chrome-extension",
      "opera-extension",
      "edge-extension",
      "ghostery",
      "privacy",
      "safari-extension",
      "yandex-extension"
    ],
    "readme": "# Ghostery Browser Extension\n\n[![Tests](https://github.com/ghostery/ghostery-extension/actions/workflows/test.yml/badge.svg?branch=main)](https://github.com/ghostery/ghostery-extension/actions/workflows/test.yml)\n![GitHub Release](https://img.shields.io/github/v/release/ghostery/ghostery-extension)\n\nGhostery is a powerful Tracker & Adblocker browser extension with over 100 million downloads. Block ads, stop trackers, and speed up websites. Privacy at its best.\n\n* Block all ads on websites, including YouTube and Facebook, to focus on the information that matters.\n* Stop trackers from collecting your personal data.\n* Automatically remove intrusive cookie pop-ups and express dissent to online tracking.\n* Get detailed tracker information on any website you visit, including the number of trackers, their type, and the company operating them.\n* Preview tracker information on search engine result pages to make informed choices.\n* Inspect the largest database of trackers, updated fast and reliably for all users.\n\n## Links\n\n* [Website](https://www.ghostery.com/)\n* [Broken Page Reports](https://github.com/ghostery/broken-page-reports/)\n* [Support](https://www.ghostery.com/support)\n* [X, formerly Twitter (@ghostery)](https://twitter.com/ghostery)\n* [Privacy Policy](https://www.ghostery.com/about-ghostery/browser-extension-privacy-policy/)\n\n## Development\n\nFirst, install dependencies and download additional resources (e.g. block lists):\n\n```bash\nnpm ci\n```\n\nThen, start the local version of the extension in the supported browser:\n\n```bash\nnpm start [chromium|firefox]\n```\n\n> The build script assumes that you are using macOS, and browsers are installed in the default locations\n\nYou can add the target after the `start` command to run the extension in a different Chromium-based browser:\n\n* Opera - `npm start -- --browser=opera`\n* Edge - `npm start -- --browser=edge`\n\nTo run local version in Safari, you have to use Xcode. The project files are available in the `xcode` folder, but Appl",
    "url": "https://github.com/ghostery/ghostery-extension",
    "last_updated": "2025-09-02T09:54:04+00:00"
  },
  {
    "full_name": "apoorvalal/CVXSynth",
    "name": "CVXSynth",
    "description": "Implementation of the original synthetic control (Abadie, Diamond, and Hainmueller 2010) and elastic-net synthetic control (Doudchenko and Imbens 2016) in CVXR.",
    "language": "R",
    "topics": [],
    "readme": "# CVXSynth\n\nImplementations of the original synthetic control (Abadie, Diamond, Hainmueller 2010, 2015) and elastic net synthetic control (Doudchenko and Imbens 2016).\n\n+ [notes](https://apoorvalal.github.io/presentations/pdf/ImbensDoudchenko.pdf)\n+ [demo](https://apoorvalal.github.io/posts/09122021_ElasticNetSyntheticControl.html)\n\n## Installation\n\n```R\nlibrary(remotes)\ninstall_github(\"apoorvalal/CVXSynth\")\n```\n\n## Optimiser\n\nUses the domain-specific language [CVXR](https://cvxr.rbind.io/) with the\ncommercial solver `MOSEK` to solve for weights. `MOSEK` is free for academics\nand can be installed using installation instructions on their website. Else, an\n[alternate\nsolver](https://cvxr.rbind.io/cvxr_examples/cvxr_using-other-solvers/) can be\npassed as the `solv` argument for both `sc_solve` and `en_sc_solve` functions.\n\n\n## Example\n\n```R\nlibrary(pacman)\np_load(data.table, patchwork, ggplot2, CVXSynth, parallel)\ntheme_set(theme_minimal())\n```\n\n### data prep\n\n```R\n# %% data prep\nload('data-raw/ADH2015.RData')\nADH2015$country = factor(ADH2015$country)\npan = data.table(ADH2015)\npan[, treat := ifelse(country == \"West Germany\" & year >= 1990, 1, 0)]\ntreat_name = 'West Germany'\nT0 = pan[country == \"West Germany\" & treat != 1, nunique(year)]\nT1 = pan[country == \"West Germany\" & treat == 1, nunique(year)]\n\n# %% # number of post-treatment periods reshape to wide\nwide = pan[, .(country, year, gdp)] |> dcast(year ~ country, value.var = 'gdp')\nsetcolorder(wide, c('year', treat_name))\ny_treat_pre = wide[1:T0, 2] |> as.matrix()\ny_ctrl_pre  = wide[1:T0, -(1:2)] |> as.matrix()\n```\n\nInput `y_treat_pre` is a $T_0$-vector, and `y_ctrl_pre` is $T_0 \\times N$\nmatrix.\n\n### Original Synthetic Control\n\n`sc_solve` solves for SC weights using constrained regression enforcing\npositivity and summation to 1.\n\n```R\n# %% synthetic control\nω_sc = sc_solve(y_treat_pre, y_ctrl_pre)\nwt_table = data.frame(donor = colnames(y_ctrl_pre), wt = ω_sc)\n# %% compute and plot\nwide2 = copy(wide)\n# impute Y(0) po",
    "url": "https://github.com/apoorvalal/CVXSynth",
    "last_updated": "2025-06-10T07:37:28+00:00"
  },
  {
    "full_name": "betanalpha/stan_intro",
    "name": "stan_intro",
    "description": "Draft introduction to probability and inference aimed at the Stan manual.",
    "language": "TeX",
    "topics": [],
    "readme": "",
    "url": "https://github.com/betanalpha/stan_intro",
    "last_updated": "2025-07-31T20:25:42+00:00"
  },
  {
    "full_name": "richfitz/remoji",
    "name": "remoji",
    "description": "Emoji for R :joy_cat:",
    "language": "R",
    "topics": [],
    "readme": "# Emoji for R\n\n[![Build Status](https://travis-ci.org/richfitz/remoji.png?branch=master)](https://travis-ci.org/richfitz/remoji)\n[![Coverage Status](https://coveralls.io/repos/richfitz/remoji/badge.svg?branch=master)](https://coveralls.io/r/richfitz/remoji?branch=master)\n\nI am sorry :crying_cat_face:.\n\n```r\ndevtools::install_github(\"richfitz/remoji\")\nlibrary(remoji)\nmessage(emoji(\"cat\"))\nmessage(emoji(list_emoji(), TRUE))\nmessage(sub_emoji(\"This is silly :frowning:\"))\nemoji_table(find_emoji(\"frown\"))\nemoji_table(find_emoji(\"frown\", approximate=TRUE))\n```\n\n```\nlibrary(magrittr)\nfind_emoji(\"sun\") %>% emoji_table()\nfind_emoji(\"chart\") %>% emoji_table()\n```\n\nI am so sorry :joy_cat:\n",
    "url": "https://github.com/richfitz/remoji",
    "last_updated": "2024-04-29T06:33:47+00:00"
  },
  {
    "full_name": "PAPL-SKKU/ggplot2.SparkR",
    "name": "ggplot2.SparkR",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "# ggplot2.SparkR\n\nAs of March 2016, ggplot2.SparkR has been merged into SKKU-SKT github repository and is an upcoming release (1.0) on spark packages. This repo no longer accepts new pull requests or commits and they should now be submitted to [SKKU-SKT/ggplot2.SparkR](https://github.com/SKKU-SKT/ggplot2.SparkR); see [here](http://skku-skt.github.io/ggplot2.SparkR/) for some instructions.\n",
    "url": "https://github.com/PAPL-SKKU/ggplot2.SparkR",
    "last_updated": "2025-03-22T11:22:09+00:00"
  },
  {
    "full_name": "tdhopper/intro-to-scikit-learn",
    "name": "intro-to-scikit-learn",
    "description": ":nut_and_bolt: Materials for talk on scikit-learn",
    "language": "",
    "topics": [],
    "readme": "# Intro to Scikit-Learn\n\n* Research Triangle Analysts\n* January 2014\n* Presented by Tim Hopper\n\n__Abstract:__ Scikit-learn is an actively developed Python package providing an implementation of many machine learning algorithms (e.g. SVM, kNN, linear models, HMM, k-Means, spectral clustering). However, the benefits of Scikit-learn goes well beyond carefully implemented learning algorithms. Being built in Python, it allows easy integration with countless other Python modules for tasks such as plotting, data munging, and application development. Its consistent API across algorithms allows for rapid experimentation with multiple learning methods. Also, Scikit-learn is well documented and provides lots of examples.\n\nInstead of discussing particular machine learning algorithms provided by the package, I will focus on Scikit-learn and Python as a toolkit for solving data problems from start to finish. I will emphasize the Pipeline tool which allows the user to chain together all the steps of a machine learning pipeline including preprocessing, dimensionality reduction, feature selection, and model fitting.\n\n------\n\nA (poor quality) video of this talk is [here](https://www.youtube.com/watch?v=2kx19t8bNMU).\n\n------\n\nThe slides for this presentation are generated from _Intro to Scikit-Learn.ipynb_.\n\nTo view the slides in a browser run the following command:\n  \n```\nipython nbconvert Intro\\ to\\ Scikit-Learn.ipynb --to slides --post serve\n```\n",
    "url": "https://github.com/tdhopper/intro-to-scikit-learn",
    "last_updated": "2025-05-26T19:49:47+00:00"
  },
  {
    "full_name": "hartator/wayback-machine-downloader",
    "name": "wayback-machine-downloader",
    "description": "Download an entire website from the Wayback Machine.",
    "language": "Ruby",
    "topics": [],
    "readme": "# Wayback Machine Downloader\n\n[![Gem Version](https://badge.fury.io/rb/wayback_machine_downloader.svg)](https://rubygems.org/gems/wayback_machine_downloader/)\n[![Build Status](https://travis-ci.org/hartator/wayback-machine-downloader.svg?branch=master)](https://travis-ci.org/hartator/wayback-machine-downloader)\n\nDownload an entire website from the Internet Archive Wayback Machine.\n\n## Installation\n\nYou need to install Ruby on your system (>= 1.9.2) - if you don't already have it.\nThen run:\n\n    gem install wayback_machine_downloader\n\n**Tip:** If you run into permission errors, you might have to add `sudo` in front of this command.\n\n## Basic Usage\n\nRun wayback_machine_downloader with the base url of the website you want to retrieve as a parameter (e.g., http://example.com):\n\n    wayback_machine_downloader http://example.com\n\n## How it works\n\nIt will download the last version of every file present on Wayback Machine to `./websites/example.com/`. It will also re-create a directory structure and auto-create `index.html` pages to work seamlessly with Apache and Nginx. All files downloaded are the original ones and not Wayback Machine rewritten versions. This way, URLs and links structure are the same as before.\n\n## Advanced Usage\n\n\tUsage: wayback_machine_downloader http://example.com\n\n\tDownload an entire website from the Wayback Machine.\n\n\tOptional options:\n\t    -d, --directory PATH             Directory to save the downloaded files into\n\t\t\t\t\t     Default is ./websites/ plus the domain name\n\t    -s, --all-timestamps             Download all snapshots/timestamps for a given website\n\t    -f, --from TIMESTAMP             Only files on or after timestamp supplied (ie. 20060716231334)\n\t    -t, --to TIMESTAMP               Only files on or before timestamp supplied (ie. 20100916231334)\n\t    -e, --exact-url                  Download only the url provided and not the full site\n\t    -o, --only ONLY_FILTER           Restrict downloading to urls that match this filter\n\t\t\t\t\t     (us",
    "url": "https://github.com/hartator/wayback-machine-downloader",
    "last_updated": "2025-08-30T20:08:02+00:00"
  },
  {
    "full_name": "SuperFastPython/PythonThreadingJumpStart",
    "name": "PythonThreadingJumpStart",
    "description": "Python Threading Jump-Start",
    "language": "Python",
    "topics": [
      "book",
      "concurrency",
      "python",
      "thread",
      "threading"
    ],
    "readme": "# Python Threading Jump-Start\n\n![Python Threading Jump-Start](cover.png)\n\n* <https://github.com/SuperFastPython/PythonThreadingJumpStart>\n\nThis repository provides all source code for the book:\n\n* **Python Threading Jump-Start**: _Develop Concurrent IO-bound Programs And Work With The GIL_, Jason Brownlee, 2022.\n\n\n## Source Code\nYou can access all Python .py files directly here:\n\n* [src/](src/)\n\n\n## Get the Book\n\nYou can learn more about the book here:\n\n* [Gumroad](https://superfastpython.gumroad.com/l/ptj)\n* [Amazon Paperback](https://amzn.to/3Q4BU54)\n* [Amazon Kindle](https://amzn.to/3Sn9x3w)\n* [GooglePlay](https://play.google.com/store/books/details?id=m0V_EAAAQBAJ)\n* [GoogleBooks](https://books.google.com/books/about?id=m0V_EAAAQBAJ)\n* [Goodreads](https://www.goodreads.com/book/show/61863433-python-threading-jump-start)\n\n\n### Book Blurb\n\n> Unlock concurrency with Python threads (and run 100s or 1,000s of tasks simultaneously).\n>\n> The threading module provides easy-to-use thread-based concurrency in Python.\n>\n> Unlike Python multiprocessing, the threading module is limited by the infamous Global Interpreter Lock (GIL).\n>\n> Critically, the GIL is released when performing blocking I/O. Additionally, threads can share memory make them perfectly suited to I/O-bound tasks such as reading and writing from files and socket connections.\n>\n> This is the API you need to use to make your code run faster.\n>\n> Introducing: \"Python Threading Jump-Start\". A new book designed to teach you the threading module in Python, super fast!\n>\n> You will get a rapid-paced, 7-part course to get you started and make you awesome at using the threading API.\n>\n> Each of the 7 lessons was carefully designed to teach one critical aspect of the threading module, with explanations, code snippets and worked examples.\n>\n> You will discover:\n>\n> * How to choose tasks that are well suited to threads.\n> * How to create and run new threads.\n> * How to locate and query running threads.\n> * How to use lo",
    "url": "https://github.com/SuperFastPython/PythonThreadingJumpStart",
    "last_updated": "2025-08-03T07:23:37+00:00"
  },
  {
    "full_name": "nteetor/prairie",
    "name": "prairie",
    "description": "Get web applications growing in R",
    "language": "R",
    "topics": [
      "web-framework",
      "r",
      "httpuv"
    ],
    "readme": "# prairie\n\nA framework to grow your existing R code into web applications.\n\n[travis]: https://travis-ci.org/nteetor/prairie.svg?branch=master \"@travisbyrum\"\n[coverage]: https://codecov.io/gh/nteetor/prairie/branch/master/graph/badge.svg \"grow the nanobots!\"\n[cran]: https://www.r-pkg.org/badges/version/prairie \"put down some roots\"\n\n![alt text][travis] ![alt text][coverage] ![alt text][cran]\n\nBelow is a simple prairie application,\n\n```R\napp(\n  route(\n    'get',\n    '^$',\n    function(req) {\n      res <- response()\n      \n      status(res) <- 200                     # set status\n      res[['Content-Type']] <- 'text/html'   # set a content type\n      body(res) <- 'Welcome to prairie!'     # set a body\n      \n      res\n    }\n  ),\n  # Make sure the list has values with names\n  # method, path, and handler\n  list(\n    method = c('get', 'post'),\n    path = '^data$',\n    handler = function(req) {\n      if (method(req) == 'GET') {\n        as.response(mtcars)\n      } else {\n        print(body(req))  # log to console\n        \n        res <- response()\n        body(res) <- 'Thanks for all the data!'\n        \n        res\n      }\n    }\n  )\n)\n```\n\nWork on prairie, when the project was still titled dull, began prior to the release of Shiny version 0.13.0. Prior to version 0.13.0, modularization in Shiny was cumbursome, probably ill-advised, or impossible. Rook, another framework for R, was out of date and, to my knowledge, not well-maintained, if at all. I set out to create an Express-like web framework and during the first couple months of development another package, jug, appeared. Jug takes an Express-like approach to web application contruction using R6 classes. I recommend looking at both [Rook](https://github.com/jeffreyhorner/Rook) and [jug](https://github.com/Bart6114/jug). Rook had a significant impact on the development of the powerful httpuv package. Specifically the format of the request environment available to the `call` function in an httpuv application. If you like d",
    "url": "https://github.com/nteetor/prairie",
    "last_updated": "2025-03-22T11:20:35+00:00"
  },
  {
    "full_name": "vdaas/vald",
    "name": "vald",
    "description": "Vald.  A Highly Scalable Distributed Vector Search Engine",
    "language": "Go",
    "topics": [
      "vald",
      "approximate-nearest-neighbor-search",
      "kubernetes",
      "distributed-systems",
      "nearest-neighbor-search",
      "vector-search-engine",
      "similarity-search",
      "image-search",
      "image-search-engine",
      "vector",
      "anng",
      "ngt",
      "microservices",
      "golang",
      "cloud",
      "cloud-native",
      "high-performance",
      "high-dimensional-data",
      "hacktoberfest"
    ],
    "readme": "<div align=\"center\">\n<a href=\"https://vald.vdaas.org/\">\n    <img src=\"./assets/image/readme.svg\" width=\"50%\" />\n</a>\n</div>\n\n[![License: Apache 2.0](https://img.shields.io/github/license/vdaas/vald.svg?style=flat-square)](https://opensource.org/licenses/Apache-2.0)\n[![release](https://img.shields.io/github/release/vdaas/vald.svg?style=flat-square)](https://github.com/vdaas/vald/releases/latest)\n[![CNCF Landscape](https://img.shields.io/badge/CNCF%20Landscape-5699C6)](https://landscape.cncf.io/?item=app-definition-and-development--database--vald)\n[![Go Reference](https://pkg.go.dev/badge/github.com/vdaas/vald.svg)](https://pkg.go.dev/github.com/vdaas/vald)\n[![Codacy Badge](https://img.shields.io/codacy/grade/a6e544eee7bc49e08a000bb10ba3deed?style=flat-square)](https://www.codacy.com/app/i.can.feel.gravity/vald?utm_source=github.com&utm_medium=referral&utm_content=vdaas/vald&utm_campaign=Badge_Grade)\n[![Go Report Card](https://goreportcard.com/badge/github.com/vdaas/vald?style=flat-square)](https://goreportcard.com/report/github.com/vdaas/vald)\n[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B21465%2Fvald.svg?type=small)](https://app.fossa.com/projects/custom%2B21465%2Fvald?ref=badge_small)\n[![DeepSource](https://static.deepsource.io/deepsource-badge-light-mini.svg)](https://deepsource.io/gh/vdaas/vald/?ref=repository-badge)\n[![DeepSource](https://deepsource.io/gh/vdaas/vald.svg/?label=resolved+issues&show_trend=true&token=UpNEsc0zsAfGw-MPPa6O05Lb)](https://deepsource.io/gh/vdaas/vald/?ref=repository-badge)\n[![CLA](https://cla-assistant.io/readme/badge/vdaas/vald?&style=flat-square)](https://cla-assistant.io/vdaas/vald)\n[![Artifact Hub](https://img.shields.io/badge/chart-ArtifactHub-informational?logo=helm&style=flat-square)](https://artifacthub.io/packages/chart/vald/vald)\n[![Slack](https://img.shields.io/badge/slack-join-brightgreen?logo=slack&style=flat-square)](https://join.slack.com/t/vald-community/shared_invite/zt-db2ky9o4-R_9p2sVp8xRwztVa8gfnPA)\n[!",
    "url": "https://github.com/vdaas/vald",
    "last_updated": "2025-09-02T08:20:12+00:00"
  },
  {
    "full_name": "jennybc/githug",
    "name": "githug",
    "description": "Interface to local and remote Git operations",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n[![Project Status: Wip - Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](http://www.repostatus.org/badges/0.1.0/wip.svg)](http://www.repostatus.org/#wip)\n\ngithug\n======\n\n### Welcome to Version Control!\n\n<!--[Demo](https://analovesdotcom.files.wordpress.com/2015/10/voldyhug-1440161473.gif)-->\n![Demo](img/voldyhug-1440161473.gif)\n\nThe goal of githug is to wrap you in the warm embrace of Git 🤗, from the comfort of R.\n\n*This a reboot of an earlier effort, which lives on in [branch `first-draft`](https://github.com/jennybc/githug/tree/first-draft). That branch includes a function `githug_init()` to connect a new or existing R project (usually a RStudio Project) to a newly created GitHub remote. Currently plodding my way back to that level of functionality.*\n\nInstallation\n------------\n\nYou can install githug from github with:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"jennybc/githug\")\n```\n\nExample\n-------\n\nLoad dev version of the package. *This will become `library(githug)`.*\n\n``` r\n#library(githug)\ndevtools::load_all(\".\")\n#> Loading githug\n```\n\nCreate a new Git repository and set that as working directory for the duration of this example.\n\n``` r\nrepo <- git_init(tempfile(\"githug-example-\"))\n#> * Creating directory:\n#>   /var/folders/vt/4sdxy0rd1b3b65nqssx … Qg3M4c/githug-example-104c22b3c3d11\n#> * Initialising git repository in:\n#>   /var/folders/vt/4sdxy0rd1b3b65nqssx … Qg3M4c/githug-example-104c22b3c3d11\nknitr::opts_knit$set(root.dir = repo)\n```\n\nSet (local) config variables for user and email.\nCreate two files and inspect Git status.\n\n``` r\ngit_config_local(user.name = \"louise\", user.email = \"louise@example.org\")\n\nwrite(\"Are these girls real smart or real real lucky?\", \"max.txt\")\nwrite(\"You get what you settle for.\", \"louise.txt\")\ngit_status()\n#> Not on a branch.\n#> # A tibble: 2 x 4\n#>      status       path ch",
    "url": "https://github.com/jennybc/githug",
    "last_updated": "2025-03-22T11:21:11+00:00"
  },
  {
    "full_name": "kosukeimai/emIRT",
    "name": "emIRT",
    "description": "emIRT: EM Algorithms for Estimating Item Response Theory Models",
    "language": "C++",
    "topics": [],
    "readme": "# emIRT: EM Algorithms for Estimating Item Response Theory Models [![Build Status](https://travis-ci.org/kosukeimai/emIRT.svg?branch=master)](https://travis-ci.org/kosukeimai/emIRT)  [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/emIRT)](https://cran.r-project.org/package=emIRT) ![CRAN downloads](http://cranlogs.r-pkg.org/badges/grand-total/emIRT)\n\nVarious Expectation-Maximization (EM) algorithms are implemented for item\nresponse theory (IRT) models. The current implementation includes IRT models for\nbinary and ordinal responses, along with dynamic and hierarchical IRT models\nwith binary responses. Variational algorithms for scaling network and text data\nare also included.  The details about the methods implemented in this package are \ndescribed in Imai, Lo, and Olmsted. (2016). \"[Fast Estimation of Ideal Points with Massive Data.](https://imai.fas.harvard.edu/research/fastideal.html)\" *American Political Science Review*, \nVol. 110, No. 4 (December), pp. 631-656.\n\nThe current release of the R package is available on\n[CRAN](https://cran.r-project.org/web/packages/emIRT/).\n\nThe github versions of the R package are available with\n\n    library(\"devtools\")\n    install_github(\"kosukeimai/emIRT\")\n    install_github(\"kosukeimai/emIRT\", ref =\"development\")\n",
    "url": "https://github.com/kosukeimai/emIRT",
    "last_updated": "2024-12-16T15:53:02+00:00"
  },
  {
    "full_name": "floodsung/Deep-Learning-Papers-Reading-Roadmap",
    "name": "Deep-Learning-Papers-Reading-Roadmap",
    "description": "Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!",
    "language": "Python",
    "topics": [
      "deep-learning"
    ],
    "readme": "# Deep Learning Papers Reading Roadmap\n\n>If you are a newcomer to the Deep Learning area, the first question you may have is \"Which paper should I start reading from?\"\n\n>Here is a reading roadmap of Deep Learning papers!\n\nThe roadmap is constructed in accordance with the following four guidelines:\n\n- From outline to detail\n- From old to state-of-the-art\n- from generic to specific areas\n- focus on state-of-the-art\n\nYou will find many papers that are quite new but really worth reading.\n\nI would continue adding papers to this roadmap.\n\n\n---------------------------------------\n\n# 1 Deep Learning History and Basics\n\n## 1.0 Book\n\n**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. \"**Deep learning**.\" An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:\n\n## 1.1 Survey\n\n**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"**Deep learning**.\" Nature 521.7553 (2015): 436-444. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) **(Three Giants' Survey)** :star::star::star::star::star:\n\n## 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)\n\n**[2]** Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"**A fast learning algorithm for deep belief nets**.\" Neural computation 18.7 (2006): 1527-1554. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)**(Deep Learning Eve)** :star::star::star:\n\n**[3]** Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. \"**Reducing the dimensionality of data with neural networks**.\" Science 313.5786 (2006): 504-507. [[pdf]](http://www.cs.toronto.edu/~hinton/science.pdf) **(Milestone, Show the promise of deep learning)** :star::star::star:\n\n## 1.3 ImageNet Evolution（Deep Learning broke out from here）\n\n**[4]** Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"**Imagenet classification with deep convolutional neural networks**.\" Advances in neural information proc",
    "url": "https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap",
    "last_updated": "2025-09-02T08:25:35+00:00"
  },
  {
    "full_name": "benmarwick/JSTORr",
    "name": "JSTORr",
    "description": "Simple text mining of journal articles from JSTOR's Data for Research service",
    "language": "R",
    "topics": [],
    "readme": "JSTORr  [![Build Status](https://travis-ci.org/benmarwick/JSTORr.svg?branch=master)](https://travis-ci.org/benmarwick/JSTORr)\n======\n\nSimple exploratory text mining and document clustering of journal articles from JSTOR's Data for Research service.\n\nObjective\n----\nThe aim of this package is provide some simple functions in `R` to explore changes in word frequencies over time in a specific journal archive. It is designed to solve the problem of finding patterns and trends in the unstructured text content of a large number of scholarly journals articles from the JSTOR archive.\n\nCurrently there are functions to explore changes in:\n- a single word (ie. plot the relative frequency of a 1-gram over time)\n- two words independantly (ie. plot the relative frequency of two 1-grams over time)\n- sets of words (ie. plot the relative frequency of a single group of mulitple 1-grams over time)\n- correlations between two words over time (ie. plot the correlation of two 1-grams over time)\n- correlations between two sets of words over time (ie. plot the correlation two sets of multiple 1-grams over time)\n- all of the above with bigrams (a sequence of two words)\n- the most frequent words by n-year ranges of documents (ie. top words in all documents published in 2-5-10 year ranges, whatever you like)\n- the top n words correlated a word by n-year ranges of documents (ie. the top 20 words associated with the word 'pirate' in 5 year ranges)\n- various methods (k-means, PCA, affinity propagation) to detect clusters in a set of documents containing a word or set of words\n- topic models with the `lda` package for full `R` solution or the Java-based MALLET program (if installing that is an option, currently implemented here for Windows only) \n\nThis package will be useful to researchers who want to explore the history of ideas in an academic field, and investigate changes in word and phrase use over time, and between different journals. \n\nHow to install\n----\nFirst, make sure you've got Hadley Wi",
    "url": "https://github.com/benmarwick/JSTORr",
    "last_updated": "2025-07-30T05:32:07+00:00"
  },
  {
    "full_name": "notnews/uk_not_news",
    "name": "uk_not_news",
    "description": "Not News: Provision of Apolitical News in the British News Media",
    "language": "Jupyter Notebook",
    "topics": [
      "news",
      "british-news-media",
      "nlp",
      "machine-learning"
    ],
    "readme": "### Not News: Provision of Apolitical News in British News Media\n\nWhat proportion of news is *not news*? What proportion of news stories are about topics unrelated to public affairs, very broadly construed, and instead are about things like cooking, sports, travel, movie reviews and such. Using a large corpus of text from web pages from hundreds of news outlets from the U.K., we tally the provision of *not news.* We describe how the provision of *not news* varies across outlets and over time.\n\n#### Scripts, Figures, and Write-up\n\n* [Scripts](scripts/)\n    - [Coding News Media Outlets](scripts/00_uk_coding_outlets.ipynb)\n    - [Summary of the Data](scripts/01_subset_summarize_uk_news_media_data_by_label.ipynb)\n    - [Produce Summary Table for the Data](scripts/01a_describe_data.R)\n    - [Not News Classifier](scripts/02_url_classify_uk.ipynb)\n    - [Pretty Model Validation Tables, Top Coefficients, etc.](scripts/02a_model_stats_interp.R)\n    - [Share of Not News, Over Time, etc.](scripts/03_describe_not_news.R) (see also [here](scripts/02_url_classify_uk.ipynb))\n\n* [Tables](tabs/)\n* [Graphs](figs/)\n* [Manuscript (.tex, .bib, pdf)](ms/)\n\n#### Data\n\nGiven copyright issues, we cannot share the full-text of news articles publicly. A dataset without the story text but including the URL, source,  date, predicted and training labels can be [found here](https://doi.org/10.7910/DVN/VZ8DB3). \n\nWe are happy to share the raw article text data under the following conditions:\n\n* you will not share the data with anyone else, and \n* you will only use it for research purposes \n\nTo request the data, please fill out this [form](https://goo.gl/forms/WMv6qtmr5H4IehgF3). If your request is approved, you will get read access to a file in a Google Coldline Storage bucket for a month. The bucket is setup such that the requester pays---you will need to create a project that can be used for billing.\n\n#### Authors\n\nSuriyan Laohaprapanon and Gaurav Sood\n\n#### Contribute to the Project\n\nIf you see",
    "url": "https://github.com/notnews/uk_not_news",
    "last_updated": "2020-08-13T05:34:42+00:00"
  },
  {
    "full_name": "cbahlai/OSRR_course",
    "name": "OSRR_course",
    "description": "The Open Science and Reproducible Research course",
    "language": "",
    "topics": [],
    "readme": "## The Open Science and Reproducible Research course##\nThis repo will contain the materials for an open science and reproducible research course targetted at organismal ecologists. The plan is  to create a framework for students to be led along the open science path, over a typical semester and using real data, leading to a final publication. The idea is to provide an even balance of technical skill (eg: how to set up version control using R Studio) with more philosophical aspects (eg: authorship and credit in collaborative projects)\n\nThe initial materials will be developed in conjunction with a real-world offering of the OSRR course, held here at Michigan State, but I'd like to encourage everyone to branch, fork, PR when you have something to add- the end goal is to create materials for a distributed-delivery, customizable course that can be offered to the full breadth of early career scientists- to get all of us started out in a more collaborative, more open environment.\n\nFind the community and tweet to us (me) at #OSRRcourse\n\nOh! I should cite a license. Let's go with [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/legalcode)\n",
    "url": "https://github.com/cbahlai/OSRR_course",
    "last_updated": "2024-08-27T18:35:46+00:00"
  },
  {
    "full_name": "jblas-project/jblas",
    "name": "jblas",
    "description": "Linear Algebra for Java",
    "language": "Java",
    "topics": [
      "java",
      "lapack",
      "linear-algebra",
      "machine-learning",
      "matrix-library"
    ],
    "readme": "jblas is a matrix library for Java which uses existing high\nperformance BLAS and LAPACK libraries like ATLAS.\n\n* Version 1.2.5, August 22, 2020\n* Version 1.2.4, May 11, 2015\n* Version 1.2.3, February 13, 2013\n* Version 1.2.2, December 17, 2012\n* Version 1.2.1\n* Version 1.2.0, January 7, 2011\n* Version 1.1.1\n* Version 1.1, August 16, 2010\n* Version 1.0.2, February 26, 2010\n* Version 1.0.1, January 14, 2010\n* Version 1.0, December 22, 2009\n* Version 0.3, September 17, 2009\n* Version 0.2, May 8, 2009\n* Version 0.1, March 28, 2009\n\nsee also the file RELEASE_NOTES\n\nHomepage: http://jblas.org\n\nINSTALL\n-------\n\nIn principle, all you need is the jblas-1.2.5.jar in your\nclasspath. jblas-1.2.5.jar will then automagically extract your platform\ndependent native library to a tempfile and load it from there. You can\nalso put that file somewhere in your load path ($LD_LIBRARY_PATH for\nLinux, %PATH for Windows).\n\nOr, use the following dependency in maven\n\n```HTML\n  <dependency>\n    <groupId>org.jblas</groupId>\n    <artifactId>jblas</artifactId>\n    <version>1.2.5</version>\n  </dependency>\n```\n\nBUILDING\n--------\n\nIf you only work on the java part, you can use maven to recompile from the sources.\nIn addition to that you need an installation of\nruby for some scripts which automaticall generate code. Then, you just\ntype \"mvn package\" on the command line.\n\nIf you want to build jblas from the sources including the native part,\nyou need to set up quite a few things:\n\nYou will need some implementation of blas and lapack. jblas is tested\nwith either plain lapack, or ATLAS\n(http://math-atlas.sourceforge.net/). You also need the Fortran\nsources for BLAS and LAPACK, available, for example from\nhttp://www.netlib.org/lapack/lapack-lite-3.1.1.tgz.\n\nIf you still want to build the source your own, see INSTALL for\nfurther details.\n\n\nHOW TO GET STARTED\n------------------\n\nHave a look at javadoc/index.html and\njavadoc/org/jblas/DoubleMatrix.html\n\nIf you want to validate your installation and get some ",
    "url": "https://github.com/jblas-project/jblas",
    "last_updated": "2025-08-13T11:08:29+00:00"
  },
  {
    "full_name": "bokeh/bokeh",
    "name": "bokeh",
    "description": "Interactive Data Visualization in the browser, from  Python",
    "language": "TypeScript",
    "topics": [
      "bokeh",
      "python",
      "interactive-plots",
      "javascript",
      "visualization",
      "plotting",
      "plots",
      "data-visualisation",
      "notebooks",
      "jupyter",
      "visualisation",
      "numfocus"
    ],
    "readme": "<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/bokeh/pm/main/assets/logos/SVG/bokeh-logo-white-text-no-padding.svg\">\n  <img src=\"https://raw.githubusercontent.com/bokeh/pm/main/assets/logos/SVG/bokeh-logo-black-text-no-padding.svg\" alt=\"Bokeh logo -- text is white in dark theme and black in light theme\" height=60/>\n</picture>\n\n----\n\n[Bokeh](https://bokeh.org) is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics and affords high-performance interactivity across large or streaming datasets. Bokeh can help anyone who wants to create interactive plots, dashboards, and data applications quickly and easily.\n\n<table>\n\n<tr>\n\n  <td>Package</td>\n\n  <td>\n    <img src=\"https://img.shields.io/pypi/v/bokeh?label=Version&color=ECD078&style=for-the-badge\"\n         alt=\"Latest package version\" />\n  </td>\n\n  <td>\n    <a href=\"https://docs.bokeh.org/en/latest/docs/first_steps/installation.html\">\n    <img src=\"https://img.shields.io/pypi/pyversions/bokeh?color=ECD078&style=for-the-badge\"\n         alt=\"Supported Python versions\" />\n    </a>\n  </td>\n\n  <td>\n    <a href=\"https://github.com/bokeh/bokeh/blob/-/LICENSE.txt\">\n    <img src=\"https://img.shields.io/github/license/bokeh/bokeh.svg?color=ECD078&style=for-the-badge\"\n         alt=\"Bokeh license (BSD 3-clause)\" />\n    </a>\n  </td>\n\n</tr>\n\n<tr>\n\n  <td>Project</td>\n\n  <td>\n    <img src=\"https://img.shields.io/github/contributors-anon/bokeh/bokeh?color=ECD078&style=for-the-badge\"\n         alt=\"Github contributors\" />\n  </td>\n\n  <td>\n    <a href=\"https://numfocus.org\">\n    <img src=\"https://img.shields.io/badge/sponsor-numfocus-ECD078?style=for-the-badge\"\n         alt=\"Link to NumFOCUS\" />\n    </a>\n  </td>\n\n  <td>\n    <a href=\"https://docs.bokeh.org/en/latest/\">\n    <img src=\"https://img.shields.io/badge/documentation-latest-ECD078?style=for-the-badge\"\n         alt=\"Link to documentation\" />\n    </a>\n  </td>\n\n</",
    "url": "https://github.com/bokeh/bokeh",
    "last_updated": "2025-09-02T05:52:01+00:00"
  },
  {
    "full_name": "PedramNavid/trump_speeches",
    "name": "trump_speeches",
    "description": "All of Trump's Speeches from June 2015 to November 9, 2016",
    "language": "Python",
    "topics": [],
    "readme": "# trump_speeches\nAll of Trump's Speeches from June 2015 to November 9, 2016\n\nMissing line breaks and other formatting, but should work well for most text analysis purposes. Enjoy. \n",
    "url": "https://github.com/PedramNavid/trump_speeches",
    "last_updated": "2024-01-04T16:09:04+00:00"
  },
  {
    "full_name": "saschagobel/legislatoR",
    "name": "legislatoR",
    "description": "Interface to the Comparative Legislators Database",
    "language": "R",
    "topics": [
      "legislators",
      "politicians",
      "dataset",
      "wikipedia",
      "data",
      "parliament",
      "r",
      "political-science",
      "politics"
    ],
    "readme": "# legislatoR: Interface to the Comparative <img src=\"images/sticker.jpg\" width=\"160\" align=\"right\" /> <br /> Legislators Database\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/legislatoR)](https://cran.r-project.org/package=legislatoR)\n[![GitHub release version](https://img.shields.io/github/release/saschagobel/legislatoR.svg?style=flat)](https://github.com/saschagobel/legislatoR/releases)\n[![CRAN_Download_Badge](https://cranlogs.r-pkg.org/badges/grand-total/legislatoR)](https://cran.rstudio.com/web/packages/legislatoR/index.html)\n\nlegislatoR is a package for the software environment R that facilitates access to the [Comparative Legislators Database (CLD)](https://complegdatabase.com/). The CLD includes political, sociodemographic, career, online presence, public attention, and visual information for over 67,000 contemporary and historical politicians from 16 countries. Data are also available for download in .csv and .sqlite formats at the [CLD's Dataverse](https://dataverse.harvard.edu/dataverse/cld).\n\n## Content and data structure\nThe CLD covers the following countries and time periods:\n\n| Country                              | Legislative sessions        | Politicians (unique*) | Integrated with    |\n| ------------------------------------ | --------------------------- | -------------------- | ------------------ |\n| Austria (Nationalrat)                | all 27<br /> (1920-2019)        | 1,923                | [ParlSpeech V2](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L4OAKN) (Rauh/Schwalbach 2020)      |\n| Brazil (Câmara dos Deputados)        | 38-57<br /> (1947-2022)     | 3,474                |                    |\n| Canada (House of Commons)            | all 44<br /> (1867-2021)        | 4,567                |                    |\n| Czech Republic (Poslanecka Snemovna) | all 9<br /> (1992-2021)         | 1,",
    "url": "https://github.com/saschagobel/legislatoR",
    "last_updated": "2025-05-16T09:04:23+00:00"
  },
  {
    "full_name": "jlevy/the-art-of-command-line",
    "name": "the-art-of-command-line",
    "description": "Master the command line, in one page",
    "language": "",
    "topics": [
      "bash",
      "unix",
      "documentation",
      "linux",
      "macos",
      "windows"
    ],
    "readme": "🌍\n*[Čeština](README-cs.md) ∙ [Deutsch](README-de.md) ∙ [Ελληνικά](README-el.md) ∙ [English](README.md) ∙ [Español](README-es.md) ∙ [Français](README-fr.md) ∙ [Indonesia](README-id.md) ∙ [Italiano](README-it.md) ∙ [日本語](README-ja.md) ∙ [한국어](README-ko.md) ∙ [polski](README-pl.md) ∙ [Português](README-pt.md) ∙ [Română](README-ro.md) ∙ [Русский](README-ru.md) ∙ [Slovenščina](README-sl.md) ∙ [Українська](README-uk.md) ∙ [简体中文](README-zh.md) ∙ [繁體中文](README-zh-Hant.md)*\n\n\n# The Art of Command Line\n\n*Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. –[jlevy](https://github.com/jlevy), [Holloway](https://www.holloway.com). Thank you!*\n\n- [Meta](#meta)\n- [Basics](#basics)\n- [Everyday use](#everyday-use)\n- [Processing files and data](#processing-files-and-data)\n- [System debugging](#system-debugging)\n- [One-liners](#one-liners)\n- [Obscure but useful](#obscure-but-useful)\n- [macOS only](#macos-only)\n- [Windows only](#windows-only)\n- [More resources](#more-resources)\n- [Disclaimer](#disclaimer)\n\n\n![curl -s 'https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md' | egrep -o '`\\w+`' | tr -d '`' | cowsay -W50](cowsay.png)\n\nFluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot.\n\nThis work is the result of [many authors and translators](AUTHORS.md).\nSome of this\n[original",
    "url": "https://github.com/jlevy/the-art-of-command-line",
    "last_updated": "2025-09-02T09:56:41+00:00"
  },
  {
    "full_name": "santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning",
    "name": "Abstractive-Summarization-With-Transfer-Learning",
    "description": "Abstractive summarisation using Bert as encoder and Transformer Decoder",
    "language": "Python",
    "topics": [
      "bert",
      "summarization",
      "abstractive-summarization",
      "abstractive-text-summarization",
      "bert-model",
      "transformer",
      "transfer-learning",
      "nlp",
      "nlg"
    ],
    "readme": "<h3>Abstractive summarization using bert as encoder and transformer decoder</h3>\n\nI have used a text generation library called Texar , Its a beautiful library with a lot of abstractions, i would say it to be \nscikit learn for text generation problems.\n\nThe main idea behind this architecture is to use the transfer learning from pretrained BERT a masked language model ,\nI have replaced the Encoder part with BERT Encoder and the deocder is trained from the scratch.\n\nOne of the advantages of using Transfomer Networks is training is much faster than LSTM based models as we elimanate sequential behaviour in Transformer models.\n\nTransformer based models generate more gramatically correct  and coherent sentences.\n\n\n<h3>To run the model</h3>\n<pre>\nwget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip \nunzip uncased_L-12_H-768_A-12.zip\n\nPlace the story and summary files under data folder with the following names.\n-train_story.txt\n-train_summ.txt\n-eval_story.txt\n-eval_summ.txt\neach story and summary must be in a single line (see sample text given.)\n\n\nStep1:\nRun Preprocessing\n<b>python preprocess.py</b>\n\nThis creates two tfrecord files under the data folder.\n\nStep 2:\n<b>python main.py</b>\n\nConfigurations for the model can be changes from config.py file\n\nStep 3:\nInference \nRun the command <b>python inference.py</b>\nThis code runs a flask server \nUse postman to send the POST request @http://your_ip_address:1118/results\nwith two form parameters story,summary\n\n\n\n</pre>\n\n\n",
    "url": "https://github.com/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning",
    "last_updated": "2025-07-20T11:18:03+00:00"
  },
  {
    "full_name": "astral-sh/ruff",
    "name": "ruff",
    "description": "An extremely fast Python linter and code formatter, written in Rust.",
    "language": "Rust",
    "topics": [
      "linter",
      "pep8",
      "python",
      "python3",
      "rust",
      "rustpython",
      "static-analysis",
      "static-code-analysis",
      "style-guide",
      "styleguide",
      "ruff"
    ],
    "readme": "<!-- Begin section: Overview -->\n\n# Ruff\n\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![image](https://img.shields.io/pypi/v/ruff.svg)](https://pypi.python.org/pypi/ruff)\n[![image](https://img.shields.io/pypi/l/ruff.svg)](https://github.com/astral-sh/ruff/blob/main/LICENSE)\n[![image](https://img.shields.io/pypi/pyversions/ruff.svg)](https://pypi.python.org/pypi/ruff)\n[![Actions status](https://github.com/astral-sh/ruff/workflows/CI/badge.svg)](https://github.com/astral-sh/ruff/actions)\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&logoColor=white)](https://discord.com/invite/astral-sh)\n\n[**Docs**](https://docs.astral.sh/ruff/) | [**Playground**](https://play.ruff.rs/)\n\nAn extremely fast Python linter and code formatter, written in Rust.\n\n<p align=\"center\">\n  <picture align=\"center\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://user-images.githubusercontent.com/1309177/232603514-c95e9b0f-6b31-43de-9a80-9e844173fd6a.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg\">\n    <img alt=\"Shows a bar chart with benchmark results.\" src=\"https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg\">\n  </picture>\n</p>\n\n<p align=\"center\">\n  <i>Linting the CPython codebase from scratch.</i>\n</p>\n\n- ⚡️ 10-100x faster than existing linters (like Flake8) and formatters (like Black)\n- 🐍 Installable via `pip`\n- 🛠️ `pyproject.toml` support\n- 🤝 Python 3.13 compatibility\n- ⚖️ Drop-in parity with [Flake8](https://docs.astral.sh/ruff/faq/#how-does-ruffs-linter-compare-to-flake8), isort, and [Black](https://docs.astral.sh/ruff/faq/#how-does-ruffs-formatter-compare-to-black)\n- 📦 Built-in caching, to avoid re-analyzing unchanged files\n- 🔧 Fix support, for automatic error correctio",
    "url": "https://github.com/astral-sh/ruff",
    "last_updated": "2025-09-02T08:01:55+00:00"
  },
  {
    "full_name": "gojiplus/tuber",
    "name": "tuber",
    "description": ":sweet_potato: Access YouTube from R",
    "language": "R",
    "topics": [
      "youtube-api",
      "video",
      "access-youtube",
      "youtube-oauth",
      "caption",
      "youtube"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n## :sweet\\_potato: tuber: Access YouTube API via R\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/tuber)](https://cran.r-project.org/package=tuber)\n![](http://cranlogs.r-pkg.org/badges/grand-total/tuber)\n\nAccess YouTube API via R. Get comments posted on YouTube videos, get\ninformation on how many times a video has been liked, search for videos\nwith particular content, and much more. You can also get closed captions\nof videos you own. To learn more about the YouTube API, see\n<https://developers.google.com/youtube/v3/>.\n\n### Installation\n\nTo get the current development version from GitHub:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"soodoku/tuber\", build_vignettes = TRUE)\n```\n\nTo get a quick overview of some important functions in tuber, check out\n[this article](https://gojiplus.github.io/tuber/articles/tuber-ex.html).\n\n### Using tuber\n\nTo get going, get the application id and password from the Google\nDeveloper Console (see\n<https://developers.google.com/youtube/v3/getting-started>). Enable all\nthe YouTube APIs. Then set the application id and password via the\n`yt_oauth` function. For more information about YouTube OAuth, see\n[YouTube OAuth\nGuide](https://developers.google.com/youtube/v3/guides/authentication).\n\n``` r\nyt_oauth(\"app_id\", \"app_password\")\n```\n\nIf your session cannot open a browser window for authentication, pass\n`use_oob = TRUE` to `yt_oauth()` so that authentication can be completed\nvia an out-of-band code.\n\nTo force re-authentication at any time, delete the `.httr-oauth` file in\nyour working directory.\n\n**Note:** If you are on ubuntu, you may have to run the following before\ndoing anything:\n\n    httr::set_config(httr::config( ssl_verifypeer = 0L ) )\n\n**Get Statistics of a Video**\n\n``` r\nget_stats(video_id = \"N708P-A45D0\")\n```\n\n**Get Information About a Video**\n\n``` r\nget_video_details(video_id = \"N708P-A45D0\")\n```\n\n**Get Captions of a Video**\n\n```",
    "url": "https://github.com/gojiplus/tuber",
    "last_updated": "2025-08-30T10:38:56+00:00"
  },
  {
    "full_name": "edgararuiz-zz/dbplot",
    "name": "dbplot",
    "description": "Simplifies plotting of database and sparklyr data",
    "language": "R",
    "topics": [
      "visualization",
      "r",
      "ggplot2",
      "databases",
      "sparklyr",
      "dbplot",
      "rlang"
    ],
    "readme": "\n# dbplot <img src=\"man/figures/logo.png\" align=\"right\" alt=\"\" width=\"220\" />\n\n[![Build\nStatus](https://travis-ci.org/edgararuiz/dbplot.svg?branch=master)](https://travis-ci.org/edgararuiz/dbplot)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/dbplot)](https://cran.r-project.org/package=dbplot)\n[![Coverage\nstatus](https://codecov.io/gh/edgararuiz/dbplot/branch/master/graph/badge.svg)](https://codecov.io/github/edgararuiz/dbplot?branch=master)\n\n  - [Installation](#installation)\n  - [Connecting to a data source](#connecting-to-a-data-source)\n  - [Example](#example)\n  - [`ggplot`](#ggplot)\n      - [Histogram](#histogram)\n      - [Raster](#raster)\n      - [Bar Plot](#bar-plot)\n      - [Line plot](#line-plot)\n      - [Boxplot](#boxplot)\n  - [Calculation functions](#calculation-functions)\n  - [`db_bin()`](#db_bin)\n\nLeverages `dplyr` to process the calculations of a plot inside a\ndatabase. This package provides helper functions that abstract the work\nat three levels:\n\n1.  Functions that ouput a `ggplot2` object\n2.  Functions that outputs a `data.frame` object with the calculations\n3.  Creates the formula needed to calculate bins for a Histogram or a\n    Raster plot\n\n## Installation\n\nYou can install the released version from CRAN:\n\n``` r\n# install.packages(\"dbplot\")\n```\n\nOr the the development version from GitHub, using the `remotes` package:\n\n``` r\n# install.packages(\"remotes\")\n# remotes::install_github(\"edgararuiz/dbplot\")\n```\n\n## Connecting to a data source\n\n  - For more information on how to connect to databases, including Hive,\n    please visit <http://db.rstudio.com>\n\n  - To use Spark, please visit the `sparklyr` official website:\n    <http://spark.rstudio.com>\n\n## Example\n\nIn addition to database connections, the functions work with `sparklyr`.\nA local `RSQLite` database will be used for the examples in this README.\n\n``` r\nlibrary(DBI)\nlibrary(odbc)\nlibrary(dplyr)\n\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ndb_flights <- copy_to(con, nycflights13::f",
    "url": "https://github.com/edgararuiz-zz/dbplot",
    "last_updated": "2024-11-01T22:39:53+00:00"
  },
  {
    "full_name": "twitter/AnomalyDetection",
    "name": "AnomalyDetection",
    "description": "Anomaly Detection with R",
    "language": "R",
    "topics": [],
    "readme": "# AnomalyDetection R package\n\n[![Build Status](https://travis-ci.org/twitter/AnomalyDetection.png)](https://travis-ci.org/twitter/AnomalyDetection)\n[![Pending Pull-Requests](http://githubbadges.herokuapp.com/twitter/AnomalyDetection/pulls.svg?style=flat)](https://github.com/twitter/AnomalyDetection/pulls)\n[![Github Issues](http://githubbadges.herokuapp.com/twitter/AnomalyDetection/issues.svg)](https://github.com/twitter/AnomalyDetection/issues)\n\nAnomalyDetection is an open-source R package to detect anomalies which is\nrobust, from a statistical standpoint, in the presence of seasonality and an\nunderlying trend. The AnomalyDetection package can be used in wide variety of\ncontexts. For example, detecting anomalies in system metrics after a new\nsoftware release, user engagement post an A/B test, or for problems in\neconometrics, financial engineering, political and social sciences.\n\n## How the package works\n\nThe underlying algorithm – referred to as Seasonal Hybrid ESD (S-H-ESD) builds\nupon the Generalized ESD test for detecting anomalies. Note that S-H-ESD can\nbe used to detect both global as well as local anomalies. This is achieved by\nemploying time series decomposition and using robust statistical metrics, viz.,\nmedian together with ESD. In addition, for long time series (say, 6 months of\nminutely data), the algorithm employs piecewise approximation - this is rooted\nto the fact that trend extraction in the presence of anomalies in non-trivial -\nfor anomaly detection.\n\nBesides time series, the package can also be used to detect anomalies in a\nvector of numerical values. We have found this very useful as many times the\ncorresponding timestamps are not available. The package provides rich\nvisualization support. The user can specify the direction of anomalies, the\nwindow of interest (such as last day, last hour), enable/disable piecewise\napproximation; additionally, the x- and y-axis are annotated in a way to assist\nvisual data analysis.\n\n## How to get started\n\nInstall ",
    "url": "https://github.com/twitter/AnomalyDetection",
    "last_updated": "2025-08-08T14:06:56+00:00"
  },
  {
    "full_name": "google/fhir",
    "name": "fhir",
    "description": "FHIR Protocol Buffers",
    "language": "C++",
    "topics": [
      "fhir",
      "healthcare",
      "hl7"
    ],
    "readme": "# Introduction\n\nFhirProto is Google’s implementation of the [FHIR Standard](http://hl7.org/fhir/) for Health Care data using [Protocol Buffers](https://developers.google.com/protocol-buffers).  By leveraging Google’s core data storage format, FhirProto provides a type-safe, strongly validated FHIR format with cross-language support at a fraction of the size on disk, making it a great data model to develop your Health Care application on.  Structured Codes and Extensions guarantee that your data will be in the correct format.  Support for generating and validating against custom Implementation Guides allow you to customize FhirProto to your dataset and requirements.  Parsing and Printing libraries make it easy to go back and forth between FhirProto format and JSON.\n\n# Getting Started\nWe think the best way to get an idea of how FhirProto works is to get in and start playing with it.  To that end, we provide [https://github.com/google/fhir-examples](https://github.com/google/fhir-examples). This repo contains a script for using [Synthea](https://github.com/synthetichealth/synthea) to create a synthetic FHIR JSON dataset, and then shows some examples of parsing, printing, validating, profiling and querying.  The repo also contains a [walkthrough](https://github.com/google/fhir-examples/blob/master/EXAMPLES.md) of many of the examples.\n\nA Reference User Guide with in-depth descriptions of different concepts can be found [here](https://github.com/google/fhir-examples/blob/master/USERGUIDE.md).\n\n## Trademark\nFHIR® is the registered trademark of HL7 and is used with the permission of HL7. Use of the FHIR trademark does not constitute endorsement of this product by HL7.\n\n",
    "url": "https://github.com/google/fhir",
    "last_updated": "2025-09-01T00:29:30+00:00"
  },
  {
    "full_name": "Data4Democracy/usa-dashboard",
    "name": "usa-dashboard",
    "description": "A dashboard of key metrics for the USA",
    "language": "Python",
    "topics": [],
    "readme": "# USA Dashboard\n\nWe're building a dashboard of key metrics for the USA because: *If you can't measure it, you can't manage it*.  When complete, you'll be able to see how well the country is doing along a number of metrics at a glance.  We strive to paint an objective, centralized picture of what's currently going on, with very timely updates.  We hope that by making this information easily visible and available that we can come to a collective understanding about whether the country is thriving or not.\n\n**Sound good? Get involved** Check out information about [Data for Democracy](https://medium.com/data-for-democracy) and [email Jonathon Morgan](mailto:jonathon@datafordemocracy.org) for an invitation.\n\n**Maintainers:** \n - @devinaconley\n - @sraithel\n\nContents:\n---\n[Background](#background)  \n[Contributing](#contributing)  \n[Metrics](#metrics)  \n[Architecture](#architecture)  \n\nBackground\n---\n**The problem that this solves** Often we only pay attention to data when someone decides to perform an analysis and share it with us.  If we automate key analyses and pay attention on a regular basis, we'll all be more familiar with the health of the country and know what to prioritize.  We'll also be able to hold politicians accountable for their actions if we see that metrics change abruptly.\n\n**Doesn't the government already do this?** Yes, the government collects original data and produces regular reports.  We want to centralize this information and make it easier to digest for citizens.  For instance, crime data is available from each city, but only the FBI periodically merges it all and produces a *yearly* report.  We can do better than this in 2017.\n\nContributing\n---\nWe have several streams of work going on right now, each attached to a milestone or issue:\n\n- [Bring an easy metric end-to-end](https://github.com/Data4Democracy/usa-dashboard/milestone/5): Build out the entire pipeline for scraping, analyzing and publishing data. This should be a valuable proof of concept, ",
    "url": "https://github.com/Data4Democracy/usa-dashboard",
    "last_updated": "2024-01-04T16:10:29+00:00"
  },
  {
    "full_name": "edrlab/thorium-reader",
    "name": "thorium-reader",
    "description": "A cross platform desktop reading app, based on the Readium Desktop toolkit",
    "language": "TypeScript",
    "topics": [
      "typescript",
      "electron",
      "reader",
      "epub-reader",
      "epub",
      "epub3",
      "lcp",
      "react",
      "redux",
      "opds",
      "opds-feed"
    ],
    "readme": "# Thorium Reader\n\nThorium Reader is an easy to use EPUB reading application for Windows 10/10S, MacOS and Linux. After importing e-books from a directory or OPDS feed, you'll be able to read on any screen size, customize layout settings, navigate via the table of contents or page list, set bookmarks ... A great care is taken to ensure the accessibility of the application for visual impaired people using NVDA, JAWS or Narrator.\n\nFree application. No ads. No private data flowing anywhere.\n\nThis project is in constant evolution, corrections and new features will be added soon and your support is welcome for that. The application is based on the open-source Readium Desktop toolkit.\n\nMore information can be found in the [Landing page](https://thorium.edrlab.org/), within the [online support documentation](https://thorium.edrlab.org/en/th3/800_collaborating/802_localizing/). Users can [Add Documentation catalog to Thorium (OPDS link)](opds://edrlab.github.io/publications/feeds/thorium31_documentation.json) or [browse English documentation inline within the Readium web reader](https://thorium.edrlab.org/en/onlinedoc).\n\nIt is currently localized in following 28 languages:\n\n* (en) English\n* (fr) Français (French)\n* (fi) Suomi (Finnish)\n* (de) Deutsch (German)\n* (es) Español (Spanish)\n* (nl) Nederlands (Dutch)\n* (ja) 日本語 (Japanese)\n* (ka) ქართული (Georgian)\n* (lt) Lietuvių (Lithuanian)\n* (pt-BR) Português Brasileiro (Portuguese - Brazil)\n* (pt-PT) Português (Portuguese - Portugal)\n* (zh-CN) 简体中文 - 中国 (Simplified Chinese - China)\n* (zh-TW) 繁體中文 - 台灣 (Traditional Chinese - Taiwan)\n* (it) Italiano (Italian)\n* (ru) Русский (Russian)\n* (ko) 한국어 (Korean)\n* (sv) Svenska (Swedish)\n* (ca) Catalan\n* (gl) Galician\n* (eu) Euskadi (Basque)\n* (el) ελληνικός (Greek)\n* (bg) български (Bulgarian)\n* (hr) Hrvatski (Croatian)\n* (da) Dansk (Danish)\n* (sl) Slovenščina (Slovene)\n* (cs) čeština (Czech)\n* (ar) عَرَبِيّ (Arabic)\n\nSince february 2025 we use Weblate project Thorium as the main tool for ",
    "url": "https://github.com/edrlab/thorium-reader",
    "last_updated": "2025-09-02T09:20:53+00:00"
  },
  {
    "full_name": "microsoft/unilm",
    "name": "unilm",
    "description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities",
    "language": "Python",
    "topics": [
      "nlp",
      "pre-trained-model",
      "unilm",
      "minilm",
      "layoutlm",
      "layoutxlm",
      "beit",
      "document-ai",
      "trocr",
      "beit-3",
      "foundation-models",
      "xlm-e",
      "deepnet",
      "llm",
      "multimodal",
      "mllm",
      "kosmos",
      "kosmos-1",
      "textdiffuser",
      "bitnet"
    ],
    "readme": "<!--# Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities-->\n## [aka.ms/GeneralAI](https://aka.ms/GeneralAI)\n# Hiring\nWe are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and General AI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to <a href=\"mailto:fuwei@microsoft.com\" class=\"x-hidden-focus\">fuwei@microsoft.com</a>.\n\n# Foundation Architecture\n### TorchScale - A Library of Foundation Architectures ([repo](https://github.com/microsoft/torchscale))\n\nFundamental research to develop new architectures for foundation models and AI, focusing on modeling generality and capability, as well as training stability and efficiency.\n\n> Stability - [**DeepNet**](https://github.com/microsoft/unilm/tree/master/deepnet): scaling Transformers to 1,000 Layers and beyond\n\n> Generality - [**Foundation Transformers (Magneto)**](https://arxiv.org/abs/2210.06423): towards true general-purpose modeling across tasks and modalities (including language, vision, speech, and multimodal)\n\n> Capability - A [**Length-Extrapolatable**](https://arxiv.org/abs/2212.10554) Transformer\n\n> Efficiency & Transferability - [**X-MoE**](https://github.com/microsoft/unilm/tree/master/xmoe): scalable & finetunable sparse Mixture-of-Experts (MoE)\n\n### The Revolution of Model Architecture\n\n> [**BitNet**](https://arxiv.org/abs/2310.11453): 1-bit Transformers for Large Language Models\n\n> [**RetNet**](https://arxiv.org/abs/2307.08621): Retentive Network: A Successor to Transformer for Large Language Models\n\n> [**LongNet**](https://arxiv.org/abs/2307.02486): Scaling Transformers to 1,000,000,000 Tokens\n\n# Foundation Models\n\n### The Evolution of (M)LLM (Multimodal LLM)\n\n> [**Kosmos-2.5**](https://github.com/microsoft/unilm/tree/master/kosmos-2.5): **A Multimodal Literate Model**\n\n> [**Kosmos-2**](https://github.com/microsoft/unilm/tree/master/kosmos-2)",
    "url": "https://github.com/microsoft/unilm",
    "last_updated": "2025-09-02T07:09:57+00:00"
  },
  {
    "full_name": "giuseppec/iml",
    "name": "iml",
    "description": "iml: interpretable machine learning R package",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- badges: start -->\n\n[![r-cmd-check](https://github.com/giuseppec/iml/actions/workflows/r-cmd-check.yml/badge.svg)](https://github.com/giuseppec/iml/actions)\n[![CRAN Status\nBadge](https://www.r-pkg.org/badges/version-ago/iml)](https://CRAN.R-project.org/package=iml)\n[![CRAN\nDownloads](https://cranlogs.r-pkg.org/badges/grand-total/iml)](https://CRAN.R-project.org/package=iml)\n[![codecov.io](https://codecov.io/github/christophM/iml/coverage.svg?branch=master)](https://codecov.io/github/christophM/iml?branch=master)\n[![DOI](http://joss.theoj.org/papers/10.21105/joss.00786/status.svg)](https://doi.org/10.21105/joss.00786)\n<!-- badges: end -->\n\n# iml\n\n`iml` is an R package that interprets the behavior and explains\npredictions of machine learning models.\n<img src=\"https://github.com/christophM/iml/blob/master/man/figures/iml.png?raw=true\" align=\"right\" height=140/>\nIt implements model-agnostic interpretability methods - meaning they can\nbe used with any machine learning model.\n\n## Features\n\n- Feature importance\n- Partial dependence plots\n- Individual conditional expectation plots (ICE)\n- Accumulated local effects\n- Tree surrogate\n- LocalModel: Local Interpretable Model-agnostic Explanations\n- Shapley value for explaining single predictions\n\nRead more about the methods in the [Interpretable Machine\nLearning](https://christophm.github.io/interpretable-ml-book/agnostic.html)\nbook.\n\n## Tutorial\n\nStart an interactive notebook tutorial by clicking on this badge\n[![Binder](http://mybinder.org/badge.svg)](http://beta.mybinder.org/v2/gh/christophM/iml/master?filepath=./notebooks/tutorial-intro.ipynb)\n\n## Installation\n\nThe package can be installed directly from CRAN and the development\nversion from GitHub:\n\n``` r\n# Stable version\ninstall.packages(\"iml\")\n\n# Development version\nremotes::install_github(\"christophM/iml\")\n```\n\n## News\n\nChanges of the packages can be accessed in the [NEWS\nfile](https://christophm.github.io/iml/news/index.html).\n\n## Quickstart\n\nFirst we train a Random",
    "url": "https://github.com/giuseppec/iml",
    "last_updated": "2025-07-25T15:41:19+00:00"
  },
  {
    "full_name": "Pakillo/grateful",
    "name": "grateful",
    "description": "Facilitate citation of R packages",
    "language": "R",
    "topics": [
      "citation-generator",
      "r",
      "r-package",
      "software-citation"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# grateful: Facilitate citation of R packages\n\n<!-- badges: start -->\n\n[![](https://www.r-pkg.org/badges/version/grateful)](https://cran.r-project.org/package=grateful)\n![](https://img.shields.io/github/r-package/v/Pakillo/grateful)\n[![r-universe\nversion](https://pakillo.r-universe.dev/grateful/badges/version)](https://pakillo.r-universe.dev/grateful)\n[![r-universe\nstatus](https://pakillo.r-universe.dev/grateful/badges/checks)](https://pakillo.r-universe.dev/grateful)\n[![R-CMD-check](https://github.com/Pakillo/grateful/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/Pakillo/grateful/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/Pakillo/grateful/graph/badge.svg)](https://app.codecov.io/gh/Pakillo/grateful)\n<a href=\"https://diffify.com/R/grateful\" target=\"_blank\"><img src=\"https://diffify.com/diffify-badge.svg\" alt=\"The diffify page for the R package grateful\" style=\"width: 100px; max-width: 100%;\"></a>\n[![](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![](https://cranlogs.r-pkg.org/badges/grand-total/grateful)](https://cran.r-project.org/package=grateful)\n[![](https://cranlogs.r-pkg.org/badges/grateful)](https://cran.r-project.org/package=grateful)\n<!-- \n[![HitCount since 2024-06-05](https://hits.dwyl.com/Pakillo/grateful.svg?style=flat-square)](https://hits.dwyl.com/Pakillo/grateful)\n[![HitCount since 2024-06-05](https://hits.dwyl.com/Pakillo/grateful.svg?style=flat-square&show=unique)](https://hits.dwyl.com/Pakillo/grateful)\n--> <!-- badges: end -->\n\nThe goal of **grateful** is to make it very easy to cite R and the R\npackages used in any analyses, so that package aut",
    "url": "https://github.com/Pakillo/grateful",
    "last_updated": "2025-08-29T06:13:59+00:00"
  },
  {
    "full_name": "nanxstats/liftr",
    "name": "liftr",
    "description": "🐳 Containerize R Markdown documents for continuous reproducibility",
    "language": "R",
    "topics": [
      "docker",
      "reproducible-research",
      "rmarkdown",
      "containerization",
      "dynamic-documents",
      "reproducible-science",
      "knitr",
      "liftr",
      "statistical-computing"
    ],
    "readme": "# liftr <img src=\"man/figures/logo.png\" align=\"right\" width=\"120\" />\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/nanxstats/liftr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/nanxstats/liftr/actions/workflows/R-CMD-check.yaml)\n[![CRAN Version](https://www.r-pkg.org/badges/version/liftr)](https://cran.r-project.org/package=liftr)\n[![Downloads from the RStudio CRAN mirror](https://cranlogs.r-pkg.org/badges/liftr)](https://cran.r-project.org/package=liftr)\n<!-- badges: end -->\n\nliftr aims to solve the problem of _persistent reproducible reporting_.\nTo achieve this goal, it extends the [R Markdown](https://rmarkdown.rstudio.com)\nmetadata format, and uses Docker to containerize and render R Markdown documents.\n\n## Paper\n\nTo cite this work and the related software, please use\n\nNüst D, Eddelbuettel D, Bennett D et al. (2020). [The Rockerverse: Packages and Applications for Containerisation with R](https://doi.org/10.32614/RJ-2020-007). _The R Journal_ 12 (1), 437-461.\n\n## Installation\n\nInstall liftr from CRAN:\n\n```r\ninstall.packages(\"liftr\")\n```\n\nOr try the development version on GitHub:\n\n```r\nremotes::install_github(\"nanxstats/liftr\")\n```\n\n[Browse the vignettes](https://nanx.me/liftr/articles/) or the [demo video](https://vimeo.com/212815497) for a quick-start.\n\n## Workflow\n\n<img src=\"man/figures/liftr-workflow.png\" width=\"100%\" alt=\"Containerize R Markdown Documents with liftr\">\n\n## Events\n\n| Time            | Event                   | Location                         |\n|:----------------|:------------------------|:---------------------------------|\n| July 30, 2018 | [JSM 2018](https://ww2.amstat.org/meetings/JSM/2018/onlineprogram/AbstractDetails.cfm?abstractid=329348) ([talk](https://nanx.me/talks/jsm2018-liftr-nanxiao.pdf)) | Vancouver, Canada |\n| July 27, 2017 | [BioC 2017](https://bioconductor.org/help/course-materials/2017/BioC2017/) ([poster](https://nanx.me/posters/dockflow-poster-bioc2017.pdf) for [dockflow.org](https://dockflo",
    "url": "https://github.com/nanxstats/liftr",
    "last_updated": "2025-04-17T00:52:52+00:00"
  },
  {
    "full_name": "GuangchuangYu/hexSticker",
    "name": "hexSticker",
    "description": ":sparkles: Hexagon sticker in R",
    "language": "R",
    "topics": [
      "stickers",
      "hexagon-sticker",
      "rstats",
      "ggplot2",
      "visualization",
      "logo"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# hexSticker: create hexagon sticker in R\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/hexSticker?color=green)](https://cran.r-project.org/package=hexSticker)\n[![](http://cranlogs.r-pkg.org/badges/grand-total/hexSticker?color=green)](https://cran.r-project.org/package=hexSticker)\n[![](http://cranlogs.r-pkg.org/badges/hexSticker?color=green)](https://cran.r-project.org/package=hexSticker)\n[![](http://cranlogs.r-pkg.org/badges/last-week/hexSticker?color=green)](https://cran.r-project.org/package=hexSticker)\n\n## :writing_hand: Author\n\nGuangchuang YU <https://yulab-smu.top>\n\nSchool of Basic Medical Sciences, Southern Medical University\n\n[![saythanks](https://img.shields.io/badge/say-thanks-ff69b4.svg)](https://saythanks.io/to/GuangchuangYu)\n[![](https://img.shields.io/badge/follow%20me%20on-WeChat-green.svg)](https://guangchuangyu.github.io/blog_images/biobabble.jpg)\n\n------------------------------------------------------------------------\n\n## :arrow_double_down: Installation\n\nInstall the hexSticker package via CRAN:\n\n``` r\ninstall.packages(\"hexSticker\")\n```\n\nYou can also install the package via the Github repository.\n\n``` r\n# install.package(\"remotes\")   #In case you have not installed it.\nremotes::install_github(\"GuangchuangYu/hexSticker\")\n```\n\n## Fail to install\n\n### imageMagick\n\n`imageMagick` is required for installing `hexSticker`. If you have not\ninstalled it, please try the following\n[approaches](https://www.imagemagick.org/script/download.php).\n\n### Fail to load ‘sysfonts’\n\nIn Mac OS, you may need to re-install `sysfont` to properly load it.\n\nBe sure to install `xquartz` first.\n\n``` r\nbrew update && brew install homebrew/cask/xquartz\n```\n\n------------------------------------------------------------------------\n\n## Examples\n\n> `sticker` function will produce a file with dimension exactly for\n> printing according to <http://hexb.in/sticker.html>\n\n### base plot\n\n``` r\nlibrary(hexS",
    "url": "https://github.com/GuangchuangYu/hexSticker",
    "last_updated": "2025-08-28T07:29:49+00:00"
  },
  {
    "full_name": "wolfgarbe/SymSpell",
    "name": "SymSpell",
    "description": "SymSpell: 1 million times faster spelling correction & fuzzy search through Symmetric Delete spelling correction algorithm",
    "language": "C#",
    "topics": [
      "levenshtein",
      "fuzzy-search",
      "approximate-string-matching",
      "edit-distance",
      "spellcheck",
      "spell-check",
      "levenshtein-distance",
      "damerau-levenshtein",
      "spelling",
      "fuzzy-matching",
      "word-segmentation",
      "chinese-text-segmentation",
      "chinese-word-segmentation",
      "text-segmentation",
      "spelling-correction",
      "symspell"
    ],
    "readme": "SymSpell<br>\n[![NuGet version](https://badge.fury.io/nu/symspell.svg)](https://badge.fury.io/nu/symspell)\n[![MIT License](https://img.shields.io/github/license/wolfgarbe/symspell.svg)](https://github.com/wolfgarbe/SymSpell/blob/master/LICENSE)\n========\n\nSpelling correction & Fuzzy search: **1 million times faster** through Symmetric Delete spelling correction algorithm\n \nThe Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster ([than the standard approach with deletes + transposes + replaces + inserts](http://norvig.com/spell-correct.html)) and language independent.\n\nOpposite to other algorithms only deletes are required, no transposes + replaces + inserts.\nTransposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\nReplaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n\nThe speed comes from the inexpensive **delete-only edit candidate generation** and the **pre-calculation**.<br>\nAn average 5 letter word has about **3 million possible spelling errors** within a maximum edit distance of 3,<br>\nbut SymSpell needs to generate **only 25 deletes** to cover them all, both at pre-calculation and at lookup time. Magic!\n\nIf you like SymSpell, try [**SeekStorm**](https://github.com/SeekStorm/SeekStorm) - a sub-millisecond full-text search library & multi-tenancy server in Rust (Open Source).\n\n<br>\n\n```\nCopyright (c) 2025 Wolf Garbe\nVersion: 6.7.3\nAuthor: Wolf Garbe <wolf.garbe@seekstorm.com>\nMaintainer: Wolf Garbe <wolf.garbe@seekstorm.com>\nURL: https://github.com/wolfgarbe/symspell\nDescription: https://seekstorm.com/blog/1000x-spelling-correction/\n\nMIT License\n\nCopyright (c) 2025 Wolf Garbe\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated \ndocumentation files (the \"Software\"),",
    "url": "https://github.com/wolfgarbe/SymSpell",
    "last_updated": "2025-09-01T05:59:24+00:00"
  },
  {
    "full_name": "dell-research-harvard/AmericanStories",
    "name": "AmericanStories",
    "description": "The official Github for the American Stories dataset as in {link}",
    "language": "Python",
    "topics": [],
    "readme": "# AmericanStories\n \n The American Stories dataset is a collection of full article texts extracted from historical U.S. newspaper images. It includes nearly 20 million scans from the public domain Chronicling America collection maintained by the Library of Congress. The dataset is designed to address the challenges posed by complex layouts and low OCR quality in existing newspaper datasets.\n It was created using a novel deep learning pipeline that incorporates layout detection, legibility classification, custom OCR, and the association of article texts spanning multiple bounding boxes. It employs efficient architectures specifically designed for mobile phones to ensure high scalability.\n The dataset offers high-quality data that can be utilized for various purposes. It can be used to pre-train large language models and improve their understanding of historical English and world knowledge. \n The dataset can also be integrated into retrieval-augmented language models, making historical information more accessible, including interpretations of political events and details about people's ancestors.\n Additionally, the structured article texts in the dataset enable the use of transformer-based methods for applications such as detecting reproduced content. This significantly enhances accuracy compared to relying solely on existing OCR techniques.\n The American Stories dataset serves as an invaluable resource for developing multimodal layout analysis models and other multimodal applications. Its vast size and silver quality make it ideal for innovation and research in this domain.\n \n ## Hugging Face Dataset\nThe dataset is on the [Hugging Face Hub](https://huggingface.co/datasets/dell-research-harvard/AmericanStories). More information about the dataset can be found in the paper and the linked dataset card. \n \n ## Accessing the data\n Ensure that you have installed the datasets library from Hugging Face.  \n \n```\n!pip install datasets\n \n```\n \n There are 4 configurations possibl",
    "url": "https://github.com/dell-research-harvard/AmericanStories",
    "last_updated": "2025-08-25T04:28:38+00:00"
  },
  {
    "full_name": "sckott/fauxpas",
    "name": "fauxpas",
    "description": "fauxpas does http errors",
    "language": "R",
    "topics": [
      "http",
      "error-handling",
      "rstats",
      "r",
      "r-package"
    ],
    "readme": "fauxpas\n=======\n\n\n\n[![R-check](https://github.com/sckott/fauxpas/actions/workflows/R-check.yml/badge.svg?branch=main)](https://github.com/sckott/fauxpas/actions/workflows/R-check.yml)\n[![cran checks](https://badges.cranchecks.info/worst/fauxpas.svg)](https://github.com/sckott/fauxpas)\n[![cran version](http://www.r-pkg.org/badges/version/fauxpas)](https://cran.r-project.org/package=fauxpas)\n[![rstudio mirror downloads](https://cranlogs.r-pkg.org/badges/fauxpas)](https://github.com/r-hub/cranlogs.app)\n\n`fauxpas` does http errors\n\n* HTTP error classes more in line with Ruby/Python/Etc.\n* An error class for each HTTP status in case a user wants to\nbe specific to an HTTP status code, and general purpose handlers\nfor any error\n* Work with any of the major R http clients: `crul`, `curl`, `httr`, (maybe\n`RCurl` later)\n* Provide flexiblity for what to do on an HTTP error, including\ncustom functions and message templates\n\nInfo Links:\n\n* HTTP on wikipedia: <https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol>\n* HTTP2 on wikipedia: <https://en.wikipedia.org/wiki/HTTP/2>\n* HTTP/1.1: <https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html>\n* HTTP/2: <https://datatracker.ietf.org/doc/html/rfc7540>\n\n## Install\n\nCRAN version\n\n\n```r\ninstall.packages(\"fauxpas\")\n```\n\nDev version\n\n\n```r\nremotes::install_github(\"sckott/fauxpas\")\n```\n\n\n```r\nlibrary(\"fauxpas\")\n```\n\n## use with crul\n\n\n```r\nlibrary(\"crul\")\ncli <- HttpClient$new(\"https://httpbin.org/status/414\")\nres <- cli$get()\nhttp(res)\n#> Error: Request-URI Too Long (HTTP 414).\nhttp414(res)\n#> Error: Request-URI Too Long (HTTP 414).\n```\n\n\n```r\nx <- HTTPRequestURITooLong$new()\nx$do_verbose(res)\n#> Error: Request-URI Too Long (HTTP 414).\n#> - The server is refusing to service the request because the Request-URI is\n#>    longer than the server is willing to interpret. This rare condition is only likely\n#>    to occur when a client has improperly converted a POST request to a GET request\n#>    with long query information, when the clien",
    "url": "https://github.com/sckott/fauxpas",
    "last_updated": "2025-03-22T11:20:01+00:00"
  },
  {
    "full_name": "sckott/rforcats",
    "name": "rforcats",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "rforcats\n=======\n\nInspired by [jsforcats.com](http://jsforcats.com/) - __rforcats__ is aimed at teaching cats R programming.\n\n## people\n\n* Scott Chamberlain [@sckott](https://github.com/sckott)\n* Carson Sievert [@cpsievert](https://github.com/cpsievert)\n* Ben Marwick [@benmarwick](https://github.com/benmarwick)\n* Noam Ross [@noamross](https://github.com/noamross)\n* Zach Stednick [@stedy](https://github.com/stedy)\n* Jeffrey Hollister [@jhollist](https://github.com/jhollist)\n* mustafaascha [@mustafaascha](https://github.com/mustafaascha)\n* Peter Desmet [@peterdesmet](https://github.com/peterdesmet)\n* StrayChild [@StrayChild01](https://github.com/StrayChild01)\n* Kevin Cazelles [@KevCaz](https://github.com/KevCaz)\n* You?\n\n## cats\n\n* the orange fur ball at the bottom of the page: _Leo_ (aka _boom boom_, _muffin_, _goose_, _goosie_, _bumpkin_, _goosebump_, _kitten face_, _kitten butt_, _monkey_, _saba_, and _no no_)\n\n## License\n\nCC0\n\n## Meta\n\n* Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.\n",
    "url": "https://github.com/sckott/rforcats",
    "last_updated": "2024-08-23T03:01:14+00:00"
  },
  {
    "full_name": "datameet/india-election-data",
    "name": "india-election-data",
    "description": "To map publicly available datasets related to General Assembly (Lok Sabha) elections in India.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "India Election Data\n===================\n\nTo map publicly available datasets related to General Assembly (Lok Sabha) elections in India.\n\n**License:** Datasets are protected under their respective licenses. Documentation by DataMeet members is shared under [Creative Commons Attribution-ShareAlike 3.0 Unported](http://creativecommons.org/licenses/by-sa/3.0/) license.\n",
    "url": "https://github.com/datameet/india-election-data",
    "last_updated": "2025-07-19T19:26:05+00:00"
  },
  {
    "full_name": "TheWalkers/congress-turk",
    "name": "congress-turk",
    "description": "Gather US Congress legislator district office details via Amazon Mechanical Turk",
    "language": "Python",
    "topics": [],
    "readme": "congress-turk\n=============\n\nA library for gathering Congress district office data through Mechanical Turk.\n\nBackground\n----------\n\nThere is no central data source for information on US legislators' district\noffices. However, this information is useful to citizens and advocacy\norganizations wishing to contact legislators via their local offices.\n\n<https://github.com/TheWalkers/congress-legislators> contains a file \n(legislators-district-offices.yaml) with machine and human readable list of\ndistrict offices, released into the public domain from a manually compiled \nsource.\n\nHowever, maintaining this data source is a challenge. There are \n535 Representatives and Senators (in addition to non-voting delegates),\neach of which has at least one and perhaps as many as ten district offices.\nIn 2016, this amounted to nearly 1500 offices, each with address, phone and \nfax numbers, contact hours, and other information.\n\nThese data are generally found on legislators' websites, but the websites \nthemselves are not in any standard format, making machine-based data scraping\ndifficult.\n\nThis project contains scripts and pages to facilitate use of Amazon \nMechanical Turk to have human workers identify and gather district office data.\n\nOverview\n--------\n\nAt present the process is largely manual, and relies on an operator to \ndrive various pieces forward and reconcile results of Turk tasks.\n\nThere are two types of Human Intelligence Tasks (HITs) for which we rely on \nMechanical Turk:\n\n- List Offices: enter a list of district office names for a legislator\n- Office Details: for a given district office, enter the contact details\n\nThe general process is:\n\n- set up list and detail tasks on <http://requester.mturk.com/>\n- generate and upload list tasks\n- reconcile the results of list tasks\n- split list tasks into detail tasks and upload \n- reconcile results of detail tasks\n- patch or replace legislator-district-offices.yaml with results\n\nScripts provided in this repository assist with these ",
    "url": "https://github.com/TheWalkers/congress-turk",
    "last_updated": "2024-03-15T19:31:25+00:00"
  },
  {
    "full_name": "Robinlovelace/R-for-Big-Data",
    "name": "R-for-Big-Data",
    "description": "Teaching materials for handling large datasets in R",
    "language": "TeX",
    "topics": [],
    "readme": "# R-for-Big-Data\n\nTeaching materials for handling large datasets in R. Now superceded by [Efficient R Programming](https://github.com/csgillespie/efficientR) but archived for posterity.\n",
    "url": "https://github.com/Robinlovelace/R-for-Big-Data",
    "last_updated": "2024-03-08T05:49:02+00:00"
  },
  {
    "full_name": "bmschmidt/jobs_sankey",
    "name": "jobs_sankey",
    "description": "Code to make http://benschmidt.org/jobs",
    "language": "JavaScript",
    "topics": [],
    "readme": "",
    "url": "https://github.com/bmschmidt/jobs_sankey",
    "last_updated": "2022-01-05T08:58:17+00:00"
  },
  {
    "full_name": "MicheleNuijten/statcheck",
    "name": "statcheck",
    "description": "A spellchecker for statistics",
    "language": "R",
    "topics": [
      "statistics",
      "cran",
      "nhst",
      "p-values",
      "r",
      "reproducibility"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n<!-- after editing README.Rmd, run devtools::build_readme() -->\n\n# statcheck <a href='http://statcheck.io'><img src='man/figures/logo.jpg' align=\"right\" height=\"100\" /></a>\n\n<!-- badges: start -->\n\n[![](https://www.r-pkg.org/badges/version/statcheck?color=green)](https://cran.r-project.org/package=statcheck)\n[![](http://cranlogs.r-pkg.org/badges/grand-total/statcheck?color=blue)](https://cran.r-project.org/package=statcheck)\n[![](https://img.shields.io/badge/devel%20version-1.6.0-yellow.svg)](https://github.com/MicheleNuijten/statcheck)\n<!-- badges: end -->\n\n## What is statcheck?\n\n`statcheck` is a “spellchecker” for statistics. It checks whether your\n*p*-values match their accompanying test statistic and degrees of\nfreedom.\n\n`statcheck` searches for null-hypothesis significance test (NHST) in APA\nstyle (e.g., *t*(28) = 2.2, *p* \\< .05). It recalculates the p-value\nusing the reported test statistic and degrees of freedom. If the\nreported and computed p-values don’t match, `statcheck` will flag the\nresult as an error.\n\n![](man/figures/infograph.png)\n\n## What can I use statcheck for?\n\n`statcheck` is mainly useful for:\n\n1.  **Self-checks**: you can use `statcheck` to make sure your\n    manuscript doesn’t contain copy-paste errors or other\n    inconsistencies before you submit it to a journal.\n2.  **Peer review**: editors and reviewers can use `statcheck` to check\n    submitted manuscripts for statistical inconsistencies. They can ask\n    authors for a correction or clarification before publishing a\n    manuscript.\n3.  **Research**: `statcheck` can be used to automatically extract\n    statistical test results from articles that can then be analyzed.\n    You can for instance investigate whether you can predict statistical\n    inconsistencies (see e.g., Nuijten et al., 2017\n    <doi:10.1525/collabra.102>), or use it to analyze p-value\n    distributions (see e.g., Hartgerink et al., 2016\n    <doi:10.771",
    "url": "https://github.com/MicheleNuijten/statcheck",
    "last_updated": "2025-08-27T18:47:58+00:00"
  },
  {
    "full_name": "lrberge/fixest",
    "name": "fixest",
    "description": "Fixed-effects estimations",
    "language": "R",
    "topics": [],
    "readme": "\r\n# fixest: Fast and user-friendly fixed-effects estimation\r\n\r\n<a href=\"https://cran.r-project.org/web/checks/check_results_fixest.html\"><img src=\"https://badges.cranchecks.info/worst/fixest.svg\" alt=\"CRAN status\"></a>\r\n<a href=\"https://fastverse.r-universe.dev\"><img src=\"https://fastverse.r-universe.dev/badges/fixest\" alt=\"Version_ROpensci\"></a>\r\n<a href=\"https://CRAN.R-project.org/package=fixest\"><img src=\"https://www.r-pkg.org/badges/version/fixest\" alt=\"Version\"> </a>\r\n<a href=\"https://ipub.com/dev-corner/apps/r-package-downloads/\"> <img src=\"https://cranlogs.r-pkg.org/badges/fixest\" alt = \"Downloads\"> </a>\r\n\r\nThe `fixest` package offers a family of functions to perform estimations with multiple fixed-effects in both an OLS and a GLM context. Please refer to the [introduction](https://CRAN.R-project.org/package=fixest/vignettes/fixest_walkthrough.html) for a walk-through.\r\n\r\nAt the time of writing of this page (February 2020), `fixest` is the fastest existing method to perform fixed-effects estimations, often by orders of magnitude. See below for a benchmarking with the fastest alternative software. \r\n\r\n```R\r\n# To install from CRAN:\r\ninstall.packages(\"fixest\")\r\n\r\n# To install the latest stable development release:\r\ninstall.packages(\"fixest\", \r\n                 repos = c(ropensci = 'https://fastverse.r-universe.dev',\r\n                           CRAN = 'https://cloud.r-project.org'))\r\n```\r\n\r\n## Benchmarking\r\n\r\nHere is a comparison of the performance of `fixest` functions to other state of the art methods to perform estimations with multiple fixed-effects. The results are reported in the five figures below. Package `fixest` (black lines) is consistently faster in all situations.\r\n\r\n![](https://github.com/lrberge/fixest/blob/master/vignettes/images/benchmark_gaussian.png?raw=true)\r\n\r\n![](https://github.com/lrberge/fixest/blob/master/vignettes/images/benchmark_difficult.png?raw=true)\r\n\r\n![](https://github.com/lrberge/fixest/blob/master/vignettes/images/benchmark_pois",
    "url": "https://github.com/lrberge/fixest",
    "last_updated": "2025-09-01T21:34:58+00:00"
  },
  {
    "full_name": "virtualstaticvoid/heroku-buildpack-r",
    "name": "heroku-buildpack-r",
    "description": "Heroku buildpack for R - Makes deploying R on Heroku easy",
    "language": "R",
    "topics": [
      "heroku-buildpack",
      "r",
      "shiny",
      "multiple-buildpacks",
      "heroku",
      "plumber-applications",
      "renv",
      "packrat",
      "shiny-applications",
      "plumber"
    ],
    "readme": "# Heroku Buildpack: R\n\n[![CI](https://github.com/virtualstaticvoid/heroku-buildpack-r-build2/actions/workflows/main.yml/badge.svg)](https://github.com/virtualstaticvoid/heroku-buildpack-r-build2/actions/workflows/main.yml)\n\nThis is a [Heroku Buildpack][buildpacks] for applications which use [R][rproject] for\nstatistical computing and [CRAN][cran] for R packages.\n\nThe buildpack supports the [heroku-20][stack20][^20support], [heroku-22][stack22][^22support]\nand [heroku-24][stack24][^24support] stacks.\n\nIt also includes support for the [Packrat][packrat] and [renv][renv] package managers, and\nthe [Shiny][shiny] and [Plumber][plumber] web application frameworks.\n\n### Heroku Stack and R Versions\n\nThe following table lists the available combinations of Heroku Stack and R version.\nThey are [built][build2] periodically as and when the [Debian R packages][cloud-r-project]\nare available.\n\n| R / Stack | `20`[^20support] | `22`[^22support] | `24`[^24support] |\n|:---------:|:---:|:---:|:---:|\n| `4.0.0`   | □ |   |   |\n| `4.0.5`   | □ |   |   |\n| `4.1.2`   | □ | □ |   |\n| `4.1.3`   | □ |   |   |\n| `4.2.0`   |   | □ |   |\n| `4.2.1`   | ■ | ■ |   |\n| `4.4.2`   |   |   | ■ |\n\nLegend:\n\n* `■` = default version for given stack\n* `□` = available\n* empty = no package available\n\nThe default R version can be overridden by setting the `R_VERSION` environment variable.\n\n```bash\nheroku config:set R_VERSION=4.0.0\n```\n\n## Usage\n\nThe buildpack's name is [`vsv/heroku-buildpack-r`][bpurl]. Provide it when creating your\napplication on Heroku as follows:\n\n```bash\nheroku create --buildpack vsv/heroku-buildpack-r\n```\n\nYou can add it to an existing application using the `buildpacks:add` command, as follows:\n\n```bash\nheroku buildpacks:add vsv/heroku-buildpack-r\n```\n\nAlternatively, you can use the Git URL of this repository, together with the branch name.\n\n```\nhttps://github.com/virtualstaticvoid/heroku-buildpack-r.git#main\n```\n\nThe buildpack will detect your application makes use of R if it has one (or ",
    "url": "https://github.com/virtualstaticvoid/heroku-buildpack-r",
    "last_updated": "2025-06-24T04:28:22+00:00"
  },
  {
    "full_name": "notnews/top_news",
    "name": "top_news",
    "description": "Collecting URLs Daily From News Feeds of Major National News Sites 2022--",
    "language": "Jupyter Notebook",
    "topics": [
      "news",
      "abc",
      "cbs",
      "newspaper3k",
      "rss-feed",
      "la-times",
      "nbc",
      "nyt",
      "usa-today",
      "propublica",
      "npr",
      "politico",
      "wapo"
    ],
    "readme": "## Top News! URLs from News Feeds of Major National News Sites (2022-)\n\nWe automatically pull daily news data from major national news sites: ABC,  CBS, CNN, LA Times, NBC, NPR, NYT, Politico, ProPublica, USA Today, and WaPo using [Github Workflows](https://github.com/notnews/top_news/tree/main/.github/workflows). For the latest version, please take a look at the respective JSON files.\n\nAs of March 2025, we have about 700k unique URLs.\n\n### Other Scripts + Data\n\n1. The script for [aggregating the URLs](https://github.com/notnews/top_news/blob/main/agg/concat_json.py) and [March-2025 dump of URLs (.zip)](https://github.com/notnews/top_news/blob/main/agg/agg_urls.json.zip)\n   \n2. The script for downloading the article text and parsing some features using [newspaper3k](https://newspaper.readthedocs.io/en/latest/), e.g., publication date, authors, etc. and putting it in a DB is [here](https://github.com/notnews/top_news/blob/main/agg/create_db.py). The script checks the local DB before incrementally processing new data.\n  * The June 2023 full-text dump is here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZNAKK6\n  * The March 2025 dump (minus the exceptions listed below) is in the same place.\n\n3. Newspaper3k can't parse USAT, Politico, and ABC URLs. I use custom Google search to dig up the URLs and get the data. The script is [here](https://github.com/notnews/top_news/blob/main/agg/usat_downloader.py). \n\n### Get Started With Exploring the Data\n\nTo explore the DB, some code ([Jupyter NB](https://github.com/notnews/top_news/blob/main/agg/tester.ipynb)) ...\n\n```python\n\nfrom sqlite_utils import Database\nfrom itertools import islice\n\ndb = Database(\"../cbs.db\")\nprint(\"Tables:\", db.table_names())\n```\n\n```\nTables: ['../cbs_stories']\n```\n\n#### Table Schema\n\n```python\nschema = db[table_name].schema\nprint(\"Schema:\\n\")\nprint(schema)\n```\n\n```\nSchema:\n\nCREATE TABLE [../cbs_stories] (\n   [url] TEXT PRIMARY KEY,\n   [source] TEXT,\n   [publish_date] TEXT,\n   [",
    "url": "https://github.com/notnews/top_news",
    "last_updated": "2025-09-02T09:32:53+00:00"
  },
  {
    "full_name": "Kanaries/pygwalker",
    "name": "pygwalker",
    "description": "PyGWalker: Turn your dataframe into an interactive UI for visual analysis",
    "language": "Python",
    "topics": [
      "data-analysis",
      "pandas",
      "tableau",
      "tableau-alternative",
      "visualization",
      "data-exploration",
      "dataframe",
      "matplotlib",
      "plotly"
    ],
    "readme": "[English](README.md) | [Español](./docs/README.es.md) | [Français](./docs/README.fr.md) | [Deutsch](./docs/README.de.md) | [中文](./docs/README.zh.md) | [Türkçe](./docs/README.tr.md) | [日本語](./docs/README.ja.md) | [한국어](./docs/README.ko.md) | [Русский](./docs/README.ru.md)\n\n\n<p align=\"center\"><a href=\"https://github.com/Kanaries/pygwalker\"><img width=100% alt=\"\" src=\"https://github.com/user-attachments/assets/f90db669-6e5a-45d3-942e-547c9d0471c9\" /></a></p>\n\n<h2 align=\"center\">PyGWalker: A Python Library for Exploratory Data Analysis with Visualization</h2>\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2406.11637\">\n      <img src=\"https://img.shields.io/badge/arXiv-2406.11637-b31b1b.svg\" height=\"18\" align=\"center\">\n    </a>\n    <a href=\"https://badge.fury.io/py/pygwalker\">\n        <img src=\"https://badge.fury.io/py/pygwalker.svg\" alt=\"PyPI version\" height=\"18\" align=\"center\" />\n    </a>\n    <a href=\"https://mybinder.org/v2/gh/Kanaries/pygwalker/main\">\n      <img src=\"https://mybinder.org/badge_logo.svg\" alt=\"binder\" height=\"18\" align=\"center\" />\n    </a>\n    <a href=\"https://pypi.org/project/pygwalker\">\n      <img src=\"https://img.shields.io/pypi/dm/pygwalker\" alt=\"PyPI downloads\" height=\"18\" align=\"center\" />\n    </a>\n    <a href=\"https://anaconda.org/conda-forge/pygwalker\"> <img src=\"https://anaconda.org/conda-forge/pygwalker/badges/version.svg\" alt=\"conda-forge\" height=\"18\" align=\"center\" /> </a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://discord.gg/Z4ngFWXz2U\">\n      <img alt=\"discord invitation link\" src=\"https://dcbadge.vercel.app/api/server/Z4ngFWXz2U?style=flat\" align=\"center\" />\n    </a>\n    <a href='https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fpublish.twitter.com%2F&ref_src=twsrc%5Etfw&screen_name=kanaries_data&tw_p=followbutton'>\n        <img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/kanaries_data?style=social\" alt='Twitter' align=\"center\" />\n    </a>\n    <a href=\"https://kanaries-community.slack.com/joi",
    "url": "https://github.com/Kanaries/pygwalker",
    "last_updated": "2025-09-01T20:40:42+00:00"
  },
  {
    "full_name": "ropensci/tabulapdf",
    "name": "tabulapdf",
    "description": "Bindings for Tabula PDF Table Extractor Library",
    "language": "R",
    "topics": [
      "tabula",
      "tabular-data",
      "pdf",
      "java",
      "pdf-document",
      "r",
      "r-package",
      "ropensci",
      "rstats",
      "peer-reviewed"
    ],
    "readme": "\n# tabulapdf: Extract tables from PDF documents <img src=\"man/figures/logo.svg\" align=\"right\" height=\"139\" alt=\"\" />\n\n[![R-CMD-check](https://github.com/ropensci/tabulapdf/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ropensci/tabulapdf/actions/workflows/R-CMD-check.yaml)\n[![](https://badges.ropensci.org/42_status.svg)](https://github.com/ropensci/software-review/issues/42)\n[![BuyMeACoffee](https://raw.githubusercontent.com/pachadotdev/buymeacoffee-badges/main/bmc-donate-yellow.svg)](https://buymeacoffee.com/pacha)\n\n**tabulapdf** provides R bindings to the [Tabula java\nlibrary](https://github.com/tabulapdf/tabula-java/), which can be used\nto computationaly extract tables from PDF documents.\n\nNote: tabulapdf is released under the MIT license, as is Tabula itself.\n\n## Installation\n\ntabulapdf depends on [rJava](https://cran.r-project.org/package=rJava),\nwhich implies a system requirement for Java. This can be frustrating,\nespecially on Windows. The preferred Windows workflow is to use\n[Chocolatey](https://chocolatey.org/) to obtain, configure, and update\nJava. You need do this before installing rJava or attempting to use\ntabulapdf. More on [this](#installing-java-on-windows-with-chocolatey)\nand [troubleshooting](#troubleshooting) below.\n\ntabulapdf is available on CRAN, and it can also be installed from\nrOpenSci’s R-Universe:\n\n``` r\n# either\ninstall.packages(\"tabulapdf\")\n\n# or\ninstall.packages(\"tabulapdf\", repos = c(\"https://ropensci.r-universe.dev\", \"https://cloud.r-project.org\"))\n```\n\nTo install the latest development version:\n\n``` r\nif (!require(remotes)) install.packages(\"remotes\")\n\n# on 64-bit Windows\nremotes::install_github(c(\"ropensci/tabulapdf\"), INSTALL_opts = \"--no-multiarch\")\n\n# elsewhere\nremotes::install_github(c(\"ropensci/tabulapdf\"))\n```\n\n## Code Examples\n\nThe main function, `extract_tables()` provides an R clone of the Tabula\ncommand line application:\n\n``` r\nlibrary(tabulapdf)\nf <- system.file(\"examples\", \"data.pdf\", package = \"tabulap",
    "url": "https://github.com/ropensci/tabulapdf",
    "last_updated": "2025-08-29T14:50:18+00:00"
  },
  {
    "full_name": "worldbank/DIME-Resources",
    "name": "DIME-Resources",
    "description": "Repo for all the DIME Analytics/DIME resources like trainings and all.",
    "language": "",
    "topics": [],
    "readme": "# DIME Resources\n\nThis is a deprecated repository for DIME Analytics/DIME resources. It is kept not break links shared in the past. \n\nFor up-to-date content, please see: \n- https://osf.io/wzjtk/ for all training material\n- https://www.worldbank.org/en/research/dime/data-and-analytics for public good resources and event\n",
    "url": "https://github.com/worldbank/DIME-Resources",
    "last_updated": "2025-08-25T16:46:42+00:00"
  },
  {
    "full_name": "EmilHvitfeldt/paletteer",
    "name": "paletteer",
    "description": "🎨🎨🎨 Collection of most color palettes in a single R package",
    "language": "R",
    "topics": [
      "rstats",
      "r",
      "color-palette",
      "palettes"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# paletteer <img src='man/figures/logo.png' style=\"float:right\" height=\"139\" />\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/emilhvitfeldt/paletteer/workflows/R-CMD-check/badge.svg)](https://github.com/emilhvitfeldt/paletteer/actions)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/paletteer)](https://cran.r-project.org/package=paletteer)\n[![CRAN_Download_Badge](http://cranlogs.r-pkg.org/badges/paletteer)](https://CRAN.R-project.org/package=paletteer)\n[![Codecov test\ncoverage](https://codecov.io/gh/emilhvitfeldt/paletteer/branch/main/graph/badge.svg)](https://app.codecov.io/gh/emilhvitfeldt/paletteer?branch=main)\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n<!-- badges: end -->\n\nThe goal of **paletteer** is to be a comprehensive collection of color\npalettes in R using a common interface. Think of it as the “caret of\npalettes”.\n\n**Notice** This version is not backwards compatible with versions \\<=\n0.2.1. Please refer to the end of the readme for breaking changes\n\n## Installation\n\nYou can install the released version of **paletteer** from\n[CRAN](https://CRAN.R-project.org) with:\n\n``` r\ninstall.packages(\"paletteer\")\n```\n\nIf you want the development version instead then install directly from\nGitHub:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"EmilHvitfeldt/paletteer\")\n```\n\n## Palettes\n\nThe palettes are divided into 2 groups; *discrete* and *continuous*. For\ndiscrete palette you have the choice between the *fixed width palettes*\nand *dynamic palettes*. Most common of the two are the fixed width\npalettes which have a set amount of colors which doesn’t change when the\nnumber of colors requested vary like the following palettes:\n\n![](man/figures/README-unnamed-chunk-2-1.png)<!-- -->\n\non the other hand we have the dynamic palettes where the colors of the\npalette depend on th",
    "url": "https://github.com/EmilHvitfeldt/paletteer",
    "last_updated": "2025-09-02T03:51:29+00:00"
  },
  {
    "full_name": "justingrimmer/SOC",
    "name": "SOC",
    "description": "This is the repository for the Social Inquiry Course",
    "language": "TeX",
    "topics": [],
    "readme": "# SOC\n",
    "url": "https://github.com/justingrimmer/SOC",
    "last_updated": "2024-02-21T14:13:00+00:00"
  },
  {
    "full_name": "johndharrison/seleniumPipes",
    "name": "seleniumPipes",
    "description": "An R client implementing w3c webdriver",
    "language": "R",
    "topics": [],
    "readme": "seleniumPipes\n==========================\n| CRAN version       | Travis build status    | SauceTests  | Coverage |\n| :-------------: |:-------------:|:-------------:|:-------------:|\n| [![](http://www.r-pkg.org/badges/version/seleniumPipes)](https://CRAN.R-project.org/package=seleniumPipes) | [![Build Status](https://travis-ci.org/johndharrison/seleniumPipes.svg?branch=master)](https://travis-ci.org/johndharrison/seleniumPipes) | [![Sauce Test Status](https://saucelabs.com/buildstatus/seleniumPipes)](https://saucelabs.com/u/seleniumPipes) | [![codecov](https://codecov.io/gh/johndharrison/seleniumPipes/branch/master/graph/badge.svg)](https://codecov.io/gh/johndharrison/seleniumPipes)|\n\n\n \n\n#### Selenium test status\n\n\n[![Sauce Test Status](https://saucelabs.com/browser-matrix/seleniumPipes.svg)](https://saucelabs.com/u/seleniumPipes)\n\n##### *A lightweight implementation of w3c webdriver specification*\n\n### Introduction\n\nseleniumPipes is a lightweight implementation of the [w3c webdriver specification](https://w3c.github.io/webdriver/webdriver-spec.html).\nIt has been built utilising `xml2`, `httr` and `magrittr` so provides an alternative for users who are familiar with piping.\n\n### Install\n\nTo install seleniumPipes from CRAN\n\n```\ninstall.packages(\"seleniumPipes\")\n```\n\n\nTo install the current developement version of seleniumPipes run:\n\n```\ndevtools::install_github(\"johndharrison/seleniumPipes\")\n```\n\n### Getting started\n\nThe easiest way to start is to look at the Basic operations vignette:\n\n* [seleniumpipes: Basic Operation](http://rpubs.com/johndharrison/seleniumPipes-basic)\n\nFailing this a few basic examples are presented below:\n\nGet started using `seleniumPipes` you can look at the following example\n\n```\nlibrary(seleniumPipes)\nlibrary(RSelenium) # start a server with utility function\nselServ <- RSelenium::startServer()\nremDr <- remoteDr()\nremDr %>% go(url = \"http://www.google.com\")\nremDr %>% go(url = \"http://www.bbc.com\")\nremDr %>% back()\nremDr %>% forward()\nremDr %>%",
    "url": "https://github.com/johndharrison/seleniumPipes",
    "last_updated": "2025-03-22T11:21:30+00:00"
  },
  {
    "full_name": "Mauricio8583/Pacotes_processamento_python",
    "name": "Pacotes_processamento_python",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# image_processing\n\nDescription:\n    The package image_processing is used to:\n        Processing:\n            - Histogram matching\n            - Structural similarity\n            - Resize image\n        Utils:\n            - Read image\n            - Save image\n            - Plot image\n            - Plot result\n            - Plot histogram\n\n## Instalation\n Use the package manager [pip] to install package_name\n\n pip install package_name\n\n## Usage\n python\n  from package_name.module1_name import file1_name\n  file1_name.my_function()\n\n## Author\nMauricio Oliveira\n",
    "url": "https://github.com/Mauricio8583/Pacotes_processamento_python",
    "last_updated": "2025-01-15T15:28:50+00:00"
  },
  {
    "full_name": "keras-team/keras",
    "name": "keras",
    "description": "Deep Learning for humans",
    "language": "Python",
    "topics": [
      "deep-learning",
      "tensorflow",
      "neural-networks",
      "machine-learning",
      "data-science",
      "python",
      "jax",
      "pytorch"
    ],
    "readme": "# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and macOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will ",
    "url": "https://github.com/keras-team/keras",
    "last_updated": "2025-09-01T21:29:51+00:00"
  },
  {
    "full_name": "soodoku/soodoku",
    "name": "soodoku",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "[![Website](https://img.shields.io/badge/Website-gsood.com-4063D8?style=flat)](https://gsood.com)\n[![Blog](https://img.shields.io/badge/Blog-gojiberries.io-7B2D26?style=flat)](https://gojiberries.io)\n[![Stats](https://img.shields.io/badge/Stats-GitHub-3A3F58?style=flat&logo=github)](https://github.com/gojiplus/allstar/blob/main/stats.md)\n\n[![Econometrics and Stats.](https://github.com/finite-sample.png?size=15)](https://github.com/finite-sample) &nbsp;[Econometrics and Stats.](https://github.com/finite-sample): Advanced Models for Calibration, Less Greedy Methods for Optimization, Pareto-Improving ML, Methods for Increasing 'Stability,' etc.\n\n[![Metascience](https://github.com/recite.png?size=15)](https://github.com/recite) &nbsp;[Metascience](https://github.com/recite): Tools to flag retracted articles in citations, research on how often retracted articles or articles with major errors are approvingly cited, tools for counting how often software is used based on replication files\n\n[![South Asia](https://github.com/in-rolls.png?size=15)](https://github.com/in-rolls/) &nbsp;[Data, Research, and Tools Focused on South Asia](https://github.com/in-rolls/): Parsed Electoral Rolls (~1B people), Local Election datasets with reservation status, Land records data, Ration data, Hindi-English transliteration tools, Effect of quotas in local elections, MNREGA\n\n[![Online Safety](https://github.com/themains.png?size=15)](https://github.com/themains) &nbsp;[Online Safety](https://github.com/themains):  Domain content classification tools, Generative password models using real-world data, Research on data breach patterns (including politicians) and privacy\n\n[![Geosensing](https://github.com/geosensing.png?size=15)](https://github.com/geosensing) &nbsp;[Geographically Distributed Data Collection](https://github.com/geosensing): Tools for randomly sampling locations on streets, Heuristic route planning, Google Street View for assessing the quality of public infrastructure and demogra",
    "url": "https://github.com/soodoku/soodoku",
    "last_updated": "2025-07-09T02:42:35+00:00"
  },
  {
    "full_name": "r-lib/keyring",
    "name": "keyring",
    "description": ":closed_lock_with_key: Access the system credential store from R",
    "language": "C",
    "topics": [
      "r",
      "keyring",
      "security"
    ],
    "readme": "\n# keyring\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/r-lib/keyring/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/keyring/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/keyring/graph/badge.svg)](https://app.codecov.io/gh/r-lib/keyring)\n[![](https://www.r-pkg.org/badges/version/keyring)](https://www.r-pkg.org/pkg/keyring)\n[![CRAN RStudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/keyring)](https://www.r-pkg.org/pkg/keyring)\n<!-- badges: end -->\n\nkeyring provides a way to securely manage secrets using your operating\nsystem’s credential store. Once a secret is defined, it persists in a\n“keyring” across multiple R sessions. keyring is an alternative to using\nenvironment variables that’s a bit more secure because your secret is\nnever stored in plain text, meaning that you can for instance never\naccidentally upload it to GitHub. For more security, you can also store\nsecrets in a custom keyring that always requires a password to unlock.\n\nkeyring currently supports:\n\n- The macOS Keychain (`backend_macos`).\n- The Windows Credential Store (`backend_wincred`).\n- The Linux Secret Service API (`backend_secret_service`).\n\nIt also provides two backends that are available on all platforms:\n\n- Encrypted files (`backend_file`)\n- Environment variables (`backend_env`).\n\n## Installation\n\nInstall the package from CRAN:\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"keyring\")\n```\n\nWe recommend using pak to install keyring as it will ensure that Linux\nsystem requirements are automatically installed (for instance Ubuntu\nrequires `libsecret-1-dev` and `libssl-dev`).\n\nTo install the development version from GitHub, use:\n\n``` r\npak::pak(\"r-lib/keyring\")\n```\n\n## Usage\n\nThe simplest usage only requires `key_set()` and `key_get()`:\n\n``` r\n# Interactively save a secret. This avoids typing the value of the secret\n# into the console as this could be recorded in your `.Rhistory`\nkey_set(\"secret-name\")\n\n# Later ",
    "url": "https://github.com/r-lib/keyring",
    "last_updated": "2025-08-19T05:57:55+00:00"
  },
  {
    "full_name": "jina-ai/clip-as-service",
    "name": "clip-as-service",
    "description": "🏄 Scalable embedding, reasoning, ranking for images and sentences with CLIP",
    "language": "Python",
    "topics": [
      "bert",
      "sentence-encoding",
      "deep-learning",
      "clip-model",
      "clip-as-service",
      "bert-as-service",
      "cross-modal-retrieval",
      "multi-modality",
      "neural-search",
      "openai",
      "pytorch",
      "onnx",
      "cross-modality",
      "image2vec",
      "sentence2vec"
    ],
    "readme": "<p align=\"center\">\n<a href=\"https://clip-as-service.jina.ai\"><img src=\"https://github.com/jina-ai/clip-as-service/blob/main/docs/_static/logo-light.svg?raw=true\" alt=\"CLIP-as-service logo: The data structure for unstructured data\" width=\"200px\"></a>\n<br><br><br>\n</p>\n\n\n<p align=center>\n<a href=\"https://pypi.org/project/clip_server/\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/clip_server?label=Release&style=flat-square\"></a>\n<a href=\"https://discord.jina.ai\"><img src=\"https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square\"></a>\n<a href=\"https://codecov.io/gh/jina-ai/clip-as-service\"><img alt=\"Codecov branch\" src=\"https://img.shields.io/codecov/c/github/jina-ai/clip-as-service/main?logo=Codecov&logoColor=white&style=flat-square\"></a>\n<a href=\"https://colab.research.google.com/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb\"><img src=\"https://img.shields.io/badge/Host-on%20Google%20Colab%20(GPU/TPU)-brightgreen?style=flat-square&logo=googlecolab&&logoColor=white\" alt=\"Host on Google Colab with GPU/TPU support\"></a>\n</p>\n\n<!-- start elevator-pitch -->\n\nCLIP-as-service is a low-latency high-scalability service for embedding images and text. It can be easily integrated as a microservice into neural search solutions.\n\n⚡ **Fast**: Serve CLIP models with TensorRT, ONNX runtime and PyTorch w/o JIT with 800QPS<sup>[*]</sup>. Non-blocking duplex streaming on requests and responses, designed for large data and long-running tasks. \n\n🫐 **Elastic**: Horizontally scale up and down multiple CLIP models on single GPU, with automatic load balancing.\n\n🐥 **Easy-to-use**: No learning curve, minimalist design on client and server. Intuitive and consistent API for image and sentence embedding. \n\n👒 **Modern**: Async client support. Easily switch between gRPC, HTTP, WebSocket protocols with TLS and compression.\n\n🍱 **Integration**: Smooth integration with neural search ecosystem including [Jina](https://github.com/jina",
    "url": "https://github.com/jina-ai/clip-as-service",
    "last_updated": "2025-09-02T07:54:44+00:00"
  },
  {
    "full_name": "google-research-datasets/dakshina",
    "name": "dakshina",
    "description": "The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. For each language, the dataset includes a large collection of native script Wikipedia text, a romanization lexicon of words in the native script with attested romanizations, and some full sentence parallel data in both a native script of the language and the basic Latin alphabet.",
    "language": "",
    "topics": [],
    "readme": "# Dakshina Dataset\n\nThe Dakshina dataset is a collection of text in both Latin and native scripts\nfor 12 South Asian languages. For each language, the dataset includes a large\ncollection of native script Wikipedia text, a romanization lexicon which\nconsists of words in the native script with attested romanizations, and some\nfull sentence parallel data in both a native script of the language and the\nbasic Latin alphabet.\n\nDataset URL:\n[https://github.com/google-research-datasets/dakshina](https://github.com/google-research-datasets/dakshina)\n\nIf you use or discuss this dataset in your work, please cite our paper (bibtex\ncitation below).  A PDF link for the paper can be found at\n[https://www.aclweb.org/anthology/2020.lrec-1.294](https://www.aclweb.org/anthology/2020.lrec-1.294).\n\n```\n@inproceedings{roark-etal-2020-processing,\n    title = \"Processing {South} {Asian} Languages Written in the {Latin} Script:\n    the {Dakshina} Dataset\",\n    author = \"Roark, Brian and\n      Wolf-Sonkin, Lawrence and\n      Kirov, Christo and\n      Mielke, Sabrina J. and\n      Johny, Cibu and\n      Demir{\\c{s}}ahin, I{\\c{s}}in and\n      Hall, Keith\",\n    booktitle = \"Proceedings of The 12th Language Resources and Evaluation Conference (LREC)\",\n    year = \"2020\",\n    url = \"https://www.aclweb.org/anthology/2020.lrec-1.294\",\n    pages = \"2413--2423\"\n}\n```\n\n## Data links ##\n\nFile | Download | Version | Date | Notes\n---- | :------: | :-------: | :--------: | :------\n**dakshina_dataset_v1.0.tar** | [link](https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar) | 1.0 | 05/27/2020 | Initial data release\n\n\n## Data Organization\n\nThere are 12 languages represented in the dataset: Bangla (`bn`), Gujarati\n(`gu`), Hindi (`hi`), Kannada (`kn`), Malayalam (`ml`), Marathi (`mr`), Punjabi\n(`pa`), Sindhi (`sd`), Sinhala (`si`), Tamil (`ta`), Telugu (`te`) and Urdu\n(`ur`).\n\nAll data is derived from Wikipedia text. Each language has its own\ndirectory, in which there are three subdirectorie",
    "url": "https://github.com/google-research-datasets/dakshina",
    "last_updated": "2025-08-19T10:47:37+00:00"
  },
  {
    "full_name": "cpsievert/plotly_book",
    "name": "plotly_book",
    "description": "plotly for R book",
    "language": "HTML",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/cpsievert/plotly_book.png?branch=master)](https://travis-ci.org/cpsievert/plotly_book)\n\n# plotly for R book\n\nView it here -- <http://cpsievert.github.io/plotly_book>\n\n## Build the book\n\n```\n$ Rscript -e \"devtools::install_github('cpsievert/plotly_book')\"\n$ make\n```\n",
    "url": "https://github.com/cpsievert/plotly_book",
    "last_updated": "2025-08-07T05:29:03+00:00"
  },
  {
    "full_name": "rocker-org/rocker-versioned",
    "name": "rocker-versioned",
    "description": "Run current & prior versions of R using docker",
    "language": "Dockerfile",
    "topics": [],
    "readme": "[![license](https://img.shields.io/badge/license-GPLv2-blue.svg)](https://opensource.org/licenses/GPL-2.0)\n[![Project Status: Moved to https://github.com/rocker-org/rocker-versioned2 – The project has been moved to a new location, and the version at that location should be considered authoritative.](https://www.repostatus.org/badges/latest/moved.svg)](https://github.com/rocker-org/rocker-versioned2)\n[![DOI](https://zenodo.org/badge/25048007.svg)](https://zenodo.org/badge/latestdoi/25048007)\n\n\nVisit [rocker-project.org](https://rocker-project.org) for more about available Rocker images, configuration, and use. \n\n# MOVED to [`rocker-versioned2`](https://github.com/rocker-org/rocker-versioned2)\n\nThis repository contains archived build scripts from the 3.x versioned series only.  Visit  [`rocker-versioned2`](https://github.com/rocker-org/rocker-versioned2) for current build scripts for 4.x versioned images.\n\n\n## Version-stable Rocker images for R 3.x\n\n![rocker](https://avatars0.githubusercontent.com/u/9100160?v=3&s=200)\n\n***For documentation for R >= 4.0.0, for images `r-ver`, `rstudio`, `tidyverse`, `verse`, `geospatial`, `shiny`, and `binder`, please see the [`rocker-versioned2` repository](https://github.com/rocker-org/rocker-versioned2).*** \n\n\nimage            | description                               | size   | metrics | build status \n---------------- | ----------------------------------------- | ------ | ------- | --------------\n[r-ver](https://hub.docker.com/r/rocker/r-ver)            |  Version-stable base R & src build tools  | [![](https://images.microbadger.com/badges/image/rocker/r-ver.svg)](https://microbadger.com/images/rocker/r-ver) | [![](https://img.shields.io/docker/pulls/rocker/r-ver.svg)](https://hub.docker.com/r/rocker/r-ver) |  [![](https://img.shields.io/docker/automated/rocker/r-ver.svg)](https://hub.docker.com/r/rocker/r-ver/builds)\n[rstudio](https://hub.docker.com/r/rocker/rstudio)          |  Adds rstudio                             | [![](h",
    "url": "https://github.com/rocker-org/rocker-versioned",
    "last_updated": "2025-07-24T01:26:18+00:00"
  },
  {
    "full_name": "plotly/plotly.R",
    "name": "plotly.R",
    "description": "An interactive graphing library for R",
    "language": "R",
    "topics": [
      "r",
      "ggplot2",
      "javascript",
      "data-visualization",
      "d3js",
      "shiny",
      "plotly",
      "webgl",
      "rstats",
      "r-package"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<img src=\"man/figures/plotly.png\" width=\"200\" />\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/ropensci/plotly/workflows/R-CMD-check/badge.svg)](https://github.com/plotly/plotly.R/actions)\n[![CRAN Status](https://www.r-pkg.org/badges/version/plotly)](https://cran.r-project.org/package=plotly)\n[![CRAN Downloads](https://cranlogs.r-pkg.org/badges/grand-total/plotly)](https://cranlogs.r-pkg.org/badges/grand-total/plotly)\n[![monthly](https://cranlogs.r-pkg.org/badges/plotly)](https://cranlogs.r-pkg.org/badges/plotly)\n<!-- badges: end -->\n\nAn R package for creating interactive web graphics via the open source\nJavaScript graphing library\n[plotly.js](https://github.com/plotly/plotly.js).\n\n<div align=\"center\">\n  <a href=\"https://dash.plotly.com/project-maintenance\">\n    <img src=\"https://dash.plotly.com/assets/images/maintained-by-community.png\" width=\"400px\" alt=\"Maintained by the Plotly Community\">\n  </a>\n</div>\n\n## Installation\n\nInstall from CRAN:\n\n``` r\ninstall.packages(\"plotly\")\n```\n\nOr install the latest development version (on GitHub) via `{remotes}`:\n\n``` r\nremotes::install_github(\"plotly/plotly\")\n```\n\n## Getting started\n\n### Web-based ggplot2 graphics\n\nIf you use [ggplot2](https://github.com/tidyverse/ggplot2), `ggplotly()`\nconverts your static plots to an interactive web-based version\\!\n\n``` r\nlibrary(plotly)\ng <- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") + \n  xlim(1, 6) + ylim(40, 100)\nggplotly(g)\n```\n\n![<https://i.imgur.com/G1rSArP.gifv>](https://i.imgur.com/G1rSArP.gif)\n\nBy default, `ggplotly()` tries to replicate the static ggplot2 version\nexactly (before any interaction occurs), but sometimes you need greater\ncontrol over the interactive behavior. The `ggplotly()` function itself\nhas some convenient “high-level” arguments, such as `dynamicTicks`,\nwhich tells plotly.js to dynamically recompute axes, when app",
    "url": "https://github.com/plotly/plotly.R",
    "last_updated": "2025-09-01T12:33:53+00:00"
  },
  {
    "full_name": "CenterForAssessment/randomNames",
    "name": "randomNames",
    "description": "Function to generate random gender and ethnicity correct first and/or last names. Names are chosen proportionally based upon their probability of appearing in a large scale data base of real names.",
    "language": "R",
    "topics": [
      "r",
      "random-names",
      "random-name-generators",
      "cran"
    ],
    "readme": "randomNames\n===========\n\n\n[![R-CMD-check](https://github.com/CenterForAssessment/randomNames/workflows/R-CMD-check/badge.svg)](https://github.com/CenterForAssessment/randomNames/actions)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/centerforassessment/randomNames?branch=master&svg=true)](https://ci.appveyor.com/project/centerforassessment/randomNames)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/randomNames)](http://cran.r-project.org/package=randomNames)\n[![Development Version](https://img.shields.io/badge/devel-1.5--0.1-brightgreen.svg)](https://github.com/CenterForAssessment/randomNames)\n[![Rstudio mirror downloads](http://cranlogs.r-pkg.org/badges/grand-total/randomNames)](https://github.com/metacran/cranlogs.app)\n[![License](http://img.shields.io/badge/license-GPL%203-brightgreen.svg?style=flat)](https://github.com/CenterForAssessment/randomNames/blob/master/LICENSE.md)\n[![https://gitter.im/CenterForAssessment/randomNames](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/CenterForAssessment/randomNames?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\n# Overview\n\nThe **randomNames** package contains a single function `randomNames` which creates random gender/ethnicity correct first and/or last names where names are proportionally sampled based upon their frequency in a large scale database.\n\n\n# Installation\n\n## From [CRAN](https://CRAN.R-project.org/package=randomNames)\n\nTo install the latest stable release of **randomNames** from [CRAN](https://CRAN.R-project.org/package=randomNames)\n\n```R\n> install.packages(\"randomNames\")\n```\n\n## From [Github](https://github.com/CenterForAssessment/randomNames/)\n\nTo install the development release of **randomNames** from [GitHub](https://github.com/CenterForAssessment/randomNames/):\n\n```R\n> devtools::install_github(\"CenterForAssessment/randomNames\")\n```\n\n# Usage\n\n```\n> randomNames(5) ## 5 last, first names\n[1] \"Mossberg, Cassie\"  \"Mendiaz, Vic",
    "url": "https://github.com/CenterForAssessment/randomNames",
    "last_updated": "2024-12-09T01:17:35+00:00"
  },
  {
    "full_name": "jrpettus/streamlit-buffett",
    "name": "streamlit-buffett",
    "description": "Snowflake LLM-based text to SQL and document retrieval in Streamlit",
    "language": "Python",
    "topics": [],
    "readme": "# streamlit-buffett\n\nThis contains the **\"Ask the Oracle of Omaha\"** Streamlit application that I built for the 2023 Snowflake Summit Streamlit Hackathon. This app took home 2nd place (https://discuss.streamlit.io/t/announcing-the-winners-of-the-summit-hackathon/43344)\n\nThe Streamlit App can be viewed  [here](https://jrpettus-streamlit-buffett-buffett-app-hqw5pq.streamlit.app/)\n\n<img width=\"1554\" alt=\"image\" src=\"https://github.com/jrpettus/streamlit-buffett/assets/11303737/d75c2532-36e9-4ce2-819f-d567d5eebf05\">\n\n<img width=\"843\" alt=\"image\" src=\"https://github.com/jrpettus/streamlit-buffett/assets/11303737/897a41b3-5924-49e5-ad1d-77c9b738f032\">\n\n<img width=\"985\" alt=\"image\" src=\"https://github.com/jrpettus/streamlit-buffett/assets/11303737/eb72a6e6-2389-4050-a47d-ef1577a5159b\">\n\n\n## Application Architecture\n![architecture](https://github.com/jrpettus/streamlit-buffett/blob/main/assets/buffett-app-architecture.png)\n\nNote I may take this down depending on costs in the future.\n",
    "url": "https://github.com/jrpettus/streamlit-buffett",
    "last_updated": "2025-07-28T22:13:49+00:00"
  },
  {
    "full_name": "cynkra/fledge",
    "name": "fledge",
    "description": "Wings for your R packages: Streamline the process of versioning R packages and updating NEWS",
    "language": "R",
    "topics": [
      "r",
      "package-creation",
      "changelog",
      "git"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# fledge\n\n> Smoother change tracking and versioning for R packages.\n\n<!-- badges: start -->\n\n[![rcc](https://github.com/cynkra/fledge/workflows/rcc/badge.svg)](https://github.com/cynkra/fledge/actions) [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental) [![CRAN status](https://www.r-pkg.org/badges/version/fledge)](https://cran.r-project.org/package=fledge) [![Codecov test coverage](https://codecov.io/gh/cynkra/fledge/branch/main/graph/badge.svg)](https://app.codecov.io/gh/cynkra/fledge?branch=main)\n\n<!-- badges: end -->\n\nDo you want to provide a changelog ([NEWS.md](https://blog.r-hub.io/2020/05/08/pkg-news/#why-write-the-changelog-as-newsmd)) more informative than “bug fixes and performance improvements” (`https://twitter.com/EmilyKager/status/1413628436984188933`) to the users of your package?\n\nWays to achieve that are:\n\n- Update NEWS.md right before release by reading through commit messages. Not necessarily fun!\n\n- Update the changelog in every commit e.g. in every PR. Now, if there are several feature PRs around that update the changelog, you’ll have a few clicks to make to tackle conflicts. Easy enough, but potentially annoying.\n\n- Use fledge to\n\n  - fill the `NEWS.md` for you based on informative commit messages,\n  - (optionally) increase the version number in `DESCRIPTION` (e.g. useful in bug reports with session information!),\n  - (optionally) create git tags (more coarse-grained history compared to top-level merges see [fledge tag list on GitHub](https://github.com/cynkra/fledge/tags)).\n\nUsing fledge is a discipline, a few habits, that are worth learning!\n\nWhat you need to do in practice is, **no matter your fledge commitment level**:\n\n- For important commit messages you want recorded in the changelog, you can\n\n  - Use the [conventional commits](https://www.conventionalcommits.org/en",
    "url": "https://github.com/cynkra/fledge",
    "last_updated": "2025-08-30T16:02:29+00:00"
  },
  {
    "full_name": "cmusatyalab/openface",
    "name": "openface",
    "description": "Face recognition with deep neural networks.",
    "language": "Lua",
    "topics": [
      "deep-learning",
      "face-recognition",
      "facenet"
    ],
    "readme": "# OpenFace • [![Build Status][travis-image]][travis] [![Release][release-image]][releases] [![License][license-image]][license] [![Gitter][gitter-image]][gitter]\n\n*Free and open source face recognition with\ndeep neural networks.*\n\n\n[travis-image]: https://travis-ci.org/cmusatyalab/openface.svg?branch=master\n[travis]: http://travis-ci.org/cmusatyalab/openface\n\n[release-image]: http://img.shields.io/badge/release-0.2.1-blue.svg?style=flat\n[releases]: https://github.com/cmusatyalab/openface/releases\n\n[license-image]: http://img.shields.io/badge/license-Apache--2-blue.svg?style=flat\n[license]: LICENSE\n\n[gitter-image]: https://badges.gitter.im/Join%20Chat.svg\n[gitter]: https://gitter.im/cmusatyalab/openface\n\n---\n\n+ Website: http://cmusatyalab.github.io/openface/\n+ [API Documentation](http://openface-api.readthedocs.org/en/latest/index.html)\n+ Join the\n  [cmu-openface group](https://groups.google.com/forum/#!forum/cmu-openface)\n  or the\n  [gitter chat](https://gitter.im/cmusatyalab/openface)\n  for discussions and installation issues.\n+ Development discussions and bugs reports are on the\n  [issue tracker](https://github.com/cmusatyalab/openface/issues).\n\n---\n\nThis research was supported by the National Science Foundation (NSF)\nunder grant number CNS-1518865.  Additional support\nwas provided by the Intel Corporation, Google, Vodafone, NVIDIA, and the\nConklin Kistler family fund.  Any opinions, findings, conclusions or\nrecommendations expressed in this material are those of the authors\nand should not be attributed to their employers or funding sources.\n\n# What's in this repository?\n+ [batch-represent](https://github.com/cmusatyalab/openface/tree/master/batch-represent): Generate representations from\n  a batch of images. [Example directory structure.](https://gist.github.com/bamos/f03037f5df7e05ad0cc8)\n+ [demos/web](https://github.com/cmusatyalab/openface/tree/master/demos/web): Real-time web demo.\n+ [demos/compare.py](https://github.com/cmusatyalab/openface/tree/master/demos",
    "url": "https://github.com/cmusatyalab/openface",
    "last_updated": "2025-09-01T06:52:46+00:00"
  },
  {
    "full_name": "jennybc/2015-06-28_r-summit-talk",
    "name": "2015-06-28_r-summit-talk",
    "description": "Talk at R Summit and Workshop about using R Markdown and GitHub in your workflow",
    "language": "",
    "topics": [],
    "readme": "# 2015-06-28_r-summit-talk\n\nTalk at [R Summit and Workshop](http://info.cbs.dk/rsummit2015/)\n\nIn case it's not clear from the slides, here's the main gist: Based on my experience ...\n\n  * as an instructor, where I use R Markdown and GitHub heavily in graduate courses, and\n  * as an R power user, working to become more of a developer\n  \nI've formed strong opinions about workflows for R Markdown + GitHub and what the big wins are.\n\n  * Make it a habit to render `.Rmd` and `.R` files to Markdown and, maybe, HTML. Places those files where people can see them, e.g. on GitHub! They help people decide whether they need or want to obtain and run your code.\n  * Exploit Markdown and other browsability features of GitHub to make your source repo do double duty as a decent project webpage. Refine your policies about never commiting an intermediate or final product.\n  * Advanced GitHub searching, coupled with Winston Chang's [mirror of R source](https://github.com/wch/r-source/) and Gábor Csárdi's [mirror of CRAN](https://github.com/cran/), helps you implement \"read the source\" in an efficient manner.\n  * GitHub Issues are a very versatile way to faciliate conversations that can be, by turns, conversational or technical and code-based.\n  * Never pick `NA` as your username for anything.\n\nWant to see the slides?\n\n  * PDF is in this repo: [2015-06-28_bryan-r-summit-talk.pdf](2015-06-28_bryan-r-summit-talk.pdf)\n  * View same over [on SpeakerDeck](https://speakerdeck.com/jennybc/talk-at-r-summit-and-workshop-about-using-r-markdown-and-github-in-your-workflow)\n\nLinks\n========================================================\n\nThis talk sort of evolved from my [Feb 2015 talk at the Fields Institute](https://github.com/jennybc/2015-02-23_bryan-fields-talk). So you might want to check out that too.\n\nSTAT 545: <http://stat545-ubc.github.io>\n\n__Rough__ notes on GitHub usage in STAT 545:\n\n  * <http://stat545-ubc.github.io/bit004_stat545-use-of-github.html>\n  * 2015-06: GitHub [recently announ",
    "url": "https://github.com/jennybc/2015-06-28_r-summit-talk",
    "last_updated": "2024-05-21T21:09:12+00:00"
  },
  {
    "full_name": "hrbrmstr/xslt",
    "name": "xslt",
    "description": "lightweight XSLT processing package for R based on xmlwrapp",
    "language": "R",
    "topics": [],
    "readme": "This repo is pretty much defunct since the creation of https://github.com/ropensci/xslt\n",
    "url": "https://github.com/hrbrmstr/xslt",
    "last_updated": "2023-01-28T21:30:51+00:00"
  },
  {
    "full_name": "MiniZinc/libminizinc",
    "name": "libminizinc",
    "description": "The MiniZinc compiler",
    "language": "MiniZinc",
    "topics": [],
    "readme": "<!-- PROJECT LOGO -->\n<br />\n<p align=\"center\">\n  <a href=\"https://www.minizinc.org/\">\n    <img src=\"https://www.minizinc.org/MiniZn_logo.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n  </a>\n\n  <h3 align=\"center\">MiniZinc</h3>\n\n  <p align=\"center\">\n    A high-level constraint modelling language that allows you to easily\n    express and solve discrete optimisation problems.\n    <br />\n    <a href=\"https://www.minizinc.org/\"><strong>Visit our website »</strong></a>\n    <br />\n    <br />\n    <a href=\"https://www.minizinc.org/doc-latest/\">View Documentation</a>\n    ·\n    <a href=\"https://github.com/MiniZinc/libminizinc/issues\">Report Bug</a>\n    ·\n    <a href=\"https://github.com/MiniZinc/libminizinc/issues\">Request Feature</a>\n  </p>\n</p>\n\n<!-- TABLE OF CONTENTS -->\n\n## Table of Contents\n\n- [About the Project](#about-the-project)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Usage](#usage)\n- [Building](#building)\n  - [Prerequisites](#prerequisites)\n  - [Compilation](#compilation)\n- [Testing](#testing)\n- [License](#license)\n- [Acknowledgements](#acknowledgements)\n- [Contact](#contact)\n\n<!-- ABOUT THE PROJECT -->\n\n## About The Project\n\nMiniZinc is a free and open-source constraint modeling language.\n\nYou can use MiniZinc to model constraint satisfaction and optimisation problems\nin a high-level, solver-independent way, taking advantage of a large library of\npre-defined constraints. Your model is then compiled into FlatZinc, a solver\ninput language that is understood by a wide range of solvers.\n\nMiniZinc is developed at Monash University with support from OPTIMA.\n\n<!-- GETTING STARTED -->\n\n## Getting Started\n\nTo get a MiniZinc up and running follow these simple steps.\n\n### Installation\n\nThe recommended way to install _MiniZinc_ is by the use of the bundled binary\npackages. These packages are available for machines running Linux, Mac, and\nWindows.\n\nThe latest release can be found on [the MiniZinc\nwebsite](http://www.minizinc.org/software.html).\n\n###",
    "url": "https://github.com/MiniZinc/libminizinc",
    "last_updated": "2025-08-30T08:42:37+00:00"
  },
  {
    "full_name": "notnews/top10",
    "name": "top10",
    "description": "Top 10 News! Scraping and Parsing Home pages and Top 10 Lists on News Sites",
    "language": "Python",
    "topics": [
      "news",
      "data",
      "nytimes",
      "news-sites"
    ],
    "readme": "## Top 10 News! Scraping and Parsing Home pages and Top 10 Lists on News Sites\n\nWe scraped and parsed the homepages, politics pages, and top10 lists of prominent news sites for 2012 and 2016--2017. We did all this in 2016--2017, and hence the 2012 data exclusively comes from Internet Archive. For 2016--2017, the data mostly comes from scraping live sites but some of the data---where we realized much too late that we wanted to scrape the site---also comes from Internet Archive.\n\n## Data\n\nFor summary of the data, see [here](data_summary.md). The raw data (HTML files) and the CSVs with the parsed data are posted [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OTJMYQ).\n\n## Scripts\n\nThe scripts scraping the Internet Archive still run nicely. The scripts for scraping current homepages, politics pages, and top10 lists have largely survived except format changes mean they may don't work nowadays.\n\nTo learn how to we set up live scraping and parsing the data, including setting up monitoring, see [here](scripts.md).\n\n## License\n\nReleased under the [MIT License](https://opensource.org/licenses/MIT)\n\n## Authors\n\nSuriyan Laohaprapanon and Gaurav Sood\n",
    "url": "https://github.com/notnews/top10",
    "last_updated": "2024-02-23T19:34:44+00:00"
  },
  {
    "full_name": "andialbrecht/sqlparse",
    "name": "sqlparse",
    "description": "A non-validating SQL parser module for Python",
    "language": "Python",
    "topics": [],
    "readme": "python-sqlparse - Parse SQL statements\n======================================\n\n|buildstatus|_\n|coverage|_\n|docs|_\n|packageversion|_\n\n.. docincludebegin\n\nsqlparse is a non-validating SQL parser for Python.\nIt provides support for parsing, splitting and formatting SQL statements.\n\nThe module is compatible with Python 3.8+ and released under the terms of the\n`New BSD license <https://opensource.org/licenses/BSD-3-Clause>`_.\n\nVisit the project page at https://github.com/andialbrecht/sqlparse for\nfurther information about this project.\n\n\nQuick Start\n-----------\n\n.. code-block:: sh\n\n   $ pip install sqlparse\n\n.. code-block:: python\n\n   >>> import sqlparse\n\n   >>> # Split a string containing two SQL statements:\n   >>> raw = 'select * from foo; select * from bar;'\n   >>> statements = sqlparse.split(raw)\n   >>> statements\n   ['select * from foo;', 'select * from bar;']\n\n   >>> # Format the first statement and print it out:\n   >>> first = statements[0]\n   >>> print(sqlparse.format(first, reindent=True, keyword_case='upper'))\n   SELECT *\n   FROM foo;\n\n   >>> # Parsing a SQL statement:\n   >>> parsed = sqlparse.parse('select * from foo')[0]\n   >>> parsed.tokens\n   [<DML 'select' at 0x7f22c5e15368>, <Whitespace ' ' at 0x7f22c5e153b0>, <Wildcard '*' … ]\n   >>>\n\nLinks\n-----\n\nProject page\n   https://github.com/andialbrecht/sqlparse\n\nBug tracker\n   https://github.com/andialbrecht/sqlparse/issues\n\nDocumentation\n   https://sqlparse.readthedocs.io/\n\nOnline Demo\n   https://sqlformat.org/\n\n\nsqlparse is licensed under the BSD license.\n\nParts of the code are based on pygments written by Georg Brandl and others.\npygments-Homepage: http://pygments.org/\n\n.. |buildstatus| image:: https://github.com/andialbrecht/sqlparse/actions/workflows/python-app.yml/badge.svg\n.. _buildstatus: https://github.com/andialbrecht/sqlparse/actions/workflows/python-app.yml\n.. |coverage| image:: https://codecov.io/gh/andialbrecht/sqlparse/branch/master/graph/badge.svg\n.. _coverage: https://codecov.io/gh/andialbrecht/",
    "url": "https://github.com/andialbrecht/sqlparse",
    "last_updated": "2025-08-31T08:46:02+00:00"
  },
  {
    "full_name": "fecgov/FEC",
    "name": "FEC",
    "description": "A general discussion forum for FEC.gov. This is the best place to submit general feedback.",
    "language": "",
    "topics": [],
    "readme": "## Campaign finance for everyone\nThe Federal Election Commission (FEC) releases information to the public about money that’s raised and spent in federal elections — that’s elections for US President, Senate, and House of Representatives.\n\nAre you interested in seeing how much money a candidate raised? Or spent? How much debt they took on? Who contributed to their campaign? The FEC is the authoritative source for that information.\n\n[FEC.gov](https://www.fec.gov/) began a collaboration between [18F](http://18f.gsa.gov) and the FEC to make campaign finance information more accessible (and understandable) to all users. The FEC continues to improve the site and add functionality.\n\n## FEC repositories\nWe welcome you to explore, make suggestions, and contribute to our code.\n\nThis repository, [FEC](https://github.com/fecgov/fec), is a general discussion forum. We [compile feedback](https://github.com/fecgov/fec/issues) from the FEC.gov feedback widget here. This is the best place to submit general feedback.\n\n### All repositories\n- [FEC](https://github.com/fecgov/fec): a general discussion forum. We [compile feedback](https://github.com/fecgov/fec/issues) from the FEC.gov feedback widget here, and this is the best place to submit general feedback.\n- [openFEC](https://github.com/fecgov/openfec): the first RESTful API for the Federal Election Commission\n- [fec-cms](https://github.com/fecgov/fec-cms): the content management system (CMS) for the new FEC.gov\n- [fec-proxy](https://github.com/fecgov/fec-proxy): the proxy application to manage and route requests coming to the new FEC.gov site\n- [fec-proxy-redirect](https://github.com/fecgov/fec-proxy-redirect): the proxy application to redirect legacy application URLs\n- [fec-infrastructure](https://github.com/fecgov/fec-infrastructure): manages the gov cloud RDS instances\n- [fec-pattern-library](https://github.com/fecgov/fec-pattern-library): pattern library for fec.gov\n- [fec-dns](https://github.com/fecgov/fec-dns): manages DNS for",
    "url": "https://github.com/fecgov/FEC",
    "last_updated": "2025-09-01T21:35:53+00:00"
  },
  {
    "full_name": "qodo-ai/pr-agent",
    "name": "pr-agent",
    "description": "🚀 PR-Agent: An AI-Powered 🤖 Tool for Automated Pull Request Analysis, Feedback, Suggestions and More! 💻🔍 (For more advanced: check Qodo Merge)",
    "language": "Python",
    "topics": [
      "gpt-4",
      "openai",
      "codereview",
      "pull-request",
      "code-review",
      "pull-requests",
      "coding-assistant",
      "devtools"
    ],
    "readme": "<div align=\"center\">\n\n<div align=\"center\">\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://www.qodo.ai/wp-content/uploads/2025/02/PR-Agent-Purple-2.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://www.qodo.ai/wp-content/uploads/2025/02/PR-Agent-Purple-2.png\">\n  <img src=\"https://codium.ai/images/pr_agent/logo-light.png\" alt=\"logo\" width=\"330\">\n\n</picture>\n<br/>\n\n[Installation Guide](https://qodo-merge-docs.qodo.ai/installation/) |\n[Usage Guide](https://qodo-merge-docs.qodo.ai/usage-guide/) |\n[Tools Guide](https://qodo-merge-docs.qodo.ai/tools/) |\n[Qodo Merge](https://qodo-merge-docs.qodo.ai/overview/pr_agent_pro/) 💎\n\n## Open-Source AI-Powered Code Review Tool\n\n**PR-Agent** is a free, open-source tool that provides AI-powered code review and PR assistance. Run it locally, on your CI/CD, or self-host it.\n\n**[Qodo Merge](https://qodo-merge-docs.qodo.ai/overview/pr_agent_pro/) 💎** is the hosted, enterprise version with additional features, zero-setup, and priority support.\n\n---\n\n### Quick Start Options\n\n| **Option** | **Best For** | **Setup Time** | **Cost** |\n|------------|--------------|----------------|----------|\n| **[PR-Agent (Open Source)](#-quick-start-for-pr-agent-open-source)** | Developers who want full control, self-hosting, or custom integrations | 5-15 minutes | Free |\n| **[Qodo Merge](#-try-qodo-merge-zero-setup)** | Teams wanting zero-setup, enhancing the open-source features, additional enterprise features, and managed hosting | 2 minutes | Free tier available |\n</div>\n\n[![Static Badge](https://img.shields.io/badge/Chrome-Extension-violet)](https://chromewebstore.google.com/detail/qodo-merge-ai-powered-cod/ephlnjeghhogofkifjloamocljapahnl)\n[![Static Badge](https://img.shields.io/badge/Pro-App-blue)](https://github.com/apps/qodo-merge-pro/)\n[![Static Badge](https://img.shields.io/badge/OpenSource-App-red)](https://github.com/apps/qodo-merge-pro-for-open-source/)\n[![Discord](https://badgen.net/badge/icon/disco",
    "url": "https://github.com/qodo-ai/pr-agent",
    "last_updated": "2025-09-02T09:26:47+00:00"
  },
  {
    "full_name": "Cimbali/pympress",
    "name": "pympress",
    "description": "Pympress is a simple yet powerful PDF reader designed for dual-screen presentations",
    "language": "Python",
    "topics": [
      "presentation",
      "python",
      "pdf-viewer",
      "beamer",
      "presenter",
      "slide",
      "poppler",
      "projector",
      "pdf-reader",
      "gtk",
      "vlc",
      "pygi"
    ],
    "readme": "# ![Pympress logo](https://raw.githubusercontent.com/Cimbali/pympress/master/pympress/share/pixmaps/pympress-32.png) What is Pympress?\n\nPympress is a PDF presentation tool designed for dual-screen setups such as presentations and public talks.\nHighly configurable, fully-featured, and portable\n\nIt comes with many great features ([more below](#functionalities)):\n- supports embedded gifs (out of the box), videos, and audios (with VLC or Gstreamer integration)\n- text annotations displayed in the presenter window\n- natively supports beamer's *notes on second screen*, as well as Libreoffice notes pages!\n\nPympress is a free software, distributed under the terms of the GPL license (version 2 or, at your option, any later version).\n\nPympress was originally created and maintained by [Schnouki](https://github.com/Schnouki), on [his repo](https://github.com/Schnouki/pympress).\n\nHere is what the 2 screen setup looks like, with a big notes slide next to 2 small slides (current and next) on the presenter side:\n![A screenshot with Pympress’ 2 screens](https://pympress.github.io/resources/pympress-screenshot.png)\n\n# Installing [![github version badge][github_version]][github_release]\n\n- Ubuntu ![ubuntu logo][ubuntu] 20.04 focal or newer, Debian ![debian logo][debian] 11 Bullseye or newer\n  [![ubuntu version badge][ubuntu_version]][ubuntu_package] [![debian version badge][debian_version]][debian_package] (maintained by [@mans0954](https://github.com/mans0954))\n\n      apt-get install pympress libgtk-3-0 libpoppler-glib8 libcairo2 python3-gi python3-gi-cairo gobject-introspection libgirepository-1.0-1 gir1.2-gtk-3.0 gir1.2-poppler-0.18\n\n- RPM-based Linux (Fedora ![fedora logo][fedora] CentOS ![centos logo][centos] Mageia ![mageia logo][mageia] OpenSuse ![suse logo][suse] RHEL) [![Copr build version][copr_build_version]][copr_package]\n\n  You can get pympress from the [pympress COPR repo][copr_repo] of your system.\n  With yum or dnf, simply do:\n\n  ```sh\n  dnf copr enable cimbali/pympress",
    "url": "https://github.com/Cimbali/pympress",
    "last_updated": "2025-09-01T14:15:20+00:00"
  },
  {
    "full_name": "astral-sh/ty",
    "name": "ty",
    "description": "An extremely fast Python type checker and language server, written in Rust.",
    "language": "Python",
    "topics": [],
    "readme": "# ty\n\n[![ty](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json)](https://github.com/astral-sh/ty)\n[![PyPI](https://img.shields.io/pypi/v/ty.svg)](https://pypi.python.org/pypi/ty)\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&logoColor=white)](https://discord.com/invite/astral-sh)\n\nAn extremely fast Python type checker and language server, written in Rust.\n\n> [!WARNING]\n>\n> ty is in preview and is not ready for production use.\n>\n> We're working hard to make ty stable and feature-complete, but until then, expect to encounter bugs,\n> missing features, and fatal errors.\n\n## Getting started\n\nTry out the [online playground](https://play.ty.dev), or run ty with\n[uvx](https://docs.astral.sh/uv/guides/tools/#running-tools) to get started quickly:\n\n```shell\nuvx ty\n```\n\nFor other ways to install ty, see the [installation](https://docs.astral.sh/ty/installation/) documentation.\n\nIf you do not provide a subcommand, ty will list available commands — for detailed information about\ncommand-line options, see the [CLI reference](https://docs.astral.sh/ty/reference/cli/).\n\nUse the `check` command to run the type checker:\n\n```shell\nuvx ty check\n```\n\nty will run on all Python files in the working directory and or subdirectories. If used from a\nproject, ty will run on all Python files in the project (starting in the directory with the\n`pyproject.toml`)\n\nYou can also provide specific paths to check:\n\n```shell\nuvx ty check example.py\n```\n\nWhen type checking, ty will find installed packages in the active virtual environment (via\n`VIRTUAL_ENV`) or discover a virtual environment named `.venv` in the project root or working\ndirectory. It will not find packages in non-virtual environments without specifying the target path\nwith `--python`. See the [module discovery](https://docs.astral.sh/ty/modules/) documentation for\ndetails.\n\n## Learning more\n\nTo learn more about using ty, see the [documentation](https:/",
    "url": "https://github.com/astral-sh/ty",
    "last_updated": "2025-09-02T09:38:16+00:00"
  },
  {
    "full_name": "codelucas/newspaper",
    "name": "newspaper",
    "description": "newspaper3k is a news, full-text, and article metadata extraction in Python 3. Advanced docs:",
    "language": "HTML",
    "topics": [
      "python",
      "news",
      "crawler",
      "crawling",
      "scraper",
      "news-aggregator"
    ],
    "readme": "Newspaper3k: Article scraping & curation\n========================================\n\n.. image:: https://badge.fury.io/py/newspaper3k.svg\n    :target: http://badge.fury.io/py/newspaper3k.svg\n        :alt: Latest version\n\n.. image:: https://travis-ci.org/codelucas/newspaper.svg\n        :target: http://travis-ci.org/codelucas/newspaper/\n        :alt: Build status\n\n.. image:: https://coveralls.io/repos/github/codelucas/newspaper/badge.svg?branch=master\n        :target: https://coveralls.io/github/codelucas/newspaper\n        :alt: Coverage status\n\n\nInspired by `requests`_ for its simplicity and powered by `lxml`_ for its speed:\n\n    \"Newspaper is an amazing python library for extracting & curating articles.\"\n    -- `tweeted by`_ Kenneth Reitz, Author of `requests`_\n\n    \"Newspaper delivers Instapaper style article extraction.\" -- `The Changelog`_\n\n.. _`tweeted by`: https://twitter.com/kennethreitz/status/419520678862548992\n.. _`The Changelog`: http://thechangelog.com/newspaper-delivers-instapaper-style-article-extraction/\n\n**Newspaper is a Python3 library**! Or, view our **deprecated and buggy** `Python2 branch`_\n\n.. _`Python2 branch`: https://github.com/codelucas/newspaper/tree/python-2-head\n\nA Glance:\n---------\n\n.. code-block:: pycon\n\n    >>> from newspaper import Article\n\n    >>> url = 'http://fox13now.com/2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/'\n    >>> article = Article(url)\n\n.. code-block:: pycon\n\n    >>> article.download()\n\n    >>> article.html\n    '<!DOCTYPE HTML><html itemscope itemtype=\"http://...'\n\n.. code-block:: pycon\n\n    >>> article.parse()\n\n    >>> article.authors\n    ['Leigh Ann Caldwell', 'John Honway']\n\n    >>> article.publish_date\n    datetime.datetime(2013, 12, 30, 0, 0)\n\n    >>> article.text\n    'Washington (CNN) -- Not everyone subscribes to a New Year's resolution...'\n\n    >>> article.top_image\n    'http://someCDN.com/blah/blah/blah/file.png'\n\n    >>> article.movies\n    ['http://youtube.com/path/to/link.com', ...]\n\n.. code-block:",
    "url": "https://github.com/codelucas/newspaper",
    "last_updated": "2025-09-01T08:37:01+00:00"
  },
  {
    "full_name": "kaskada-ai/kaskada",
    "name": "kaskada",
    "description": "Modern, open-source event-processing",
    "language": "Rust",
    "topics": [
      "cep",
      "event-processing",
      "olap-engine",
      "streaming",
      "complex-event-processing",
      "data-science"
    ],
    "readme": "\n# Kaskada: Modern, open-source event-processing\n\n<p align=\"center\">\n  <a href=\"https://github.com/kaskada-ai/kaskada/actions/workflows/ci_python.yml\">\n    <img src=\"https://github.com/kaskada-ai/kaskada/actions/workflows/ci_python.yml/badge.svg\" alt=\"Python CI\" style=\"max-width: 100%;\">\n  </a>\n  <a href=\"https://github.com/kaskada-ai/kaskada/actions/workflows/ci_with_rust_nightly.yml\">\n    <img src=\"https://github.com/kaskada-ai/kaskada/actions/workflows/ci_with_rust_nightly.yml/badge.svg\" alt=\"Rust CI (Nightly)\" style=\"max-width: 100%;\">\n  </a>\n  <a href=\"https://github.com/kaskada-ai/kaskada/actions/workflows/ci_notebooks.yml\">\n    <img src=\"https://github.com/kaskada-ai/kaskada/actions/workflows/ci_notebooks.yml/badge.svg\" alt=\"Notebooks CI\" style=\"max-width: 100%;\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://kaskada.io\">kaskada.io</a>\n  |\n  <a href=\"https://kaskada.io/guide/\">Docs</a>\n</p>\n\nKaskada is a unified event processing engine that provides all the power of stateful stream processing in a high-level, declarative query language designed specifically for reasoning about events in bulk and in real time.\n\nKaskada's query language builds on the best features of SQL to provide a more expressive way to compute over events. Queries are simple and declarative. Unlike SQL, they are also concise, composable, and designed for processing events. By focusing on the event-processing use case, Kaskada's query language makes it easier to reason about when things happen, state at specific points in time, and how results change over time.\n\nKaskada is implemented as a modern compute engine designed for processing events in bulk or real-time. Written in Rust and built on Apache Arrow, Kaskada can compute most workloads without the complexity and overhead of distributed execution.\n\nRead more at [kaskada.io](https://kaskada.io).\nSee the [docs](https://kaskada.io/guide/) to get started.\n\n## Features\n\n- **Stateful aggregations**: Aggregate events to produce a continuous",
    "url": "https://github.com/kaskada-ai/kaskada",
    "last_updated": "2025-08-10T00:55:30+00:00"
  },
  {
    "full_name": "rstudio/learnr",
    "name": "learnr",
    "description": "Interactive Tutorials with R Markdown",
    "language": "R",
    "topics": [
      "interactive",
      "python",
      "r",
      "r-package",
      "rmarkdown",
      "rstats",
      "shiny",
      "teaching",
      "tutorial",
      "sql"
    ],
    "readme": "# learnr <a href='https://rstudio.github.io/learnr/'><img src='man/figures/logo.png' align=\"right\" height=\"138\" /></a>\n\n<!-- badges: start -->\n\n[![R build\nstatus](https://github.com/rstudio/learnr/workflows/R-CMD-check/badge.svg)](https://github.com/rstudio/learnr)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/learnr)](https://CRAN.R-project.org/package=learnr)\n[![learnr downloads per\nmonth](http://cranlogs.r-pkg.org/badges/learnr)](http://www.rpackages.io/package/learnr)\n[![DOI](https://zenodo.org/badge/71377580.svg)](https://zenodo.org/badge/latestdoi/71377580)\n<br /> [![GitHub\nDiscussions](https://img.shields.io/github/discussions/rstudio/learnr?logo=github&style=social)](https://github.com/rstudio/learnr/discussions)\n[![RStudio\ncommunity](https://img.shields.io/badge/community-teaching-blue?style=social&logo=rstudio&logoColor=75AADB)](https://community.rstudio.com/c/teaching/13)\n[![RStudio\ncommunity](https://img.shields.io/badge/community-learnr-blue?style=social&logo=rstudio&logoColor=75AADB)](https://community.rstudio.com/new-topic?title=&category_id=13&tags=learnr&body=%0A%0A%0A%20%20--------%0A%20%20%0A%20%20%3Csup%3EReferred%20here%20by%20%60learnr%60%27s%20GitHub%3C/sup%3E%0A&u=barret)\n<!-- badges: end -->\n\nThe **learnr** package makes it easy to turn any [R\nMarkdown](https://rmarkdown.rstudio.com/) document into an interactive\ntutorial. Tutorials consist of content along with interactive components\nfor checking and reinforcing understanding. Tutorials can include any or\nall of the following:\n\n1.  Narrative, figures, illustrations, and equations.\n\n2.  Code exercises (R code chunks that users can edit and execute\n    directly).\n\n3.  Quiz questions.\n\n4.  Videos (supported services include YouTube and Vimeo).\n\n5.  Interactive Shiny components.\n\nTutorials automatically preserve work done within them, so if a user\nworks on a few exercises or questions and returns to the tutorial later\nthey can pick up right where they left off.\n\nLearn more about the **le",
    "url": "https://github.com/rstudio/learnr",
    "last_updated": "2025-09-01T22:21:51+00:00"
  },
  {
    "full_name": "mgymrek/docker-reproducibility-example",
    "name": "docker-reproducibility-example",
    "description": "Example of providing a docker container for reproducible analyses",
    "language": "R",
    "topics": [],
    "readme": "docker-reproducibility-example\n==============================\n\nExample of providing a docker container for reproducible analyses. This repository is intended to show how Docker can be used to easily reproduce figures, results, etc. from computational publications. The R scripts and data examples are taken or modified from [Transcriptome Sequencing from Diverse Human Populations Reveals Differentiated Regulatory Architecture]\n(http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1004353) (see below) and are not my original work. \n\n# Running the docker\n```\ndocker run -p 49000:8787 -d mgymrek/docker-reproducibility-example\n```\n\n(You can replace 49000 with another open port if you want.) Navigate in your web browser to http://0.0.0.0:49000. This will open up rstudio. The username and password are both \"guest\". You can open files in the ```scripts/``` directory and run them to reproduce figures S7B and S1 from Martin et al.\n\n# Running on a Mac\nIf you are running on a Mac, you should have already installed, initialized, and started boot2docker using the instructions [here](http://docs.docker.com/installation/mac/). Instructions are the same as above, except instead of using \"0.0.0.0\" as the IP address, run:\n```\nboot2docker ip\n```\nto get the IP, then navigate in the browser to ```$IP:49000```.\n\n# Code\nNote: posted R code is a modified version of the code posted by the authors of Martin et al: [Transcriptome Sequencing from Diverse Human Populations Reveals Differentiated Regulatory Architecture]\n(http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1004353). Original code can be found [here](http://gbsc-share.stanford.edu/HGDP_RNAseq/scripts/). Code is posted with submission from the original authors. Data files were copied from their site.\n\n# Notes\n\n* You might get an rstudio error \"Error in plot.new() : figure margins too large\" if the plot area is too small (e.g. if you're using a small screen or if you squished the plot area to be ",
    "url": "https://github.com/mgymrek/docker-reproducibility-example",
    "last_updated": "2018-03-23T17:43:25+00:00"
  },
  {
    "full_name": "jeremyjbowers/pyopenfec",
    "name": "pyopenfec",
    "description": "A Python wrapper for the OpenFEC API.",
    "language": "Python",
    "topics": [
      "python",
      "fec",
      "elections",
      "data",
      "journalism",
      "government",
      "accountability",
      "openfec-api",
      "candidate"
    ],
    "readme": "# PyOpenFec\nA Python wrapper for the OpenFEC API. Documentation for this API can be found [here](https://api.open.fec.gov/developers)\n\n## Installation\n\n 1. Clone this repository\n```\ngit clone https://github.com/jeremyjbowers/pyopenfec.git\n```\n   \n 2. navigate into the new directory\n```\ncd pyopenfec\n```\n\n 3. (optional) start the virtual environment you'd like to install to\n\n 4. run\n```\npython setup.py install\n```\n\n_Dependencies include [six](https://pypi.python.org/pypi/six) and [requests](https://pypi.python.org/pypi/requests) (will be installed by `setup.py`)_\n\n## Examples\n\n### Candidates\n\n#### Candidate\nThe `Candidate` class holds fields for each candidate in the OpenFEC API.\n\nA number of class and instance methods are available.\n\n##### count\nThe `Candidate.count()` method will return the number of Candidate objects available for a given query. Note: This method returns an integer representing the number of items available in the OpenFEC API. It does not return a list of objects.\n```\nfrom pyopenfec import Candidate\ncandidate_count = Candidate.count(cycle=2016, office=\"P\", candidate_status=\"C\")\n```\n\n#### fetch\nThe `Candidate.fetch()` method will return a list of Candidate objects available for a given query. This method will automatically page through the results and return all objects available in the OpenFEC API.\n```\nfrom pyopenfec import Candidate\ncandidate_count = Candidate.count(cycle=2016, office=\"P\", candidate_status=\"C\")\ncandidates = Candidate.fetch(cycle=2016, office=\"P\", candidate_status=\"C\")\nfor candidate in candidates:\n    print(\"{name}, {party}\".format(name=candidate.name, party=candidate.party))\n```\n### Committees\ntktk\n\n### Reports\ntktk\n",
    "url": "https://github.com/jeremyjbowers/pyopenfec",
    "last_updated": "2024-09-25T15:13:38+00:00"
  },
  {
    "full_name": "LazerLab/DomainDemo",
    "name": "DomainDemo",
    "description": "DomainDemo: a dataset of domain-sharing activities among different demographic groups on Twitter",
    "language": "Python",
    "topics": [],
    "readme": "# Introduction\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15151613.svg)](https://doi.org/10.5281/zenodo.15151613)\n\nThis repository contains the code and part of the data for the dataset [DomainDemo: a dataset of domain-sharing activities among different demographic groups on Twitter](https://doi.org/10.1038/s41597-025-05604-6).\n\n# Directory Structure\n\n- [/code](/code): for example scripts to load the data and workflow to generate derived metrics\n- [/data](/data): for data\n\n# Data access\n\nDomainDemo contains the following versions:\n- `DomainDemo-multivariate`: multivariate version of the dataset\n- `DomainDemo-univariate`: univariate version of the dataset\n- `derived_metrics`: derived metrics for the domains\n\nAll these versions are hosted on [Zenodo](https://zenodo.org/record/15151613).\nDue to the sensitive nature of `DomainDemo-multivariate` and `DomainDemo-univariate`, researchers interested in accessing them need to apply for access.\nDetailed instructions are available on the Zenodo page.\n\nThe `derived_metrics` is available for public access.\nThese metrics quantify different aspects, such as localness and audience partisanship, for over 129,000 domains.\nFor details, please refer to the [derived metrics](/data/derived_metrics) folder.\n\nWe also provide an interactive app to allow everyone to explore the data.\nThe app is hosted on [domaindemoexplorer.streamlit.app](https://domaindemoexplorer.streamlit.app/).\n\n# Citation\n\nIf you use this dataset in your research, please cite the following paper:\n\n```bibtex\n@article{yang2025domaindemo,\n\tauthor       = {Kai-Cheng Yang and Pranav Goel and Alexi Quintana-Mathé and Luke Horgan and Stefan D. McCabe and Nir Grinberg and Kenneth Joseph and David Lazer},\n\ttitle        = {DomainDemo: a dataset of domain-sharing activities among different demographic groups on Twitter},\n\tjournal      = {Scientific Data},\n\tyear         = {2025},\n\tvolume       = {12},\n\tnumber       = {1},\n\tpages        = {1251},\n\tdoi          = {10.1038/s",
    "url": "https://github.com/LazerLab/DomainDemo",
    "last_updated": "2025-08-07T02:55:25+00:00"
  },
  {
    "full_name": "richfitz/remake",
    "name": "remake",
    "description": "Make-like declarative workflows in R",
    "language": "R",
    "topics": [],
    "readme": "# remake\n\n[![Build Status](https://travis-ci.org/richfitz/remake.svg?branch=master)](https://travis-ci.org/richfitz/remake)\n[![Build status](https://ci.appveyor.com/api/projects/status/yltv2lpn046a7e93/branch/master?svg=true)](https://ci.appveyor.com/project/richfitz/remake/branch/master)\n[![Coverage Status](https://coveralls.io/repos/richfitz/remake/badge.svg?branch=master)](https://coveralls.io/r/richfitz/remake?branch=master)\n\nMake-like build management, reimagined for R.\n\nSee below for [installation instructions](#installation).\n\n# The idea\n\n\"[make](http://en.wikipedia.org/wiki/Make_(software))\",\nwhen it works, is wonderful.  Being able to change part of a complicated system and the re-make, updating only the parts of the system that have changed is great.  While it gets some use It's very heavily tailored towards building software though.  While make can be used to create reproducible research workflows (e.g. [here](http://www.bioinformaticszen.com/post/decomplected-workflows-makefiles/) and [here](http://kbroman.org/minimal_make/)), it is a challenge.\n\nThe idea here is to re-imagine a set of ideas from make but built for R.  Rather than having a series of calls to different instances of R (as happens if you run make on R scripts), the idea is to define pieces of a pipeline within an R session.  Rather than being language agnostic (like make must be), `remake` is unapologetically R focussed.\n\n**Note**: This package is under heavy development (as of May 2015), so things may change under you if you start using this now.  However, the core format seems to be working on some nontrivial cases that we are using in our own work.  At the same time, if you're willing to have things change around a bit feel free to start using this and post [issues](https://github.com/richfitz/remake/issues) with problems/friction/ideas etc and the package will reflect your workflow more.\n\n**Note**: Between versions `0.1` and `0.2.0` the database format has changed.  This will require re",
    "url": "https://github.com/richfitz/remake",
    "last_updated": "2025-02-19T21:48:05+00:00"
  },
  {
    "full_name": "khakieconomics/scale_issues",
    "name": "scale_issues",
    "description": "Scale issues blog post",
    "language": "",
    "topics": [],
    "readme": "",
    "url": "https://github.com/khakieconomics/scale_issues",
    "last_updated": "2016-11-14T12:51:34+00:00"
  },
  {
    "full_name": "notnews/horse_race",
    "name": "horse_race",
    "description": "Replication Data and Scripts ",
    "language": "Stata",
    "topics": [],
    "readme": "## Replication Data and Scripts for \"The Supply of Media Slant Across Outlets and Demand for Slant Within Outlets: Evidence from US Presidential Campaign News\"\n\n### Article\n\nhttps://www.sciencedirect.com/science/article/abs/pii/S0176268020300252\n",
    "url": "https://github.com/notnews/horse_race",
    "last_updated": "2023-08-28T19:04:30+00:00"
  },
  {
    "full_name": "ucbrise/clipper",
    "name": "clipper",
    "description": "A low-latency prediction-serving system",
    "language": "C++",
    "topics": [],
    "readme": "# Clipper\n\n[![Build Status](https://amplab.cs.berkeley.edu/jenkins/buildStatus/icon?job=Clipper)](https://amplab.cs.berkeley.edu/jenkins/job/Clipper/) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n\n<img src=\"images/clipper-logo.png\" width=\"200\">\n\n## Note: Clipper is not actively maintained currently. It is available as a research artifact.\n\n## What is Clipper?\n\nClipper is a prediction serving system that sits between user-facing applications and a wide range of commonly used machine learning models and frameworks. Learn more about Clipper and view documentation at our website <http://clipper.ai>.\n\n## What does Clipper do?\n\n* Clipper **simplifies integration of machine learning techniques** into user facing applications by providing a simple standard REST interface for prediction and feedback across a wide range of commonly used machine learning frameworks.  *Clipper makes product teams happy.*\n\n\n* Clipper **simplifies model deployment** and **helps reduce common bugs** by using the same tools and libraries used in model development to render live predictions.  *Clipper makes data scientists happy.*\n\n* Clipper **improves throughput** and ensures **reliable millisecond latencies** by introducing adaptive batching, caching, and straggler mitigation techniques.  *Clipper makes the infra-team less unhappy.*\n\n* Clipper **improves prediction accuracy** by introducing state-of-the-art bandit and ensemble methods to intelligently select and combine predictions and achieve real-time personalization across machine learning frameworks.  *Clipper makes users happy.*\n\n\n## Quickstart\n\n**Note: This quickstart works for the latest version of code. For a quickstart that works with the released version of Clipper available on PyPi, go to our [website](http://clipper.ai)**\n\n> This quickstart requires [Docker](https://www.docker.com/) and supports Python 2.7, 3.5, 3.6 and 3.7.\n\n\n#### Clipper Example Code\n* Basic que",
    "url": "https://github.com/ucbrise/clipper",
    "last_updated": "2025-08-27T05:56:29+00:00"
  },
  {
    "full_name": "everypolitician/everypolitician-data",
    "name": "everypolitician-data",
    "description": "data for national legislatures worldwide",
    "language": "Ruby",
    "topics": [
      "ruby",
      "everypolitician",
      "political",
      "civic-tech",
      "data",
      "dataset",
      "politics",
      "politicians"
    ],
    "readme": "**The EveryPolitician project is currently on hold. See [this blog post](https://www.mysociety.org/2019/06/26/placing-everypolitician-on-hold/) for more information.**\n\n# everypolitician-data\n\nThis is the data repo for EveryPolitician. It contains the data powering [EveryPolitician.org](http://everypolitician.org/), and other sites such as [Gender-Balance.org](http://www.gender-balance.org/).\n\n## Want to use the data?\n\n* [general information about how to _use_ the data](http://everypolitician.org/technical.html)\n* if you want to download it, get it from:\n  - human? go via the [EveryPolitician website](http://everypolitician.org)\n  - program? use the RawGit CDN, via links in `countries.json`, which we [explain here](http://docs.everypolitician.org/repo_structure.html)\n* [what's in the data?](http://docs.everypolitician.org/data_summary.html)\n\n## Want to contribute data?\n\n* [high-level information about how to contribute](http://everypolitician.org/contribute.html)\n\nThis repo is where we store the data, but we have a process for adding it — please don't\nsubmit Pull Requests with data. Instead, if you know of data or data sources we are not\nusing, please get in touch: here's\n[how to contribute](http://everypolitician.org/contribute.html). The bottom line is: we use\n[multiple online sources](http://docs.everypolitician.org/sources.html), and we regularly\nretrieve data from those sources so we can automatically keep up-to-date if and when they change.\nIf you can help us by providing more sources, great!\n\nThis document is for developers actively working _on_ the project, rather than consuming data from it.\n\n## Building the data for a legislature\n\n1. From within the directory for the legislature it should usually be enough to run `bundle exec rake clean default`.\n\n    * To re-refetch the data from a given source first, set the REBUILD_SOURCE environment variable to something matching the filename of the required source: e.g. `REBUILD_SOURCE=official bundle exec rake clean ",
    "url": "https://github.com/everypolitician/everypolitician-data",
    "last_updated": "2025-08-28T23:26:02+00:00"
  },
  {
    "full_name": "deniederhut/workshop_Rintensive",
    "name": "workshop_Rintensive",
    "description": "D-Lab R-intensive teaching materials",
    "language": "HTML",
    "topics": [],
    "readme": "---\ntitle: Materials for D-Lab's R for Data Science\nauthor: Dillon Niederhut\n---\n\nThis repository contains the instructor materials for the D-Lab's R intensive.\n\n## If you are a student:\n\nYou can download the contents of this repository with:\n\n```\ngit clone https://github.com/dlab-berkeley/r-for-data-science.git\n```\n\nor, by clicking the \"Download Zip\" button and then extracting the `.zip` file.\n\nThe instructor of this workshop series will lead you through the activities for each day.\n\n## If you are a D-Lab instructor\n\nYou'll see accumulated teaching notes and examples for each day's topics in the instructor folder. For your convenience, these are available as .Rmd, commented .R files, PDF documents, and HTML slides. The meta-document for this workshop series, which explains the logic behind the structure and topics, can be viewed [at the D-Lab guides repository](https://github.com/dlab-berkeley/guides/blob/master/r.pdf)\n\nFor information on contributing to this repository, see `CONTRIBUTING.md`\n\n## If you are a D-Lab facilitator\n\nThe standard Drupal workshop descriptions and facetweet postings for this workshop series are in `PUBLICITY.md`\n\n## Description\n\n* `data/` : data necessary for interactive coding examples\n* `examples/` \n    * `save_console_output.R` : R code for saving console output to pdf\n* `instructor/` : teaching notes\n* `scripts/`\n    * `feedback_cleaner.R` : used to clean data for use in Day 3\n    * `regenrate_files.R` : for regenerating `.R` and `.pdf` files from `.Rmd`\n\n## Topics:\n\nThis workshop series covers:\n\n1. Interacting with R\n2. Datatypes\n3. Data structures\n4. Reading data\n5. Sanitizing data\n6. Missing data\n7. Reshaping data\n8. Summary statistics\n9. Plotting\n10. Linear models\n11. Non-parametric models\n12. Functions\n13. Loops\n14. Parallelization\n15. Packages\n\n## Libraries\n\nThis workshop uses the following packages:\n\n* Amelia\n* devtools\n* dplyr\n* foreign\n* ggplot2\n* parallelMap\n* RCurl\n* roxygen2\n* stringr\n* tidyr\n* XML\n\n---\n_D-Lab == Data Inten",
    "url": "https://github.com/deniederhut/workshop_Rintensive",
    "last_updated": "2023-09-28T12:02:28+00:00"
  },
  {
    "full_name": "sryza/spark-timeseries",
    "name": "spark-timeseries",
    "description": "A library for time series analysis on Apache Spark",
    "language": "Scala",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/sryza/spark-timeseries.svg)](https://travis-ci.org/sryza/spark-timeseries)\n\nTime Series for Spark (The `spark-ts` Package)\n=============\n\nA Scala / Java / Python library for interacting with time series data on Apache Spark.\n\nPost questions and comments to the [Google group](https://groups.google.com/forum/#!forum/spark-ts),\nor email them directly to <mailto:spark-ts@googlegroups.com>.\n\n### Note: The spark-ts library is no longer under active development by me (Sandy).  I unfortunately no longer have bandwidth to develop features, answer all questions on the mailing list, or fix all bugs that are filed.\n\n### That said, I remain happy to review pull requests and do whatever I can to aid others in advancing the library.\n\n\nDocs are available at http://sryza.github.io/spark-timeseries.\n\nOr check out the [Scaladoc](http://sryza.github.io/spark-timeseries/0.3.0/scaladocs/index.html), [Javadoc](http://sryza.github.io/spark-timeseries/0.3.0/apidocs/index.html), or [Python doc](http://sryza.github.io/spark-timeseries/0.3.0/pydoc/py-modindex.html).\n\nThe aim here is to provide\n\n* A set of abstractions for manipulating large time series data sets, similar to\n  what's provided for smaller data sets in\n  [Pandas](http://pandas.pydata.org/pandas-docs/dev/timeseries.html),\n  [Matlab](http://www.mathworks.com/help/matlab/time-series.html), and R's\n  [zoo](http://cran.r-project.org/web/packages/zoo/index.html) and \n  [xts](http://cran.r-project.org/web/packages/xts/index.html) packages.\n* Models, tests, and functions that enable dealing with time series from a statistical perspective,\n  similar to what's provided in [StatsModels](http://statsmodels.sourceforge.net/devel/tsa.html)\n  and a variety of Matlab and R packages.\n\nThe library sits on a few other excellent Java and Scala libraries.\n\n* [Breeze](https://github.com/scalanlp/breeze) for NumPy-like, BLAS-able linear algebra.\n* [java.time](https://docs.oracle.com/javase/8/docs/api/index.html?ja",
    "url": "https://github.com/sryza/spark-timeseries",
    "last_updated": "2025-08-26T12:34:20+00:00"
  },
  {
    "full_name": "voxmedia/data-projects",
    "name": "data-projects",
    "description": "Scripts and data for various Vox Media stories and news projects",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Data projects at Vox Media\n\n\n## What makes for a good data project? \n\nGood data projects are an extension of the reporting process that rely on quantitative information to inform and educate. They should help flesh out a story and reveal information by means of analysis that would not otherwise be apparent.\n\n\n## Guidelines\n\n- Include guidelines for the processing of data, or at the very least document changes that have been made to the source and include it alongside the output.\n\n- Context should always accompany data and aid in the understanding of a dataset.\n\n- Add information about how to contribute to the project. Also include contribution information for one-off projects that likely will not be updated. Example:\n\n```\nThis project is shared as-is. Bugs, issues, and pull requests may \nnot be readily addressed.\n```\n\n- List the authors and/or contact information.\n\n### Copyright and licensing\n\n- Provide origin of dataset.\n\n- Make sure we have permission to include the dataset in the repo. \n\n- Be clear on what exactly we are claiming as copyright, if relevant. For example, if we're using public domain government data, then what we claim as copyright is any script that transforms the dataset into something usable for a project. The data itself is still publicly domain. \n\n- This is the open source license that goes out with other Vox Media projects:\n\n```\nCopyright (c) 2015, Vox Media, Inc. All rights reserved.\n\nBSD license\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list \nof conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this \nlist of conditions and the following disclaimer in the documentation and/or other \nmaterials provided with the distribution.\n\nNeither the name of the copyright holder nor the names of its contributors ",
    "url": "https://github.com/voxmedia/data-projects",
    "last_updated": "2024-12-06T21:46:16+00:00"
  },
  {
    "full_name": "lucidrains/performer-pytorch",
    "name": "performer-pytorch",
    "description": "An implementation of Performer, a linear attention-based transformer, in Pytorch",
    "language": "Python",
    "topics": [
      "artificial-intelligence",
      "deep-learning",
      "attention-mechanism",
      "attention",
      "transformers"
    ],
    "readme": "<img src=\"./favor+.png\" width=\"500px\"></img>\n\n## Performer - Pytorch\n\n[![PyPI version](https://badge.fury.io/py/performer-pytorch.svg)](https://badge.fury.io/py/performer-pytorch)\n\nAn implementation of <a href=\"https://arxiv.org/abs/2009.14794\">Performer</a>, a linear attention-based transformer variant with a **F**ast **A**ttention **V**ia positive **O**rthogonal **R**andom features approach (FAVOR+).\n\n## Install\n\n```bash\n$ pip install performer-pytorch\n```\n\nThen you must run the following, if you plan on training an autoregressive model\n\n```bash\n$ pip install -r requirements.txt\n```\n\n## Usage\n\nPerformer Language Model\n\n```python\nimport torch\nfrom performer_pytorch import PerformerLM\n\nmodel = PerformerLM(\n    num_tokens = 20000,\n    max_seq_len = 2048,             # max sequence length\n    dim = 512,                      # dimension\n    depth = 12,                     # layers\n    heads = 8,                      # heads\n    causal = False,                 # auto-regressive or not\n    nb_features = 256,              # number of random features, if not set, will default to (d * log(d)), where d is the dimension of each head\n    feature_redraw_interval = 1000, # how frequently to redraw the projection matrix, the more frequent, the slower the training\n    generalized_attention = False,  # defaults to softmax approximation, but can be set to True for generalized attention\n    kernel_fn = torch.nn.ReLU(),    # the kernel function to be used, if generalized attention is turned on, defaults to Relu\n    reversible = True,              # reversible layers, from Reformer paper\n    ff_chunks = 10,                 # chunk feedforward layer, from Reformer paper\n    use_scalenorm = False,          # use scale norm, from 'Transformers without Tears' paper\n    use_rezero = False,             # use rezero, from 'Rezero is all you need' paper\n    ff_glu = True,                  # use GLU variant for feedforward\n    emb_dropout = 0.1,              # embedding dropout\n    ff_dropout =",
    "url": "https://github.com/lucidrains/performer-pytorch",
    "last_updated": "2025-08-22T10:12:25+00:00"
  },
  {
    "full_name": "iamlemec/fastreg",
    "name": "fastreg",
    "description": "Fast sparse regressions with advanced formula syntax. OLS, GLM, Poisson, Maxlike, and more. High-dimensional fixed effects.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "<br />\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/iamlemec/fastreg/master/content/fastreg_path.svg\" alt=\"fastreg logo\"></img>\n</div>\n\n<br />\n\nFast sparse regressions with advanced formula syntax. Good for high-dimensional fixed effects. Installation and usage are described below. Detailed documentation can be found [further down](#documentation).\n\n**New**: generalized linear models and maximum likelihood estimation with JAX.\n\n### Install\n\n\nTo install from PyPI with pip:\n``` bash\npip install fastreg\n```\n\nTo install directly from GitHub:\n\n``` bash\npip install git+https://github.com/iamlemec/fastreg\n```\n\nAlternatively, you can clone this repository locally and run\n\n``` bash\npip install -e .\n```\n\nOptionally, for the maximum likelihood routines, you'll need `jax` and `optax` as well. See [here](https://github.com/google/jax) for detailed instructions.\n\n### Usage\n\nFirst import the necessary functions\n\n``` python\nimport fastreg as fr\nfrom fastreg import I, R, C\n```\n\nCreate some testing data\n\n``` python\ndata = fr.dataset(N=100_000, K1=10, K2=100, models=['linear', 'poisson'])\n```\n\n|     |     y0 |     y |     x1 |     x2 | id1   |   id2 |\n|----:|-------:|------:|-------:|-------:|:------|------:|\n|   0 |  0.140 | 3.450 | -0.260 |  0.958 | E     |    37 |\n|   1 | -0.552 | 0.955 |  0.334 | -1.046 | I     |    65 |\n|   2 | -0.683 | 1.517 |  0.067 | -0.631 | I     |    10 |\n| ... |        |       |        |        |       |       |\n\nWe can construct formulas to define our specification. To make a real `Factor` on `x1`, use `R('x1')` or more conveniently `R.x1`. These can then be combined into `Term`s with `*` and then into `Formula`s with `+`. Regress `y0` on `1`, `x1`, and `x2` given `pandas` DataFrame `data`:\n\n``` python\nfr.ols(y=R.y0, x=I+R.x1+R.x2, data=data)\n```\n\n|    |   coeff |   stderr |   low95 |   high95 |   pvalue |\n|:---|--------:|---------:|--------:|---------:|---------:|\n| I  |   0.099 |    0.003 |   0.093 |    0.105 |    0.000 |\n| x1 |   0",
    "url": "https://github.com/iamlemec/fastreg",
    "last_updated": "2025-05-22T14:58:15+00:00"
  },
  {
    "full_name": "ALShum/rwunderground",
    "name": "rwunderground",
    "description": "A simple R package to get historical and forecast weather data",
    "language": "R",
    "topics": [
      "weather-data",
      "weather",
      "weather-underground",
      "weather-history"
    ],
    "readme": "[![Build Status](https://travis-ci.org/ALShum/rwunderground.svg)](https://travis-ci.org/ALShum/rwunderground)\n\n# Weather Underground R API\n\nThis is an R interface to weather underground's [API](http://www.wunderground.com/weather/api).  \nIn order to use this library please [register](http://www.wunderground.com/weather/api/d/login.html) for an API key.\nThe free-tier should be sufficient if you aren't calling the API more than a 500 times per day.  Please note that the free tier also limits requests to 10 per minute.  If you are grabbing weather for a large date range using `history_range` then by default `limit = 10` will limit the calls to a maximum of 10 per minute.  This package\nhas functions that follow the [online api](http://www.wunderground.com/weather/api/d/docs).\n\n## Install and Setup\n\nThis package is officially on CRAN; install using `install.packages(\"rwunderground\")`.\n\nTo install the latest version please use `devtools`.  If you don't have devtools install using `install.packages(\"devtools\")`.  Afterwards install `rwunderground` using devtools: `devtools::install_github(\"ALShum/rwunderground\")`.\n\nOnce you have your API key as indicated above you can set the key in R using: `rwunderground::set_api_key(\"YOUR KEY\")`.  You only have to do this once per R session.  Alternatively you can save the key in your local .Renviron file by adding the line `WUNDERGROUNDID = 'your key here'`.\n\n## Locations\nFor any of the API functions you must first specify a location -- the first argument of all the API functions is a location.  Locations can be specified by the airport code, zip code, personal weather station ID or simply by specifying state and city (if in US) or country and city (if outside US).  The `set_location` function will validate locations and format things correctly or you can use a (correctly formatted) string.\n\n### Locations by country/state/city\nSetting the location to Honolulu, HI:\n`set_location(territory = \"Hawaii\", city = \"Honolulu\")`.  \n\nSetting the ",
    "url": "https://github.com/ALShum/rwunderground",
    "last_updated": "2025-08-01T15:15:24+00:00"
  },
  {
    "full_name": "mathurinm/celer",
    "name": "celer",
    "description": "Fast solver for L1-type problems: Lasso, sparse Logisitic regression, Group Lasso, weighted Lasso, Multitask Lasso, etc.",
    "language": "Python",
    "topics": [],
    "readme": "# celer\n\n![build](https://github.com/mathurinm/celer/workflows/build/badge.svg)\n![coverage](https://codecov.io/gh/mathurinm/celer/branch/main/graphs/badge.svg?branch=main)\n![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)\n[![Downloads](https://static.pepy.tech/badge/celer)](https://pepy.tech/project/celer)\n[![Downloads](https://pepy.tech/badge/celer/month)](https://pepy.tech/project/celer)\n[![PyPI version](https://badge.fury.io/py/celer.svg)](https://pypi.org/project/celer)\n\n\n``celer`` is a Python package that solves Lasso-like problems and provides estimators that follow the ``scikit-learn`` API. Thanks to a tailored implementation, ``celer`` provides a fast solver that tackles large-scale datasets with millions of features **up to 100 times faster than ``scikit-learn``**.\n\nCurrently, the package handles the following problems:\n\n\n| Problem                       | Support Weights | Native cross-validation\n| -----------                   | -----------     |----------------\n| Lasso                         | ✓               | ✓\n| ElasticNet                    | ✓               | ✓\n| Group Lasso                   | ✓               | ✓\n| Multitask Lasso               | ✕               | ✓\n| Sparse Logistic regression    | ✕               | ✕\n\nIf you are interested in other models, such as non convex penalties (SCAD, MCP), sparse group lasso, group logistic regression, Poisson regression, Tweedie regression, have a look at our companion package [``skglm``](https://github.com/scikit-learn-contrib/skglm)\n\n## Cite\n\n``celer`` is licensed under the [BSD 3-Clause](https://github.com/mathurinm/celer/blob/main/LICENSE). Hence, you are free to use it.\nIf you do so, please cite:\n\n\n```bibtex\n@InProceedings{pmlr-v80-massias18a,\n  title     = {Celer: a Fast Solver for the Lasso with Dual Extrapolation},\n  author    = {Massias, Mathurin and Gramfort, Alexandre and Salmon, Joseph},\n  booktitle = {Proceedings of the 35th International Conference on Machine Learning}",
    "url": "https://github.com/mathurinm/celer",
    "last_updated": "2025-08-19T18:04:45+00:00"
  },
  {
    "full_name": "great-expectations/great_expectations",
    "name": "great_expectations",
    "description": "Always know what to expect from your data.",
    "language": "Python",
    "topics": [
      "pipeline-tests",
      "dataquality",
      "datacleaning",
      "datacleaner",
      "data-science",
      "data-profiling",
      "pipeline",
      "pipeline-testing",
      "cleandata",
      "dataunittest",
      "data-unit-tests",
      "eda",
      "exploratory-data-analysis",
      "exploratory-analysis",
      "exploratorydataanalysis",
      "data-quality",
      "data-engineering",
      "pipeline-debt",
      "data-profilers",
      "mlops"
    ],
    "readme": "[![Python Versions](https://img.shields.io/pypi/pyversions/great_expectations.svg)](https://pypi.python.org/pypi/great_expectations)\n[![PyPI](https://img.shields.io/pypi/v/great_expectations)](https://pypi.org/project/great-expectations/#history)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/great-expectations)](https://pypistats.org/packages/great-expectations)\n[![Build Status](https://img.shields.io/azure-devops/build/great-expectations/bedaf2c2-4c4a-4b37-87b0-3877190e71f5/1)](https://dev.azure.com/great-expectations/great_expectations/_build/latest?definitionId=1&branchName=develop)\n[![pre-commit.ci Status](https://results.pre-commit.ci/badge/github/great-expectations/great_expectations/develop.svg)](https://results.pre-commit.ci/latest/github/great-expectations/great_expectations/develop)\n[![codecov](https://codecov.io/gh/great-expectations/great_expectations/graph/badge.svg?token=rbHxgTxYTs)](https://codecov.io/gh/great-expectations/great_expectations)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5683574.svg)](https://doi.org/10.5281/zenodo.5683574)\n[![Twitter Follow](https://img.shields.io/twitter/follow/expectgreatdata?style=social)](https://twitter.com/expectgreatdata)\n[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://greatexpectations.io/slack)\n[![Contributors](https://img.shields.io/github/contributors/great-expectations/great_expectations)](https://github.com/great-expectations/great_expectations/graphs/contributors)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n\n<!-- <<<Super-quickstart links go here>>> -->\n\n<img align=\"right\" src=\"./docs/docusaurus/static/img/gx-mark-160.png\">\n\n## About GX Core\n\nGX Core combines the collective wisdom of thousands of community members with a proven track record in data quality deployments worldwide, wrapped into a super-simple package for data te",
    "url": "https://github.com/great-expectations/great_expectations",
    "last_updated": "2025-09-02T03:58:08+00:00"
  },
  {
    "full_name": "ramnathv/slidify",
    "name": "slidify",
    "description": "Generate reproducible html5 slides from R markdown",
    "language": "R",
    "topics": [],
    "readme": "![slidify_logo](https://f.cloud.github.com/assets/346288/650134/894eadd0-d455-11e2-8be5-8d463050f4ef.png) \n\nSlidify helps you create and publish beautiful HTML5 presentations from [RMarkdown](http://goo.gl/KKdaf)\n\n## Getting Started\n\n\n### Install ###\n\nSlidify is still under heavy development. You can install it from `github` using the `devtools` package. You will also need `slidifyLibraries` which contains all external libraries required by `slidify`.\n\n```r\ninstall_github('ramnathv/slidify')\ninstall_github('ramnathv/slidifyLibraries')\n```\n\n### Initialize ###\n\nYou can initialize a presentation by running `create`. This will create a scaffold for your presentation and open an Rmd file for you to edit. \n\n```r\nlibrary(slidify)\nauthor('mydeck')\n```\n\n### Author ###\n\nWrite your presentation in RMarkdown, using a newline followed by three dashes to separate slides. You can mix markdown with code chunks to create a reproducible slide deck. \n\n### Generate ###\n\nGenerate your presentation by running `slidify`. This will create a static HTML5 presentation that you can open locally in your browser.\n\n```r\nslidify('index.Rmd')\n```\n\n### Publish ###\n\n```r\n# publish to github\n# create an empty repo on github. replace USER and REPO with your repo details\npublish(user = USER, repo = REPO) \n\n# publish to rpubs\npublish(title = 'My Deck', 'index.html', host = 'rpubs')\n```\n\n---\n\n## Customize ##\n\nSlidify is designed to be modular and provides a high degree of customization for the more advanced user. You can access the defaults using `slidifyDefaults()`. It is possible to override options by passing it to `slidify` as a named list or as a `yaml` file.\n\n```text\nframework      : slide generation framework to use\ntheme          : theme to use for styling slide content\nhighlighter    : tool to use for syntax highlighting\nhitheme        : style to use for syntax highlighting\nmode           : selfcontained, standalone, draft\nurl            : paths to lib\nwidgets        : widgets to include\n```\n\n\nS",
    "url": "https://github.com/ramnathv/slidify",
    "last_updated": "2025-07-20T19:29:27+00:00"
  },
  {
    "full_name": "marcan/takeover.sh",
    "name": "takeover.sh",
    "description": "Wipe and reinstall a running Linux system via SSH, without rebooting. You know you want to.",
    "language": "Shell",
    "topics": [],
    "readme": "# takeover.sh\n\nA script to completely take over a running Linux system remotely, allowing you\nto log into an in-memory rescue environment, unmount the original root\nfilesystem, and do anything you want, all without rebooting. Replace one distro\nwith another without touching a physical console.\n\n## WARNING WARNING WARNING WARNING\n\nThis is experimental. Do not use this script if you don't understand exactly\nhow it works. Do not use this script on any system you care about. Do not use\nthis script on any system you expect to be up. Do not run this script unless\nyou can afford to get physical access to fix a botched takeover. If anything\ngoes wrong, your system will most likely panic.\n\nThat said, this script will not (itself) make any permanent changes to your\nexisting root filesystem (assuming you run it from a tmpfs), so as long as you\ncan remotely reboot your box using an out-of-band mechanism, you *should* be OK.\nBut don't blame me if it eats your dog.\n\nThis script does not have any provisions for exiting *out* of the new\nenvironment back into something sane. You *will* have to reboot when you're\ndone. If you get anything wrong, your machine won't boot. Tough luck.\n\nThis is not a guide for newbies. I'm deliberately not giving you commands you\ncan copy and paste. If you can't figure out what to do exactly without\nhandholding, this script is not for you.\n\n## Compatibility\n\nThis script is designed for init systems that support the `telinit u` command to\nreload the init binary. This includes sysvinit and systemd. If your init system\nworks a different way, you will have to adapt it, or this might not work at\nall. You're on your own here.\n\nYou should always test this in a VM first. You can grab a tarball of your live\nroot filesystem, extract it into a VM image, get your VM up and running (boot\nloader setup is left as an exercise for the reader), then try the process there\nand see if it works. Hint: `mount --bind / /mnt` will get you a view of your\nroot filesystem on `/mnt`",
    "url": "https://github.com/marcan/takeover.sh",
    "last_updated": "2025-08-30T21:38:51+00:00"
  },
  {
    "full_name": "PaddlePaddle/PaddleOCR",
    "name": "PaddleOCR",
    "description": "Awesome multilingual OCR and Document Parsing toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)",
    "language": "Python",
    "topics": [
      "ocr",
      "crnn",
      "ocrlite",
      "db",
      "chineseocr",
      "pdf2markdown",
      "pp-ocr",
      "pp-structure",
      "document-parsing",
      "chatocr"
    ],
    "readme": "<div align=\"center\">\n  <p>\n      <img width=\"100%\" src=\"./docs/images/Banner.png\" alt=\"PaddleOCR Banner\">\n  </p>\n\nEnglish | [简体中文](./readme/README_cn.md) | [繁體中文](./readme/README_tcn.md) | [日本語](./readme/README_ja.md) | [한국어](./readme/README_ko.md) | [Français](./readme/README_fr.md) | [Русский](./readme/README_ru.md) | [Español](./readme/README_es.md) | [العربية](./readme/README_ar.md)\n\n[![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)\n[![arXiv](https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2507.05595)\n[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr/month)](https://pepy.tech/project/paddleocr)\n[![PyPI Downloads](https://static.pepy.tech/badge/paddleocr)](https://pepy.tech/project/paddleocr)\n[![Used by](https://img.shields.io/badge/Used%20by-5.8k%2B%20repositories-blue)](https://github.com/PaddlePaddle/PaddleOCR/network/dependents)\n\n![python](https://img.shields.io/badge/python-3.8~3.12-aff.svg)\n![os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg)\n![hardware](https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg)\n[![License](https://img.shields.io/badge/license-Apache_2.0-green)](./LICENSE)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/PaddlePaddle/PaddleOCR)\n\n\n**PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding**\n\n</div>\n\n# PaddleOCR\n[![Framework](https://img.shields.io/badge/PaddlePaddle-3.0-orange)](https://www.paddlepaddle.org.cn/en)\n[![Accuracy](https://img.shields.io/badge/Recognition%20Accuracy-🏆-green)](#)\n[![Multi-Language](https://img.shields.io/badge/Support_Languages-80+-brightgreen)](#)\n[![Handwriting](https://img.shields.io/badge/Handwriting-✓-success)](#)\n[![Hardware](https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%",
    "url": "https://github.com/PaddlePaddle/PaddleOCR",
    "last_updated": "2025-09-02T09:56:23+00:00"
  },
  {
    "full_name": "gsbDBI/ExperimentData",
    "name": "ExperimentData",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "This repo contains data for the publicly available datasets listed below.\nEach subfolder also contains papers and readme files (if any) associated with that dataset.\nThe sample R file gettingDataIntoR.R shows how the data can be read into R.\n\n\n==========\n==========\n\nDatasets:\n\n==========\n\nAdvertising\n\nData used for the paper\nWhat is advertising content worth?\nBertrand, Karlan, Mullainithan, Shafir and Zinman (2010)\n\n----------\n\nCharitable\n\nData used for the paper\nDoes Price matter in charitable giving?\nEvidence from a large-Scale Natural Field experiment\nDean karlan and John List \n\n----------\n\neSTI\n\nData used for the paper\nWilson E, Free C, Morris TP, Syred J, Ahamed I, Menon-Johansson AS, et al. (2017) Internet-accessed sexually transmitted infection (eSTI) testing and results service: A randomised, single-blind, controlled trial. PLoS Med 14(12): e1002479. https://doi.org/10.1371/journal.pmed.1002479\n\n----------\n\nIV datasets\n\nData used for the paper\nDoes compulsory school attendance affect schooling and earnings?\nAngrist and Krueger (1991)\nand related papers.\n\n----------\n\nMobilization\n\nData for the paper\nComparing Experimental and Matching Methods Using a Large-Scale Voter Mobilization\nExperiment\nKevin Arceneaux, Alan S. Gerber and Donald P. Green (2006)\n\n----------\n\nProject STAR\n\nData from the project\nTennessee's Student Teacher Achievement Ratio (STAR) project\nC.M. Achilles; Helen Pate Bain; Fred Bellott; Jayne Boyd-Zaharias; Jeremy Finn; John Folger; John Johnston; Elizabeth Word, 2008.\n\n----------\n\nSchool vouchers\n\nData for the paper\nVouchers for Private Schooling in Colombia: Evidence from a Randomized Natural Experiment\nAngrist, Bettinger, Bloom, King, and Kremer (2002)\n\n----------\n\nSecrecy\n\nData for the paper\nBallot Secrecy Concerns and Voter Mobilization: New Experimental Evidence about Message\nSource, Context, and the Duration of Mobilization Effects\nGerber, Hubers, Biggers, Hendry (2014)\n\n\n----------\n\nSocial\n\nData for the paper\nSocial Pressure and Voter ",
    "url": "https://github.com/gsbDBI/ExperimentData",
    "last_updated": "2025-09-01T02:17:01+00:00"
  },
  {
    "full_name": "kosukeimai/wru",
    "name": "wru",
    "description": "Who Are You? Bayesian Prediction of Racial Category Using Surname and Geolocation",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# wru: Who Are You? Bayesian Prediction of Racial Category Using Surname and Geolocation <img src=\"man/figures/logo.png?raw=TRUE\" align=\"right\" height=\"138\" alt=\"Package logo\" />\n\n[![R-CMD-check](https://github.com/kosukeimai/wru/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/kosukeimai/wru/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version-last-release/wru)](https://cran.r-project.org/package=wru)\n![CRAN downloads](http://cranlogs.r-pkg.org/badges/grand-total/wru)\n\nThis R package implements the methods proposed in Imai, K. and Khanna,\nK. (2016). “[Improving Ecological Inference by Predicting Individual\nEthnicity from Voter Registration\nRecord](http://imai.princeton.edu/research/race.html).” Political\nAnalysis, Vol. 24, No. 2 (Spring), pp. 263-272. [doi:\n10.1093/pan/mpw001](https://dx.doi.org/10.1093/pan/mpw001).\n\n## Installation\n\nYou can install the released version of **wru** from\n[CRAN](https://cran.r-project.org/package=wru) with:\n\n``` r\ninstall.packages(\"wru\")\n```\n\nOr you can install the development version of **wru** from\n[GitHub](https://github.com/kosukeimai/wru) with:\n\n``` r\n# install.packages(\"pak\")\npak::pkg_install(\"kosukeimai/wru\")\n```\n\n## Using wru\n\nHere is a simple example that predicts the race/ethnicity of voters\nbased only on their surnames.\n\n``` r\nlibrary(wru)\nfuture::plan(future::multisession)\npredict_race(voter.file = voters, surname.only = TRUE)\n```\n\nThe above produces the following output, where the last five columns are\nprobabilistic race/ethnicity predictions (e.g., `pred.his` is the\nprobability of being Hispanic/Latino):\n\n     VoterID    surname state CD county  tract block age sex party PID place    pred.whi    pred.bla     pred.his    pred.asi    pred.oth\n           1     Khanna    NJ 12    021 004000  3001  29   0   Ind   0 74000 0.045110474 0.003067623 0.0068522723 0.860411906 0.084557725\n           2   ",
    "url": "https://github.com/kosukeimai/wru",
    "last_updated": "2025-08-30T14:17:24+00:00"
  },
  {
    "full_name": "modelcontextprotocol/python-sdk",
    "name": "python-sdk",
    "description": "The official Python SDK for Model Context Protocol servers and clients",
    "language": "Python",
    "topics": [],
    "readme": "# MCP Python SDK\n\n<div align=\"center\">\n\n<strong>Python implementation of the Model Context Protocol (MCP)</strong>\n\n[![PyPI][pypi-badge]][pypi-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Python Version][python-badge]][python-url]\n[![Documentation][docs-badge]][docs-url]\n[![Specification][spec-badge]][spec-url]\n[![GitHub Discussions][discussions-badge]][discussions-url]\n\n</div>\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [MCP Python SDK](#mcp-python-sdk)\n  - [Overview](#overview)\n  - [Installation](#installation)\n    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)\n    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)\n  - [Quickstart](#quickstart)\n  - [What is MCP?](#what-is-mcp)\n  - [Core Concepts](#core-concepts)\n    - [Server](#server)\n    - [Resources](#resources)\n    - [Tools](#tools)\n      - [Structured Output](#structured-output)\n    - [Prompts](#prompts)\n    - [Images](#images)\n    - [Context](#context)\n      - [Getting Context in Functions](#getting-context-in-functions)\n      - [Context Properties and Methods](#context-properties-and-methods)\n    - [Completions](#completions)\n    - [Elicitation](#elicitation)\n    - [Sampling](#sampling)\n    - [Logging and Notifications](#logging-and-notifications)\n    - [Authentication](#authentication)\n    - [FastMCP Properties](#fastmcp-properties)\n    - [Session Properties and Methods](#session-properties-and-methods)\n    - [Request Context Properties](#request-context-properties)\n  - [Running Your Server](#running-your-server)\n    - [Development Mode](#development-mode)\n    - [Claude Desktop Integration](#claude-desktop-integration)\n    - [Direct Execution](#direct-execution)\n    - [Streamable HTTP Transport](#streamable-http-transport)\n      - [CORS Configuration for Browser-Based Clients](#cors-configuration-for-browser-based-clients)\n    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)\n      - [StreamableHTTP serve",
    "url": "https://github.com/modelcontextprotocol/python-sdk",
    "last_updated": "2025-09-02T09:41:41+00:00"
  },
  {
    "full_name": "carlislerainey/governor-tax-proposals",
    "name": "governor-tax-proposals",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/carlislerainey/governor-tax-proposals",
    "last_updated": "2015-07-09T20:13:35+00:00"
  },
  {
    "full_name": "duncantl/Rtesseract",
    "name": "Rtesseract",
    "description": "Interface to tesseract OCR system.",
    "language": "R",
    "topics": [],
    "readme": "# Rtesseract package\n\nThis is an R interface to the tesseract OCR (Optical Character Recognition) system.\n\ntesseract is available at https://code.google.com/p/tesseract-ocr/.\n\nMore recent versions are available on github\n  https://github.com/tesseract-ocr/tesseract\n\nInstalling tesseract involves first installing leptonica\nhttp://www.leptonica.com/.\n\nThis is currently a basic interface to the essential functionality, with some\nadded R functionality to visualize the results.\n\n1. Of course, the package provides functionality to get the recognized text.\nHowever, it also allows us to do this at various different levels, e.g.\nword, character, line\n3. We can create a searchable and selectable PDF version of the image(s).\n3. We can output the results of the OCR to a tab-separated-value file, an HTML (hocr) file, a BoxText, a UNLV, or a OSD file.\n2. We can also use different page segmentation modes so that we can detect/recognize\nlines on the image which is useful for processing tables where the lines separate\nrows or columns\n3. We can get the confidence for each recognized text element to understand whether it is \n  a good match or not.\n3. We can get the location and dimensions of each of the text elements. Again, this is \n necessary for processing tables and other structured content.\n3. We can display the matched text, the associated confidences to see spatial patterns.\n Also, we can overlay this on the original image to see patterns.\n3. We can restrict the recognition to a sub-rectangle of the image.\n3. The package provides lower-level access to the C++ API, allowing for more fine-grained and efficient\n use and flexible programmatic access.\n3. We can set and query many variables controlling tesseract's behavior.\n3. We can query details about the image.\n3. We can manipulate an image as an array of pixels\n3. We interface to numerous leptonica routines to process images, e.g., convert to gray scale or\n   binary images, rotate and transpose images\n3. Functionality to read ima",
    "url": "https://github.com/duncantl/Rtesseract",
    "last_updated": "2024-09-04T16:06:01+00:00"
  },
  {
    "full_name": "unitedstates/congressional-record",
    "name": "congressional-record",
    "description": "A parser for the Congressional Record.",
    "language": "HTML",
    "topics": [],
    "readme": "[![Build Status](https://github.com/unitedstates/congressional-record/actions/workflows/ci.yml/badge.svg)](https://github.com/unitedstates/congressional-record/actions/workflows/ci.yml)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://github.com/prettier/prettier)\n\n# congressional-record\n\nThis tool converts HTML files containing the text of the Congressional Record into structured text data. It is particularly useful for identifying speeches by members of Congress.\n\nFrom the repository root, type `python -m congressionalrecord.cli -h` for instructions.\n\n- It outputs JSON\n- Instances of speech are tagged with the speaker's bioguideid wherever possible\n- Instances of speech are recorded as \"turns,\" such that each subsequent instance of speech by a Member counts as a new \"turn.\"\n\nThis software is released as-is under the BSD3 License, with no warranty of any kind.\n\n# installation\n\nIn Python 3 using `venv` for e.g.:\n\n```\ngit clone https://github.com/unitedstates/congressional-record.git\ncd congressional-record\npython3 -m venv .venv\n.venv/bin/python -m pip install -e .\n```\n\nthen `.venv/bin/python -m congressionalrecord.cli -h` to see usage instructions.\n\n# Recommended citation:\n\nJudd, Nicholas, Dan Drinkard, Jeremy Carbaugh, and Lindsay Young. _congressional-record: A parser for the Congressional Record._ Chicago, IL: 2017.\n",
    "url": "https://github.com/unitedstates/congressional-record",
    "last_updated": "2025-08-22T20:09:28+00:00"
  },
  {
    "full_name": "tidymodels/broom",
    "name": "broom",
    "description": "Convert statistical analysis objects from R into tidy format",
    "language": "R",
    "topics": [
      "r",
      "tidy-data",
      "modeling"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# broom <img src=\"man/figures/logo.png\" align=\"right\" width=\"100\" />\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/broom)](https://CRAN.R-project.org/package=broom)\n[![Downloads](https://cranlogs.r-pkg.org/badges/broom)](https://CRAN.R-project.org/package=broom)\n[![R-CMD-check](https://github.com/tidymodels/broom/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/broom/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n## Overview\n\n`broom` summarizes key information about models in tidy `tibble()`s.\n`broom` provides three verbs to make it convenient to interact with\nmodel objects:\n\n- `tidy()` summarizes information about model components\n- `glance()` reports information about the entire model\n- `augment()` adds informations about observations to a dataset\n\nFor a detailed introduction, please see `vignette(\"broom\")`.\n\n`broom` tidies 100+ models from popular modelling packages and almost\nall of the model objects in the `stats` package that comes with base R.\n`vignette(\"available-methods\")` lists method availability.\n\nIf you aren’t familiar with tidy data structures and want to know how\nthey can make your life easier, we highly recommend reading Hadley\nWickham’s [Tidy Data](https://www.jstatsoft.org/v59/i10).\n\n## Installation\n\n``` r\n# we recommend installing the entire tidyverse\n# modeling set, which includes broom:\ninstall.packages(\"tidymodels\")\n\n# alternatively, to install just broom:\ninstall.packages(\"broom\")\n\n# to get the development version from GitHub:\ninstall.packages(\"pak\")\npak::pak(\"tidymodels/broom\")\n```\n\nIf you find a bug, please file a minimal reproducible example in the\n[issues](https://github.com/tidymodels/broom/issues).\n\n## Usage\n\n`tidy()` produces a `tibble()` where each row contains information about\nan important component of the model. For regression models, this often\ncorresponds to regression coefficients. This i",
    "url": "https://github.com/tidymodels/broom",
    "last_updated": "2025-08-25T16:25:00+00:00"
  },
  {
    "full_name": "lorien/grab",
    "name": "grab",
    "description": "Web Scraping Framework",
    "language": "Python",
    "topics": [
      "web-scraping",
      "http-client",
      "framework",
      "python",
      "pycurl",
      "asynchronous",
      "network",
      "urllib3",
      "spider",
      "crawler",
      "crawling",
      "scraping",
      "python-library",
      "python3"
    ],
    "readme": "# Grab\n\n## Update (2025 year)\n\nI have reset all project files to the state of recent pypi release 0.6.41 dated by june 2018.\n\nIf you need most recent state of the project before reset, use the commit tagged as \"cancelled-refactoring\".\n\n## Support\n\nYou are welcome to talk about web scraping and data processing in these Telegram chat groups: [@grablab](https://t.me/grablab) (English) and [@grablab\\_ru](https://t.me/grablab_ru) (Russian)\n\nTo report a bug create new issue in https://github.com/lorien/grab/issues\n\nDocumentation: https://grab.readthedocs.io/en/stable/\n\n\n## What is Grab?\n\nGrab is a python web scraping framework. Grab provides a number of helpful methods\nto perform network requests, scrape web sites and process the scraped content:\n\n* Automatic cookies (session) support\n* HTTP and SOCKS proxy with/without authorization\n* Keep-Alive support\n* IDN support\n* Tools to work with web forms\n* Easy multipart file uploading\n* Flexible customization of HTTP requests\n* Automatic charset detection\n* Powerful API to extract data from DOM tree of HTML documents with XPATH queries\n* Asynchronous API to make thousands of simultaneous queries. This part of\n  library called Spider. See list of spider fetures below.\n* Python 3 ready\n\nSpider is a framework for writing web-site scrapers. Features:\n\n* Rules and conventions to organize the request/parse logic in separate\n  blocks of codes\n* Multiple parallel network requests\n* Automatic processing of network errors (failed tasks go back to task queue)\n* You can create network requests and parse responses with Grab API (see above)\n* HTTP proxy support\n* Caching network results in permanent storage\n* Different backends for task queue (in-memory, redis, mongodb)\n* Tools to debug and collect statistics\n\n\n## Installation\n\nRun `pip install -U grab`\n\nSee details about installing Grab on different platforms here https://grab.readthedocs.io/en/stable/usage/installation.html\n\n\n## Grab Example\n\n```python\nimport logging\n\nfrom grab import Gra",
    "url": "https://github.com/lorien/grab",
    "last_updated": "2025-08-31T16:05:41+00:00"
  },
  {
    "full_name": "UKPLab/sentence-transformers",
    "name": "sentence-transformers",
    "description": "State-of-the-Art Text Embeddings",
    "language": "Python",
    "topics": [],
    "readme": "<!--- BADGES: START --->\n[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-models-yellow)](https://huggingface.co/models?library=sentence-transformers)\n[![GitHub - License](https://img.shields.io/github/license/UKPLab/sentence-transformers?logo=github&style=flat&color=green)][#github-license]\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/sentence-transformers?logo=pypi&style=flat&color=blue)][#pypi-package]\n[![PyPI - Package Version](https://img.shields.io/pypi/v/sentence-transformers?logo=pypi&style=flat&color=orange)][#pypi-package]\n[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=sentence-transformers)][#docs-package]\n<!-- [![PyPI - Downloads](https://img.shields.io/pypi/dm/sentence-transformers?logo=pypi&style=flat&color=green)][#pypi-package] -->\n\n[#github-license]: https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE\n[#pypi-package]: https://pypi.org/project/sentence-transformers/\n[#conda-forge-package]: https://anaconda.org/conda-forge/sentence-transformers\n[#docs-package]: https://www.sbert.net/\n<!--- BADGES: END --->\n\n# Sentence Transformers: Embeddings, Retrieval, and Reranking\n\nThis framework provides an easy method to compute embeddings for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models ([quickstart](https://sbert.net/docs/quickstart.html#sentence-transformer)), to calculate similarity scores using Cross-Encoder (a.k.a. reranker) models ([quickstart](https://sbert.net/docs/quickstart.html#cross-encoder)) or to generate sparse embeddings using Sparse Encoder models ([quickstart](https://sbert.net/docs/quickstart.html#sparse-encoder)). This unlocks a wide range of applications, including [semantic search](https://sbert.net/examples/applications/semantic-search/README.html), [semantic textual similarity](https://sbert.net/docs/sentence_transformer/usage/semant",
    "url": "https://github.com/UKPLab/sentence-transformers",
    "last_updated": "2025-09-02T07:22:26+00:00"
  },
  {
    "full_name": "minimaxir/automl-gs",
    "name": "automl-gs",
    "description": "Provide an input CSV and a target field to predict, generate a model + code to run it.",
    "language": "Python",
    "topics": [
      "python",
      "tensorflow",
      "keras",
      "xgboost",
      "machine-learning",
      "automl"
    ],
    "readme": "# automl-gs\n\n![console gif](docs/console-demo.gif)\n\nGive an input CSV file and a target field you want to predict to automl-gs, and get a trained high-performing machine learning or deep learning model plus native Python code pipelines allowing you to integrate that model into any prediction workflow. No black box: you can see *exactly* how the data is processed, how the model is constructed, and you can make tweaks as necessary.\n\n![demo output](docs/demo-output.png)\n\nautoml-gs is an AutoML tool which, unlike Microsoft's [NNI](https://github.com/Microsoft/nni), Uber's [Ludwig](https://github.com/uber/ludwig), and [TPOT](https://github.com/EpistasisLab/tpot), offers a *zero code/model definition interface* to getting an optimized model and data transformation pipeline in multiple popular ML/DL frameworks, with minimal Python dependencies (pandas + scikit-learn + your framework of choice). automl-gs is designed for citizen data scientists and engineers without a deep statistical background under the philosophy that you don't need to know any modern data preprocessing and machine learning engineering techniques to create a powerful prediction workflow.\n\nNowadays, the cost of computing many different models and hyperparameters is much lower than the opportunity cost of an data scientist's time. automl-gs is a Python 3 module designed to abstract away the common approaches to transforming tabular data, architecting machine learning/deep learning models, and performing random hyperparameter searches to identify the best-performing model. This allows data scientists and researchers to better utilize their time on model performance optimization.\n\n* Generates native Python code; no platform lock-in, and no need to use automl-gs after the model script is created.\n* Train model configurations super-fast *for free* using a **TPU** and TensorFlow in Google Colaboratory. (in Beta: you can access the Colaboratory notebook [here](https://colab.research.google.com/drive/1sbF8cqnOsdz",
    "url": "https://github.com/minimaxir/automl-gs",
    "last_updated": "2025-08-29T00:03:24+00:00"
  },
  {
    "full_name": "dat/pyner",
    "name": "pyner",
    "description": "Python interface to the Stanford Named Entity Recognizer",
    "language": "Python",
    "topics": [],
    "readme": "# PyNER \n\nThe Python interface to the [Stanford Named Entity Recognizer](https://github.com/dat/stanford-ner).\n\n## Project Homepage\n\n* [Stanford Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml)\n\n## Installation\n\n    $ python setup.py install\n\n## Basic Usage\n\n    >>> import ner\n    >>> tagger = ner.HttpNER(host='localhost', port=8080)\n    >>> tagger.get_entities(\"University of California is located in California, United States\")\n    {'LOCATION': ['California', 'United States'],\n     'ORGANIZATION': ['University of California']}\n    >>> tagger.json_entities(\"Alice went to the Museum of Natural History.\")\n    '{\"ORGANIZATION\": [\"Museum of Natural History\"], \"PERSON\": [\"Alice\"]}'\n\n## Online Demo\n\n* [Graphical demo of several models](http://nlp.stanford.edu:8080/ner/)\n\n## License\n\nBSD License\n\n## Author\n\nPyNER is developed by maintained by Dat Hoang.\nIt can be found here: http://github.com/dat/pyner\n\n",
    "url": "https://github.com/dat/pyner",
    "last_updated": "2025-03-20T12:50:51+00:00"
  },
  {
    "full_name": "aryan096/Uttar-Pradesh-2015-2021-Panchayat-Candidates",
    "name": "Uttar-Pradesh-2015-2021-Panchayat-Candidates",
    "description": "Code for creation of dataset of Panchayat candidates (village level) in the Indian state of Uttar Pradesh during the 2015 and 2021 elections",
    "language": "Python",
    "topics": [],
    "readme": "# Uttar Pradesh Panchayat Candidates Dataset - 2015, 2021\n\nCode for creation of dataset of Panchayat candidates (village level) in the Indian state of Uttar Pradesh during the 2015 and 2021 elections\n\nLink to csv file with 2015 and 2021 data - https://docs.google.com/spreadsheets/d/1N0ckOqUFFjrlGK6mWwV6mLplNetOrN03VyTtCFuQu8I/edit?usp=sharing\n\n# List of Variables\n\n<img width=\"500\" alt=\"Screenshot 2022-01-24 at 10 01 28 PM\" src=\"https://user-images.githubusercontent.com/16442168/150903096-99dc3a29-28a7-4404-9ea8-ccf84801d08a.png\">\n",
    "url": "https://github.com/aryan096/Uttar-Pradesh-2015-2021-Panchayat-Candidates",
    "last_updated": "2024-10-03T20:09:15+00:00"
  },
  {
    "full_name": "abresler/markovifyR",
    "name": "markovifyR",
    "description": "Markovify wrapper for R",
    "language": "R",
    "topics": [
      "r",
      "markov-chain",
      "text",
      "text-generator"
    ],
    "readme": "MarkovifyR\n================\n\n`markovifyR` : R wrapper for Markovify\n\nRef: <https://github.com/jsvine/markovify>\n\n> *\"Markovify is a simple, extensible Markov chain generator. Right now, its main use is for building Markov models of large corpora of text, and generating random sentences from that.\"*\n\nThis package requires Python and markovify to be installed.\n\nTo install markovify in R you can run:\n\n``` r\nsystem(\"pip install markovify\")\n```\n\nThe following functions are implemented:\n\n-   `generate_markovify_model:` Generates a markov model\n-   `markovify_text`: Generates text from a markov model\n-   `generate_sentence_starting_with`: Generates text, if possible, with your specified start word\n-   `generate_start_words`: Produces a data frame with the starting words for each input sentence\n-   \n\n### Installation\n\n``` r\ndevtools::install_github(\"abresler/markovifyR\")\n```\n\n``` r\noptions(width=120)\n```\n\n### Usage\n\n``` r\nlibrary(markovifyR)\nlibrary(dplyr)\n```\n\n### Generate New Peter Linneman \"Life Lessons\"\"\n\nHere we are going to showcase how to use the package to create new [Life Lessons](asbcllc.com/reflections/peter_linneman/) from my favorite professor from college Peter Linneman.\n\n#### Step 1 -- Build the Corpus\n\n``` r\ndata(\"linneman_lessons\")\n\nlessons <-\n  linneman_lessons %>% \n  pull(textLesson)\n\nlessons %>% str()\n```\n\n    ##  chr [1:101] \"You always have time for what is important to you\" ...\n\n#### Step 2 -- Build the Model\n\n``` r\nmarkov_model <-\n  generate_markovify_model(\n    input_text = lessons,\n    markov_state_size = 2L,\n    max_overlap_total = 25,\n    max_overlap_ratio = .85\n  )\n```\n\n### Step 3 -- Generate the Text\n\n``` r\nmarkovify_text(\n  markov_model = markov_model,\n  maximum_sentence_length = NULL,\n  output_column_name = 'textLinnemanBot',\n  count = 25,\n  tries = 100,\n  only_distinct = TRUE,\n  return_message = TRUE\n)\n```\n\n    ## textLinnemanBot: “What is the bet?” is the bet?” is the outcome of hard work every day; not a eureka experience\n\n    ## textLinne",
    "url": "https://github.com/abresler/markovifyR",
    "last_updated": "2025-03-22T11:14:23+00:00"
  },
  {
    "full_name": "aws/amazon-sagemaker-examples",
    "name": "amazon-sagemaker-examples",
    "description": "Example 📓 Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using 🧠 Amazon SageMaker. ",
    "language": "Jupyter Notebook",
    "topics": [
      "sagemaker",
      "aws",
      "reinforcement-learning",
      "machine-learning",
      "deep-learning",
      "examples",
      "jupyter-notebook",
      "mlops",
      "data-science",
      "training",
      "inference"
    ],
    "readme": "![SageMaker](https://github.com/aws/amazon-sagemaker-examples/raw/main/_static/sagemaker-banner.png)\n\n# :exclamation::fire: Announcing SageMaker-Core: A New Python SDK for Amazon SageMaker :fire::exclamation:\n\n## Introduction\nToday, Amazon SageMaker is excited to announce the release of SageMaker-Core, a new Python SDK that provides an object-oriented interface for interacting with SageMaker resources such as TrainingJob, Model, and Endpoint. This SDK introduces the resource chaining feature, allowing developers to pass resource objects as parameters, eliminating manual parameter specification and simplifying code management. SageMaker-Core abstracts low-level details like resource state transitions and polling logic, achieving full parity with SageMaker APIs. It also includes usability improvements such as auto code completion, comprehensive documentation, and type hints, enhancing the overall developer experience.\n\n## Use Case\nSageMaker-Core is ideal for ML practitioners who seek full customization of AWS primitives for their ML workloads. SageMaker-Core is an improvement over Boto3, providing a more intuitive and efficient way to manage SageMaker resources. By providing an intuitive object-oriented interface and resource chaining, the SDK allows for seamless integration and management of SageMaker resources. This flexibility, combined with intelligent defaults enables developers to tailor their ML workloads according to their needs. Comprehensive documentation, and type hints help developers write code faster and with fewer errors without navigating complex API documentation.\n\n## Call to Action\nTo learn more about SageMaker-Core, visit the [documentation](https://sagemaker-core.readthedocs.io) and [example notebooks](https://github.com/aws/amazon-sagemaker-examples/tree/default/sagemaker-core). Get started today by integrating SageMaker-Core into your machine learning workflows and experience the benefits of a streamlined and efficient development process.\n\n\n# Am",
    "url": "https://github.com/aws/amazon-sagemaker-examples",
    "last_updated": "2025-09-02T09:10:26+00:00"
  },
  {
    "full_name": "wzchen/probability_cheatsheet",
    "name": "probability_cheatsheet",
    "description": "A comprehensive 10-page probability cheatsheet that covers a semester's worth of introduction to probability.",
    "language": "TeX",
    "topics": [],
    "readme": "This cheatsheet is a 10-page reference in probability that covers a semester's worth of introductory probability.\n\nThe cheatsheet is based off of Harvard's introductory probability course, Stat 110. It is co-authored by former Stat 110 Teaching Fellow William Chen and Stat 110 Professor Joe Blitzstein.\n\nLinks\n-------\n\n* [Probability Cheatsheet PDF](http://www.wzchen.com/probability-cheatsheet/)\n\n\nScreenshots\n-------\n\n![First Page](http://i.imgur.com/Oa73huL.jpg)\n![Second Page](http://i.imgur.com/dyvW2rB.jpg)\n\n\nLicense\n-------\n\nThis work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.][by-nc-sa].\n\n[![Creative Commons License][by-nc-sa-img]][by-nc-sa]\n\nPlanned Additions\n-------\n\n* Monty Hall\n* St. Petersburg Paradox\n\nChangelog\n-------\n\n### 2.0 / 2015-09-04\n\nContributor: Joe Blitzstein\n\nSummary: A huge thanks goes to Professor Joe Blitzstein (@stat110) for his work in polishing up the cheatsheet, adding colors and figures, and officially using it in his Stat 110 class (which is the class that inspired this cheatsheet to begin with). The cheatsheet is now a joint work by William and Professor Blitzstein.\n\nAdded\n\n* Figures\n* Probability of an Intersection or Union\n* Distributions in R Section\n* Colors\n\nFixes\n\n* Extensive polishing and rephrasing for clarity, consistency, and quality.\n\nMoved\n\n* Inequalities moved from the back reference sheet to inside the cheatsheet under \"Distribution Properties\"\n* Some sections reordered\n\nRemoved\n\n* Repeated definitions of PMF and CDF\n* Calculating Probability (2) from Example Problems\n* MGF -- Distribution Matching from Example Problems\n\n### 1.1.1 / 2015-03-20\n\nSummary: Cleaned up sections, moved things around to more logical locations, and fixed one error in formula. Also removed some left margins, which allowed room for a Settlers of Catan diagram and the hat-matching problem.\n\nAdded\n\n* Settlers of Catan diagram\n* Hat-matching problem\n\nRemoved\n\n* Unused theorems and commands\n* Le",
    "url": "https://github.com/wzchen/probability_cheatsheet",
    "last_updated": "2025-09-02T03:57:09+00:00"
  },
  {
    "full_name": "johnmyleswhite/MLNotes",
    "name": "MLNotes",
    "description": "Very concise notes on machine learning and statistics.",
    "language": "R",
    "topics": [],
    "readme": "# MLNotes\nNotes written for myself to keep ML/stats theory ideas organized. Will hopefully be useful to others.\n\nTo best take advantage of these notes, you should download the HTML files and view them in any browswer that supports MathJax. Eventually I'll organize them more clearly and maybe even make a PDF, but for now I'm just keeping ideas in a file named based on the central theme of that file.\n\n# Compile Notes\nTo make sure the notes you're using are up-to-date, it's probably a good idea to compile the notes using the knitr package in R. Once it's installed, you can just run:\n\n    bash compile.sh\n\nwhich will generate the appropriate HTML files.\n\n",
    "url": "https://github.com/johnmyleswhite/MLNotes",
    "last_updated": "2025-06-06T19:16:02+00:00"
  },
  {
    "full_name": "randy3k/R-Box",
    "name": "R-Box",
    "description": "R package for Sublime Text 3",
    "language": "Python",
    "topics": [
      "sublime-text",
      "r"
    ],
    "readme": "## R-Box is succeeded by R-IDE\n\nThe next iteration of R-Box - `R-IDE` at https://github.com/REditorSupport/sublime-ide-r is out. R-IDE utilizes the [language server protocol](https://github.com/tomv564/LSP) and [languageserver](https://github.com/REditorSupport/languageserver) to provide more long term suport for the R language.\n\n\nR package for Sublime Text\n------------\n\n<a href=\"https://packagecontrol.io/packages/R-Box\"><img src=\"https://packagecontrol.herokuapp.com/downloads/R-Box.svg\"></a>\n<a href=\"https://www.paypal.me/randy3k/5usd\" title=\"Donate to this project using Paypal\"><img src=\"https://img.shields.io/badge/paypal-donate-blue.svg\" /></a>\n<a href=\"https://liberapay.com/randy3k/donate\"><img src=\"http://img.shields.io/liberapay/receives/randy3k.svg?logo=liberapay\"></a>\n\nImprove your R coding experiences with Sublime Text!\n\n[Old README](OLDREADME.md)",
    "url": "https://github.com/randy3k/R-Box",
    "last_updated": "2024-11-04T13:38:02+00:00"
  },
  {
    "full_name": "avehtari/BDA_course_Aalto",
    "name": "BDA_course_Aalto",
    "description": "Bayesian Data Analysis course at Aalto",
    "language": "TeX",
    "topics": [
      "bayes",
      "bayesian",
      "bayesian-data-analysis",
      "bayesian-inference",
      "bayesian-methods",
      "bayesian-workflow"
    ],
    "readme": "[![Build Status](https://travis-ci.org/avehtari/BDA_course_Aalto.svg?branch=master)](https://travis-ci.org/avehtari/BDA_course_Aalto)\n\n# Bayesian Data Analysis course material\n\nThis repository has course material for Bayesian Data Analysis course at Aalto (CS-E5710). See [the course web pages for more information](https://avehtari.github.io/BDA_course_Aalto/).\n\nThe course material in the repo can be used in other courses. Text and videos licensed under CC-BY-NC 4.0. Code licensed under BSD-3.\n\n[The electronic version of the course book\nBayesian Data Analysis, 3rd ed, by Andrew Gelman, John Carlin, Hal\nStern, David Dunson, Aki Vehtari, and Donald Rubin](https://users.aalto.fi/~ave/BDA3.pdf) is available for non-commercial purposes. Hard copies are available from [the publisher](https://www.crcpress.com/Bayesian-Data-Analysis/Gelman-Carlin-Stern-Dunson-Vehtari-Rubin/p/book/9781439840955) and many book stores.\nSee also [home page for the\nbook](http://www.stat.columbia.edu/~gelman/book/), [errata for the\nbook](http://www.stat.columbia.edu/~gelman/book/errata_bda3.txt), and [chapter_notes](chapter_notes).\n\nThe material will be updated during the course. Exercise instructions and slides will be updated at latest on Monday of the corresponding week. The best way to stay updated is to clone the repo and pull before checking new material. If you don't want to learn git and can't find the Download ZIP link, click [here](https://github.com/avehtari/BDA_course_Aalto/archive/master.zip).\n\n## Acknowledgements\n\nThe course material has been greatly improved by the previous and\ncurrent course assistants (in alphabetical order): Michael Riis\nAndersen, Paul Bürkner, Akash Daka, Alejandro Catalina, Kunal Ghosh,\nMeenal Jhajharia, Andrew Johnson, Noa Kallioinen, Joona Karjalainen,\nDavid Kohns, Juho Kokkala, Leevi Lindgren, Yann McLatchie, Måns\nMagnusson, Anton Mallasto, Janne Ojanen, Topi Paananen, Markus\nPaasiniemi, Juho Piironen, Anna Riha, Jaakko Riihimäki, Niko Siccha,\nEero Siivola, ",
    "url": "https://github.com/avehtari/BDA_course_Aalto",
    "last_updated": "2025-09-01T13:40:49+00:00"
  },
  {
    "full_name": "tlfvincent/political-liars",
    "name": "political-liars",
    "description": "A lightweight app to visualize the truth of statements in Politifact",
    "language": "R",
    "topics": [],
    "readme": "# political_liars\nA lightweight app to visualize the truth of statements in Politifact\n\n## Collecting the data\n```\npython scrape_politifact.py\n```\n\n## Running the app\nIf you are working locally, you can simply type the following into your R console:\n```\nlibrary(shiny)\nrunApp('/app')\n```\n\nIf you are working on a remote machine and would like to reproduce the app on a host, make sure you have `Docker` installed on your system and type the following:\n\n```\ndocker build -t name_of_your_image .\n```\n\nfollowed by\n\n```\ndocker run -p 3838:3838 -ti name_of_your_image\n```\n\nThe app should then be publicly visible at the following URL `http://your_machine_ip:3838/app/`\n\n",
    "url": "https://github.com/tlfvincent/political-liars",
    "last_updated": "2020-11-13T00:39:53+00:00"
  },
  {
    "full_name": "niderhoff/nlp-datasets",
    "name": "nlp-datasets",
    "description": "Alphabetical list of free/public domain datasets with text data for use in Natural Language Processing (NLP)",
    "language": "",
    "topics": [],
    "readme": "# nlp-datasets\nAlphabetical list of free/public domain datasets with text data for use in Natural Language Processing (NLP). Most stuff here is just raw unstructured text data, if you are looking for annotated corpora or Treebanks refer to the sources at the bottom.\n\n## Datasets (English, multilang)\n*   [Apache Software Foundation Public Mail Archives](http://aws.amazon.com/de/datasets/apache-software-foundation-public-mail-archives/): all publicly available Apache Software Foundation mail archives as of July 11, 2011 (200 GB)\n\n*   [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm): consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. 681,288 posts and over 140 million words. (298 MB)\n\n*   [Amazon Fine Food Reviews [Kaggle]](https://www.kaggle.com/snap/amazon-fine-food-reviews): consists of 568,454 food reviews Amazon users left up to October 2012. [Paper](http://i.stanford.edu/~julian/pdfs/www13.pdf). (240 MB)\n\n*   [Amazon Reviews](https://snap.stanford.edu/data/web-Amazon.html): Stanford collection of 35 million amazon reviews. (11 GB)\n\n*   [ArXiv](http://arxiv.org/help/bulk_data_s3): All the Papers on archive as fulltext (270 GB) + sourcefiles (190 GB).\n\n*   [CLiPS Stylometry Investigation (CSI) Corpus](http://www.clips.uantwerpen.be/datasets/csi-corpus): a yearly expanded corpus of student texts in two genres: essays and reviews. The purpose of this corpus lies primarily in stylometric research, but other applications are possible. (on request)\n\n*   [ClueWeb09 FACC](http://lemurproject.org/clueweb09/FACC1/): [ClueWeb09](http://lemurproject.org/clueweb09/) with Freebase annotations (72 GB)\n\n*   [ClueWeb11 FACC](http://lemurproject.org/clueweb12/FACC1/): [ClueWeb11](http://lemurproject.org/clueweb12/) with Freebase annotations (92 GB)\n\n*   [Common Crawl Corpus](http://aws.amazon.com/de/datasets/common-crawl-corpus/): web crawl data composed of over 5 billion web pages (541 TB)\n\n*   [Cornell Movie Dialog C",
    "url": "https://github.com/niderhoff/nlp-datasets",
    "last_updated": "2025-09-01T13:52:51+00:00"
  },
  {
    "full_name": "PraxTube/assignment-manager",
    "name": "assignment-manager",
    "description": "A terminal based assignment-manager written in python. It can handle both one-time tasks and recurring assignments. Also allows you to track your progress.",
    "language": "Python",
    "topics": [
      "assignment",
      "assignment-manager",
      "manager",
      "python",
      "task",
      "task-manager"
    ],
    "readme": "# Assignment Manager\n\nA terminal based assignment-manager written in python.\nIt can handle both one-time tasks and recurring\nassignments. Also allows you to track your progress.\n\n<p align=\"center\">\n    <img src=\"docs/demo/showcase.gif\" alt=\"animated\" />\n</p>\n\n## Features\n\nThe following features are present in the current version\n\n- Reoccuring assignments\n- Track progress on each assignment\n- Display assignments in sorted table\n- Make back up of assignment data\n- Edit existing assignments\n\nAnd these features still need to be implemented\n\n- [ ] Special class of one-time assignments\n- [ ] Undo last action taken\n\n## Prerequisites\n\nAll you need in order to use this repo is `python>=3.8`.\nNote that there are some python dependencies, however\nmost of them should be available for all OS's.\n\n## Installation\n\nIf you have [pipx](https://pypa.github.io/pipx/), run\n\n```\npipx install assignment-manager\n```\n\notherwise you can just run\n\n```\npip install assignment-manager\n```\n\nNote that if you are using `pip`, you may want to\n[setup a venv](https://docs.python.org/3/library/venv.html).\n\n## Usage\n\nYou use the following two keywords as entry points\n\n```\nassman\nassignment-manager\n```\n\nTo view the help panel, write\n\n```\n$ assman --help\n\n Usage: assman [OPTIONS] COMMAND [ARGS]...                                                          \n\n╭─ Options ────────────────────────────────────────────────────────────────────────────────────────╮\n│ --install-completion        [bash|zsh|fish|powershell|pwsh]  Install completion for the          │\n│                                                              specified shell.                    │\n│                                                              [default: None]                     │\n│ --show-completion           [bash|zsh|fish|powershell|pwsh]  Show completion for the specified   │\n│                                                              shell, to copy it or customize the  │\n│                                                       ",
    "url": "https://github.com/PraxTube/assignment-manager",
    "last_updated": "2025-01-15T15:29:49+00:00"
  },
  {
    "full_name": "jainammm/TableNet",
    "name": "TableNet",
    "description": "Unofficial implementation of \"TableNet: Deep Learning model for end-to-end Table detection and Tabular data extraction from Scanned Document Images\"",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# TableNet\n\nUnofficial implementation of ICDAR 2019 paper : _TableNet: Deep Learning model for end-to-end Table detection and Tabular data extraction from Scanned Document Images._ \n\n[__Paper__](https://arxiv.org/abs/2001.01469)\n\n## Overview\n**Paper: TableNet: Deep Learning model for end-to-end Table detection and Tabular data extraction from Scanned Document Images**\n\nTableNet is a modern deep learning architecture that was proposed by a team from TCS Research year in the year 2019. The main motivation was to extract information from scanned tables through mobile phones or cameras.\n\nThey proposed a solution that includes accurate detection of the tabular region within an image and subsequently detecting and extracting information from the rows and columns of the detected table.\n\n**Architecture:** The architecture is based out of Long et al., an encoder-decoder model for semantic segmentation. The same encoder/decoder network is used as the FCN architecture for table extraction. The images are preprocessed and modified using the Tesseract OCR. \n\nSource: [Nanonets](https://nanonets.com/blog/table-extraction-deep-learning/#tablenet?&utm_source=nanonets.com/blog/&utm_medium=blog&utm_content=Table%20Detection,%20Information%20Extraction%20and%20Structuring%20using%20Deep%20Learning)\n\n\n![architecture](https://github.com/jainammm/TableNet/raw/master/architecture.png)\n\n## How to run\n```\npip install -r requirements.txt\n```\n\n1. Download the Marmot Dataset from the link given in readme.\n1. Run `data_preprocess/generate_mask.py` to generate Table and Column Mask of corresponding images.\n1. Follow the `TableNet.ipynb` notebook to train and test the model.\n\n## Challenges\n* Require a very decent System with a good GPU for accurate result on High pixel images. \n\n## Dataset\n\nDownload the dataset provided in paper : [Marmot Dataset](https://drive.google.com/drive/folders/1QZiv5RKe3xlOBdTzuTVuYRxixemVIODp). \n",
    "url": "https://github.com/jainammm/TableNet",
    "last_updated": "2025-08-15T13:38:48+00:00"
  },
  {
    "full_name": "heike/stat590f",
    "name": "stat590f",
    "description": "Topics in statistical computing @ ISU",
    "language": "HTML",
    "topics": [],
    "readme": "stat590f\n========\n\nTopics in statistical computing @ ISU\n\nSpring 2015\n---\nTime & Place: Snedecor 2102, T 4-5\n\nTimeline & Topics:\n\n\n\n\n- Jan 20: discussion of topics and timeline \n- Jan 27: very basic things in python (Jason, Alex, Ian)\n- Feb 3: numpy, scipy, how does python work in general? (Alex), \n- Feb 10: pandas (Carson) - the python version of Hadley-verse\n- Feb 17: scikit learn (Jason) \n- Feb 24: scikit again?\n- Mar 3:  ggply: Grammar of Graphics in python, other packages for static graphics\n- Mar 10: [interactive graphics (bokeh, seaborn)](http://heike.github.io/stat590f/bokeh) (Andee)\n- Mar 24: opencv (Eric), python interface, computer vision\n- Mar 31: ipython\n- Apr 7: Django, Flask - alternatives to  shiny (Eric)\n- Apr 14: Beautiful soup - webscraping (Carson) \n- Apr 21: sympy (Andee): symbolic mathematics\n- Apr 28: mapreduce in python (), python interpreters - speedup?\n\n\n\n\nResources: \n- [https://www.python.org/]\n- Python notebooks: [http://ipython.org/notebook.html]\n\nFall 2014\n---\nTime & Place: Snedecor 1109, W 12-1\n\nTimeline & Topics:\n\n- Aug 27: installation of Julia, discussion of topics and timeline\n- Sep 3: <a href=\"intro/intro.jl\">intro</a> to Julia (Heike)\n- Sep 10: graphics capabilities: e.g. gadfly (Andee,  Sam T, Carson):\n<a href=\"http://heike.github.io/stat590f/gadfly/andee-graphics/#/\">overview</a>, <a href=\"http://heike.github.io/stat590f/gadfly/samty-gadflyfun/gadflyintro.txt\">gadfly examples</a>, and <a href=\"http://heike.github.io/stat590f/gadfly/carson-knitr/index.html\">embedding Julia in HTML with knitr</a>\n- Sep 17: <a href=\"https://github.com/heike/stat590f/blob/master/ml/Julia%20ML.ipynb\">ML capabilities</a> (Sam H) [detecting text in images: first steps with Julia https://www.kaggle.com/c/street-view-getting-started-with-julia]\n- Sep 24: ML lab\n- Oct 1: Python and Julia, <a href=\"https://github.com/heike/stat590f/blob/master/ipython/ipython_intro.html\">IPython notebooks</a> (Carson, Sam T)\n- Oct 8: [IJulia](http://heike.github.io/stat59",
    "url": "https://github.com/heike/stat590f",
    "last_updated": "2024-01-05T04:32:42+00:00"
  },
  {
    "full_name": "vsego/call-args",
    "name": "call-args",
    "description": "Call functions and create objects by easily assigning values of keyword arguments from some object's attributes of or items from some dictionary",
    "language": "Python",
    "topics": [],
    "readme": "# CallArgs\n\nCall functions and create objects by easily assigning values of keyword arguments from some object's attributes of or items from some dictionary.\n\n## Content\n\n1. [Usage](#usage)\n2. [Variable keyword arguments](#variable-keyword-arguments)\n3. [Extra arguments](#extra-arguments)\n4. [Extending the package's functionality](#extending-the-packages-functionality)\n\n## Usage\n\nWhen calling a function `f` with keyword arguments contained in a dictionary `d`, one can do something like this:\n\n```python\nresult = f(a=d[\"a\"], b=d[\"b\"],...)\n```\n\nor simply unpack `d`:\n\n```python\nresult = f(**d)\n```\n\nSimilarly, using attributes of object `obj`, one can do\n\n```python\nresult = f(a=obj.a, b=obj.b,...)\n```\n\nor simply unpack d:\n\n```python\nresult = f(**vars(d))\n```\n\nIn both of these cases, the first approach can get tedious if there are many arguments and/or when adding new ones, while the second approach is problematic if a dictionary/object has some items/attributes that `f` does not accept (which is almost always the case with objects' attributes).\n\nInstead, the above calls can be done like this:\n\n```python\nfrom call_args import call_args_dict\nresult = call_args_dict(f, d)\n```\n\nor\n\n```python\nfrom call_args import call_args_attr\nresult = call_args_attr(f, obj)\n```\n\nThis can be useful in cases when a dictionary or an object exists specifically for the use with the given callable. For example, when using with command-line arguments:\n\n```python\nparser = argparse.ArgumentParser(\n    prog=splitext(basename(sys.argv[0]))[0],\n    description=sys.modules[__name__].__doc__,\n)\n...\nargs = parser.parse_args()\ncall_args_attr(ClassThatDoesTheJob, args).run()\n```\n\nThe `call_args_*` functions filter out the values that `f` would not understand, as well as all the private ones (those with names starting with underscore `_`).\n\n## Variable keyword arguments\n\nIf the callable allows variable keyword arguments, then the whole source will be used. This means that\n\n```python\ndef f(**kwargs):\n    ...",
    "url": "https://github.com/vsego/call-args",
    "last_updated": "2025-01-15T15:28:10+00:00"
  },
  {
    "full_name": "linanqiu/word2vec-sentiments",
    "name": "word2vec-sentiments",
    "description": "Tutorial for Sentiment Analysis using Doc2Vec in gensim (or \"getting 87% accuracy in sentiment analysis in under 100 lines of code\")",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "\n# Sentiment Analysis using Doc2Vec\n\nWord2Vec is dope. In short, it takes in a corpus, and churns out vectors for each of those words. What's so special about these vectors you ask? Well, similar words are near each other. Furthermore, these vectors represent how we use the words. For example, `v_man - v_woman` is approximately equal to `v_king - v_queen`, illustrating the relationship that \"man is to woman as king is to queen\". This process, in NLP voodoo, is called **word embedding**. These representations have been applied widely. This is made even more awesome with the introduction of Doc2Vec that represents not only words, but entire sentences and documents. Imagine being able to represent an entire sentence using a fixed-length vector and proceeding to run all your standard classification algorithms. Isn't that amazing?\n\nHowever, Word2Vec documentation is shit. The C-code is nigh unreadable (700 lines of highly optimized, and sometimes weirdly optimized code). I personally spent a lot of time untangling Doc2Vec and crashing into ~50% accuracies due to implementation mistakes. This tutorial aims to help other users get off the ground using Word2Vec for their own research. We use Word2Vec for **sentiment analysis** by attempting to classify the Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/). The specific data set used is available for download at http://ai.stanford.edu/~amaas/data/sentiment/.\n\n## Show Me The Code\n\nThe IPython Notebook (code + tutorial) can be found in `word2vec-sentiments.ipynb`\n\nThe code to just run the Doc2Vec and save the model as `imdb.d2v` can be found in `run.py`. Should be useful for running on computer clusters.\n\n## What Does This Repo Contain\n\n- `test-neg.txt` `test-pos.txt` `train-neg.txt` `train-pos.txt` `train-unsup.txt` Training and testing data. Explained in more detail in the notebook.\n- `word2vec-sentiment.ipynb` The notebook (code + tutorial)\n- `run.py` Just the code\n\n## License\n\nCopy",
    "url": "https://github.com/linanqiu/word2vec-sentiments",
    "last_updated": "2025-07-04T15:16:10+00:00"
  },
  {
    "full_name": "emilyinamillion/supreme-court-topics-overtime",
    "name": "supreme-court-topics-overtime",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "## Supreme Court Topics Over Time\n\nThis is a project by @emilyinamillion that utilizes topic modeling with Non Negative Matrix factorization (python) and has an accompanying D3.js visualization. \n\nIf you came here for the topic modeling, [one of my ipython notebooks](https://github.com/emilyinamillion/supreme-court-topics-overtime/blob/master/pipeline/Step4_pipeline_Model-testing.ipynb) walks through the other algorithms I tried as well as my reasoning for why I settled on NMF.\n\nIf you came here for the D3.js, the visualization folder is the entire contents of the live D3.js visualization currently available on Bl.ocks. The visualization lives [here](https://emilyinamillion.me/supreme-court-topics-visualization/index.html).\n\n:v:\n\n\nQuickstart \n\n```\n# Create the conda environment from the file\nconda env create -f environment.yml\n\n# Activate the environment\nconda activate supreme-court-topics\n\n# Download required NLTK data and spaCy model\npython -c \"import nltk; nltk.download('stopwords'); nltk.download('names')\"\npython -m spacy download en_core_web_sm\n```",
    "url": "https://github.com/emilyinamillion/supreme-court-topics-overtime",
    "last_updated": "2025-08-07T19:09:26+00:00"
  },
  {
    "full_name": "edsu/anon",
    "name": "anon",
    "description": "tweet about anonymous Wikipedia edits from particular IP address ranges",
    "language": "JavaScript",
    "topics": [
      "wikipedia",
      "twitter"
    ],
    "readme": "# anon\n\n[![Build Status](https://secure.travis-ci.org/edsu/anon.svg)](http://travis-ci.org/edsu/anon)\n[![Gitter chat](https://badges.gitter.im/edsu/anon.svg)](https://gitter.im/edsu/anon)\n\nanon will watch Wikipedia for *anonymous* edits from a set of named IP ranges\nand will tweet when it notices one. It was inspired by\n[@parliamentedits](https://twitter.com/parliamentedits) and was used to make\n[@congressedits](https://twitter.com/congressedits) available until the account\nwas suspended by Twitter in 2018. An archive of the @congressedits tweets up\nuntil that point is [available](https://edsu.github.io/congressedits-archive/).\nFor more about why the @congressedits accounts was suspended see [this\narticle](http://thewikipedian.net/2019/01/17/congressedits-twitter-suspended/)\nfrom The Wikipedian. anon is now being used by a [community](#community) of\nusers to post selected Wikipedia edits to Twitter.\n\nanon can also send updates on [GNU Social /\nMastodon](https://github.com/tootsuite/mastodon) (see below)\n\n## Run\n\nTo run anon you will need to:\n\n1. install [Node](http://nodejs.org) (v6 or higher)\n1. `git clone https://github.com/edsu/anon.git`\n1. `cd anon`\n1. `npm install`\n1. `cp config.json.template config.json`\n1. add twitter credentials for your bot to `config.json` (make sure the Twitter\napp you create has read/write permission so it can tweet)\n1. add IP ranges/names to `config.json`\n1. modify status template if desired\n1. `./anon.js\n\n### IP Ranges\n\nYou will notice in the example `config.json.template` that you can configure\nip address ranges using a netmask:\n\n    \"143.231.0.0/16\"\n\nor with an array of start/end IP addresses:\n\n    [\"143.231.0.0\", \"143.231.255.255\"]\n\nThese two are equivalent, but the former is a bit faster, and easier to read.\nThe latter is convenient if your range is difficult to express using a netmask.\n\nIf you would like your configuration file to reference the IP addresses in \nthe external file just use the filename. So instead of:\n\n```javascript",
    "url": "https://github.com/edsu/anon",
    "last_updated": "2025-07-21T05:17:31+00:00"
  },
  {
    "full_name": "karthik/rdrop2",
    "name": "rdrop2",
    "description": "Dropbox Interface from R",
    "language": "R",
    "topics": [
      "dropbox-interface",
      "r",
      "data-storage"
    ],
    "readme": "# rdrop2 - Dropbox interface from R  ![a_box](docs/rdrop2-logo.png)  \n\n\n\n# 🚨🚨🚨 Call for maintainers 🚨🚨🚨\n\nThe package is currently not maintained and up for adoption. If you are interested in taking over as maintainer, please send an email to karthik.ram@gmail.com. \n\nIf we can't find another maintainer before something breaks on CRAN, the package will be archived. 🙏\n🚨🚨🚨\n\n\n---\n\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n [![Build Status](https://travis-ci.org/karthik/rdrop2.svg)](https://travis-ci.org/karthik/rdrop2)  [![Coverage Status](https://coveralls.io./repos/karthik/rdrop2/badge.svg)](https://coveralls.io/r/karthik/rdrop2) [![](http://cranlogs.r-pkg.org/badges/rdrop2)](http://cran.rstudio.com/web/packages/rdrop2/index.html)  [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/rdrop2)](http://cran.r-project.org/web/packages/rdrop2) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.998912.svg)](https://doi.org/10.5281/zenodo.998912)\n\n__Maintainers:__ Karthik Ram (@karthik) and Clayton Yochum (@ClaytonJY)\n\n\nThis package provides programmatic access to Dropbox from R. The functions in this package provide access to a full suite of file operations, including dir/copy/move/delete operations, account information and the ability to upload and download files from any Dropbox account.  \n\n\n### Installation  \n\n\n```\n# Current CRAN version (0.8.1)\ninstall.packages('rdrop2')\n\n# or the development version (0.8.1.9999)\ndevtools::install_github(\"karthik/rdrop2\")\n```\n\n### Authentication\n\n```r\nlibrary(rdrop2)\ndrop_auth()\n# This will launch your browser and request access to your Dropbox account. You will be prompted to log in if you aren't already logged in.\n# Once completed, close your browser window and return to R to complete authentication. \n# The credentials are automatically cached (you can prevent this) for future",
    "url": "https://github.com/karthik/rdrop2",
    "last_updated": "2025-06-08T16:00:40+00:00"
  },
  {
    "full_name": "johnmyleswhite/BanditsBook",
    "name": "BanditsBook",
    "description": "Code for my book on Multi-Armed Bandit Algorithms",
    "language": "R",
    "topics": [],
    "readme": "# Code to Accompany the Book \"Bandit Algorithms for Website Optimization\"\n\nThis repo contains code in several languages that implements several standard algorithms for solving the Multi-Armed Bandits Problem, including:\n\n* epsilon-Greedy\n* Softmax (Boltzmann)\n* UCB1\n* UCB2\n* Hedge\n* Exp3\n\nIt also contains code that provides a testing framework for bandit algorithms based around simple Monte Carlo simulations.\n\n# Languages\n\nThis codebase is split up by language. In most languages, there are parallel implementations of the core algorithms and infrastructure for testing the algorithms:\n\n* Python\n* Julia\n* Ruby\n\nIn R, there is a body of code for visualizing the results of simulations and analyzing those results. The R code would benefit from some refactoring to make it DRYer.\n\nIf you're interested in seeing how some of these algorithms would be implemented in Javascript, you should try out Mark Reid's code: http://mark.reid.name/code/bandits/\n\nIf you're looking for Java code, try Dani Sola's work: https://github.com/danisola/bandit\n\nIf you're looking for Scala code, try everpeace(Shingo Omura)'s work: https://github.com/everpeace/banditsbook-scala\n\nIf you're looking for Go code, try Rany Keddo's work: https://github.com/purzelrakete/bandit\n\nIf you're looking for Clojure code, try Paul Ingles's work: https://github.com/pingles/clj-bandit\n\nIf you're looking for Swift code, see https://github.com/crenwick/Swiper\n\nFor a Flask implementation, see https://github.com/DeaconDesperado/flask_mab\n\n# Getting Started\n\nTo try out this code, you can go into the Python or Julia directories and then run the demo script.\n\nIn Python, that looks like:\n\n    python demo.py\n\nIn Julia, that looks like:\n\n    julia demo.jl\n\nYou should step through that code line-by-line to understand what the functions are doing. The book provides more in-depth explanations of how the algorithms work.\n\nThe Ruby code was contributed by Kashif Rasul. If you're interested in translating the code into another langua",
    "url": "https://github.com/johnmyleswhite/BanditsBook",
    "last_updated": "2025-08-19T18:08:05+00:00"
  },
  {
    "full_name": "vincentarelbundock/modelsummary",
    "name": "modelsummary",
    "description": "Beautiful and customizable model summaries in R.",
    "language": "HTML",
    "topics": [],
    "readme": "\n\n<p align=\"center\">\n\n<a href=\"https://modelsummary.com\">\n<img src=\"man/figures/modelsummary_gallery.gif\" height = \"350\" class = \"center\">\n</a> <br> <br>\n<a href = \"https://github.com/vincentarelbundock/modelsummary/blob/main/LICENSE.md\" target = \"_blank\"><img src=\"https://img.shields.io/badge/license-GPLv3-blue\"></a>\n<a href = \"https://modelsummary.com\" target = \"_blank\"><img src=\"https://img.shields.io/static/v1?label=Website&message=Visit&color=blue\"></a>\n<a href = \"https://modelsummary.com\" target = \"_blank\"><img src=\"https://cranlogs.r-pkg.org/badges/grand-total/modelsummary\"></a>\n<br><br>\n</p>\n\n`modelsummary` creates tables and plots to present *descriptive\nstatistics* and to summarize *statistical models* in `R`.\n\n> modelsummary is a package to summarize data and statistical models in\n> R. It supports over one hundred types of models out-of-the-box, and\n> allows users to report the results of those models side-by-side in a\n> table, or in coefficient plots. It makes it easy to execute common\n> tasks such as computing robust standard errors, adding significance\n> stars, and manipulating coefficient and model labels. Beyond model\n> summaries, the package also includes a suite of tools to produce\n> highly flexible data summary tables, such as dataset overviews,\n> correlation matrices, (multi-level) cross-tabulations, and balance\n> tables (also known as “Table 1”). The appearance of the tables\n> produced by modelsummary can be customized using external packages\n> such as kableExtra, gt, flextable, or huxtable; the plots can be\n> customized using ggplot2. Tables can be exported to many output\n> formats, including HTML, LaTeX, Text/Markdown, Microsoft Word,\n> Powerpoint, Excel, RTF, PDF, and image files. Tables and plots can be\n> embedded seamlessly in rmarkdown, knitr, or Sweave dynamic documents.\n> The modelsummary package is designed to be simple, robust, modular,\n> and extensible [(Arel-Bundock,\n> 2022)](https://doi.org/10.18637/jss.v103.i01).\n\n## What?\n\n`models",
    "url": "https://github.com/vincentarelbundock/modelsummary",
    "last_updated": "2025-08-27T21:44:04+00:00"
  },
  {
    "full_name": "smicallef/spiderfoot",
    "name": "spiderfoot",
    "description": "SpiderFoot automates OSINT for threat intelligence and mapping your attack surface.",
    "language": "Python",
    "topics": [
      "footprinting",
      "osint",
      "threatintel",
      "python",
      "infosec",
      "intelligence-gathering",
      "osint-reconnaissance",
      "pentesting",
      "threat-intelligence",
      "security-tools",
      "information-gathering",
      "cybersecurity",
      "cti",
      "osint-framework",
      "attacksurface",
      "osint-tool",
      "hacking",
      "information-security",
      "recon"
    ],
    "readme": "<a href=\"https://www.spiderfoot.net/r.php?u=aHR0cHM6Ly93d3cuc3BpZGVyZm9vdC5uZXQv&s=os_gh\"><img src=\"https://www.spiderfoot.net/wp-content/themes/spiderfoot/img/spiderfoot-wide.png\"></a>\n\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/smicallef/spiderfoot/master/LICENSE)\n[![Python Version](https://img.shields.io/badge/python-3.7+-green)](https://www.python.org)\n[![Stable Release](https://img.shields.io/badge/version-4.0-blue.svg)](https://github.com/smicallef/spiderfoot/releases/tag/v4.0)\n[![CI status](https://github.com/smicallef/spiderfoot/workflows/Tests/badge.svg)](https://github.com/smicallef/spiderfoot/actions?query=workflow%3A\"Tests\")\n[![Last Commit](https://img.shields.io/github/last-commit/smicallef/spiderfoot)](https://github.com/smicallef/spiderfoot/commits/master)\n[![Codecov](https://codecov.io/github/smicallef/spiderfoot/coverage.svg)](https://codecov.io/github/smicallef/spiderfoot)\n[![Twitter Follow](https://img.shields.io/twitter/follow/spiderfoot?label=follow&style=social)](https://twitter.com/spiderfoot)\n[![Discord](https://img.shields.io/discord/770524432464216074)](https://discord.gg/vyvztrG)\n\n**SpiderFoot** is an open source intelligence (OSINT) automation tool. It integrates with just about every data source available and utilises a range of methods for data analysis, making that data easy to navigate. \n\nSpiderFoot has an embedded web-server for providing a clean and intuitive web-based interface but can also be used completely via the command-line.  It's written in **Python 3** and **MIT-licensed**.\n\n<img src=\"https://www.spiderfoot.net/wp-content/uploads/2022/04/opensource-screenshot-v4.png\" />\n\n### FEATURES\n\n- Web based UI or CLI\n- Over 200 modules (see below)\n- Python 3.7+\n- YAML-configurable [correlation engine](/correlations/README.md) with [37 pre-defined rules](/correlations)\n- CSV/JSON/GEXF export\n- API key export/import\n- SQLite back-end for custom querying\n- Highly configurable\n- Full",
    "url": "https://github.com/smicallef/spiderfoot",
    "last_updated": "2025-09-02T05:17:17+00:00"
  },
  {
    "full_name": "jrblevin/deft",
    "name": "deft",
    "description": "Deft for Emacs",
    "language": "Emacs Lisp",
    "topics": [
      "emacs",
      "emacs-mode",
      "note-taking",
      "search",
      "plain-text",
      "filter-string"
    ],
    "readme": "# Deft for Emacs [![MELPA badge][melpa-badge]][melpa-link] [![MELPA stable badge][melpa-stable-badge]][melpa-stable-link] [![Travis CI Build Status][travis-badge]][travis-link]\n\n  [melpa-link]: https://melpa.org/#/deft\n  [melpa-stable-link]: https://stable.melpa.org/#/deft\n  [melpa-badge]: https://melpa.org/packages/deft-badge.svg\n  [melpa-stable-badge]: https://stable.melpa.org/packages/deft-badge.svg\n  [travis-link]: https://travis-ci.org/jrblevin/deft\n  [travis-badge]: https://travis-ci.org/jrblevin/deft.svg?branch=master\n\n<!-- Automatically generated from comments in deft.el. -->\n\nDeft is an Emacs mode for quickly browsing, filtering, and editing\ndirectories of plain text notes, inspired by Notational Velocity.\nIt was designed for increased productivity when writing and taking\nnotes by making it fast and simple to find the right file at the\nright time and by automating many of the usual tasks such as\ncreating new files and saving files.\n\n![Deft Screencast](https://jblevins.org/projects/deft/deft-v0.6.gif)\n\nObtaining Deft\n--------------\n\nDeft is open source software and may be freely distributed and\nmodified under the BSD license.  The latest stable release is\nversion 0.8, released on January 12, 2018.\n\n**Installation via MELPA Stable**\n\nThe recommended way to install Deft is to obtain the stable version\nfrom [MELPA Stable](https://stable.melpa.org/#/deft) using\n`package.el`.  First, configure `package.el` and the MELPA Stable\nrepository by adding the following to your `.emacs`, `init.el`, or\nequivalent startup file:\n\n    (require 'package)\n    (add-to-list 'package-archives\n                 '(\"melpa-stable\" . \"https://stable.melpa.org/packages/\"))\n    (package-initialize)\n\nThen, after restarting Emacs or evaluating the above statements, issue\nthe following command: <kbd>M-x package-install RET deft RET</kbd>.\n\n[MELPA Stable]: http://stable.melpa.org/\n\n**Direct Download**\n\nAlternatively you can manually download and install Deft.\nFirst, download the latest stable",
    "url": "https://github.com/jrblevin/deft",
    "last_updated": "2025-09-02T09:44:07+00:00"
  },
  {
    "full_name": "censusreporter/notebooks",
    "name": "notebooks",
    "description": "A collection of Jupyter notebooks demonstrating ways to analyze Census data",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# notebooks\n\nWith the recent announcement that [Github would directly render Jupyter notebooks](http://blog.jupyter.org/2015/05/07/rendering-notebooks-on-github/), \nit seems interesting to try an experiment.\n\nCensus Reporter invites you to make pull requests for notebooks demonstrating methods for analyzing Census data. \nWhat that looks like remains to be seen! Join in if you want to help figure it out.\n\n# index\n\n* [Basic Census Reporter API with Pandas](Basic Census Reporter API with Pandas.ipynb)\n* [Looking at Black/White income gap in cities / FiveThirtyEight article](538_race_income_gap.ipynb)\n\n\n<hr>\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "url": "https://github.com/censusreporter/notebooks",
    "last_updated": "2024-08-16T22:29:20+00:00"
  },
  {
    "full_name": "lucyparsons/BARTArrests",
    "name": "BARTArrests",
    "description": "A repository of data around arrests on BART",
    "language": "Python",
    "topics": [],
    "readme": "# BART Arrest data\n\nWe are interested in knowing more about BART police's arrest data following a smug comment\nmade by a BART board member on Twitter. Do we decided to do a public records request for it.\n\n```\nLast night on Twitter a board member, Nick Josefowitz stated: \n\n\"Using data, better analysis, and new strategies, @SFBART police were able to increase arrests by 40% year over year. We need this approach at SFPD, and as Supervisor I intend to make accountability and modern police tactics paramount. #NoMoreExcuses\"\n\nhttps://twitter.com/josefow/status/979069932896997376?s=21\n```\n\n# California Public Records Act Request\n\nBelow is the exact CPRA request we sent to BART:\n\n```\nGreeting FOIA officers:\n\nLast night on Twitter a board member, Nick Josefowitz stated:\n\n“Using data, better analysis, and new strategies, @SFBART police were able to increase arrests by 40% year over year. We need this approach at SFPD, and as Supervisor I intend to make accountability and modern police tactics paramount. #NoMoreExcuses”\n\nhttps://twitter.com/josefow/status/979069932896997376?s=21\n\nTherefore, under  to the California Record Act, I am requesting:\n\nFor the years between 2012-2017 (the full calendar year beginning Jan 1 to Dec 31st) I request the record for each year:\n\n* Records sufficient to show the number of arrests on BART\n* Records sufficient to show the race of an an individual or individuals arrested.\n* Records sufficient to show the charges against an an individual or individuals arrested.\n* For a given arrest, the unique case number referred for prosecution.\n* (If known) the outcome of each case\n\nPlease acknowledge this request is being made by a member of the press and I request you waive all fees in the public interest. Please do not hesitate to reach out to me if you have any question about the size or scope of this request.\n```\n\n### Scripts\n\n* `detect_pdf.py` - simplified version of [Google's PDF OCR sample script](https://github.com/GoogleCloudPlatform/python-docs-sam",
    "url": "https://github.com/lucyparsons/BARTArrests",
    "last_updated": "2018-11-09T00:50:14+00:00"
  },
  {
    "full_name": "nlmatics/llmsherpa",
    "name": "llmsherpa",
    "description": "Developer APIs to Accelerate LLM Projects",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# LLM Sherpa\n\nLLM Sherpa provides strategic APIs to accelerate large language model (LLM) use cases.\n\n## What's New\n\n> [!IMPORTANT]\n> llmsherpa back end service is now fully open sourced under Apache 2.0 Licence. See [https://github.com/nlmatics/nlm-ingestor](https://github.com/nlmatics/nlm-ingestor)\n> - You can now run your own servers using a docker image!\n> - Support for different file formats: DOCX, PPTX, HTML, TXT, XML\n> - OCR Support is built in\n> - Blocks now have co-ordinates - use bbox propery of blocks such as sections\n> - A new indent parser to better align all headings in a document to their corresponding level\n> - The free server and paid server are not updated with latest code and users are requested to spawn their own servers using instructions in nlm-ingestor\n\n## LayoutPDFReader\n\nMost PDF to text parsers do not provide layout information. Often times, even the sentences are split with arbritrary CR/LFs making it very difficult to find paragraph boundaries. This poses various challenges in chunking and adding long running contextual information such as section header to the passages while indexing/vectorizing PDFs for LLM applications such as retrieval augmented generation (RAG). \n\nLayoutPDFReader solves this problem by parsing PDFs along with hierarchical layout information such as:\n\n1. Sections and subsections along with their levels.\n2. Paragraphs - combines lines.\n3. Links between sections and paragraphs.\n4. Tables along with the section the tables are found in.\n5. Lists and nested lists.\n6. Join content spread across pages.\n7. Removal of repeating headers and footers.\n8. Watermark removal.\n\nWith LayoutPDFReader, developers can find optimal chunks of text to vectorize, and a solution for limited context window sizes of LLMs. \n\nYou can experiment with the library directly in **Google Colab** [here](https://colab.research.google.com/drive/1hx5Y2TxWriAuFXcwcjsu3huKyn39Q2id?usp=sharing)\n\nHere's a [writeup](https://open.substack.com/pub/ambikasukla/p/e",
    "url": "https://github.com/nlmatics/llmsherpa",
    "last_updated": "2025-09-02T04:54:09+00:00"
  },
  {
    "full_name": "mermaid-js/mermaid",
    "name": "mermaid",
    "description": "Generation of diagrams like flowcharts or sequence diagrams from text in a similar manner as markdown",
    "language": "TypeScript",
    "topics": [
      "documentation",
      "flowchart",
      "javascript",
      "typescript",
      "uml-diagrams",
      "diagrams",
      "diagrams-as-code",
      "mindmap"
    ],
    "readme": "<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/mermaid-js/mermaid/develop/docs/public/favicon.svg\" height=\"150\">\n</p>\n<h1 align=\"center\">\nMermaid\n</h1>\n<p align=\"center\">\nGenerate diagrams from markdown-like text.\n<p>\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/mermaid\"><img src=\"https://img.shields.io/npm/v/mermaid?color=ff3670&label=\"></a>\n<p>\n\n<p align=\"center\">\n<a href=\"https://mermaid.live/\"><b>Live Editor!</b></a>\n</p>\n<p align=\"center\">\n <a href=\"https://mermaid.js.org\">📖 Documentation</a> | <a href=\"https://mermaid.js.org/intro/\">🚀 Getting Started</a> | <a href=\"https://www.jsdelivr.com/package/npm/mermaid\">🌐 CDN</a> | <a href=\"https://discord.gg/sKeNQX4Wtj\" title=\"Discord invite\">🙌 Join Us</a>\n</p>\n<p align=\"center\">\n<a href=\"./README.zh-CN.md\">简体中文</a>\n</p>\n<p align=\"center\">\nTry Live Editor previews of future releases: <a href=\"https://develop.git.mermaid.live/\" title=\"Try the mermaid version from the develop branch.\">Develop</a> | <a href=\"https://next.git.mermaid.live/\" title=\"Try the mermaid version from the next branch.\">Next</a>\n</p>\n\n<br>\n<br>\n\n[![NPM](https://img.shields.io/npm/v/mermaid)](https://www.npmjs.com/package/mermaid)\n[![Build CI Status](https://github.com/mermaid-js/mermaid/actions/workflows/build.yml/badge.svg)](https://github.com/mermaid-js/mermaid/actions/workflows/build.yml)\n[![npm minified gzipped bundle size](https://img.shields.io/bundlephobia/minzip/mermaid)](https://bundlephobia.com/package/mermaid)\n[![Coverage Status](https://codecov.io/github/mermaid-js/mermaid/branch/develop/graph/badge.svg)](https://app.codecov.io/github/mermaid-js/mermaid/tree/develop)\n[![CDN Status](https://img.shields.io/jsdelivr/npm/hm/mermaid)](https://www.jsdelivr.com/package/npm/mermaid)\n[![NPM Downloads](https://img.shields.io/npm/dm/mermaid)](https://www.npmjs.com/package/mermaid)\n[![Join our Discord!](https://img.shields.io/static/v1?message=join%20chat&color=9cf&logo=discord&label=discord)](https://discord.gg/sKeNQX",
    "url": "https://github.com/mermaid-js/mermaid",
    "last_updated": "2025-09-02T10:04:47+00:00"
  },
  {
    "full_name": "allenai/s2orc",
    "name": "s2orc",
    "description": "S2ORC: The Semantic Scholar Open Research Corpus:  https://www.aclweb.org/anthology/2020.acl-main.447/",
    "language": "Python",
    "topics": [],
    "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/allenai/s2orc/master/assets/logo.svg\" alt=\"Logo of S2ORC, pronounced stork\" width=\"20%\">\n</p>\n\n\n# S2ORC: The Semantic Scholar Open Research Corpus\n\nS2ORC is a general-purpose corpus for NLP and text mining research over scientific papers.\n\n* **[Download instructions](#download-instructions)**.\n* S2ORC was developed by [Kyle Lo](https://kyleclo.github.io/) and [Lucy Lu Wang](https://llwang.net/) at the [Allen Institute for AI](https://allenai.org/). It is now being maintained as a product offering by the API team at [Semantic Scholar](https://www.semanticscholar.org/product/api).\n* S2ORC is released under the [ODC-By 1.0](https://opendatacommons.org/licenses/by/1-0/).  By using S2ORC, you agree to the terms in the license.\n* Please cite [our ACL 2020 paper](https://www.aclweb.org/anthology/2020.acl-main.447) if you use S2ORC for your project.  See the [BibTeX](#citation). You can also watch [our 12 min ACL 2020 talk](https://slideslive.com/38929131/s2orc-the-semantic-scholar-open-research-corpus).\n\n\n\n## News and Releases\n\n⭐ **S2ORC now available through S2 API**\n\nIt's Jan 2023; happy new year! After years of managing S2ORC as a research project, it has now been adopted as a core dataset offering through the [Semantic Scholar Public API](https://www.semanticscholar.org/product/api). Please look for the instructions under \"Bulk Dataset\" for download! \n\nS2ORC is now available through the [Semantic Scholar Public API](https://www.semanticscholar.org/product/api) as a \"Bulk Dataset\". It is continuously being rebuilt so if you access it through there, you'll get access to **new** papers as well!\n\n**Software Release: 2021-02-01**\n\n- Released [s2orc-doc2json](https://github.com/allenai/s2orc-doc2json) to support parsing of PDF and LaTeX to JSON format.\n\n\n**S2ORC Release: 2020-07-05**\n\n- Released a new version of S2ORC containing papers up until 2020-04-14, bringing full text coverage from 8M to 12M.\n- Lifted so",
    "url": "https://github.com/allenai/s2orc",
    "last_updated": "2025-09-02T03:13:26+00:00"
  },
  {
    "full_name": "daattali/timevis",
    "name": "timevis",
    "description": "📅 Create interactive timeline visualizations in R",
    "language": "R",
    "topics": [
      "shiny",
      "shiny-r",
      "r",
      "rstats",
      "r-package"
    ],
    "readme": "<h3 align=\"center\">timevis</h3>\n<h4 align=\"center\">\n  📅 Create interactive timeline visualizations in R\n  <br><br>\n  <a href=\"https://daattali.com/shiny/timevis-demo/\">Demo</a>\n  &middot;\n  by <a href=\"https://deanattali.com\">Dean Attali</a>\n</h4>\n\n<p align=\"center\">\n  <a href=\"https://github.com/daattali/timevis/actions\">\n    <img src=\"https://github.com/daattali/timevis/workflows/R-CMD-check/badge.svg\" alt=\"R build status\" />\n  </a> \n  <a href=\"https://cran.r-project.org/package=timevis\">\n    <img src=\"https://www.r-pkg.org/badges/version/timevis\" alt=\"CRAN version\" />\n  </a>\n  <a href=\"https://paypal.me/daattali/20\">\n    <img src=\"http://i.imgur.com/vCIGFrH.png\" />\n  </a>\n</p>\n\n---\n\n<img src=\"inst/img/hex.png\" width=\"170\" align=\"right\"/>\n\n{timevis} lets you create rich and *fully interactive* timeline\nvisualizations in R. Timelines can be included in Shiny apps or R\nmarkdown documents.\n{timevis} includes an extensive API to manipulate a timeline after\ncreation, and supports getting data out of the visualization into R.\nThis package is based on the [visjs](https://visjs.github.io/vis-timeline/docs/timeline/) Timeline\nJavaScript library.\n\n**Need Shiny help? [I'm available for consulting](https://attalitech.com/).**<br/>\n**If you find {timevis} useful, please consider [supporting my work](https://github.com/sponsors/daattali)! ❤**\n\n<p align=\"center\">\n  <a style=\"display: inline-block;\" href=\"https://github.com/sponsors/daattali\">\n    <img height=\"35\" src=\"https://i.imgur.com/034B8vq.png\" />\n  </a>\n</p>\n\n> This package is part of a larger ecosystem of packages with a shared vision: solving common Shiny issues and improving Shiny apps with minimal effort, minimal code changes, and clear documentation. Other packages for your Shiny apps:\n\n| Package | Description | Demo |\n|---|---|---|\n| [shinyjs](https://deanattali.com/shinyjs/) | 💡 Easily improve the user experience of your Shiny apps in seconds | [🔗](https://deanattali.com/shinyjs/overview#demo) |\n| [shinyalert](http",
    "url": "https://github.com/daattali/timevis",
    "last_updated": "2025-08-18T23:07:20+00:00"
  },
  {
    "full_name": "TennisVisuals/tennis-match-data",
    "name": "tennis-match-data",
    "description": "Collections of CSV files containing three levels of match data: basic, point-by-point, stats",
    "language": "",
    "topics": [],
    "readme": "# tennis-match-data\nCollections of CSV files containing three levels of match data: basic, point-by-point, stats\n",
    "url": "https://github.com/TennisVisuals/tennis-match-data",
    "last_updated": "2024-10-21T08:31:50+00:00"
  },
  {
    "full_name": "narwhals-dev/narwhals",
    "name": "narwhals",
    "description": "Lightweight and extensible compatibility layer between dataframe libraries!",
    "language": "Python",
    "topics": [
      "cudf",
      "ibis",
      "pandas",
      "polars",
      "pyarrow",
      "dask",
      "duckdb",
      "pyspark"
    ],
    "readme": "# Narwhals\n\n<h1 align=\"center\">\n\t<img\n\t\twidth=\"400\"\n\t\talt=\"narwhals_small\"\n\t\tsrc=\"https://github.com/user-attachments/assets/968545af-ea0f-48bb-8377-144e93f7abf8\">\n</h1>\n\n[![PyPI version](https://badge.fury.io/py/narwhals.svg)](https://badge.fury.io/py/narwhals)\n[![Downloads](https://static.pepy.tech/badge/narwhals/month)](https://pepy.tech/project/narwhals)\n[![Trusted publishing](https://img.shields.io/badge/Trusted_publishing-Provides_attestations-bright_green)](https://peps.python.org/pep-0740/)\n[![PYPI - Types](https://img.shields.io/pypi/types/narwhals)](https://pypi.org/project/narwhals)\n\nExtremely lightweight and extensible compatibility layer between dataframe libraries!\n\n- **Full API support**: cuDF, Modin, pandas, Polars, PyArrow.\n- **Lazy-only support**: Dask, DuckDB, Ibis, PySpark, SQLFrame. Work in progress: Daft.\n\nSeamlessly support all, without depending on any!\n\n- ✅ **Just use** [a subset of **the Polars API**](https://narwhals-dev.github.io/narwhals/api-reference/), no need to learn anything new\n- ✅ **Zero dependencies**, Narwhals only uses what\n  the user passes in so your library can stay lightweight\n- ✅ Separate **lazy** and eager APIs, use **expressions**\n- ✅ Support pandas' complicated type system and index, without\n  either getting in the way\n- ✅ **100% branch coverage**, tested against pandas and Polars nightly builds\n- ✅ **Negligible overhead**, see [overhead](https://narwhals-dev.github.io/narwhals/overhead/)\n- ✅ Let your IDE help you thanks to **full static typing**, see [typing](https://narwhals-dev.github.io/narwhals/api-reference/typing/)\n- ✅ **Perfect backwards compatibility policy**,\n  see [stable api](https://narwhals-dev.github.io/narwhals/backcompat/) for how to opt-in\n\nGet started!\n\n- [Read the documentation](https://narwhals-dev.github.io/narwhals/)\n- [Chat with us on Discord!](https://discord.gg/V3PqtB4VA4)\n- [Join our community call](https://calendar.google.com/calendar/embed?src=27ff6dc5f598c1d94c1f6e627a1aaae680e2fac88f848bda",
    "url": "https://github.com/narwhals-dev/narwhals",
    "last_updated": "2025-09-02T08:34:26+00:00"
  },
  {
    "full_name": "lucaong/minisearch",
    "name": "minisearch",
    "description": "Tiny and powerful JavaScript full-text search engine for browser and Node",
    "language": "TypeScript",
    "topics": [
      "fuzzy-search",
      "javascript",
      "search",
      "fulltext-search",
      "search-engine",
      "autosuggestion",
      "autocomplete",
      "text-search",
      "edge-computing",
      "typo-tolerance"
    ],
    "readme": "# MiniSearch\n\n[![CI Build](https://github.com/lucaong/minisearch/workflows/CI%20Build/badge.svg)](https://github.com/lucaong/minisearch/actions)\n[![Coverage Status](https://coveralls.io/repos/github/lucaong/minisearch/badge.svg?branch=master)](https://coveralls.io/github/lucaong/minisearch?branch=master)\n[![Minzipped Size](https://badgen.net/bundlephobia/minzip/minisearch)](https://bundlephobia.com/result?p=minisearch)\n[![npm](https://img.shields.io/npm/v/minisearch?color=%23ff00dd)](https://www.npmjs.com/package/minisearch)\n[![npm downloads](https://img.shields.io/npm/dw/minisearch)](https://www.npmjs.com/package/minisearch)\n[![types](https://img.shields.io/npm/types/minisearch)](https://lucaong.github.io/minisearch/classes/MiniSearch.MiniSearch.html)\n\n`MiniSearch` is a tiny but powerful in-memory fulltext search engine written in\nJavaScript. It is respectful of resources, and it can comfortably run both in\nNode and in the browser.\n\nTry out the [demo application](https://lucaong.github.io/minisearch/demo/).\n\nFind the complete [documentation and API reference\nhere](https://lucaong.github.io/minisearch/classes/MiniSearch.MiniSearch.html),\nand more background about `MiniSearch`, including a comparison with other\nsimilar libraries, in [this blog\npost](https://lucaongaro.eu/blog/2019/01/30/minisearch-client-side-fulltext-search-engine.html).\n\n`MiniSearch` follows [semantic versioning](https://semver.org/spec/v2.0.0.html),\nand documents releases and changes in the\n[changelog](https://github.com/lucaong/minisearch/blob/master/CHANGELOG.md).\n\n\n## Use case\n\n`MiniSearch` addresses use cases where full-text search features are needed\n(e.g. prefix search, fuzzy search, ranking, boosting of fields…), but the data\nto be indexed can fit locally in the process memory. While you won't index the\nwhole Internet with it, there are surprisingly many use cases that are served\nwell by `MiniSearch`. By storing the index in local memory, `MiniSearch` can\nwork offline, and can process queri",
    "url": "https://github.com/lucaong/minisearch",
    "last_updated": "2025-09-02T07:54:10+00:00"
  },
  {
    "full_name": "wch/harbor",
    "name": "harbor",
    "description": "An R package for controlling docker containers on local and remote hosts",
    "language": "R",
    "topics": [],
    "readme": "harbor\n================\n\n[![Travis-CI Build Status](https://travis-ci.org/hrbrmstr/harbor.svg?branch=master)](https://travis-ci.org/hrbrmstr/harbor)\n\nTools to Manage 'Docker' Images and Containers\n\n``` r\nlibrary(harbor)\n\ndocker_pull(image=\"hello-world\")\n```\n\n    ## Using default tag: latest\n    ## latest: Pulling from library/hello-world\n    ## 78445dd45222: Already exists\n    ## Digest: sha256:c5515758d4c5e1e838e9cd307f6c6a0d620b5e07e6f927b07d05f6d12a1ac8d7\n    ## Status: Downloaded newer image for hello-world:latest\n\n``` r\nres <- docker_run(image = \"hello-world\", capture_text = TRUE)\n\ncat(attr(res, \"output\"))\n```\n\n    ## \n    ## Hello from Docker!\n    ## This message shows that your installation appears to be working correctly.\n    ## \n    ## To generate this message, Docker took the following steps:\n    ##  1. The Docker client contacted the Docker daemon.\n    ##  2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    ##  3. The Docker daemon created a new container from that image which runs the\n    ##     executable that produces the output you are currently reading.\n    ##  4. The Docker daemon streamed that output to the Docker client, which sent it\n    ##     to your terminal.\n    ## \n    ## To try something more ambitious, you can run an Ubuntu container with:\n    ##  $ docker run -it ubuntu bash\n    ## \n    ## Share images, automate workflows, and more with a free Docker ID:\n    ##  https://cloud.docker.com/\n    ## \n    ## For more examples and ideas, visit:\n    ##  https://docs.docker.com/engine/userguide/\n",
    "url": "https://github.com/wch/harbor",
    "last_updated": "2025-08-05T22:14:01+00:00"
  },
  {
    "full_name": "GA-20/py-aws-lambda-toolkit",
    "name": "py-aws-lambda-toolkit",
    "description": "Lightweight Python toolkit for easily and quickly creating AWS Lambda functions with the Serverless Framework.",
    "language": "Python",
    "topics": [],
    "readme": "# Py-AWS-Lambda-Toolkit\n\nThis lightweight Python toolkit streamlines the creation of AWS Lambda functions using the Serverless Framework. It offers a range of features to enhance your development process:\n\n- **DynamoDB**: shortcuts: Avoid boilerplate code for DynamoDB operations.\n- **HTTP**: event processing: Get the event of the HTTP request and parse it, splitting it into the path, query string, body, headers, etc.\n- **HTTP**: response shortcuts: Create HTTP responses with the correct format and status code.\n- **JWT**: authentication: Create and verify JWT tokens as easily as possible.\n- **Logger**: Log messages with a format that is easy to read.\n- **Mappers**: Remove specified fields from a dictionary or list of dictionaries.\n- **Parser**: Convert dictionary keys to snake case or camel case.\n- **Password**: hashing: Hash passwords with salt and verify them.\n- **DynamoDB**: scan builder: Build a scan query with specified filters.\n- **Validator**: Validate a dictionary with specified rules.\n\n## Installation\n\n**Attention**: This package is currently undergoing maintenance. To test the package's modules, please access them directly from this repository.\n\nInstall the package with pip:\n\n```bash\npip install py-aws-lambda-toolkit\n```\n\n## Usage\n\nUse the package in your code:\n\n```python\nimport logging\nfrom py_aws_lambda_toolkit.http_event import process_event\nfrom py_aws_lambda_toolkit.http_response import create_response\nfrom py_aws_lambda_toolkit.status import StatusCode\nfrom py_aws_lambda_toolkit.logger import logging\n\nstatus = StatusCode()\n\ndef handler(event, context):\n    # Process the event\n    event_data = process_event(event)\n    event_body = event_data.get(\"body\", {})\n\n    logging.info(\"Event body: %s\", event_body)\n\n    # Create a response\n    response = create_response(\n        { \"ok\": True, \"message\": \"Processed event successfully\" },\n        status=status.code_200_success,\n    )\n\n    return response\n\n```\n\n## Contributing\n\nContributions are welcome! For bug rep",
    "url": "https://github.com/GA-20/py-aws-lambda-toolkit",
    "last_updated": "2025-01-15T15:29:36+00:00"
  },
  {
    "full_name": "ourresearch/unpaywall-extension",
    "name": "unpaywall-extension",
    "description": "Firefox/Chrome extension that gives you a link to a free PDF when you view scholarly articles",
    "language": "JavaScript",
    "topics": [],
    "readme": "# unpaywall\nRepository for the Unpaywall browser extension for Chrome and Firefox. More documentation is available on the website\nat https://unpaywall.org/products/extension",
    "url": "https://github.com/ourresearch/unpaywall-extension",
    "last_updated": "2025-08-23T19:29:39+00:00"
  },
  {
    "full_name": "leandropls/phonetree",
    "name": "phonetree",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# PhoneTree\n\nPhoneTree is a Python framework for creating text-based menu systems, resembling phone tree systems or rudimentary chatbots. It allows you to easily create menus and actions, manage user input and output, and maintain state between interactions.\n\n## Features\n\n- Simple decorator-based syntax for defining menus and actions\n- Optional \"ask\" and \"tell\" callbacks for handling user input and output\n- State management for passing data between menus and actions\n\n## Installation\n\nTo install PhoneTree, simply use pip:\n\n```\npip install phonetree\n```\n\n## Usage\n\nHere's an example of how to use PhoneTree to create a simple menu system:\n\n```python\nimport phonetree\nfrom phonetree import Ask, Tell\n\n@phonetree.menu()\ndef main_menu(state: dict) -> dict:\n    \"\"\"Main menu.\"\"\"\n    return {\"interactions\": state.get(\"interactions\", 0) + 1}\n\n@main_menu.menu(\"First Submenu\")\ndef first_submenu(state: dict) -> dict:\n    \"\"\"First Submenu menu.\"\"\"\n    # here goes the code that runs when you enter the submenu\n    ...\n\n    return {\"interactions\": state.get(\"interactions\", 0) + 1}\n\n@first_submenu.action(\"Do something\")\ndef do_something(state: dict, ask: Ask, tell: Tell) -> dict:\n    \"\"\"Some action\"\"\"\n    anything = ask(\"Is there anything you want to say?\")\n    print(\"user answered: \" + anything)\n    tell(\"Alright! Thank you!\")\n    return {\"interactions\": state.get(\"interactions\", 0) + 1}\n\n@first_submenu.action(\"Do something else\")\ndef do_something_else(state: dict, ask: Ask, tell: Tell) -> dict:\n    \"\"\"Some action\"\"\"\n    color = ask(\"What's your favorite color?\")\n    print(\"User said \" + color + \" is their favorite color.\")\n    tell(\"Alright! Nice to know!\")\n    return {\"interactions\": state.get(\"interactions\", 0) + 1, \"favorite_color\": color}\n\n@main_menu.menu(\"Second submenu\")\ndef second_submenu(state: dict, tell: Tell) -> dict:\n    \"\"\"Second submenu.\"\"\"\n    tell(\"Welcome to second submenu!\")\n    return {\"interactions\": state.get(\"interactions\", 0) + 1}\n```\n\n### Defining Menus and Act",
    "url": "https://github.com/leandropls/phonetree",
    "last_updated": "2025-01-15T15:29:18+00:00"
  },
  {
    "full_name": "baidu-research/warp-ctc",
    "name": "warp-ctc",
    "description": "Fast parallel CTC.",
    "language": "Cuda",
    "topics": [],
    "readme": "![Baidu Logo](/doc/baidu-research-logo-small.png)\n\n[In Chinese 中文版](README.zh_cn.md)\n\n# warp-ctc\n\nA fast parallel implementation of CTC, on both CPU and GPU.\n\n## Introduction\n\n[Connectionist Temporal Classification](http://www.cs.toronto.edu/~graves/icml_2006.pdf)\nis a loss function useful for performing supervised learning on sequence data,\nwithout needing an alignment between input data and labels.  For example, CTC\ncan be used to train\n[end-to-end](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf)\n[systems](http://arxiv.org/pdf/1408.2873v2.pdf) for\n[speech recognition](http://arxiv.org/abs/1512.02595),\nwhich is how we have been using it at Baidu's Silicon Valley AI Lab.\n\n![DSCTC](/doc/deep-speech-ctc-small.png)\n\nThe illustration above shows CTC computing the probability of an output\nsequence \"THE CAT \", as a sum over all possible alignments of input sequences\nthat could map to \"THE CAT \", taking into account that labels may be duplicated\nbecause they may stretch over several time steps of the input data (represented by\nthe spectrogram at the bottom of the image).\nComputing the sum of all such probabilities explicitly would be prohibitively costly due to the\ncombinatorics involved, but CTC uses dynamic programming to dramatically\nreduce the complexity of the computation. Because CTC is a differentiable function,\nit can be used during standard SGD training of deep neural networks.\n\nIn our lab, we focus on scaling up recurrent neural networks, and CTC loss is an\nimportant component. To make our system efficient, we parallelized the CTC\nalgorithm, as described in [this paper](http://arxiv.org/abs/1512.02595).\nThis project contains our high performance CPU and CUDA versions of the CTC loss,\nalong with bindings for [Torch](http://torch.ch/).\nThe library provides a simple C interface, so that it is easy to\nintegrate into deep learning frameworks.\n\nThis implementation has improved training scalability beyond the\nperformance improvement from a faster parallel CTC i",
    "url": "https://github.com/baidu-research/warp-ctc",
    "last_updated": "2025-09-02T02:32:19+00:00"
  },
  {
    "full_name": "21dotco/two1-python",
    "name": "two1-python",
    "description": "The 21 command line interface and two1 bitcoin library. Send and receive BTC over HTTP.",
    "language": "Python",
    "topics": [],
    "readme": "# 21: Build the Machine-Payable Web [![Build Status](https://travis-ci.org/21dotco/two1-python.svg?branch=master)](https://travis-ci.org/21dotco/two1-python)\n\n![21 logo](docs/img/21_banner.png \"21\")\n\n21 is an open source Python library and command line interface for\nquickly building machine-payable web services. It allows you to\naccomplish three major tasks:\n\n  - Get bitcoin on any device\n  - Add bitcoin micropayments to any Django or Flask app\n  - Earn bitcoin on every HTTP request\n\nThe package includes:\n\n - an HD wallet to securely manage your bitcoin\n - crypto and bitcoin libraries to build bitcoin/blockchain applications\n - a [micropayment-channels](https://21.co/learn/intro-to-micropayment-channels/) client and server\n - commands for mining, buying, and earning bitcoin, as well as requesting it from the 21 faucet\n - tools for publishing machine-payable endpoints to the [21 Marketplace](https://21.co/mkt)\n - containers that allow your machine to sell machine resources for bitcoin\n\nand much more.\n\n## Security\n\n_Please note that the 21 software is in beta_. To protect the security\nof your systems while using 21, we highly recommend you install the\nsoftware on a device other than your main laptop (e.g. 21 Bitcoin\nComputer, an old laptop, or an Amazon Virtual Machine) while the\nproduct is still in beta. You can read more security-related\ninformation [here](https://21.co/learn/security/). Please send an\nemail to [security@21.co](mailto://support@21.co) regarding any issue\nconcerning security.\n\n## Installation\n[Create an account](https://21.co) or install the library and CLI\n(python3.4+ is required):\n\n``` bash\n$ curl https://21.co | sh\n```\n\n`two1` can also be installed via pip:\n\n``` bash\n$ sudo pip3 install two1\n```\n\nStart with the command line tool:\n\n``` bash\n$ 21 help\n```\n\nThen read the [Introduction to 21](https://21.co/learn/intro-to-21/) guide\nand the `two1`\n[documentation](https://21.co/learn/#reference-21-library).\n\n## Developers\nTo edit and run the two1 source",
    "url": "https://github.com/21dotco/two1-python",
    "last_updated": "2025-03-26T15:05:34+00:00"
  },
  {
    "full_name": "mannau/boilerpipeR",
    "name": "boilerpipeR",
    "description": "Interface to the boilerpipe Java library by Christian Kohlschutter  (http://code.google.com/p/boilerpipe/)",
    "language": "R",
    "topics": [],
    "readme": "# boilerpipeR\n<!-- badges: start -->\n  [![R build status](https://github.com/mannau/boilerpipeR/workflows/R-CMD-check/badge.svg)](https://github.com/mannau/boilerpipeR/actions)\n  <!-- badges: end -->\n\n**boilerpipeR** is an R-package which provides an interface to [boilerpipe](https://github.com/kohlschutter/boilerpipe), a Java library written by Christian Kohlschütter [1]. It supports the generic extraction of main text content from HTML files and therefore removes ads, side-bars and headers from the HTML source content. The extraction heuristics from *boilerpipe* show a robust performance for a wide range of web site templates.\n\n\n## Install\nTo install the [latest version from CRAN](http://cran.r-project.org/web/packages/boilerpipeR/index.html) simply \n```python\ninstall.packages(\"boilerpipeR\")\n```\n\nUsing the **devtools** package you can easily install the latest development version of **boilerpipeR** from github with\n\n```python\nlibrary(devtools)\ninstall_github(\"mannau/boilerpipeR\")\n```\n\nWindows users need to use the following command to install from github:\n\n```python\nlibrary(devtools)\ninstall_github(\"mannau/boilerpipeR\", args = \"--no-multiarch\")\n```\n\n## Usage\nTo download and extract the main text from e.g. the R-Studio blog you can use the following commands:\n```python\nlibrary(boilerpipeR)\n\nurl <- \"http://blog.rstudio.org/2014/05/09/reshape2-1-4/\"\nmaintext <- ArticleExtractor(url, asText = FALSE)\ncat(maintext)\n```\n\n## References\n[1] Christian Kohlschütter, [Exploiting Links and Text Structure on the Web — A Quantitative Approach to Improving Search Quality](http://www.kohlschutter.com/pdf/Dissertation-Kohlschuetter.pdf), PhD Thesis\n\n## License\nboilerpipe and **boilerpipeR** are both released under the [Apache Version 2 License](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n\n",
    "url": "https://github.com/mannau/boilerpipeR",
    "last_updated": "2025-03-22T10:58:34+00:00"
  },
  {
    "full_name": "jhwjhw0123/Imbalance-XGBoost",
    "name": "Imbalance-XGBoost",
    "description": "XGBoost for label-imbalanced data: XGBoost with weighted and focal loss functions",
    "language": "Python",
    "topics": [],
    "readme": "# Imbalance-XGBoost\nThis software includes the codes of Weighted Loss and Focal Loss [1] implementation for [XGBoost](https://github.com/dmlc/xgboost) [2] in binary classification problems. The principal reason for us to use Weighted and Focal Loss functions is to address the problem of label-imbalanced data. The original XGBoost program provides a convenient way to customize the loss function, but one needs to compute the first and second order derivatives to implement them. The major contribution of the software is the derivation of the gradients and the implementations of them.\n\n<!-- ## Software Update\n**The project has been posted on github for several months, and now a correponding API on Pypi is released. Special thanks to @icegrid and @shaojunchao for help correct errors in the previous versions. The codes are now updated to version 0.7 and it now allows users to specify the weighted parameter \\alpha and focal parameter \\gamma outside the script. Also it supports higher version of XGBoost now.** <br /> -->\n\n## Software Update\n**Version 0.8.1: The package now supports early stopping, you can specify this by `early_stopping_rounds` when initializing the object.**\n\n## Version Notification\n**From version 0.7.0 on Imbalance-XGBoost starts to support higher versions of XGBoost and removes supports of versions earlier than 0.4a30(XGBoost>=0.4a30). This contradicts the previous requirement of XGBoost<=0.4a30. Please choose the version that fits your system accordingly.** <br />\n**Starting from version 0.8.1, the package now requires xgboost to have a newer version of >=1.1.1. This is due to some changes on deprecated arguments of the XGBoost.**\n\n## Installation\nInstalling with Pypi is the easiest way, you can run: <br />\n\n```\npip install imbalance-xgboost\n```\nIf you have multiple versions of Python, make sure you're using Python 3 (run with `pip3 install imbalance-xgboost`). The program are designated for Python 3.5 and 3.6. That being said, an (incomplete) test does",
    "url": "https://github.com/jhwjhw0123/Imbalance-XGBoost",
    "last_updated": "2025-08-31T17:10:44+00:00"
  },
  {
    "full_name": "leeper/textcolor",
    "name": "textcolor",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# Summarize Text Models by Visualizing a Source Text\n\n**textcolor** is a package for something...\n\n```{r}\nlibrary(\"textcolor\")\n\n# do something\n```\n\n\n## Installation\n\n[![CRAN](https://www.r-pkg.org/badges/version/textcolor)](https://cran.r-project.org/package=textcolor)\n![Downloads](https://cranlogs.r-pkg.org/badges/textcolor)\n[![Travis Build Status](https://travis-ci.org/leeper/textcolor.png?branch=master)](https://travis-ci.org/leeper/textcolor)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/PROJECTNUMBER?svg=true)](https://ci.appveyor.com/project/leeper/textcolor)\n[![codecov.io](https://codecov.io/github/leeper/textcolor/coverage.svg?branch=master)](https://codecov.io/github/leeper/textcolor?branch=master)\n\nThis package is not yet on CRAN. To install the latest development version you can install from Github:\n\n```R\nif (!require(\"ghit\")) {\n    install.packages(\"ghit\")\n}\nghit::install_github(\"leeper/textcolor\")\n```\n\n",
    "url": "https://github.com/leeper/textcolor",
    "last_updated": "2017-10-12T13:30:36+00:00"
  },
  {
    "full_name": "jimfleming/numerai",
    "name": "numerai",
    "description": "Code from my experiments on Numerai",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Numerai Experiments\n\nFolder structure:\n\n- ensemble.py - combines multiple predictions using geometric mean\n- fit_tsne.py - uses [this t-SNE implementation](https://github.com/danielfrg/tsne) for 2D embedding (does not work in 3D)\n- search_params.py - uses `RandomSearchCV` for hyperparameter search\n- tpot_test.py - runs [tpot](https://github.com/rhiever/tpot) over the data\n- tpot_pipeline.py - best tpot model\n- notebooks/ - contains Jupyter notebooks\n- bh_tsne/ - is the original C++ t-SNE implementation with scripts for converting the csvs to the format the binary expects\n- models/ - various model implementations\n  - adverarial/ - generative adversarial model that saves the learned features for each sample\n  - autoencoder/ - simple autoencoder with regular and denoising variants (also saves learned features)\n  - classifier/ - simple neural network classifier\n  - pairwise/ - pairwise model implementation described in the blog post\n  - pipeline/ - various scikit-learn models\n    - estimators.py - custom wrappers around `KernelPCA` and `Isomap` that fit on a small portion of the training samples to avoid memory errors\n    - transformers.py - contains `ItemSelector` which allows for selecting data by a key when building pipelines ([source](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html))\n    - fm.py - factorization machines\n    - lr.py - logistic regression with t-SNE features\n    - pairwise.py - sklearn variant of the pairwise model\n    - simple.py - simple logistic regression with polynomial features\n",
    "url": "https://github.com/jimfleming/numerai",
    "last_updated": "2025-07-26T18:05:38+00:00"
  },
  {
    "full_name": "ollama/ollama",
    "name": "ollama",
    "description": "Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.",
    "language": "Go",
    "topics": [
      "llama",
      "llm",
      "llama2",
      "llms",
      "go",
      "golang",
      "ollama",
      "mistral",
      "gemma",
      "llama3",
      "llava",
      "phi4",
      "deepseek",
      "gemma3",
      "qwen",
      "gemma3n",
      "gpt-oss"
    ],
    "readme": "<div align=\"center\">\n  <a href=\"https://ollama.com\">\n    <img alt=\"ollama\" width=\"240\" src=\"https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n  </a>\n</div>\n\n# Ollama\n\nGet up and running with large language models.\n\n### macOS\n\n[Download](https://ollama.com/download/Ollama.dmg)\n\n### Windows\n\n[Download](https://ollama.com/download/OllamaSetup.exe)\n\n### Linux\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n[Manual install instructions](https://github.com/ollama/ollama/blob/main/docs/linux.md)\n\n### Docker\n\nThe official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.\n\n### Libraries\n\n- [ollama-python](https://github.com/ollama/ollama-python)\n- [ollama-js](https://github.com/ollama/ollama-js)\n\n### Community\n\n- [Discord](https://discord.gg/ollama)\n- [Reddit](https://reddit.com/r/ollama)\n\n## Quickstart\n\nTo run and chat with [Gemma 3](https://ollama.com/library/gemma3):\n\n```shell\nollama run gemma3\n```\n\n## Model library\n\nOllama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library')\n\nHere are some example models that can be downloaded:\n\n| Model              | Parameters | Size  | Download                         |\n| ------------------ | ---------- | ----- | -------------------------------- |\n| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |\n| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |\n| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |\n| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |\n| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |\n| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |\n| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |\n| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |\n| Llam",
    "url": "https://github.com/ollama/ollama",
    "last_updated": "2025-09-02T09:49:42+00:00"
  },
  {
    "full_name": "cvxpy/cvxpylayers",
    "name": "cvxpylayers",
    "description": "Differentiable convex optimization layers",
    "language": "Python",
    "topics": [],
    "readme": "![cvxpylayers logo](cvxpylayers_logo.png)\n[![Build Status](https://travis-ci.org/cvxgrp/cvxpylayers.svg?branch=master)](https://travis-ci.org/cvxgrp/cvxpylayers)\n[![Build Status](https://ci.appveyor.com/api/projects/status/dhtbi9fb96mce56i/branch/master?svg=true)](https://ci.appveyor.com/project/sbarratt/cvxpylayers/branch/master)\n\n# cvxpylayers\n\ncvxpylayers is a Python library for constructing differentiable convex\noptimization layers in PyTorch, JAX, and TensorFlow using CVXPY.\nA convex optimization layer solves a parametrized convex optimization problem\nin the forward pass to produce a solution.\nIt computes the derivative of the solution with respect to\nthe parameters in the backward pass.\n\nThis library accompanies our [NeurIPS 2019 paper](https://web.stanford.edu/~boyd/papers/pdf/diff_cvxpy.pdf)\non differentiable convex optimization layers.\nFor an informal introduction to convex optimization layers, see our\n[blog post](https://locuslab.github.io/2019-10-28-cvxpylayers/).\n\nOur package uses [CVXPY](https://github.com/cvxgrp/cvxpy) for specifying\nparametrized convex optimization problems.\n\n- [Installation](#installation)\n- [Usage](#usage)\n- [Examples](#examples)\n- [Contributing](#contributing)\n- [Projects using cvxpylayers](#projects-using-cvxpylayers)\n- [License](#contributing)\n- [Citing](#citing)\n\n## Installation\n\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install\ncvxpylayers.\n\n```bash\npip install cvxpylayers\n```\n\nOur package includes convex optimization layers for\nPyTorch, JAX, and TensorFlow 2.0;\nthe layers are functionally equivalent. You will need to install\n[PyTorch](https://pytorch.org),\n[JAX](https://github.com/google/jax), or\n[TensorFlow](https://www.tensorflow.org)\nseparately, which can be done by following the instructions on their websites.\n\ncvxpylayers has the following dependencies:\n* Python 3\n* [NumPy](https://pypi.org/project/numpy/)\n* [CVXPY](https://github.com/cvxgrp/cvxpy) >= 1.1.a4\n* [PyTorch](https://pytorch.org) >= 1.0, ",
    "url": "https://github.com/cvxpy/cvxpylayers",
    "last_updated": "2025-09-02T05:52:14+00:00"
  },
  {
    "full_name": "dell-research-harvard/linktransformer",
    "name": "linktransformer",
    "description": "A convenient way to link, deduplicate, aggregate and cluster data(frames) in Python using deep learning",
    "language": "Python",
    "topics": [
      "deep-learning",
      "entity-matching",
      "entity-resolution",
      "huggingface-transformers",
      "nlp",
      "python",
      "record-linkage",
      "sentence-transformers",
      "transformers"
    ],
    "readme": "# LinkTransformer\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![arXiv](https://img.shields.io/badge/arXiv-2309.00789-b31b1b.svg)](https://arxiv.org/abs/2309.00789)\n\n![Linkktransformers demo](https://github.com/dell-research-harvard/linktransformer/assets/60428420/15162828-b0fb-4ee3-8a0f-fdf3371d10a0)\n\n\nLinkTransformer is a Python library for merging and deduplicating data frames using language model embeddings. It leverages popular Sentence Transformer (or any HuggingFace) models to generate embeddings for text data and provides functions to perform efficient 1:1, 1:m, and m:1 merges based on the similarity of embeddings. Additionally, the package includes utilities for clustering and data preprocessing. It also includes modifications to Sentence Transformers that allow for logging training runs on weights and biases.\n\n- [Paper](https://arxiv.org/abs/2309.00789)\n- [Website](https://linktransformer.github.io/)\n- [Demo Video](https://www.youtube.com/watch?v=Sn47nmCvV9M)\n- Tutorials\n  + [Link Records with LinkTransformer](https://colab.research.google.com/drive/1OqUB8sqpUvrnC8oa_1RoOUzV6DaAKL4N?usp=sharing)\n  + [Train your own LinkTransformer Model](https://colab.research.google.com/drive/1tHitPGjMMI2Nvh4wwA8rdcbYfbLaJDvg?usp=sharing)\n  + [Classify text with LinkTransformer](https://colab.research.google.com/drive/1hSh_p8j7LP2RfdtxrPslOfnogC_CbYw5?usp=sharing)\n- [Feature Deck](https://www.dropbox.com/scl/fi/dquxru8bndlyf9na14cw6/A-python-package-to-do-easy-record-linkage-using-Transformer-models.pdf?rlkey=fiv7j6c0vgl901y940054eptk&dl=0)\n\nMore tutorials are coming soon!\n\n## News\n\n- If you like this resource and want to help build it further, we would appreciate contributions. Currently, we wish to make the API compatible with the latest sentence-transformers updates. \n- A bug on package import on Google Colab is now fixed. \n- If you are just looking to demo the package on your data, try out this [Hugg",
    "url": "https://github.com/dell-research-harvard/linktransformer",
    "last_updated": "2025-08-24T18:45:26+00:00"
  },
  {
    "full_name": "minimaxir/big-list-of-naughty-strings",
    "name": "big-list-of-naughty-strings",
    "description": "The Big List of Naughty Strings is a list of strings which have a high probability of causing issues when used as user-input data.",
    "language": "Python",
    "topics": [],
    "readme": "# Big List of Naughty Strings\nThe Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer [walks into a bar](http://www.sempf.net/post/On-Testing1).\n\n## Why Test Naughty Strings?\n\nEven multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a [zero-width space](https://en.wikipedia.org/wiki/Zero-width_space) (U+200B) on Twitter:\n\n![](http://i.imgur.com/HyDg2eV.gif)\n\nAlthough this is not a malicious error, and typical users aren't Tweeting weird unicode, an \"internal server error\" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.\n\n## Usage\n\n`blns.txt` consists of newline-delimited strings and comments which are preceded with `#`. The comments divide the strings into sections for easy manual reading and copy/pasting into input forms. For those who want to access the strings programmatically, a `blns.json` file is provided containing an array with all the comments stripped out (the `scripts` folder contains a Python script used to generate the `blns.json`).\n\n## Contributions\n\nFeel free to send a pull request to add more strings, or additional sections. However, please do not send pull requests with very-long strings (255+ characters), as that makes the list much more difficult to view.\n\nLikewise, please do not send pull requests which compromise *manual usability of the file*. This includes the [EICAR test string](https://en.wikipedia.org/wiki/EICAR_test_file), which can cause the file to be flagged by antivirus scanners, and files which alter the encoding of `blns.txt`. Also, do not send a null character (U+0000) string, as it [chang",
    "url": "https://github.com/minimaxir/big-list-of-naughty-strings",
    "last_updated": "2025-09-01T21:54:39+00:00"
  },
  {
    "full_name": "MuseumofModernArt/collection",
    "name": "collection",
    "description": "The Museum of Modern Art (MoMA) collection data",
    "language": "",
    "topics": [],
    "readme": "The Museum of Modern Art (MoMA) Collection\n===================\n\nThe Museum of Modern Art (MoMA) acquired its first artworks in 1929, the year it was established. Today, the Museum’s evolving collection contains almost 200,000 works from around the world spanning the last 150 years. The collection includes an ever-expanding range of visual expression, including painting, sculpture, printmaking, drawing, photography, architecture, design, film, and media and performance art.\n\nMoMA is committed to helping everyone understand, enjoy, and use our collection. The Museum’s [website](http://www.moma.org/collection) features 105,272 artworks from 28,005 artists. This research dataset contains 159,773 records, representing all of the works that have been accessioned into MoMA’s collection and cataloged in our database. It includes basic metadata for each work, including title, artist, date made, medium, dimensions, and date acquired by the Museum. Some of these records have incomplete information and are noted as “not Curator Approved.”\n\nThe Artists dataset contains 15,721 records, representing all the artists who have work in MoMA's collection and have been cataloged in our database. It includes basic metadata for each artist, including name, nationality, gender, birth year, death year, Wiki QID, and Getty ULAN ID.\n\nAt this time, both datasets are available in CSV format, encoded in UTF-8. While UTF-8 is the standard for multilingual character encodings, it is not correctly interpreted by Excel on a Mac. Users of Excel on a Mac can convert the UTF-8 to UTF-16 so the file can be imported correctly. The datasets are also available in JSON.\n\nThis datasets are placed in the public domain using a [CC0 License](https://creativecommons.org/publicdomain/zero/1.0/).\n\nFor a roundup of how people have used our data so far, visit our [Medium post](https://medium.com/@foe/here-s-a-roundup-of-how-people-have-used-our-data-so-far-80862e4ce220#.f6272outn). We love adding to the list, so ple",
    "url": "https://github.com/MuseumofModernArt/collection",
    "last_updated": "2025-09-02T08:57:20+00:00"
  },
  {
    "full_name": "hrbrmstr/aquarium",
    "name": "aquarium",
    "description": "🐟🐠🐡🎣 Validate 'Phishing' 'URLs' with the 'PhishTank' Service",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "phishing",
      "phishtank",
      "r-cyber"
    ],
    "readme": "\n# aquarium\n\nValidate ‘Phishing’ ‘URLs’ with the ‘PhishTank’ Service\n\n## Description\n\n‘PhishTank’ \\<www.phishtank.com\\> is a free community site where anyone\ncan submit, verify, track and share ‘phishing’ data. Methods are\nprovided to test if a ‘URL’ is classsified as a ‘phishing’ site and to\ndownload aggregated ‘phishing’ ‘URL’ databases.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `pt_check_url`: Check an individual URL against PhishTank’s database\n  - `pt_read_db`: Retrieve a complete copy of the current PhishTank\n    database\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/aquarium\")\n```\n\n## Usage\n\n``` r\nlibrary(aquarium)\nlibrary(hrbrthemes)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"aquarium\")\n```\n\n    ## [1] '0.1.0'\n\n### Test a URL\n\n``` r\nx <- pt_check_url(\"http://www.seer.revpsi.org/hhh/1/\")\n\nx\n```\n\n    ## # A tibble: 1 x 11\n    ##   timestamp           serverid status requestid  url   in_database phish_id phish_detail_pa… verified verified_at valid\n    ##   <dttm>              <chr>    <lgl>  <chr>      <chr> <chr>       <chr>    <chr>            <lgl>    <chr>       <lgl>\n    ## 1 2018-05-06 12:54:41 a8fc4c9b NA     172.31.97… http… TRUE        5604930  http://www.phis… TRUE     2018-04-27… TRUE\n\n``` r\nglimpse(x)\n```\n\n    ## Observations: 1\n    ## Variables: 11\n    ## $ timestamp         <dttm> 2018-05-06 12:54:41\n    ## $ serverid          <chr> \"a8fc4c9b\"\n    ## $ status            <lgl> NA\n    ## $ requestid         <chr> \"172.31.97.117.5aef3351720d56.76914316\"\n    ## $ url               <chr> \"http://www.seer.revpsi.org/hhh/1/\"\n    ## $ in_database       <chr> \"TRUE\"\n    ## $ phish_id          <chr> \"5604930\"\n    ## $ phish_detail_page <chr> \"http://www.phishtank.com/phish_detail.php?phish_id=5604930\"\n    ## $ verified          <lgl> TRUE\n    ## $ verified_at       <chr> \"2018-04-27T19:38:43+00:00\"\n    ## $ valid             <lgl> TRUE\n\n### Get the databases\n\n``` r\nx <- pt_read_db(.progress = FALSE)\n\nx\n",
    "url": "https://github.com/hrbrmstr/aquarium",
    "last_updated": "2025-03-22T11:07:16+00:00"
  },
  {
    "full_name": "soodoku/nireland",
    "name": "nireland",
    "description": "Replication Data And Scripts for How Can You Think That?: Deliberation and the Learning of Opposing Arguments",
    "language": "R",
    "topics": [],
    "readme": "## Replication Data And Scripts For \"How Can You Think That?: Deliberation and the Learning of Opposing Arguments\"\n\n### Data\n\n* [NINIS](data/ninis/)\n* [Original Data](data/orig_data)\n* [Open Ended](data/open_ended)\n\n### Scripts\n\n* [Scripts](scripts/)\n\n### Paper\n\n* [Paper](https://github.com/soodoku/ireland/tree/main/ms)\n\n\n",
    "url": "https://github.com/soodoku/nireland",
    "last_updated": "2022-04-29T04:57:17+00:00"
  },
  {
    "full_name": "SMAPPNYU/smapp-toolkit",
    "name": "smapp-toolkit",
    "description": "Python library for interacting with smapp collections",
    "language": "Python",
    "topics": [],
    "readme": "```\n                                       _              _ _    _ _   \n ___ _ __ ___   __ _ _ __  _ __       | |_ ___   ___ | | | _(_) |_ \n/ __| '_ ` _ \\ / _` | '_ \\| '_ \\ _____| __/ _ \\ / _ \\| | |/ / | __|\n\\__ \\ | | | | | (_| | |_) | |_) |_____| || (_) | (_) | |   <| | |_ \n|___/_| |_| |_|\\__,_| .__/| .__/       \\__\\___/ \\___/|_|_|\\_\\_|\\__|\n                    |_|   |_|                                      \n```\n\nThis is an user-friendly python package for interfacing with large collections of tweets. Developped at the SMaPP lab at New York University.\n\n\n- [MongoTweetCollection](https://github.com/SMAPPNYU/smapp-toolkit#mongotweetcollection)\n- [BSONTweetCollection](https://github.com/SMAPPNYU/smapp-toolkit#bsontweetcollection)\n- [Shared Collection Functions](https://github.com/SMAPPNYU/smapp-toolkit#shared-collection-functions)\n  - [containing](https://github.com/SMAPPNYU/smapp-toolkit#containing)\n  - [count](https://github.com/SMAPPNYU/smapp-toolkit#count)\n  - [texts](https://github.com/SMAPPNYU/smapp-toolkit#texts)\n  - [term_counts](https://github.com/SMAPPNYU/smapp-toolkit#term_counts)\n  - [sample](https://github.com/SMAPPNYU/smapp-toolkit#sample)\n  - [apply_labels](https://github.com/SMAPPNYU/smapp-toolkit#apply_labels)\n  - [since](https://github.com/SMAPPNYU/smapp-toolkit#since)\n  - [until](https://github.com/SMAPPNYU/smapp-toolkit#until)\n  - [language](https://github.com/SMAPPNYU/smapp-toolkit#language)\n  - [user_lang_contains](https://github.com/SMAPPNYU/smapp-toolkit#user_lang_contains)\n  - [excluding_retweets](https://github.com/SMAPPNYU/smapp-toolkit#excluding_retweets)\n  - [user_location_containing](https://github.com/SMAPPNYU/smapp-toolkit#user_location_containing)\n  - [field_containing](https://github.com/SMAPPNYU/smapp-toolkit#field_containing)\n  - [geo_enabled](https://github.com/SMAPPNYU/smapp-toolkit#geo_enabled)\n  - [non_geo_enabled](https://github.com/SMAPPNYU/smapp-toolkit#non_geo_enabled)\n  - [limit](https://github.com/SMAPPNYU/smapp-toolkit#lim",
    "url": "https://github.com/SMAPPNYU/smapp-toolkit",
    "last_updated": "2023-09-24T05:17:54+00:00"
  },
  {
    "full_name": "dharmaturtle/LexisNexisMassEmailer",
    "name": "LexisNexisMassEmailer",
    "description": "Emails Lexis Nexis search results with attached documents",
    "language": "Python",
    "topics": [],
    "readme": "# Lexis Nexis Mass Emailer\n\nThis uses a headless browser (Selenium/PhantomJS) to email articles in Lexis Nexis's database about specified subjects between specified date ranges to an email address. The email is necessary because downloading files with PhantomJS is virtually impossible. This script uses parallel processing to expedite matters. Included are support scripts to download the attachments and rename them according to their contents.\n\n## Caution\n\n**Be careful when running multiple instances of this program. You may be blacklisted from Lexis Nexis. This has happened before!**\n\nI successfully ran 3 instances of this script (12 concurrent threads, given the 4 cores) without getting black listed. Your mileage may vary. Just because you're running this on a university network does **NOT** mean your traffic will go unnoticed by Lexis Nexis.\n\n## Programming style\n\nThis was written primarily for personal use, so some conventions like line length limits are ignored.\n\n## How to use\n\nYou will need to run this on a pre-authorized network like a university connection. You may try to use your university or institution's VPN to run this at home. This program accepts input from four text files, `1.txt`, `2.txt`, `3.txt`, and `4.txt`. Format of the text files should be as follows: `UnitedStates,('04/11/2002', '04/11/2002')`. Essentially `\"countryname\",\"tuple of date range\"`. You will then want to download the files from the emails with `dl_attachments.py` and optionally rename the documents according to their contents with `rename.py`\n",
    "url": "https://github.com/dharmaturtle/LexisNexisMassEmailer",
    "last_updated": "2024-01-24T18:48:16+00:00"
  },
  {
    "full_name": "arslanbilal/git-cheat-sheet",
    "name": "git-cheat-sheet",
    "description": ":octocat: git and git flow cheat sheet",
    "language": "",
    "topics": [
      "git-flow",
      "git",
      "cheatsheet",
      "github"
    ],
    "readme": "# Git and Git Flow Cheat Sheet \n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n<p align=\"center\">\n    <img alt=\"Git\" src=\"./Img/git-logo.png\" height=\"190\" width=\"455\">\n</p>\n\n---\n\n## 📖 About\n\nThis comprehensive Git cheat sheet helps you master Git commands without memorizing everything. Whether you're a beginner or an experienced developer, this guide provides quick reference to essential Git operations.\n\n**Contributions Welcome!** Feel free to:\n- Fix grammar mistakes\n- Add new commands\n- Translate to your language\n- Improve explanations\n\n---\n## 📋 Table of Contents\n\n- [🔧 Setup](#-setup)\n- [⚙️ Configuration Files](#️-configuration-files)\n- [🆕 Create Repository](#-create-repository)\n- [📝 Local Changes](#-local-changes)\n- [🔍 Search](#-search)\n- [📖 Commit History](#-commit-history)\n- [📁 Move / Rename](#-move--rename)\n- [🌿 Branches & Tags](#-branches--tags)\n- [🔄 Update & Publish](#-update--publish)\n- [🔀 Merge & Rebase](#-merge--rebase)\n- [↩️ Undo](#️-undo)\n- [🌊 Git Flow](#-git-flow)\n- [🌍 Other Languages](#-other-languages)\n\n---\n\n## 🔧 Setup\n\n### View Configuration\n\n**Show current configuration:**\n```bash\ngit config --list\n```\n\n**Show repository configuration:**\n```bash\ngit config --local --list\n```\n\n**Show global configuration:**\n```bash\ngit config --global --list\n```\n\n**Show system configuration:**\n```bash\ngit config --system --list\n```\n\n### User Configuration\n\n**Set your name for version history:**\n```bash\ngit config --global user.name \"[firstname lastname]\"\n```\n\n**Set your email address:**\n```bash\ngit config --global user.email \"[valid-email]\"\n```\n\n### Display & Editor Settings\n\n**Enable automatic command line coloring:**\n```bash\ngit config --global color.ui auto\n```\n\n**Set global editor for commits:**\n```bash\ngit config --global core.editor vi\n```\n\n---\n\n## ⚙️ Configuration Files\n\n| Scope | Location | Command Flag |\n|-------|----------|--------------|\n| **Reposi",
    "url": "https://github.com/arslanbilal/git-cheat-sheet",
    "last_updated": "2025-08-31T18:00:09+00:00"
  },
  {
    "full_name": "dwillis/wapo-alerts",
    "name": "wapo-alerts",
    "description": "Data on washingtonpost.com alerts",
    "language": "Python",
    "topics": [],
    "readme": "# wapo-alerts\nData on washingtonpost.com alerts\n",
    "url": "https://github.com/dwillis/wapo-alerts",
    "last_updated": "2025-09-02T10:02:02+00:00"
  },
  {
    "full_name": "protectai/nbdefense",
    "name": "nbdefense",
    "description": "Secure Jupyter Notebooks and Experimentation Environment",
    "language": "Python",
    "topics": [
      "jupyter-notebook",
      "security-tools"
    ],
    "readme": "# 🛡️ NB Defense\n\n[![bandit](https://github.com/protectai/nbdefense/actions/workflows/bandit.yml/badge.svg)](https://github.com/protectai/nbdefense/actions/workflows/bandit.yml)\n[![build](https://github.com/protectai/nbdefense/actions/workflows/build.yml/badge.svg)](https://github.com/protectai/nbdefense/actions/workflows/build.yml)\n[![black](https://github.com/protectai/nbdefense/actions/workflows/black.yml/badge.svg)](https://github.com/protectai/nbdefense/actions/workflows/black.yml)\n[![mypy](https://github.com/protectai/nbdefense/actions/workflows/mypy.yml/badge.svg)](https://github.com/protectai/nbdefense/actions/workflows/mypy.yml)\n[![tests](https://github.com/protectai/nbdefense/actions/workflows/test.yml/badge.svg)](https://github.com/protectai/nbdefense/actions/workflows/test.yml)\n[![License: Apache 2.0](https://img.shields.io/crates/l/apa)](https://opensource.org/license/apache-2-0/)\n\n## 🏃‍♀️ Quick Start\n\n```bash\npip install nbdefense\n```\n\n## 🙋‍♂️ What is NB Defense?\n\nBrought to you by Protect AI, NB Defense is a CLI tool and SDK that encourages you to think about security throughout every step of your machine learning development process. You can use nbdefense to scan for [Secrets](https://github.com/protectai/nbdefense/blob/main/docs/docs/supported-scans/detecting-secrets.md), [Personally Identifiable Information (PII)](https://github.com/protectai/nbdefense/blob/main/docs/docs/supported-scans/detecting-PII.md), [Common Vulnerabilities and Exposures(CVE)](https://github.com/protectai/nbdefense/blob/main/docs/docs/supported-scans/detecting-CVEs.md), and [Licenses](https://github.com/protectai/nbdefense/blob/main/docs/docs/supported-scans/detecting-licenses.md) in your notebook and dependency files.\n\nNB Defense also acts as a SDK for our [Jupyter Lab Extension](https://github.com/protectai/nbdefense-jupyter). Visit our [documentation](https://github.com/protectai/nbdefense/tree/main/docs/docs), or the repository below to learn more.\n\n- [Jupyter Lab Extensio",
    "url": "https://github.com/protectai/nbdefense",
    "last_updated": "2025-08-30T07:02:58+00:00"
  },
  {
    "full_name": "rdrr1990/keras",
    "name": "keras",
    "description": "R Interface to Keras",
    "language": "HTML",
    "topics": [],
    "readme": "# R interface to Keras\n\n![](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\n[![Travis-CI Build Status](https://travis-ci.org/rstudio/keras.svg?branch=master)](https://travis-ci.org/rstudio/keras) \n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/keras)](http://cran.r-project.org/package=keras)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/fchollet/keras/blob/master/LICENSE)\n\n[Keras](https://keras.io/) is a high-level neural networks API developed with a focus on enabling fast experimentation. *Being able to go from idea to result with the least possible delay is key to doing good research.* Keras has the following key features:\n\n- Allows the same code to run on CPU or on GPU, seamlessly.\n\n- User-friendly API which makes it easy to quickly prototype deep learning models.\n\n- Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\n\n- Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\n\n- Is capable of running on top of multiple back-ends including [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/cntk), or [Theano](https://github.com/Theano/Theano).\n\nSee the package website at <https://keras.rstudio.com> for complete documentation.\n\n",
    "url": "https://github.com/rdrr1990/keras",
    "last_updated": "2021-09-23T19:34:26+00:00"
  },
  {
    "full_name": "nikhgarg/EmbeddingDynamicStereotypes",
    "name": "EmbeddingDynamicStereotypes",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "This repository contains code and data associated with [Word embeddings quantify 100 years of gender and ethnic stereotypes.](https://doi.org/10.1073/pnas.1720347115) PDF available [here](http://gargnikhil.com/files/pdfs/GSJZ18_embedstereotypes.pdf).\n\nIf you use the content in this repository, please cite:\n\nGarg, N., Schiebinger, L., Jurafsky, D. & Zou, J. Word embeddings quantify 100 years of gender and ethnic stereotypes. PNAS 201720347 (2018). doi:10.1073/pnas.1720347115\n\nTo re-run all analyses and plots:\n1. download vectors from online sources and normalize by l2 norm (links in paper and below)\n2. set up parameters to run as in run_params.csv\n3. run changes_over_time.py\n4. run create_final_plots_all.py\n\ndataset_utilities/ contains various helper scripts to preprocess files and create word vectors. From a corpus, for example LDC95T21-North-American-News, that contains many text files (each containing an article) from a given year, first run create_yrly_datasets.py to create a single text file per year (with only valid words). Then, run pipeline.py on each of these files to create vectors, potentially combining multiple years into a single training set. normalize_vectors.py contains utilities to standardize the vectors.\n\nWe have uploaded the New York Times embeddings generated for this paper. They are available at [http://stanford.edu/~nkgarg/NYTembeddings/](http://stanford.edu/~nkgarg/NYTembeddings/). 2021/04/05 update: Unfortunately, the files are no longer available. (Upon my graduation the links died, before I was able to back them up). However, the original text data is still available at [New York Times Annotated Corpus](https://catalog.ldc.upenn.edu/LDC2008T19), and so the the vectors can be trained as described in the paper. \n\nWe use the following embeddings publicly available online. If you use these embeddings, please cite the associated papers.\n\n1. [Google News, word2vec](https://code.google.com/archive/p/word2vec/)\n2. [Genre-Balanced American English (",
    "url": "https://github.com/nikhgarg/EmbeddingDynamicStereotypes",
    "last_updated": "2025-06-05T08:12:11+00:00"
  },
  {
    "full_name": "plausible/analytics",
    "name": "analytics",
    "description": "Simple, open source, lightweight and privacy-friendly web analytics alternative to Google Analytics.",
    "language": "Elixir",
    "topics": [
      "analytics",
      "privacy",
      "elixir",
      "phoenix",
      "postgresql",
      "tailwindcss",
      "clickhouse",
      "plausible-analytics",
      "google-analytics",
      "web-analytics",
      "website-stats",
      "cloud",
      "charts",
      "statistics",
      "metrics",
      "website-analytics",
      "website",
      "marketing"
    ],
    "readme": "# Plausible Analytics\n\n<p align=\"center\">\n  <a href=\"https://plausible.io/\">\n    <img src=\"https://raw.githubusercontent.com/plausible/docs/master/static/img/plausible-analytics-icon-top.png\" width=\"140px\" alt=\"Plausible Analytics\" />\n  </a>\n</p>\n<p align=\"center\">\n    <a href=\"https://plausible.io/simple-web-analytics\">Simple Metrics</a> |\n    <a href=\"https://plausible.io/lightweight-web-analytics\">Lightweight Script</a> |\n    <a href=\"https://plausible.io/privacy-focused-web-analytics\">Privacy Focused</a> |\n    <a href=\"https://plausible.io/open-source-website-analytics\">Open Source</a> |\n    <a href=\"https://plausible.io/docs\">Docs</a> |\n    <a href=\"https://github.com/plausible/analytics/blob/master/CONTRIBUTING.md\">Contributing</a>\n    <br /><br />\n</p>\n\n[Plausible Analytics](https://plausible.io/) is an easy to use, lightweight, open source and privacy-friendly alternative to Google Analytics. It doesn’t use cookies and is fully compliant with GDPR, CCPA and PECR. You can self-host Plausible Community Edition or have us manage Plausible Analytics for you in the cloud. Here's [the live demo of our own website stats](https://plausible.io/plausible.io). Made and hosted in the EU 🇪🇺\n\nWe are dedicated to making web analytics more privacy-friendly. Our mission is to reduce corporate surveillance by providing an alternative web analytics tool which doesn’t come from the AdTech world. We are completely independent and solely funded by our subscribers.\n\n![Plausible Analytics](https://plausible.io/docs/img/plausible-analytics.png)\n\n## Why Plausible?\n\nHere's what makes Plausible a great Google Analytics alternative and why we're trusted by thousands of paying subscribers to deliver their website and business insights:\n\n- **Clutter Free**: Plausible Analytics provides [simple web analytics](https://plausible.io/simple-web-analytics) and it cuts through the noise. No layers of menus, no need for custom reports. Get all the important insights on one single page. No trainin",
    "url": "https://github.com/plausible/analytics",
    "last_updated": "2025-09-02T09:49:05+00:00"
  },
  {
    "full_name": "bumble-tech/private-detector",
    "name": "private-detector",
    "description": "Bumble's Private Detector - a pretrained model for detecting lewd images",
    "language": "Python",
    "topics": [
      "bumble",
      "efficientnet",
      "image-classification",
      "tensorflow"
    ],
    "readme": "# Private Detector\n\nThis is the repo for Bumble's *Private Detector*™ model - an image classifier that can detect lewd images.\n\nThe internal repo has been heavily refactored and released as a fully open-source project to allow for the wider community to use and finetune a Private Detector model of their own. You can download the pretrained SavedModel, [Frozen Model](https://github.com/bumble-tech/private-detector/issues/7) and checkpoint [here](https://storage.googleapis.com/private_detector/private_detector_with_frozen.zip)\n\n## Model\n\nThe SavedModel can be found in `saved_model/` within `private_detector.zip` above\n\nThe model is based on Efficientnet-v2 and trained on our internal dataset of lewd images - more information can be found at the whitepaper [here](https://bumble.com/en/the-buzz/bumble-open-source-private-detector-ai-cyberflashing-dick-pics) or [here](https://medium.com/bumble-tech/bumble-inc-open-sources-private-detector-and-makes-another-step-towards-a-safer-internet-for-women-8e6cdb111d81)\n\n## Inference\n\nInference is pretty simple and an example has been given in `inference.py`. The model is released as a SavedModel so it can be deployed in many different ways, but here's a quick runthrough of one way to get it working for those less familiar with Python/Tensorflow.\n\nFirst you need to install [Python](https://www.python.org/downloads/) and [Conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) on your system and go to the Terminal/Command Prompt on your machine\n\nThen you can use the `environment.yaml` file to install the necessary packages to run the inference.\n\n```sh\nconda env create -f environment.yaml\nconda activate private_detector\n```\n\nOnce that's set up, you can run the inference script. Simply replace the sample `.jpg` file paths below with your own\n\n```sh\npython3 inference.py \\\n    --model saved_model/ \\\n    --image_paths \\\n        Yes_samples/1.jpg \\\n        Yes_samples/2.jpg \\\n        Yes_samples/3.jpg \\\n        Yes",
    "url": "https://github.com/bumble-tech/private-detector",
    "last_updated": "2025-08-27T12:32:07+00:00"
  },
  {
    "full_name": "andosa/treeinterpreter",
    "name": "treeinterpreter",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "===============================\nTreeInterpreter\n===============================\n\nPackage for interpreting scikit-learn's decision tree and random forest predictions.\nAllows decomposing each prediction into bias and feature contribution components as described in http://blog.datadive.net/interpreting-random-forests/. For a dataset with ``n`` features, each prediction on the dataset is decomposed  as ``prediction = bias + feature_1_contribution + ... + feature_n_contribution``.\n\nIt works on scikit-learn's\n\n* DecisionTreeRegressor\n* DecisionTreeClassifier\n* ExtraTreeRegressor\n* ExtraTreeClassifier\n* RandomForestRegressor\n* RandomForestClassifier\n* ExtraTreesRegressor\n* ExtraTreesClassifier\n\nFree software: BSD license\n\nDependencies\n------------\n\n- scikit-learn 0.17+\n\n\nInstallation\n------------\nThe easiest way to install the package is via ``pip``::\n\n    $ pip install treeinterpreter\n\nUsage\n-----\n::\n\n from treeinterpreter import treeinterpreter as ti\n # fit a scikit-learn's regressor model\n rf = RandomForestRegressor()\n rf.fit(trainX, trainY)\n \n prediction, bias, contributions = ti.predict(rf, testX)\n \nPrediction is the sum of bias and feature contributions::\n \n assert(numpy.allclose(prediction, bias + np.sum(contributions, axis=1)))\n assert(numpy.allclose(rf.predict(testX), bias + np.sum(contributions, axis=1)))\n\n\nMore usage examples at http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/.\n\n \n",
    "url": "https://github.com/andosa/treeinterpreter",
    "last_updated": "2025-07-04T13:52:38+00:00"
  },
  {
    "full_name": "jmarshallnz/talks",
    "name": "talks",
    "description": "Statistics talks by Jonathan Marshall",
    "language": "HTML",
    "topics": [],
    "readme": "## Jonathan's talks ##\n\nA simple repository containing the source (markdown, tex etc.) of various talks I've done.\n\nMost of these are done using [RStudio](http://rstudio.org) (or at least the [rmarkdown package](http://rmarkdown.rstudio.com)).\n\nFeel free to use these as a basis for your own talks.\n",
    "url": "https://github.com/jmarshallnz/talks",
    "last_updated": "2025-03-16T12:45:31+00:00"
  },
  {
    "full_name": "swarm-lab/Rvision",
    "name": "Rvision",
    "description": "Basic computer vision library for R",
    "language": "HTML",
    "topics": [
      "r",
      "computer-vision",
      "opencv"
    ],
    "readme": "# Rvision - A computer vision library for R <img src=\"man/figures/logo.png\" align=\"right\" alt=\"\" width=\"120\" />\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/swarm-lab/Rvision/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/swarm-lab/Rvision/actions/workflows/R-CMD-check.yaml)\n[![Codecov test coverage](https://codecov.io/gh/swarm-lab/Rvision/branch/master/graph/badge.svg)](https://app.codecov.io/gh/swarm-lab/Rvision?branch=master)\n[![CRAN status](https://www.r-pkg.org/badges/version/Rvision)](https://CRAN.R-project.org/package=Rvision)\n[![CRANLOGS downloads](https://cranlogs.r-pkg.org/badges/Rvision)](https://cran.r-project.org/package=Rvision)\n[![DOI](https://zenodo.org/badge/62589719.svg)](https://zenodo.org/badge/latestdoi/62589719)\n<!-- badges: end -->\n\n## Description\n\n[`Rvision`](https://github.com/swarm-lab/Rvision) is a growing computer vision \nlibrary for [`R`](https://cran.r-project.org). It is based on the powerful \n[`OpenCV`](http://opencv.org/) library for C/C++, the state-of-the-art for \ncomputer vision in the open source world. \n\nThe ultimate goal of [`Rvision`](https://github.com/swarm-lab/Rvision) is to \nprovide `R` users with all the necessary functions to read and manipulate images, \nvideos and camera streams, with an emphasis on speed (thanks to `OpenCV`). In \nthis respect, it is different from all the other image manipulations packages \nfor `R` that either can not quickly and directly access frames from videos and \ncamera streams or are limited in their processing speed and/or volume. \n\n---\n\n## Quick start guides \n\n+ [1 - Installation instructions](https://swarm-lab.github.io/Rvision/articles/z1_install.html)\n+ [2 - Input/output operations](https://swarm-lab.github.io/Rvision/articles/z2_io.html)\n+ [3 - Basic operations](https://swarm-lab.github.io/Rvision/articles/z3_basic.html)\n+ [4 - In-place, copy, and target operations](https://swarm-lab.github.io/Rvision/articles/z4_inplace.html)\n+ [5 - GPU operations](https:/",
    "url": "https://github.com/swarm-lab/Rvision",
    "last_updated": "2025-08-25T16:34:43+00:00"
  },
  {
    "full_name": "berkeley-scf/gpu-workshop-2016",
    "name": "gpu-workshop-2016",
    "description": "Materials for workshop on GPU computation for statistics, data science, machine learning applications. ",
    "language": "HTML",
    "topics": [],
    "readme": "# gpu-workshop-2016\nMaterials for workshop on GPU computation for statistics, data science, machine learning applications. Please see gpu.html to be guided through the materials.\n\nSession 1: Monday, Feb. 1, 4:10 - 5:30 pm in Evans 1011\n* Introduction to GPU resources that are available (Savio, Amazon EC2)\n* Basics of using GPUs with C, R, and Python\n\nSession 2: Monday Feb. 8, 4:10 - 5:30 pm in Evans 1011\n* Use of packages such as Caffe, TensorFlow, etc. that use GPUs for\nback-end computation\n* Discussion of use cases by those using GPUs currently\n* Optimizing GPU usage\n\nThe workshop will be an introduction to using GPUs and will assume no\nprevious knowledge of GPUs. I will assume familiarity with either R,\nC, or Python and at least modest familiarity with operating in a UNIX\nenvironment. The goal is to get folks up to speed on using GPUs, and\nwe'll cover basic techniques for using a GPU with R, C, and Python.\n\n",
    "url": "https://github.com/berkeley-scf/gpu-workshop-2016",
    "last_updated": "2022-04-30T21:04:45+00:00"
  },
  {
    "full_name": "StylishThemes/GitHub-Dark",
    "name": "GitHub-Dark",
    "description": ":octocat: Dark GitHub style",
    "language": "CSS",
    "topics": [
      "theme",
      "css",
      "awesome",
      "dark-theme",
      "github",
      "userstyle",
      "usercss",
      "dark"
    ],
    "readme": "<p align=\"center\">\n  <img alt=\"githubdark-logo\" src=\"https://cdn.jsdelivr.net/gh/StylishThemes/logos@master/github.dark/githubdark-mini.svg\" width=\"580\">\n  <br>\n  <a href=\"https://github.com/StylishThemes/GitHub-Dark/tags\">\n    <img src=\"https://img.shields.io/github/tag/StylishThemes/GitHub-Dark.svg?label=version&style=flat\" alt=\"Version\">\n  </a>\n  <a href=\"https://github.com/StylishThemes/GitHub-Dark/stargazers\">\n    <img src=\"https://img.shields.io/github/stars/StylishThemes/GitHub-Dark.svg?style=flat\" alt=\"Stars\">\n  </a>\n  <a href=\"https://github.com/StylishThemes/GitHub-Dark/network\">\n    <img src=\"https://img.shields.io/github/forks/StylishThemes/GitHub-Dark.svg?style=flat\" alt=\"Forks\">\n  </a>\n  <a href=\"https://david-dm.org/StylishThemes/GitHub-Dark?type=dev\">\n    <img src=\"https://img.shields.io/david/dev/StylishThemes/GitHub-Dark.svg?label=devDependencies&style=flat\" alt=\"devDependencies\">\n  </a>\n  <a href=\"https://gitter.im/StylishThemes/GitHub-Dark\">\n    <img src=\"https://img.shields.io/gitter/room/StylishThemes/Github-Dark.js.svg?maxAge=2592000&style=flat\" alt=\"Gitter\">\n  </a>\n</p>\n<h2 align=\"center\">Your eyes will&nbsp;:heart:&nbsp;you.</h2>\n\n# TOC\n  * [Preview](#preview)\n  * [Installation](#installation)\n    * [Additional Userstyles](#additional-userstyles)\n    * [Supported GitHub Extensions](#supported-github-extensions)\n    * [Available Syntax Highlighting Themes](#available-syntax-highlighting-themes-demo)\n  * [Contributions and Development](#contributions-and-development)\n    * [Auto generated CSS](#auto-generated-css)\n    * [Manual override entries](#manual-override-entries)\n    * [Make targets](#make-targets)\n  * [Notes](#notes)\n\n\n## Preview\n\n![Preview of GitHub Dark](./images/screenshot.png)\n\n## Installation\n\n1. Install [Stylus for Firefox](https://addons.mozilla.org/en-US/firefox/addon/styl-us/), [Chrome](https://chrome.google.com/webstore/detail/stylus/clngdbkpkpeebahjckkjfobafhncgmne), [Opera](https://addons.opera.com/en-gb/extensions/details",
    "url": "https://github.com/StylishThemes/GitHub-Dark",
    "last_updated": "2025-09-01T12:54:02+00:00"
  },
  {
    "full_name": "wilkelab/cowplot",
    "name": "cowplot",
    "description": "cowplot: Streamlined Plot Theme and Plot Annotations for ggplot2",
    "language": "R",
    "topics": [],
    "readme": "<img width=\"120px\" alt=\"cowplot logo\" align=\"right\" src=\"man/figures/logo.png\">\n\n# cowplot – Streamlined plot theme and plot annotations for ggplot2\n\n<!-- badges: start -->\n[![R build status](https://github.com/wilkelab/cowplot/workflows/R-CMD-check/badge.svg)](https://github.com/wilkelab/cowplot/actions)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/cowplot)](https://CRAN.R-project.org/package=cowplot)\n[![CRAN\\_Downloads\\_Badge](https://cranlogs.r-pkg.org/badges/cowplot)](https://cranlogs.r-pkg.org/downloads/total/last-month/cowplot)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2533860.svg)](https://doi.org/10.5281/zenodo.2533860)\n\n<!-- badges: end -->\n\nThe cowplot package provides various features that help with creating publication-quality figures, such as a set of themes, functions to align plots and arrange them into complex compound figures, and functions that make it easy to annotate plots and or mix plots with images. The package was originally written for internal use in the Wilke lab, hence the name (Claus O. Wilke's plot package). It has also been used extensively in the book  [Fundamentals of Data Visualization.](https://clauswilke.com/dataviz/)\n\n# Installation\n\nThe cowplot package is available on [CRAN](https://cran.r-project.org/package=cowplot) and can be installed via\n\n    install.packages(\"cowplot\")\n    \nTo install the latest development version of the package using the devtools package, enter the following in your R console:\n\n    remotes::install_github(\"wilkelab/cowplot\")\n\n# Usage\n\nTo get a quick introduction to the main features of this package, read the [introductory vignette.](https://wilkelab.org/cowplot/articles/introduction.html) For a more in-depth discussion, read [all vignettes](https://wilkelab.org/cowplot/articles/index.html) and/or the [reference documentation.](https://wilkelab.org/cowplot/reference/index.html)\n\n",
    "url": "https://github.com/wilkelab/cowplot",
    "last_updated": "2025-08-25T16:29:13+00:00"
  },
  {
    "full_name": "ro31337/libretaxi",
    "name": "libretaxi",
    "description": "Open source Uber #deleteuber",
    "language": "Go",
    "topics": [
      "telegram",
      "ridesharing",
      "uber",
      "lyft",
      "transportation",
      "golang"
    ],
    "readme": "# LibreTaxi v2\n\nLibreTaxi is open-source Uber proof-of-concept that works through [Telegram](https://telegram.org/).\n\nSee it in action: https://t.me/libretaxi_bot\n\nPublic feed: https://t.me/libretaxi_all\n\nIt is closer to Craigslist rideshare rather than Uber, but it works, and works great! The app that is easy to use, supports multiple languages, fast and cool. There are tens of thousands users worldwide, and we're on the way to 1M users. So please spread the word!\n\n* [How it works - English](https://telegra.ph/LibreTaxi-20---you-will-love-it-02-02)\n* [How it works - Spanish](https://telegra.ph/LibreTaxi-20---te-va-a-enamorar-02-09)\n* [How it works - Russian](https://telegra.ph/Novaya-versiya-Libre-taksi-vam-ponravitsya-02-08)\n* [How it works - Portuguese](https://telegra.ph/LibreTaxi-20---Vai-o-amar-02-12)\n\n## Prerequisites\n\n1. [Install Go](https://golang.org/doc/install)\n2. [Install Go dep](https://github.com/golang/dep)\n3. Download the repo to `~/go/src/libretaxi`\n4. Install Docker with docker-compose\n5. Run PostgreSQL and RabbitMQ with default credentials (see connection strings below)\n```\ndocker-compose up -d\n```\n\n## Setting up RabbitMQ (for development and production)\n\n`rabbitmq:3-management` contains UI plugin for queue management. Plugin port is 8080 (15672 in container).\nLogin **guest/guest**.\n\nLogin to RabbitUI here: http://localhost:8080\n\nThere is only one queue at the moment:\n\n* `messages` queue, http://localhost:8080/#/queues/%2F/messages - picked up by message handler, enqueued by libretaxi\n\nNote that there is one message producer, and one message consumer threads (goroutines) in application.\n\nPort 5672 is RabbitMQ itself.\n\n## LibreTaxi settings\n\nInit settings for `./libretaxi.yml`:\n\n```\ntelegram_token: YOUR_TOKEN\ndb_conn_str: postgres://libretaxi:libretaxi@localhost:15432/libretaxi\nrabbit_url: amqp://127.0.0.1:8079/\nadmin_channel_chat_id: -1001324105405\npublic_channel_chat_id: -1001470847849\n```\n\nAdmin channel is the place where you shadow ban spamers",
    "url": "https://github.com/ro31337/libretaxi",
    "last_updated": "2025-09-01T05:06:16+00:00"
  },
  {
    "full_name": "trangptm/DeepCare",
    "name": "DeepCare",
    "description": "A Deep Dynamic Memory Model for Predictive Medicine",
    "language": "Python",
    "topics": [],
    "readme": "# DeepCare\nDeepCare is a deep dynamic model that reads EMR data, infer disease progression and predict future outcome.\n4 tasks are implemented:\n  - Disease progression: predict diagnoses of the next readmission\n  - Intervention recommendation: predict procedures/medications for a set of diagnoses\n  - Readmission prediction: predict if a patient will re-admit within a period\n  - High-risk prediction: predict if a patient is in high risk (3 unplanned readmissions within a period)\n  \nDeepCare uses a LSTM to model the patient's history. It treats each patient as a sequence of admissions. Unlike a typical LSTM model, DeepCare reads input from multiple sources: diagnoses, interventions (procedures/medications), admission time and admission type (unplanned or planned).\n\nLink to the paper:\nhttps://arxiv.org/abs/1602.00357\n",
    "url": "https://github.com/trangptm/DeepCare",
    "last_updated": "2025-08-10T21:24:31+00:00"
  },
  {
    "full_name": "soodoku/party_time",
    "name": "party_time",
    "description": "Replication Data and Scripts for Affect, Not Ideology: A Social Identity Perspective on Polarization",
    "language": "Scheme",
    "topics": [],
    "readme": "## Party Time: Replication Data for Affect, Not Ideology: A Social Identity Perspective on Polarization\n\nThe replication scripts have been prepared in conjunction with [Rob Lytle](https://github.com/RobLytle).\n\nWe replicate the main descriptive findings of the [paper](http://gsood.com/research/papers/AffectNotIdeology.pdf): Tables 1 and 2, and Figures 1--4.\n\n## Data and Codebooks\n\n1. [YG 2010 10 Nation](data/10nat/)\n2. [YG 2008](data/rivers/)\n3. [ANES](data/anes/)\n4. [Verba](data/verba/)\n\n----\nData for campaign effects (no corresponding scripts)\n\n5. [AP Yahoo!](data/ap_yahoo/)\n6. [Blair](data/blair/)\n7. [Wisconsin Ads 2004](data/wiscads/)\n\n## Replication Scripts\n\n1. [ANES Time Series Trimming](scripts/01_anes_cdf_trim.R): Trims [raw ANES file](data/anes/raw/anes_timeseries_cdf_dta.zip) to a more manageable size&mdash;the file needs to be zipped, but takes too long to extract each time the code is changed.\n2. [ANES Time Series Recoding](scripts/02_wrangle_cdf.R): The bulk of the recoding work for the ANES data is done here. 1. [ANES Time Series Recoding](scripts/01_anes_ts_recode.R)\n3. [Figure 1](scripts/03_fig1.R)\n4. [Figure 2](scripts/04_fig2.R)\n5. [Figure 3](scripts/05_fig3.R)\n6. [Figure 3](scripts/06_fig4.R)\n7. [Table 1](scripts/07_tab1.R)\n\nNote: Some of the scripts rely on the [goji package](https://github.com/soodoku/goji).\n\n## Quality-of-Life Scripts\n\n1. [Makefile](Makefile): Executes all scripts and compiles PDF.\n2. [Makefile DAG](makefile-dag.R): Creates a diagram of dependencies for [`Makefile`](Makefile).\n\n## Document\n\n1. [Replication PDF](doc/isl-2012-replication.pdf): PDF containing replications of figures in [Affect, Not Idoelogy](http://gsood.com/research/papers/AffectNotIdeology.pdf).\n2. [Replication LaTeX](doc/isl-2012-replication.tex): Code for the PDF.\n",
    "url": "https://github.com/soodoku/party_time",
    "last_updated": "2021-01-11T06:58:33+00:00"
  },
  {
    "full_name": "StatsReporting/stargazer",
    "name": "stargazer",
    "description": "Python implementation of the R stargazer multiple regression model creation tool",
    "language": "Jupyter Notebook",
    "topics": [
      "stargazer",
      "latex",
      "python",
      "regression-models"
    ],
    "readme": "# Stargazer\n\nThis is a python port of the R stargazer package that can be found [on CRAN](https://CRAN.R-project.org/package=stargazer). I was disappointed that there wasn't equivalent functionality in any python packages I was aware of so I'm re-implementing it here.\n\nThere is an experimental function in the [statsmodels.regression.linear_model.OLSResults.summary2](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.summary2.html) that can report single regression model results in HTML/CSV/LaTeX/etc, but it still didn't quite fulfill what I was looking for.\n\nThe python package is object oriented now with chained commands to make changes to the rendering parameters, which is hopefully more pythonic and the user doesn't have to put a bunch of arguments in a single function.\n\n## Installation\n\nYou can install this package through PyPi with `pip install stargazer` or just clone the repo and take the `stargazer.py` file since it's the only one in the package.\n\n### Dependencies\n\nIt depends on `statsmodels`, which in turn depends on several other libraries like `pandas`, `numpy`, etc\n\n## Editing Features\n\nThis library implements many of the customization features found in the original package. Examples of most can be found [in the examples jupyter notebook](https://github.com/StatsReporting/stargazer/blob/master/examples.ipynb) and a full list of the methods/features is here below:\n\n* `title`: custom title\n* `show_header`: display or hide model header data\n* `show_model_numbers`: display or hide model numbers\n* `custom_columns`: custom model names and model groupings\n* `significance_levels`: change statistical significance thresholds\n* `significant_digits`: change number of significant digits\n* `show_confidence_intervals`: display confidence intervals instead of variance\n* `dependent_variable_name`: rename dependent variable\n* `rename_covariates`: rename covariates\n* `covariate_order`: reorder covariates\n* `reset_covariate_order`: reset ",
    "url": "https://github.com/StatsReporting/stargazer",
    "last_updated": "2025-08-31T12:45:30+00:00"
  },
  {
    "full_name": "microsoft/r-server-hospital-length-of-stay",
    "name": "r-server-hospital-length-of-stay",
    "description": "Hospital length of stay prediction solution with Microsoft R Server ",
    "language": "TSQL",
    "topics": [],
    "readme": "<img src=\"Resources/Images/los.jpg\" align=\"right\" height=\"200px\">\n\n# Predicting Length of Stay in Hospitals\nPredicting patients length of stay, an important scenario in Healthcare.\n[![Deploy to Azure](https://raw.githubusercontent.com/Azure/Azure-CortanaIntelligence-SolutionAuthoringWorkspace/master/docs/images/DeployToAzure.PNG)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FMicrosoft%2Fr-server-hospital-length-of-stay%2Fmaster%2FArmTemplates%2Fhospital_arm.json)\n\n## More samples and information\n> Discover more examples at [Microsoft Machine Learning Server](https://github.com/Microsoft/ML-Server)\n\nFor all documentation, visit the [Hospital Length of Stay website](https://microsoft.github.io/r-server-hospital-length-of-stay/).\n\n**NOTE:** Please don't use \"Download ZIP\" to get this repository, as it will change the line endings in the data files. Use \"git clone\" to get a local copy of this repository. \n\n## Contributing\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n",
    "url": "https://github.com/microsoft/r-server-hospital-length-of-stay",
    "last_updated": "2025-03-29T18:38:03+00:00"
  },
  {
    "full_name": "dbohdan/structured-text-tools",
    "name": "structured-text-tools",
    "description": "A list of command-line tools for manipulating structured text data",
    "language": "",
    "topics": [
      "awk",
      "list",
      "csv",
      "tsv",
      "delimited-files",
      "text-files",
      "json",
      "yaml",
      "toml",
      "xml",
      "html",
      "ini",
      "structured-text",
      "conversion",
      "configuration-file",
      "cli"
    ],
    "readme": "# Structured text tools\n\nThe following is a list of text-based file formats and command-line tools for manipulating each.\n\n\n## Contents\n\n- [awk-like](#awk-like)\n  - [awk](#awk)\n  - [POSIX commands](#posix-commands)\n  - [SQL-based tools](#sql-based-tools)\n  - [Other tools](#other-tools)\n- [CSV](#csv)\n- [HTML](#html)\n- [JSON](#json)\n- [Markdown](#markdown)\n- [TOML](#toml)\n- [XML](#xml)\n- [YAML](#yaml)\n- [Configuration files](#configuration-files)\n  - [.env](#env)\n  - [/etc/hosts](#etchosts)\n  - [INI](#ini)\n  - [Multiple formats](#multiple-formats)\n- [Log files](#log-files)\n- [Multiformat tools](#multiformat-tools)\n- [Templating for structured text](#templating-for-structured-text)\n- [Extra: interactive TUIs](#extra-interactive-tuis)\n- [Extra: CLIs for single-file databases](#extra-clis-for-single-file-databases)\n- [License](#license)\n- [Disclosure](#disclosure)\n\n\n## awk-like\n\nTools that work with lines of fields separated by delimiters but do not necessarily support [CSV field quoting](https://en.wikipedia.org/wiki/Comma-separated_values#Basic_rules).\n\n### awk\n\nAWK/awk is a programming language and a POSIX-standard command-line tool. (You will sometimes see \"awk\" used for the tool and \"AWK\" for the language. This document follows this convention. GNU Awk uses \"Awk\".) If you run Linux, macOS, or a BSD, you almost certainly have it installed. See below for Windows.\n\n- If you already know how to program, the nawk [man page](https://www.freebsd.org/cgi/man.cgi?query=nawk&sektion=1) is a great way to learn AWK quickly. What you learn from it will apply to other implementations on different platforms. Read it first if you feel overwhelmed by the sheer size of the [GNU Awk manual](https://www.gnu.org/software/gawk/manual/gawk.html).\n- [awk.info archive](https://web.archive.org/web/20160505033644/http://awk.info/) **—** an extensive resource on Awk.\n- [\"AWK Vs NAWK Vs GAWK\"](https://www.thegeekstuff.com/2011/06/awk-nawk-gawk/) **—** a comparison of features present in differe",
    "url": "https://github.com/dbohdan/structured-text-tools",
    "last_updated": "2025-08-30T12:44:37+00:00"
  },
  {
    "full_name": "NoahFinberg/djangostarterproject",
    "name": "djangostarterproject",
    "description": "A simple dockerized Django starter app with allauth, postgres, and an sbadmin bootstrap template.",
    "language": "CSS",
    "topics": [],
    "readme": "# Django Starter Project\n\nMost webapps have a common initial configuration -- and there is no reason to spend hours doing the exact same config over and over for each new project. This is the first in a series of starter Django projects that follow common starting app patterns.\n\nFollowing this repo will enable you to setup a standard development environment, webapp, and deploy to Heroku in less than 5 minutes and with fewer than ten lines of code (seriously) -- thank you Docker.\n\nFeel free to follow along with this [Youtube video](https://www.youtube.com/watch?v=4seIp_kaaWM), where I walk through the below setup. If you find this repo helpful, please give it a star and/or like the Youtube video and subscribe to our [Django in Minutes](https://www.youtube.com/channel/UCeXWf8ttB2Q8-xTmEoTDkZg) channel. We'll be building many more starter apps. Thanks so much!\n\n## Resources\nThis starter app leverages the following resources, but you don't need to worry about installing and configuring most of these. Just see prerequisites below for what you need on your machine to make this work:\n\n  - [Docker](https://www.docker.com/) - for virtual development environment and easy deployment\n  - [Allauth](https://github.com/pennersr/django-allauth) - for user authentication\n  - [Psycogpg](https://github.com/psycopg/psycopg2) - python client for postgres db\n  - [sbadmin2](https://startbootstrap.com/theme/sb-admin-2) - A Free Bootstrap Template from [Start Bootstrap](https://startbootstrap.com/).\n  - [Heroku](http://www.heroku.com/) - for very easy production deployment.\n\n## Prerequisites\nAs a prerequisite please make sure you have the following tools already installed on your machine:\n1. [Git](https://git-scm.com/)\n2. [Docker](https://docs.docker.com/get-docker/)\n3. [Docker Compose](https://docs.docker.com/compose/install/)\n4. [Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli) - Note: you'll need to create a Heroku account and there may be a cost associated with the servers ",
    "url": "https://github.com/NoahFinberg/djangostarterproject",
    "last_updated": "2025-08-28T14:50:31+00:00"
  },
  {
    "full_name": "rajashekar/WakeWordDetector",
    "name": "WakeWordDetector",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "<h1 style=\"text-align: center;\">Wake Word Detector</h1>\n<img src=\"images/wake_word_detect.png\">\n\n# Table of Contents\n- [Background](#background)\n- [Introduction](#introduction)\n- [Related Work](#related-work)\n- [Implementation](#implementation)\n    - [Preparing labelled dataset](#preparing-labelled-dataset)\n    - [Word Alignment](#word-alignment)\n    - [Fix data imbalance](#fix-data-imbalance)\n    - [Extract audio features](#extract-audio-features)\n    - [Audio transformations](#audio-transformations)\n    - [Define model architecture](#define-model-architecture)\n    - [Train model](#train-model)\n    - [Test Model](#test-model)\n    - [Inference](#inference)\n        - [Using Pyaudio](#using-pyaudio)\n        - [Using web sockets](#using-web-sockets)\n        - [Using onnx](#using-onnx)\n        - [Using tensorflowjs](#using-tensorflowjs)\n        - [Using tflite](#using-tflite)\n- [Demo](#demo)\n- [Slides](#slides)\n- [Conclusion](#conclusion)\n- [Enhancements](#enhancements)\n\n# Background\nPersonal Assistant devices like Google Home, Alexa and Apple Homepod, will be constantly listening for specific set of wake words like “Ok, Google” or “Alexa” or “Hey Siri”, and once these sequence of words are detected it would prompt to user for next commands and respond to them appropriately.\n\n# Introduction\nTo create a open-source custom wake word detector, which will take audio as input and once the sequence of words are detected then prompt to the user. <br>\n\nGoal is to provide configurable custom detector so that anyone can use it on their own application to perform operations, once configured wake words are detected.\n\n# Related Work\n- Firefox Voice \n    - Model was trained using Mozilla Common Voice dataset, used Pytorch (refer paper [Howl](https://arxiv.org/abs/2008.09606)) library to extract audio features and to train model on res8. Custom logic MeydaMelSpectrogram was used to train the model. \n    - Used [Meyda: an audio feature extraction library for the Web\nAudio API](http://d",
    "url": "https://github.com/rajashekar/WakeWordDetector",
    "last_updated": "2025-08-09T23:42:30+00:00"
  },
  {
    "full_name": "max-mapper/linux",
    "name": "linux",
    "description": "run Linux on Yosemite easily from the CLI",
    "language": "JavaScript",
    "topics": [],
    "readme": "# linux\n\n**beta software! proceed with caution**\n\nDownload, install and run Linux on OS X in less than 60 seconds!\n\nnpm installs [hypercore linux](https://github.com/maxogden/hypercore-linux) and runs it as a daemon using the new Mac OS Yosemite hypervisor (via [hyperkit](https://github.com/moby/hyperkit)).\n\nSee [this youtube video](https://www.youtube.com/watch?v=esNlno79dBw) for a demonstration with a cool soundtrack.\n\nThis module is a low level component that is part of HyperOS, made by the team working on the [Dat](http://dat-data.com/) data version control tool. We are working on integrating the other HyperOS components to support advanced functionality like running containers, sharing filesystems etc.\n\nMac OS Yosemite only for now, Windows support coming later through Hyper-V integration (see [this issue](https://github.com/maxogden/linux/issues/4) if you wanna help)\n\n**WARNING**\n-----------\n - hyperkit is a very new project, expect bugs! You must be running OS X 10.10.3 Yosemite or later and 2010 or later Mac for this to work.\n - if you happen to be running any version of VirtualBox prior to 4.3.30 or 5.0 then hyperkit will crash your system either if VirtualBox is running or had been run previously after the last reboot (see xhyve's issues [#5](mist64/xhyve#5) and [#9](mist64/xhyve#9) for the full context). So, if you are unable to update VirtualBox to version 4.3.30 or 5, or later, and were using it in your current session please do restart your Mac before attempting to run xhyve.\n - (these warnings were borrowed from [coreos-xhyve](https://github.com/coreos/coreos-xhyve))\n \n[![js-standard-style](https://cdn.rawgit.com/feross/standard/master/badge.svg)](https://github.com/feross/standard)\n[![Build Status](https://travis-ci.org/maxogden/linux.svg?branch=master)](https://travis-ci.org/maxogden/linux)\n[![dat](http://img.shields.io/badge/Development%20sponsored%20by-dat-green.svg?style=flat)](http://dat-data.com/)\n\n### installation\n\n```\nnpm install linux -g\n```",
    "url": "https://github.com/max-mapper/linux",
    "last_updated": "2025-06-25T09:27:40+00:00"
  },
  {
    "full_name": "r-prof/jointprof",
    "name": "jointprof",
    "description": "Joint profiling of native and R code",
    "language": "C++",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\njointprof\n=========\n\n[![Travis build status](https://travis-ci.org/r-prof/jointprof.svg?branch=master)](https://travis-ci.org/r-prof/jointprof) [![Coverage status](https://codecov.io/gh/r-prof/jointprof/branch/master/graph/badge.svg)](https://codecov.io/github/r-prof/jointprof?branch=master) [![CRAN status](http://www.r-pkg.org/badges/version/jointprof)](https://cran.r-project.org/package=jointprof)\n\nThe goal of jointprof is to assist profiling R packages that include native code (C++, C, Fortran, ...). It collects profiling output simultaneously using [Rprof](https://www.rdocumentation.org/packages/utils/versions/3.4.3/topics/Rprof) and [gperftools](https://github.com/gperftools/gperftools) and provides a unified view of profiling data.\n\nSee the [guide](https://r-prof.github.io/jointprof/articles/guide.html) for a more detailed overview, or take a look at the [internals](https://r-prof.github.io/jointprof/articles/internals.html) document if you're curious.\n\nInstallation\n------------\n\n### Ubuntu\n\nOther Linux distributions may work if you install the right system dependencies ([let me know](https://github.com/r-prof/jointprof/issues) which!).\n\n1.  Install system dependencies:\n\n    ``` sh\n    sudo apt install \\\n      libgoogle-perftools-dev \\\n      libprotoc-dev libprotobuf-dev protobuf-compiler \\\n      golang-go \\\n      graphviz\n    ```\n\n2.  Install `pprof`:\n\n    ``` sh\n    go get github.com/google/pprof\n    ```\n\n3.  Install the package:\n\n    ``` r\n    # install.packages(\"remotes\")\n    remotes::install_github(\"r-prof/jointprof\")\n    ```\n\n### OS X\n\n1.  Install system dependencies:\n\n    ``` sh\n    brew install graphviz\n    ```\n\n2.  Install `gperftools` (currently from a branch, [pull request](https://github.com/gperftools/gperftools/pull/1004) pending):\n\n    ``` sh\n    git clone https://github.com/krlmlr/gperftools.git -b f-export-stack\n    cd gperftools\n    ./autogen.sh\n    ./configure\n    make\n ",
    "url": "https://github.com/r-prof/jointprof",
    "last_updated": "2025-03-22T10:55:12+00:00"
  },
  {
    "full_name": "gojiplus/notary",
    "name": "notary",
    "description": "PyPI Attestation Shield Generation Service",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Notary: PyPI Digital Attestation Shield Generation Service\n\nExample: https://notarypy.soodoku.workers.dev/badge/pydantic/2.7.2/pydantic-2.7.2-py3-none-any.whl\n\nUses the [PyPI Integrity API](https://docs.pypi.org/api/integrity/), e.g., https://pypi.org/integrity/ethnicolr/0.14.0/ethnicolr-0.14.0-py3-none-any.whl/provenance, and produces a Shields Badge based on that.\n\n# NotaryPy Badge Generator\n\nA badge generator for Python package attestations on PyPI, deployed as a Cloudflare Worker.\n\n## What it does\n\nThis service generates badges that indicate whether a Python package on PyPI has provenance attestations. It uses Shields.io to render the badges and provides a simple API endpoint to check attestation status.\n\n## Usage\n\nAdd a badge to your README by using this URL format:\n\n```markdown\n![PyPI Attestation](https://notarypy.soodoku.workers.dev/badge/PACKAGE_NAME/VERSION/FILENAME)\n```\n\nFor example:\n\n```markdown\n![PyPI Attestation](https://notarypy.soodoku.workers.dev/badge/pydantic/2.7.2/pydantic-2.7.2-py3-none-any.whl)\n```\n\n## Badge States\n\n- **Verified** (green): Package has provenance attestations\n- **None** (red): Package does not have provenance attestations\n\n## Development\n\n### Prerequisites\n\n- [Node.js](https://nodejs.org/) (v18 or higher recommended)\n- [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/)\n- Cloudflare account\n\n### Local Development\n\n1. Clone this repository\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Run the worker locally:\n   ```\n   npm run dev\n   ```\n4. Test the badge endpoint at http://localhost:8787/badge/pydantic/2.7.2/pydantic-2.7.2-py3-none-any.whl\n\n### Deployment\n\nDeploy to Cloudflare Workers:\n\n```\nnpm run deploy\n```\n\n## License\n\nMIT\n\nMade with ❤️ by GojiPlus\n",
    "url": "https://github.com/gojiplus/notary",
    "last_updated": "2025-08-23T16:17:25+00:00"
  },
  {
    "full_name": "turi-code/SFrame",
    "name": "SFrame",
    "description": "SFrame: Scalable tabular and graph data-structures built for out-of-core data analysis and machine learning.",
    "language": "C++",
    "topics": [],
    "readme": "# **THIS REPOSITORY IS DEPRECATED.**\n# THE FUNCTIONALITY HAS MOVED TO [Turi Create](https://github.com/apple/turicreate). \n\n____\n\nSFrame\n======\n<a href=\"https://pypi.python.org/pypi/SFrame\">\n   <img src=\"https://img.shields.io/pypi/v/SFrame.svg\" alt=\"latest release\" />\n</a>\n<a href=\"https://travis-ci.org/turi-code/SFrame\">\n   <img src=\"https://travis-ci.org/turi-code/SFrame.svg?branch=master\" alt=\"travis build status\" />\n</a>\n\nScalable tabular (**SFrame**, **SArray**) and graph (**SGraph**) data-structures built for out-of-core data analysis. \n\nThe SFrame package provides the complete implementation of:\n - SFrame\n - SArray\n - SGraph \n - The C++ SDK surface area (gl_sframe, gl_sarray, gl_sgraph)\n\nIntroduction\n------------\n\nThe SFrame contains the open source components [GraphLab Create](https://turi.com/products/create/) from [Turi](https://turi.com). \n\nSome documentation to help get started:\n- [Getting started with SFrame](https://turi.com/learn/gallery/notebooks/introduction_to_sframes.html)\n- [SFrame user guide](https://turi.com/learn/userguide/sframe/tabular-data.html)\n- [SGraph user guide](https://turi.com/learn/userguide/sgraph/sgraph.html) \n\nFor more details on GraphLab Create (including documentation and tutorials) see http://turi.com.\n\nSome of the key features of this package are.\n\n- A scalable column compressed disk-backed dataframe optimized for machine learning and data science needs.\n- Designed for both **tabular** (SFrame, SArray) as well as **graph** data (SGraph)\n- Support for **strictly typed** columns (int, float, str, datetime), **weakly typed** columns (schema free lists, dictionaries) as well as **specialized types** such as Image.\n- Uniform support for **missing data**.\n- Query optimization and Lazy evaluation.\n- A C++ API (gl_sarray, gl_sframe, gl_sgraph) with direct native access via the C++ SDK.\n- A Python API (SArray, SFrame, SGraph) with an indirect access via an interprocess layer.\n\nLicense\n-------\nThe SFrame Package is licensed under a BS",
    "url": "https://github.com/turi-code/SFrame",
    "last_updated": "2025-08-22T06:51:55+00:00"
  },
  {
    "full_name": "unclecode/crawl4ai",
    "name": "crawl4ai",
    "description": "🚀🤖 Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN",
    "language": "Python",
    "topics": [],
    "readme": "# 🚀🤖 Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper.\n\n<div align=\"center\">\n\n<a href=\"https://trendshift.io/repositories/11716\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11716\" alt=\"unclecode%2Fcrawl4ai | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)\n[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)\n\n[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)\n[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)\n[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/unclecode?style=flat&logo=GitHub-Sponsors&label=Sponsors&color=pink)](https://github.com/sponsors/unclecode)\n\n<p align=\"center\">\n    <a href=\"https://x.com/crawl4ai\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white\" alt=\"Follow on X\" />\n    </a>\n    <a href=\"https://www.linkedin.com/company/crawl4ai\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"Follow on LinkedIn\" />\n    </a>\n    <a href=\"https://discord.gg/jP8KfhDhyN\">\n      <img src=\"https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Join our Discord\" />\n    </a>\n  </p>\n</div>\n\nCrawl4AI turns the web into clean, LLM ready Markdown for RAG, agents, and data pipelines. Fast, controllable, battle tested by a 50k+ star community.\n\n[✨ Check out latest update v0.7.4](#-recent-updates)\n\n✨ New in v0.7.4: Revolutionary LLM Table Extraction with intelligent chunking, enhanced con",
    "url": "https://github.com/unclecode/crawl4ai",
    "last_updated": "2025-09-02T10:01:53+00:00"
  },
  {
    "full_name": "kylebutts/latex-templates",
    "name": "latex-templates",
    "description": "latex templates",
    "language": "TeX",
    "topics": [],
    "readme": "# Latex templates\n\nThis is a repository of all my latex templates. I've done my best to document every line of code for these themes so that you can understand what it does and delete/modify things as you see fit. You can see a live preview of each one here:\n\n[Slides](https://raw.githack.com/kylebutts/latex-templates/main/latex-slides/slides.pdf) | \n[Article](https://raw.githack.com/kylebutts/latex-templates/main/latex-article/article.pdf) | \n[Referee Report](https://raw.githack.com/kylebutts/latex-templates/main/referee-response/responses.pdf)\n\n\n## Latex Slides \n\n<!-- latex-slides -->\n[<img src=\"thumbnails/latex-slides-1.png\" style=\"width: 100.0%\" />](thumbnails/latex-slides-1.png)\n[<img src=\"thumbnails/latex-slides-2.png\" style=\"width: 100.0%\" />](thumbnails/latex-slides-2.png)\n[<img src=\"thumbnails/latex-slides-3.png\" style=\"width: 100.0%\" />](thumbnails/latex-slides-3.png)\n<!-- latex-slides -->\n\nThis latex slide aims for nice typography, minimal aesthetics so the focus is on the content, and a set of helpful commands including easily colored words, full-size image frames, table highlighting, multi-column layouts, and more. The theme is highly customizable with (1) a well commented `slides.sty` file so you can tweak it and make it your own and (2) by defining the colors in the top of your document:\n\n```latex\n% Define `accent`/`accent2` colors for theme customization\n\\definecolor{accent}{HTML}{006896}\n\\definecolor{accent2}{HTML}{E64173}\n\\usepackage{slides}\n```\n\n\n## Latex Article\n\n<!-- latex-article -->\n[<img src=\"thumbnails/latex-article-1.png\" style=\"width: 100.0%\" />](thumbnails/latex-article-1.png)\n[<img src=\"thumbnails/latex-article-2.png\" style=\"width: 100.0%\" />](thumbnails/latex-article-2.png)\n<!-- latex-article -->\n\n\n\nThis is how I write my papers. There's not many special bells and whistles, but I think it looks nice and clean. \n\n### Math commands\n\nI have a set of math commands in `math.sty` that borrow heavily from [Pascal Michaillat](https://github.com/",
    "url": "https://github.com/kylebutts/latex-templates",
    "last_updated": "2025-08-26T06:47:41+00:00"
  },
  {
    "full_name": "soodoku/biocong",
    "name": "biocong",
    "description": "Biographical data on members of congress (105th --- 115th). ",
    "language": "Jupyter Notebook",
    "topics": [
      "congress",
      "biography"
    ],
    "readme": "## Congressional Biographies\n\n\n## 97th ---  104th Congress\n\nWe use text from the [pdfs](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NZPPJM) (downloaded from Google Books, from where these are freely available) and then parse the text.\n\n### Scripts\n\n1. [parse](scripts/parse-biocong-from-text-all.ipynb)\n2. [clean](scripts/clean-biocong-from-text-all.ipynb)\n\n## 105th --- 115th Congress\n\nWe scrape congressional biographies for 105th to the 115th Congress from the [Congressional Directory](https://www.govinfo.gov/app/collection/cdir/). We download the biographical files, e.g.,  https://www.govinfo.gov/content/pkg/CDIR-2018-10-29/html/CDIR-2018-10-29-STATISTICALINFORMATION-2.htm and parse them to extract information such as birthdate, number of children, education, etc.  \n\n### Scripts\n\n1. [Scrapes the Congressional Directory](scripts/biocong.ipynb) produces [biocong.csv](data/biocong.csv), [biocong-browsepath.csv](data/biocong-browsepath.csv), and [html files (tar.gz)](data/cong_bio_1997_2018.tar.gz) \n2. [Download Congressional Biographies Using the API](scripts/biocong-api.ipynb) provides the script for downloading the data using the API. (It produces incomplete data so we don't use this script.)\n3. [Parse](scripts/03_parse-biocong.ipynb) iterates through biocong-browsepath.csv and parses the [html files (tar.gz)](data/cong_bio_1997_2018.tar.gz) and produces [biocong-parsed.csv](data/biocong-parsed.csv)\n4. [Clean](scripts/04_clean-biocong.ipynb) takes biocong-parsed.csv produces [biocong-cleaned.csv](data/biocong-cleaned.csv)\n\n### Data\n\nThe final dataset---[biocong-cleaned.csv](data/biocong-cleaned.csv)---has the following columns: \n\n```\n'level', 'docCount', 'browsePath', 'title', 'lastpage', 'granuleid', 'packageid', 'pdffile', 'pdf', 'text',\n 'agencyLevel', 'nodeStatus', 'textfile', 'htmlfile', 'browseline1', 'processingcode', 'nodetype', 'index.1', \n 'publishdate', 'part', 'forGpo', 'hasChildren', 'hasParents', 'rootNode', 'documentResults",
    "url": "https://github.com/soodoku/biocong",
    "last_updated": "2025-05-24T20:08:35+00:00"
  },
  {
    "full_name": "cwickham/purrr-tutorial",
    "name": "purrr-tutorial",
    "description": "A introduction to purrr",
    "language": "R",
    "topics": [
      "purrr",
      "r",
      "tutorial",
      "rstats",
      "training"
    ],
    "readme": "# A purrr tutorial\n\nThis repo hosts the materials for a [purrr](http://purrr.tidyverse.org/) tutorial.  The materials currently reflect the version planned for the useR! Brussels, Belgium, July 2017.\n\n## Upcoming in-person tutorials\n\n* [useR!](https://user2017.brussels/) Brussels, Belgium, July 4 2017\n* [London R-ladies](https://www.meetup.com/rladies-london/) London, U.K., July 12 2017 \n\nOlder versions of the materials, from prior in-person tutorials, are also available:\n\n* [Cascadia R Conf June 3 2017 (1.75 hours)](https://github.com/cwickham/purrr-tutorial/tree/v0.2) \n\n* [rstudio::conf Jan 2017 (2.25 hours)](https://github.com/cwickham/purrr-tutorial/tree/v0.1)\n\n\n## Outline\n\nCode with a lot of duplication is harder to understand, troubleshoot and maintain. The goal of this tutorial is help you remove duplication in your code by using functions that write `for` loops for you.\n\nYou'll learn to use the functions in the `purrr` package to perform iterative tasks: tasks that look like \"for each _____ do _____\".\n\nBy the end of the tutorial you'll be writing code that is more readable and easier to update and you'll be ready to solve new iteration problems faster and with fewer mistakes.\n\n## Learning Objectives\n\nBy the end of the tutorial, you'll be able to:\n\n* Move from solving a problem on a single element, to iterating that solution over many elements with `map()`.\n* Identify when to use the typed variants of `map()`: `map_lgl()`, `map_int()`, `walk()` etc.\n* Iterate over two arguments with `map2()`.\n* Leverage `purrr` to get list data into tibbles.\n* Use `purrr` to work with list columns in tibbles.\n\n## Pre-requisites\n\nDon't worry if you have never written a `for` loop, used `lapply()`, written your own function or heard of a `tibble`, this tutorial is designed to be accessible to beginners.  \n\nThat said, you should be familiar with exploring and subsetting the basic data structures in R including lists and data frames. \n\nThis is a hands-on tutorial, you'll need you",
    "url": "https://github.com/cwickham/purrr-tutorial",
    "last_updated": "2025-08-25T16:36:39+00:00"
  },
  {
    "full_name": "leifeld/texreg",
    "name": "texreg",
    "description": "Conversion of R Regression Output to LaTeX or HTML Tables",
    "language": "R",
    "topics": [
      "texreg",
      "html-tables",
      "latex",
      "latex-tables",
      "regression",
      "reporting",
      "table"
    ],
    "readme": "# texreg\n\nConversion of R Regression Output to LaTeX or HTML Tables.\n\nConverts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented.\n\n## Documentation\n\nDetails on **texreg** can be found in the following article:\n\nLeifeld, Philip (2013): **texreg**: Conversion of Statistical Model Output in R to LaTeX and HTML Tables. Journal of Statistical Software 55(8): 1-24. doi:[10.18637/jss.v055.i08](http://dx.doi.org/10.18637/jss.v055.i08)\n\nAn updated version of this paper is included as a [package vignette](https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf). Additional details on how to write custom extensions for **texreg** and manipulate **texreg** objects can be found in the following answers on StackOverflow: [[1]](http://stackoverflow.com/questions/38894044/print-beautiful-tables-for-h2o-models-in-r/39135080#39135080), [[2]](http://stackoverflow.com/questions/39397194/computing-p-values-in-spatial-econometric-models-why-are-there-inconsistencies/39479191#39479191), [[3]](http://stackoverflow.com/questions/36947477/how-can-i-use-texreg-1-36-4-for-a-relogit-model-estimated-using-zelig-v-5/36968738#36968738), [[4]](http://stackoverflow.com/questions/39143747/how-to-use-texreg-after-clmm-i-want-to-extract-random-effect-components/39507751#39507751), [[5]](http://stackoverflow.com/questions/40176607/r-how-to-get-a-proper-latex-regression-table-from-a-dataframe/40197961#40197961).\n\n## Installation\n\nThe last stable release can be installed from CRAN:\n``` r\ninstall.packages(\"texreg\")\n```\nTo install the latest development version from GitHub, use the `remotes` package:\n``` r\nremotes::install_github(\"leifeld/texreg\")\n```\n\n## Contribute to the proj",
    "url": "https://github.com/leifeld/texreg",
    "last_updated": "2025-06-15T02:06:34+00:00"
  },
  {
    "full_name": "DillonHammill/DataEditR",
    "name": "DataEditR",
    "description": "An Interactive R Package for Viewing, Entering Filtering and Editing Data",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# DataEditR <img src=\"man/figures/logo.png\" align=\"right\" alt=\"\" width=\"240\"/>\n\n<!-- badges: start -->\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![R build\nstatus](https://github.com/DillonHammill/DataEditR/workflows/R-CMD-check/badge.svg)](https://github.com/DillonHammill/DataEditR/actions?workflow=R-CMD-check)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/DataEditR)](https://CRAN.R-project.org/package=DataEditR)\n[![CRAN\\_Download\\_Badge](https://cranlogs.r-pkg.org/badges/grand-total/DataEditR)](https://cran.r-project.org/package=DataEditR)\n<!-- badges: end -->\n\nManual data entry and editing in R can be tedious, especially if you\nhave limited coding experience and are accustomed to using software with\na Graphical User Interface (GUI). **DataEditR** is an R package built on\nshiny and rhandsontable that makes it easy to interactively view, enter,\nfilter and edit data. If you are new to **DataEditR** visit\n<https://dillonhammill.github.io/DataEditR/> to get started.\n\n## Installation\n\n**DataEditR** can be installed from CRAN:\n\n``` r\ninstall.packages(\"DataEditR\")\n```\n\nThe development version of **DataEditR** can be installed directly from\nGitHub:\n\n``` r\nlibrary(devtools)\ninstall_github(\"DillonHammill/DataEditR\")\n```\n\nTo ensure that `DataEditR` works as expected, you will also need to\ninstall my fork of `rhandsontable`:\n\n``` r\ndevtools::install_github(\"DillonHammill/rhandsontable\")\n```\n\n## Usage\n\n**DataEditR** ships with a series of shiny modules, namely `dataInput`,\n`dataSelect`, `dataFilter`, `dataEdit` and `dataOutput` which have been\nwrapped up into a single function called `data_edit()` to create an",
    "url": "https://github.com/DillonHammill/DataEditR",
    "last_updated": "2025-08-11T22:04:39+00:00"
  },
  {
    "full_name": "eddelbuettel/tint",
    "name": "tint",
    "description": "Tint is not Tufte",
    "language": "R",
    "topics": [
      "r-package",
      "vignette",
      "cran",
      "markdown"
    ],
    "readme": "## tint: Tint is not Tufte\n\n[![CI](https://github.com/eddelbuettel/tint/workflows/ci/badge.svg)](https://github.com/eddelbuettel/tint/actions?query=workflow%3Aci)\n[![Package-License](https://img.shields.io/badge/license-GPL--3-brightgreen.svg?style=flat)](https://www.gnu.org/licenses/gpl-3.0.html) \n[![CRAN](https://www.r-pkg.org/badges/version/tint)](https://cran.r-project.org/package=tint) \n[![Dependencies](https://tinyverse.netlify.app/badge/tint)](https://cran.r-project.org/package=tint) \n[![Downloads](https://cranlogs.r-pkg.org/badges/tint?color=brightgreen)](https://www.r-pkg.org/pkg/tint)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/tint)](https://github.com/eddelbuettel/tint)\n\n### Motivation\n\nThe (html and pdf) styles provided by the [tufte](https://cran.r-project.org/package=tufte) package\nmake it very easy and convenient to create documents in the celebrated style of\n[Edward Tufte](https://www.edwardtufte.com/).\n\nThe clear layout, focused use of white space and unparalleled use of the margin for complementary\ninformation, including graphs, offer a novel and very valuable resource for typesetting.\n\nYet at the same time, not everybody is a fan of the yellow tint, and the fonts.  I had been looking\nfor a while for an alternative, and came across \n[envisioned css](https://github.com/nogginfuel/envisioned-css) by Jef Lippiat.  It gets a few things\nvery right: use of the beautiful \n[Roboto Condensed font](https://fonts.google.com/specimen/Roboto+Condensed) along with\na closer-to-white background.  So I _mixed_ this with the code framework provided by JJ and Yihui to\nmake it an [RMarkdown](https://rmarkdown.rstudio.com/) template you can use just by installing this\npackage. Among the small changes I made were the removal of _italics_ in subheaders and the title.\n\nSimilarly, LaTeX styles exists and the\n[tufte](https://cran.r-project.org/package=tufte) package supports both pdf\nhandouts and a book format.  We first supported the pdf handout",
    "url": "https://github.com/eddelbuettel/tint",
    "last_updated": "2025-07-12T07:38:09+00:00"
  },
  {
    "full_name": "opentofu/opentofu",
    "name": "opentofu",
    "description": "OpenTofu lets you declaratively manage your cloud infrastructure.",
    "language": "Go",
    "topics": [],
    "readme": "# OpenTofu\n\n- [HomePage](https://opentofu.org/)\n- [How to install](https://opentofu.org/docs/intro/install)\n- [Join our Slack community!](https://opentofu.org/slack)\n\n![](https://raw.githubusercontent.com/opentofu/brand-artifacts/main/full/transparent/SVG/on-dark.svg#gh-dark-mode-only)\n![](https://raw.githubusercontent.com/opentofu/brand-artifacts/main/full/transparent/SVG/on-light.svg#gh-light-mode-only)\n\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/10508/badge)](https://www.bestpractices.dev/projects/10508)\n\nOpenTofu is an OSS tool for building, changing, and versioning infrastructure safely and efficiently. OpenTofu can manage existing and popular service providers as well as custom in-house solutions.\n\nThe key features of OpenTofu are:\n\n- **Infrastructure as Code**: Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your datacenter to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used.\n\n- **Execution Plans**: OpenTofu has a \"planning\" step where it generates an execution plan. The execution plan shows what OpenTofu will do when you call apply. This lets you avoid any surprises when OpenTofu manipulates infrastructure.\n\n- **Resource Graph**: OpenTofu builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, OpenTofu builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure.\n\n- **Change Automation**: Complex changesets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what OpenTofu will change and in what order, avoiding many possible human errors.\n\n## Getting help and contributing\n\n- Have a question?\n  - Post it in [GitHub Discussions](https://github.com/orgs/opentofu/discussions)\n  - Open a [GitHub issue](h",
    "url": "https://github.com/opentofu/opentofu",
    "last_updated": "2025-09-02T09:33:02+00:00"
  },
  {
    "full_name": "m1guelpf/yt-whisper",
    "name": "yt-whisper",
    "description": "Using OpenAI's Whisper to automatically generate YouTube subtitles",
    "language": "Python",
    "topics": [
      "ffmpeg",
      "openai",
      "openai-whisper",
      "whisper",
      "youtube",
      "youtube-dl",
      "subtitles",
      "subtitles-generated",
      "transcribe"
    ],
    "readme": "# Automatic YouTube subtitle generation\n\nThis repository uses `yt-dlp` and [OpenAI's Whisper](https://openai.com/blog/whisper) to generate subtitle files for any youtube video.\n\n## Installation\n\nTo get started, you'll need Python 3.7 or newer. Install the binary by running the following command:\n\n    pip install git+https://github.com/m1guelpf/yt-whisper.git\n\nYou'll also need to install [`ffmpeg`](https://ffmpeg.org/), which is available from most package managers:\n\n```bash\n# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n```\n\n## Usage\n\nThe following command will generate a VTT file from the specified YouTube video\n\n    yt_whisper \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n\nThe default setting (which selects the `small` model) works well for transcribing English. You can optionally use a bigger model for better results (especially with other languages). The available models are `tiny`, `tiny.en`, `base`, `base.en`, `small`, `small.en`, `medium`, `medium.en`, `large`.\n\n    yt_whisper \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" --model medium\n\nAdding `--task translate` will translate the subtitles into English:\n\n    yt_whisper \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" --task translate\n\nRun the following to view all available options:\n\n    yt_whisper --help\n\n## License\n\nThis script is open-source and licensed under the MIT License. For more details, check the [LICENSE](LICENSE) file.\n",
    "url": "https://github.com/m1guelpf/yt-whisper",
    "last_updated": "2025-09-01T09:59:37+00:00"
  },
  {
    "full_name": "ntblk/whois-rdap",
    "name": "whois-rdap",
    "description": "📒 RDAP client library for RFC7482 IP address WHOIS lookups",
    "language": "JavaScript",
    "topics": [
      "whois",
      "rdap",
      "whois-lookup",
      "whois-client",
      "ipwhois"
    ],
    "readme": "## whois-rdap\n\n<img src=\"https://netblocks.org/images/art/netblocks-hardware-probes.png\" width=\"200px\" align=\"right\" />\n\n``whois-rdap`` is a fast, concurrent RDAP client library for the next-generation IP WHOIS lookup\nsystem supporting database-backed caching and distributed operation.\n\nIt has been deployed in production to deliver insights on network ownership, digital rights and the state of internet governance around the world.\n\n[![NPM Version][npm-image]][npm-url]\n\n## Synopsis\n\n<img src=\"https://netblocks.org/files/netblocks-logo.png\" width=\"200px\" align=\"left\" alt=\"NetBlocks\" style=\"margin: 0.5em;\" />\n\nwhois-rdap is a caching WHOIS client library that looks up IPv4 and IPv6\naddresses and finds registry records at ARIN, RIPE etc. Only the next-generation\nRDAP (Registration Data Access Protocol) Query Format specified by\n[RFC7482](https://tools.ietf.org/html/rfc7482) and\n[RFC7483](https://tools.ietf.org/html/rfc7483) response format are\nsupported, allowing for structured retrieval and processing of registrar\nrecords. All addresses are mapped to the IPv6 address space for consistency.\n\nThis package is maintained as part of the the\n[NetBlocks.org](https://netblocks.org) network observation framework.\n\n## Features\n\n* IPv4 and IPv6 support\n* Database-backed NoSQL storage supporting schema-free queries\n* Client implementation of RFC7482 REST protocol\n* Vendor-agnostic support for RFC7483 JSON data format and extensions\n* Legacy-free with no support for classic whois queries\n* Supports IP to ASN and ASN origin queries\n* node.js library API for embedded use in servers-side JavaScript applications\n* CLI for ipwhois with optional pretty ANSI-colored console output\n* Workaround mode for RDAP [server quirks](https://github.com/arineng/nicinfo/issues/21)\n\n## Getting started: Usage and integration\n\n### Command-line lookup utility\n\nA basic command-line utility is included that can be used for testing or to seed and exist a deployed cache instance.\n\n```bash\n$ npm install -g who",
    "url": "https://github.com/ntblk/whois-rdap",
    "last_updated": "2025-07-01T20:46:44+00:00"
  },
  {
    "full_name": "GoogleTrends/data",
    "name": "data",
    "description": "An index of all open-source data",
    "language": "JavaScript",
    "topics": [],
    "readme": "This repo contains open-source datasets behind the graphics, interactives, and analyses at [Google Trends](https://www.google.com/trends). Every day we will add new datasets behind our graphics and charts. \n\n<h3>What is the data?</h3>\nThe data primarily comes from our analysis of Google Trends, but will on occasion include other Google tools such as YouTube, Play and Waze. It is primarily:<br>\n• Aggregated<br>\n• Anonymised<br>\n• Indexed<br>\n• Normalized\n\n<h3>What can you do with it?</h3>\nThe data is deliberately designed for you to play with, explore and create visualizations. We want to know what you do so we can share it on our social channels and inspire others to play with it too.\n\n<h3>Useful links:</h3>\n• [Google Trends](https://www.google.com/trends)<br>\n• [Google News Lab](https://www.google.com/newslab)<br>\n• [@GoogleTrends](https://www.twitter.com/googletrends)<br>\n\n<h3>Contact us</h3>\nnewslabtrends@google.com\n\n",
    "url": "https://github.com/GoogleTrends/data",
    "last_updated": "2025-09-02T02:45:56+00:00"
  },
  {
    "full_name": "joelgombin/ggplot.acm",
    "name": "ggplot.acm",
    "description": "ggplot2 plots made easy for Multiple correspondence analysis (MCA)",
    "language": "R",
    "topics": [],
    "readme": "ggplot2 plots made easy for Multiple correspondence analysis (MCA)\n------------------------------------------------------------------\n\n`ggplot.acm` provides an easy way of creating a `ggplot2`-style plot for a multiple correspondence analysis run with the `FactoMineR` package (this is important, as other packages/functions may product objects with a different internal structure). Multiple options are provided, which should adress the most common uses in social sciences. The `autoplot.MCA` function extends the `autoplot` generic function, and returns a `ggplot2` object, which can then be further modified. \n",
    "url": "https://github.com/joelgombin/ggplot.acm",
    "last_updated": "2018-03-09T15:30:57+00:00"
  },
  {
    "full_name": "MineDojo/Voyager",
    "name": "Voyager",
    "description": "An Open-Ended Embodied Agent with Large Language Models",
    "language": "JavaScript",
    "topics": [
      "large-language-models",
      "embodied-learning",
      "open-ended-learning",
      "minecraft"
    ],
    "readme": "# Voyager: An Open-Ended Embodied Agent with Large Language Models\n<div align=\"center\">\n\n[[Website]](https://voyager.minedojo.org/)\n[[Arxiv]](https://arxiv.org/abs/2305.16291)\n[[PDF]](https://voyager.minedojo.org/assets/documents/voyager.pdf)\n[[Tweet]](https://twitter.com/DrJimFan/status/1662115266933972993?s=20)\n\n[![Python Version](https://img.shields.io/badge/Python-3.9-blue.svg)](https://github.com/MineDojo/Voyager)\n[![GitHub license](https://img.shields.io/github/license/MineDojo/Voyager)](https://github.com/MineDojo/Voyager/blob/main/LICENSE)\n______________________________________________________________________\n\n\nhttps://github.com/MineDojo/Voyager/assets/25460983/ce29f45b-43a5-4399-8fd8-5dd105fd64f2\n\n![](images/pull.png)\n\n\n</div>\n\nWe introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving complex\nbehaviors, and 3) a new iterative prompting mechanism that incorporates environment\nfeedback, execution errors, and self-verification for program improvement.\nVoyager interacts with GPT-4 via blackbox queries, which bypasses the need for\nmodel parameter fine-tuning. The skills developed by Voyager are temporally\nextended, interpretable, and compositional, which compounds the agent’s abilities\nrapidly and alleviates catastrophic forgetting. Empirically, Voyager shows\nstrong in-context lifelong learning capability and exhibits exceptional proficiency\nin playing Minecraft. It obtains 3.3× more unique items, travels 2.3× longer\ndistances, and unlocks key tech tree milestones up to 15.3× faster than prior SOTA.\nVoyager is able to utilize the learned skill library in a new Minecraft world to\nsolve novel tasks from scratch, while other techniques struggl",
    "url": "https://github.com/MineDojo/Voyager",
    "last_updated": "2025-09-02T07:48:30+00:00"
  },
  {
    "full_name": "nchopin/particles",
    "name": "particles",
    "description": "Sequential Monte Carlo in python",
    "language": "Python",
    "topics": [
      "particle-filter",
      "sequential-monte-carlo",
      "bayesian-inference",
      "pmcmc",
      "smc2",
      "quasi-monte-carlo",
      "kalman-filter"
    ],
    "readme": "![logo](logo.png)\n\n# particles #\n\nSequential Monte Carlo in python. \n\n## Motivation ##\n\nThis package was developed to complement the following book:\n\n[An introduction to Sequential Monte Carlo](https://www.springer.com/gp/book/9783030478445)\n\nby Nicolas Chopin and Omiros Papaspiliopoulos. \n\nIt now also implements algorithms and methods introduced after the book was\npublished, see below. \n\n## Features ##\n\n* **particle filtering**: bootstrap filter, guided filter, APF.\n\n* **resampling**: multinomial, residual, stratified, systematic and SSP. \n\n* possibility to define **state-space models** using some (basic) form of \n  probabilistic programming; see below for an example. \n\n* **SQMC** (Sequential quasi Monte Carlo);  routines for computing the Hilbert curve, \n  and generating RQMC sequences. \n\n* **FFBS (forward filtering backward sampling)**: standard, O(N^2) variant, and\n  faster variants based on either MCMC, pure rejection, or the hybrid scheme;\n  see Dau & Chopin (2022) for a discussion. The QMC version of Gerber and\n  Chopin (2017, Bernoulli) is also implemented.\n\n* **other smoothing algorithms**: fixed-lag smoothing, on-line smoothing,\n  two-filter smoothing (O(N) and O(N^2) variants).  \n\n* Exact filtering/smoothing algorithms: **Kalman** (for linear Gaussian models) \n  and **forward-backward recursions** (for finite hidden Markov models).\n\n* **Standard and waste-free SMC samplers**: SMC tempering, IBIS (a.k.a. data\n  tempering). SMC samplers for binary words (Schäfer and Chopin, 2014), with\n  application to **variable selection**.\n\n* Bayesian parameter inference for state-space models: **PMCMC** (PMMH, Particle Gibbs) \n  and **SMC^2**. \n\n* Basic support for **parallel computation** (i.e. running multiple SMC algorithms \n  on different CPU cores). \n\n* **Variance estimators** (Chan and Lai, 2013 ; Lee and Whiteley, 2018; Olsson\n  and Douc, 2019).\n\n* **nested sampling**: both the vanilla version and the SMC sampler of Salomone\n  et al (2018).\n\n## Example ##\n\nHere i",
    "url": "https://github.com/nchopin/particles",
    "last_updated": "2025-08-26T11:50:06+00:00"
  },
  {
    "full_name": "borisyankov/react-sparklines",
    "name": "react-sparklines",
    "description": "Beautiful and expressive Sparklines React component",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Beautiful and expressive sparklines component for React\n\n[![Build Status](https://travis-ci.org/borisyankov/react-sparklines.svg?branch=master)](https://travis-ci.org/borisyankov/react-sparklines)\n\nLive demos and docs: [borisyankov.github.io/react-sparklines/](http://borisyankov.github.io/react-sparklines/)\n\n![](http://borisyankov.github.io/react-sparklines/img/dynamic.gif)\n\n## Install\n\n```\nnpm install react-sparklines --save\n```\n\n## Run demo\n\n```\nnpm install\nnpm start\nhttp://localhost:8080\n```\n\n\n## Use\n\nImport the Sparklines components that you need; for example to generate a simple chart:\n\n![](http://borisyankov.github.io/react-sparklines/img/basic.png)\n\n```\nimport React from 'react';\nimport { Sparklines } from 'react-sparklines';\n...\n<Sparklines data={[5, 10, 5, 20, 8, 15]} limit={5} width={100} height={20} margin={5}>\n</Sparklines>\n```\n\nSparklines component is a container with the following properties:\n\ndata - the data set used to build the sparkline\n\nlimit - optional, how many data points to display at once\n\nwidth, height - dimensions of the generated sparkline in the SVG viewbox.  This will be automatically scaled (i.e. responsive) inside the parent container by default.\n\nsvgWidth, svgHeight - If you want absolute dimensions instead of a responsive component set these attributes.\n\n[preserveAspectRatio](https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/preserveAspectRatio) - default: 'none', set this to modify how the sparkline should scale \n\nmargin - optional, offset the chart\n\nmin, max - optional, bound the chart\n\n\n#### Basic Sparkline\n\n![](http://borisyankov.github.io/react-sparklines/img/customizable.png)\n\n```\nimport React from 'react';\nimport { Sparklines, SparklinesLine } from 'react-sparklines';\n...\n<Sparklines data={[5, 10, 5, 20]}>\n  <SparklinesLine color=\"blue\" />\n</Sparklines>\n```\n\n#### Bars\n\n![](http://borisyankov.github.io/react-sparklines/img/bars.png)\n\n\n```\nimport React from 'react';\nimport { Sparklines, SparklinesBars } from 'react-sp",
    "url": "https://github.com/borisyankov/react-sparklines",
    "last_updated": "2025-08-20T19:15:35+00:00"
  },
  {
    "full_name": "giuseppedib/LDA",
    "name": "LDA",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/giuseppedib/LDA",
    "last_updated": "2016-06-06T20:27:16+00:00"
  },
  {
    "full_name": "jantic/DeOldify",
    "name": "DeOldify",
    "description": "A Deep Learning based project for colorizing and restoring old images (and video!)",
    "language": "Python",
    "topics": [],
    "readme": "\n# DeOldify\n\n**This Reposisitory is Archived**  This project was a wild ride since I started it back in 2018.  6 years ago as of this writing (October 19, 2024)!.  It's time for me to move on and put this repo in the archives as I simply don't have the time to attend to it anymore, and frankly it's ancient as far as deep-learning projects go at this point! ~Jason\n\n**Quick Start**: The easiest way to colorize images using open source DeOldify\n(for free!) is here: [DeOldify Image Colorization on DeepAI](https://deepai.org/machine-learning-model/colorizer)\n\n**Desktop**: Want to run open source DeOldify for photos and videos on the desktop?\n* Stable Diffusion Web UI Plugin- Photos and video, cross-platform (NEW!). <https://github.com/SpenserCai/sd-webui-deoldify>\n* ColorfulSoft Windows GUI- No GPU required! Photos/Windows only. <https://github.com/ColorfulSoft/DeOldify.NET>.\nNo GPU required!\n\n**In Browser (new!)**  Check out this Onnx-based in browser implementation:  https://github.com/akbartus/DeOldify-on-Browser\n\nThe **most advanced** version of DeOldify image colorization is available here,\nexclusively.  Try a few images for free! [MyHeritage In Color](https://www.myheritage.com/incolor)\n\n**Replicate:** Image: <a href=\"https://replicate.com/arielreplicate/deoldify_image\"><img src=\"https://replicate.com/arielreplicate/deoldify_image/badge\"></a> | Video: <a href=\"https://replicate.com/arielreplicate/deoldify_video\"><img src=\"https://replicate.com/arielreplicate/deoldify_video/badge\"></a>\n\n----------------------------\n\nImage (artistic) [![Colab for images](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)\n| Video [![Colab for video](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)\n\nHaving trouble with the default image colorizer, aka \"artistic\"?  Try the\n\"stable",
    "url": "https://github.com/jantic/DeOldify",
    "last_updated": "2025-09-02T08:34:07+00:00"
  },
  {
    "full_name": "ENSTA-U2IS-AI/torch-uncertainty",
    "name": "torch-uncertainty",
    "description": "Open-source framework for uncertainty and deep learning models in PyTorch :seedling:",
    "language": "Python",
    "topics": [
      "ensembles",
      "uncertainty-quantification",
      "predictive-uncertainty",
      "uncertainty",
      "reliable-ai",
      "pytorch",
      "bayesian-network",
      "neural-networks",
      "trustworthy-machine-learning",
      "computer-vision"
    ],
    "readme": "<div align=\"center\">\n\n![TorchUncertaintyLogo](https://github.com/ENSTA-U2IS-AI/torch-uncertainty/blob/main/docs/source/_static/images/torch_uncertainty.png)\n\n[![pypi](https://img.shields.io/pypi/v/torch_uncertainty.svg)](https://pypi.python.org/pypi/torch_uncertainty)\n[![tests](https://github.com/ENSTA-U2IS-AI/torch-uncertainty/actions/workflows/run-tests.yml/badge.svg?branch=main&event=push)](https://github.com/ENSTA-U2IS-AI/torch-uncertainty/actions/workflows/run-tests.yml)\n[![Docs](https://github.com/ENSTA-U2IS-AI/torch-uncertainty/actions/workflows/build-docs.yml/badge.svg)](https://torch-uncertainty.github.io/)\n[![PRWelcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/ENSTA-U2IS-AI/torch-uncertainty/pulls)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Code Coverage](https://codecov.io/github/ENSTA-U2IS-AI/torch-uncertainty/coverage.svg?branch=master)](https://codecov.io/gh/ENSTA-U2IS-AI/torch-uncertainty)\n[![Downloads](https://static.pepy.tech/badge/torch-uncertainty)](https://pepy.tech/project/torch-uncertainty)\n[![Discord Badge](https://dcbadge.vercel.app/api/server/HMCawt5MJu?compact=true&style=flat)](https://discord.gg/HMCawt5MJu)\n</div>\n\n_TorchUncertainty_ is a package designed to help leverage [uncertainty quantification techniques](https://github.com/ENSTA-U2IS-AI/awesome-uncertainty-deeplearning) to make deep neural networks more reliable. It aims at being collaborative and including as many methods as possible, so reach out to add yours!\n\n:construction: _TorchUncertainty_ is in early development :construction: - expect changes, but reach out and contribute if you are interested in the project! **Please raise an issue if you have any bugs or difficulties and join the [discord server](https://discord.gg/HMCawt5MJu).**\n\n:books: Our webpage and documentation is available here: [torch-uncertainty.github.io](http",
    "url": "https://github.com/ENSTA-U2IS-AI/torch-uncertainty",
    "last_updated": "2025-08-28T19:22:05+00:00"
  },
  {
    "full_name": "EventRegistry/event-registry-python",
    "name": "event-registry-python",
    "description": "Python package for API access to news articles and events in the Event Registry",
    "language": "Python",
    "topics": [
      "event-registry",
      "python",
      "news",
      "news-aggregator",
      "news-feed",
      "newsapi",
      "events",
      "information-extraction",
      "news-articles"
    ],
    "readme": "Event Registry is a Python package that can be used to easily access the news data available in [Event Registry](http://eventregistry.org/) through the API. The package can be used to query for articles or events by filtering using a large set of filters, like keywords, concepts, topics, sources, sentiment, date, etc. Details about the News API are available on the [landing page of the product](https://newsapi.ai/).\n\n## Installation\n\nEvent Registry package can be installed using Python's pip installer. In the command line, simply type:\n\n    pip install eventregistry\n\nand the package should be installed. Alternatively, you can also clone the package from the [GitHub repository](https://github.com/EventRegistry/event-registry-python). After cloning it, open the command line and run:\n\n    python setup.py install\n\n### Validating installation\n\nTo ensure the package has been properly installed run python and type:\n\n```python\nimport eventregistry\n```\n\nIf you don't get any error messages, then your installation has been successful.\n\n### Updating the package\n\nAs features are added to the package you will need at some point to update it. In case you have downloaded the package from GitHub simply do a `git pull`. If you have installed it using the `pip` command, then simply run\n\n\tpip install eventregistry --upgrade\n\n### Authentication and API key\n\nWhen making queries to Event Registry you will have to use an API key that you can obtain for free. The details on how to obtain and use the key are described in the [Authorization](../../wiki/EventRegistry-class#authorization) section.\n\n## Four simple examples to get you interested\n\n**Print a list of recently articles or blog posts from *US based sources* *with positive sentiment* mentioning phrases *\"George Clooney\"* or *\"Sandra Bullock\"***\n\n```python\nfrom eventregistry import *\ner = EventRegistry(apiKey = YOUR_API_KEY)\n\n# get the USA URI\nusUri = er.getLocationUri(\"USA\")    # = http://en.wikipedia.org/wiki/United_States\n\nq = QueryA",
    "url": "https://github.com/EventRegistry/event-registry-python",
    "last_updated": "2025-08-13T17:57:17+00:00"
  },
  {
    "full_name": "rdatsci/rtcl",
    "name": "rtcl",
    "description": "R tools for the command line",
    "language": "R",
    "topics": [],
    "readme": "# rtcl: R Tools for the Command Line\n\n[![Travis build status](https://travis-ci.org/rdatsci/rtcl.svg?branch=master)](https://travis-ci.org/rdatsci/rtcl)\n[![Coverage status](https://coveralls.io/repos/github/rdatsci/rtcl/badge.svg)](https://coveralls.io/r/rdatsci/rtcl?branch=master)\n\nThis package ships some command line utilities which simplify working with R packages from the command line.\nMany commands *rtcl* provides are just wrappers around , e.g. [remotes](https://github.com/r-lib/remotes) and [testthat](https://github.com/r-lib/testthat).\nThey ensure a valid exit code which is required to use these commands for shell scripting.\nYou find additional packages via [R infrastructure](https://github.com/r-lib).\nFurthermore, *rtcl* allows you to maintain a collection of you favorite packages in a plain text file which you can add to your [dotfiles](https://dotfiles.github.io/) and share across systems.\nThis file may also contain git sources to keep you up to date with packages not yet released on CRAN.\n\n\n## Available Commands\n* `rbuild` to bundle a local package.\n* `rcheck` to check a local package.\n* `rclean` to remove `.[o|so]` files from a local package.\n* `rcov` to test the coverage of a local package.\n* `rdoc` to document a local package using roxygen.\n* `rhub` to upload a local package to rhub service.\n* `rinstall` to install a remote package, e.g. a package hosted on CRAN or GitHub.\n* `rknit` to knit a document (.Rnw, .Rmd, ...) via rknit.\n* `rmake` to make a local package (document and install).\n* `rpkgdown` to build static HTML documentation with [pkgdown](https://github.com/hadley/pkgdown).\n* `rremove` to remove (uninstall) R packages.\n* `rshine` to run a shiny app.\n* `rspell` to check spelling in generated .Rd files.\n* `rtest` to test a local package.\n* `rupdate` to update all CRAN packages on your system, install missing CRAN packages listed in your collection `~/.rtcl/packages` and update all packages with a git source.\n* `rusage` to check variable usage.",
    "url": "https://github.com/rdatsci/rtcl",
    "last_updated": "2025-03-22T10:51:09+00:00"
  },
  {
    "full_name": "nbedos/termtosvg",
    "name": "termtosvg",
    "description": "Record terminal sessions as SVG animations",
    "language": "Python",
    "topics": [
      "terminal",
      "svg",
      "cli",
      "shell",
      "animation",
      "svg-animations",
      "recorder",
      "recording"
    ],
    "readme": "**Note: As of June 2020 I do not have time to maintain termtosvg anymore and this repository is now read-only.**\n\n# termtosvg\ntermtosvg is a Unix terminal recorder written in Python that renders your command\nline sessions as standalone SVG animations.\n\n![Example](./docs/examples/awesome_window_frame_powershell.svg)\n\n* [Gallery of examples](https://nbedos.github.io/termtosvg/pages/examples.html)\n* [Gallery of templates](https://nbedos.github.io/termtosvg/pages/templates.html)\n\n## Features\n* Produce lightweight and clean looking animations or still frames embeddable on a project page\n* Custom color themes, terminal UI and animation controls via user-defined [SVG templates](man/termtosvg-templates.md)\n* Rendering of recordings in asciicast format made with asciinema\n    \n## Installation\ntermtosvg is compatible with Linux, macOS and BSD OSes, requires Python >= 3.5 and can be installed as follows using pip:\n```shell\n# Create virtualenv named '.venv'\npython3 -m venv .venv\n# Activate virtualenv\nsource .venv/bin/activate\npip3 install termtosvg\n```\nThen run termtosvg by calling either `termtosvg` or `python3 -m termtosvg`.\n\nVarious independently maintained, OS specific packages have been made available by the community:\n\n| OS       | Repository  | Installation command  |\n|----------|-------------|---|\n| Archlinux  | [Arch](https://www.archlinux.org/packages/community/any/termtosvg/)  |`pacman -S termtosvg`   |\n| FreeBSD | [ports](https://www.freshports.org/graphics/py-termtosvg) | |\n| Gentoo | [media-gfx/termtosvg](https://packages.gentoo.org/packages/media-gfx/termtosvg) | `emerge media-gfx/termtosvg`|\n| macOS  | [Homebrew](https://formulae.brew.sh/formula/termtosvg)  |`brew install termtosvg`   |\n| OpenBSD  | [ports](https://github.com/openbsd/ports/tree/master/graphics/termtosvg)  |   |\n| NixOS | [nixpkgs](https://github.com/NixOS/nixpkgs/blob/master/pkgs/tools/misc/termtosvg/) | |\n\n\n## Basic usage\nStart recording with:\n\n```\n$ termtosvg\nRecording started, enter \"exit\" co",
    "url": "https://github.com/nbedos/termtosvg",
    "last_updated": "2025-09-01T06:05:31+00:00"
  },
  {
    "full_name": "derekgreene/topic-stability",
    "name": "topic-stability",
    "description": "Stability analysis for topic models",
    "language": "Python",
    "topics": [],
    "readme": "topic-stability\n===============\n\n### Summary\nDespite the many [topic modeling algorithms](http://en.wikipedia.org/wiki/Topic_model) that have been proposed for text mining, a common challenge is selecting an appropriate number of topics for a particular data set. Choosing too few topics will produce results that are overly broad, while choosing too many will result in the over-clustering of a data into many redundant, highly-similar topics. We have developed a *stability analysis* approach to address this problem, the idea being that a model with an appropriate number of topics will be more robust to perturbations in the data. Details of this approach are described in the following paper:\n\n\tHow Many Topics? Stability Analysis for Topic Models (2014)\n\tDerek Greene, Derek O'Callaghan, Pádraig Cunningham\n\thttp://arxiv.org/abs/1404.4606\t\n\t\nThis repository contains a Python reference implementation of the above approach.\n\n### Dependencies\nTested with Python 2.7 and Python 3.5, and requiring the following packages, which are available via PIP:\n\n* Required: [numpy >= 1.8.0](http://www.numpy.org/)\n* Required: [scikit-learn >= 0.14](http://scikit-learn.org/stable/)\n* Required for NMF: [nimfa >= 1.2.x](http://nimfa.biolab.si/)\n* Required for LDA: [scipy >= 0.13](http://www.scipy.org/)\n* Required for utility tools: [prettytable >= 0.7.2](https://code.google.com/p/prettytable/)\n\nThe following dependency is bundled with this project:\n- [hungarian-algorithm 2013-11-03](https://github.com/tdedecko/hungarian-algorithm)\n \nTo run the LDA tools, an installation of Mallet 2.0 is required, which is available [here](http://mallet.cs.umass.edu/). The current code has been tested with Mallet version 2.0.8-RC3\n\n### Basic Usage\nBefore applying topic modeling to a corpus, the first step is to pre-process the corpus and store it in a suitable format. The script 'parse-text.py' can be used to parse a directory of plain text documents. Here, we parse all .txt files in the directory or sub-direct",
    "url": "https://github.com/derekgreene/topic-stability",
    "last_updated": "2025-01-22T20:39:07+00:00"
  },
  {
    "full_name": "r-devel/rcheckserver",
    "name": "rcheckserver",
    "description": "CRAN-like docker images and source code",
    "language": "Dockerfile",
    "topics": [],
    "readme": "# rcheckserver [![badge](https://github.com/r-devel/rcheckserver/actions/workflows/build.yml/badge.svg)](https://github.com/r-devel/rcheckserver/actions/workflows/build.yml)\n\n|          |  x86_64  |  ARM64   |\n|:--------:|:--------:|:--------:|\n| Debian   | [![debian](https://img.shields.io/docker/image-size/cran/debian)](https://hub.docker.com/r/cran/debian) | [![debian](https://img.shields.io/docker/image-size/cran/debian?arch=arm64)](https://hub.docker.com/r/cran/debian) |\n| Ubuntu   | [![ubuntu](https://img.shields.io/docker/image-size/cran/ubuntu)](https://hub.docker.com/r/cran/ubuntu)   | [![ubuntu](https://img.shields.io/docker/image-size/cran/ubuntu?arch=arm64)](https://hub.docker.com/r/cran/ubuntu) |\n\n> Docker images with a complete Debian CRAN check server\n\nThe CRAN team maintains a [debian meta package](http://statmath.wu.ac.at/AASC/debian) which depends on all packages that are installed on the CRAN Debian check server. This provides a reproducible 'R CMD check' environment for CRAN packages.\n\n## How to use\n\nThe CRAN team always uses the [Debian testing](https://packages.debian.org/testing/) distribution, which is also what [our Debian image](debian/Dockerfile) uses. This is a rolling branch, which means system libraries get continously updated, and may get breaking changes.\n\n```sh\n# Get the latest image\ndocker pull cran/debian\n\n# Start interactive bash shell\ndocker run --rm -it cran/debian bash\n```\n\nTo quickly test if a package can be built:\n\n```sh\ndocker run --rm -it cran/debian R -e 'install.packages(\"pdftools\")'\n```\n\nAlternatively, our [ubuntu based image](ubuntu/Dockerfile) provides a similar environment based on the latest Ubuntu (LTS) server, but with the latest R installed from [CRAN](https://cran.r-project.org/bin/linux/ubuntu/). This image is smaller and more stable, with slightly older system libraries than Debian, and only non-breaking updates.\n\n```sh\ndocker run --rm -it cran/ubuntu R -e 'install.packages(\"magick\")'\n```\n\n## Source code\n\nThe f",
    "url": "https://github.com/r-devel/rcheckserver",
    "last_updated": "2025-05-04T11:00:25+00:00"
  },
  {
    "full_name": "rstudio/sparkapi",
    "name": "sparkapi",
    "description": "Sparklyr Extensions API",
    "language": "R",
    "topics": [],
    "readme": "# sparkapi\n\nThis project was merged back to [sparklyr](http://spark.rstudio.com). Please visit http://spark.rstudio.com/extensions.html for information.\n",
    "url": "https://github.com/rstudio/sparkapi",
    "last_updated": "2025-04-21T23:37:14+00:00"
  },
  {
    "full_name": "halpo/parser",
    "name": "parser",
    "description": "R parser package",
    "language": "C",
    "topics": [],
    "readme": "The `parser` R package.\n=======================\n\nIntroduction\n------------\n\nThe `parser` package is an alternative implementation of the base R parser.\nThe expressions returned are the same but the information is presented slightly\ndifferent from the base parser.\n\nThe parser package was originally written by Romain Francois, but is now being\nmaintained by Andrew Redd.  The source code is based off the \n[GNU bison](http://www.gnu.org/software/bison/) parser project.\n\nReporting Bugs\n--------------\nThe github [issues tracker](https://github.com/halpo/parser/issues) will\nbe used to request bug fixes and feature requests.  The github fork/pull \nsystem will also be used for any submitting fixes.\n\n",
    "url": "https://github.com/halpo/parser",
    "last_updated": "2022-12-12T19:40:37+00:00"
  },
  {
    "full_name": "notnews/nbc_transcripts",
    "name": "nbc_transcripts",
    "description": "NBC transcripts 2011--2014",
    "language": "Python",
    "topics": [
      "news",
      "nbc-transcripts",
      "news-transcripts",
      "nbc-news"
    ],
    "readme": "## NBC transcripts\n\nNBC used to provide transcripts of some of its shows at the now defunct http://www.nbcnews.com/id/3719710. Check out this archive.org page [https://web.archive.org/web/20170601234403/http://www.nbcnews.com/id/3719710](https://web.archive.org/web/20170601234403/http://www.nbcnews.com/id/3719710).\n\n[nbc_crawl.py](scripts/nbc_crawl.py) crawls all the links to news transcripts. The script produces a [list of all links](data/all_links.csv). And [nbc_extract.py](scripts/nbc_extract.py) downloads and parses the news transcripts and appends some meta data and dumps it to a CSV file. \n\nThe raw html files and the final csv can be downloaded from [http://dx.doi.org/10.7910/DVN/ND1TCV](http://dx.doi.org/10.7910/DVN/ND1TCV).\n\nAnd a list of all the links along with the title of the show and the date, see [here](data/out.txt).\n\nHere's the yearly breakdown of the final dataset (5,369 rows):\n\n```\n2008 2009 2010 2011 2012 2013 2014 \n  76  434  752 1042 1164 1177  724 \n```\n\n### Notes\n\n* Scripts from 2014.\n* Some news transcripts had a typo in the date string, e.g. 'Thusday','Februrary', etc. That caused the script to fail to fill in the date column.\n\n## 🔗 Adjacent Repositories\n\n- [notnews/fox_news_transcripts](https://github.com/notnews/fox_news_transcripts) — Fox News Transcripts 2003--2025\n- [notnews/cnn_transcripts](https://github.com/notnews/cnn_transcripts) — CNN Transcripts 2000--2025\n- [notnews/stanford_tv_news](https://github.com/notnews/stanford_tv_news) — Stanford Cable TV News Dataset\n- [notnews/lacc_to_csv](https://github.com/notnews/lacc_to_csv) — Los Angeles Closed-Caption Television News Archive Data to CSV\n- [notnews/archive_news_cc](https://github.com/notnews/archive_news_cc) — Closed Caption Transcripts of News Videos from archive.org 2014--2023\n",
    "url": "https://github.com/notnews/nbc_transcripts",
    "last_updated": "2025-05-01T05:25:15+00:00"
  },
  {
    "full_name": "avehtari/BDA_py_demos",
    "name": "BDA_py_demos",
    "description": "Bayesian Data Analysis demos for Python",
    "language": "Jupyter Notebook",
    "topics": [
      "python",
      "bayesian-data-analysis",
      "bayesian-inference",
      "bayesian",
      "mcmc",
      "stan"
    ],
    "readme": "# Bayesian Data Analysis Python Demos\n\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org/repo/avehtari/bda_py_demos) to interactively run the IPython Notebooks in the browser.\n\nThis repository contains some Python demos for the book [Bayesian Data\nAnalysis, 3rd ed by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (BDA3)](http://www.stat.columbia.edu/~gelman/book/). See also [Bayesian Data Analysis course material](https://github.com/avehtari/BDA_course_Aalto).\n\nCurrently there are demos for BDA3 Chapters 2, 3, 4, 5, 6, 10 and 11. Furthermore, [PyStan](https://github.com/stan-dev/pystan) is also demoed.\n\nDemos are in jupyter notebook (.ipynb) format. These can be directly previewed in github without need\nto install or run anything.\n\nCorresponding demos were originally written for [Matlab/Octave](https://github.com/avehtari/BDA_m_demos) by [Aki Vehtari](http://users.aalto.fi/~ave/) and translated to Python by Tuomas Sivula. Some improvements were contributed by Pellervo Ruponen and Lassi Meronen. There are also corresponding [R demos](https://github.com/avehtari/BDA_R_demos).\n\n\n## Requirements\n\n- python 3\n- ipython\n- numpy\n- scipy\n- matplotlib 2\n- pandas (for some demos)\n- pystan (for some demos)\n- ArviZ (for some demos)\n",
    "url": "https://github.com/avehtari/BDA_py_demos",
    "last_updated": "2025-08-20T14:16:51+00:00"
  },
  {
    "full_name": "amaboura/panama-papers-dataset-2016",
    "name": "panama-papers-dataset-2016",
    "description": "Structured data about Panama papers collected from official ICIJ website ",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Panama Papers Dataset 2016\nStructured data about Panama papers collected from the official ICIJ website \n\n**Update:** The *full dataset* that has been released by ICIJ recently is available [here](https://offshoreleaks.icij.org/pages/database)\n\n### How were the data collected ?\nI wrote a detailed explanation [here](COLLECTION.md)\n### Dataset structure\n```\n├── de.csv\n├── en.csv\n├── es.csv\n├── fr.csv\n├── pt.csv\n└── viz-data\n    ├── 014fe3bc.json\n    ├── 02664021.json\n    ├── 034793d9.json\n    ├── 08cc5165.json\n    ├── 0a1122ef.json\n    ├── 0a3cf4fd.json\n    ├── 0dae973f.json\n\t ...\n\t ...\n\t ...\n\t├── fb706ef9.json\n    ├── fb7f4be3.json\n    └── ff9bbc4d.json\n└── csv-viz-data\n    ├── 014fe3bc-nodes.csv\n    ├── 014fe3bc-edges.csv\n    ├── 02664021-nodes.csv\n     ...\n     ...\n     ...\n    ├── fb7f4be3-edges.csv\n    ├── ff9bbc4d-nodes.csv\n    └── ff9bbc4d-edges.csv\n└── iPython NB\n    ├── Panama Project.ipynb\n```\n\n### Explore the data\nthe main data provided by ICIJ can be found in CSV file, that is available in 4 languages\n\n#### Main files\nThe CSV file is broken down to the following columns\n\n```\nstory-title\nstory-subtitle\nstory-image-if-linked-person\nstory-narrative\nstory-category-code\nstory-priority\nstory-region-codes\nstory-country-1-code\nstory-country-1-name\nstory-country-2-code\nstory-country-2-name\nstory-document-1-title\ndata-person-1-name\ndata-person-1-description-if-politician\ndata-person-1-relationship,\ndata-person-1-image\ndata-person-1-viz-publish\ndata-person-1-comment\n```\n\n#### Visualization data\nICIJ report provided a set of visualizations that you can browser through this [link](https://panamapapers.icij.org/the_power_players/)\n\nData content for these visualizations is a graph structure, each graph is encoded to a JSON file, named after the ``` data-person-1-viz-publish ``` attribute found in the main CSV file.\n\n#### Visualization nodes and edges csv data\nThe JSON data is normalized and the nodes and edges data is given in different csvs.\n\n#### iPython Notebook\nAn ",
    "url": "https://github.com/amaboura/panama-papers-dataset-2016",
    "last_updated": "2025-07-07T05:30:23+00:00"
  },
  {
    "full_name": "hadley/pryr",
    "name": "pryr",
    "description": "Pry open the covers of R",
    "language": "R",
    "topics": [],
    "readme": "# pryr\n\n<!-- badges: start -->\n[![Lifecycle: superseded](https://img.shields.io/badge/lifecycle-superseded-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#superseded)\n[![R-CMD-check](https://github.com/hadley/pryr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/hadley/pryr/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\npryr is superseded. Please use:\n\n* [rlang](https://rlang.r-lib.org/) for low-level R programming.\n* [lobstr](https://lobstr.r-lib.org/) for object sizes & comparison.\n* [sloop](https://sloop.r-lib.org/) for OOP tools.\n",
    "url": "https://github.com/hadley/pryr",
    "last_updated": "2025-08-22T18:18:26+00:00"
  },
  {
    "full_name": "mitchelloharawild/icons",
    "name": "icons",
    "description": "R package to easily insert web icons to RMarkdown",
    "language": "R",
    "topics": [
      "ozunconf17",
      "r",
      "unconf",
      "fontawesome",
      "web-icons",
      "icon-library"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# icons <a href='https://pkg.mitchelloharawild.com/icon'><img src='man/figures/logo.png' align=\"right\" height=\"138\" /></a>\n\n[![R-CMD-check](https://github.com/mitchelloharawild/icons/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/mitchelloharawild/icons/actions/workflows/R-CMD-check.yaml)\n[![Coverage\nstatus](https://codecov.io/gh/mitchelloharawild/icons/branch/master/graph/badge.svg)](https://codecov.io/gh/mitchelloharawild/icon?branch=master)\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/icon)](https://cran.r-project.org/package=icons)\n<!-- [![Downloads](http://cranlogs.r-pkg.org/badges/icons?color=brightgreen)](https://cran.r-project.org/package=icons) -->\n\nThe `icons` package for R makes adding web icons to reports,\npresentations and apps easy. It integrates many popular icon libraries\nfrom around the web with a simple interface that works with any\n`rmarkdown` output format. If a particular icon library is not\nexplicitly supported by this package, you can still use it by creating a\ncustom icon set from a folder of SVG files. Icons provide flexible means\nof digital expression, allowing expressions and functionality beyond\nwhat is possible with emoji.\n\nThe `icons` package currently provides helpful tools for downloading and\nusing icons from these libraries:\n\n- [Font Awesome](https://github.com/FortAwesome/Font-Awesome/) (Pro\n  icons can be used using custom icon sets)\n- [Ionicons](https://github.com/ionic-team/ionicons/)\n- [Academicons](https://github.com/jpswalsh/academicons)\n- [Simple Icons](https://github.com/simple-icons/simple-icons/)\n- [Google’s Material\n  Design](https://github.com/google/material-design-icons)\n- [Octicons](https://github.com/primer/octicons)\n- [Feather Icons](https://github.com/feathericons/feather)\n- [Bioicons",
    "url": "https://github.com/mitchelloharawild/icons",
    "last_updated": "2025-08-28T20:37:18+00:00"
  },
  {
    "full_name": "bfelbo/DeepMoji",
    "name": "DeepMoji",
    "description": "State-of-the-art deep learning model for analyzing sentiment, emotion, sarcasm etc.",
    "language": "Python",
    "topics": [
      "machine-learning",
      "deep-learning",
      "natural-language-processing",
      "python",
      "ai",
      "keras",
      "neural-networks",
      "nlp",
      "sentiment-analysis",
      "tensorflow",
      "text-classification"
    ],
    "readme": "### ------ Update September 2023 ------\n\nThe online demo is no longer available as it's not possible for us to renew the certificate. The code in this repo still works, but you might have to make some changes for it to work in Python 3 (see the open PRs). You can also check out the PyTorch version of this algorithm called [torchMoji](https://github.com/huggingface/torchMoji) made by HuggingFace.\n\n# DeepMoji\n\n[![DeepMoji Youtube](https://img.youtube.com/vi/u_JwYxtjzUs/0.jpg)](https://www.youtube.com/watch?v=u_JwYxtjzUs)  \n*(click image for video demonstration)*\n  \nDeepMoji is a model trained on 1.2 billion tweets with emojis to understand how language is used to express emotions. Through transfer learning the model can obtain state-of-the-art performance on many emotion-related text modeling tasks.\n  \nSee the [paper](https://arxiv.org/abs/1708.00524) or [blog post](https://medium.com/@bjarkefelbo/what-can-we-learn-from-emojis-6beb165a5ea0) for more details.\n\n## Overview\n* [deepmoji/](deepmoji) contains all the underlying code needed to convert a dataset to our vocabulary and use our model.\n* [examples/](examples) contains short code snippets showing how to convert a dataset to our vocabulary, load up the model and run it on that dataset.\n* [scripts/](scripts) contains code for processing and analysing datasets to reproduce results in the paper.\n* [model/](model) contains the pretrained model and vocabulary.\n* [data/](data) contains raw and processed datasets that we include in this repository for testing.\n* [tests/](tests) contains unit tests for the codebase.\n  \nTo start out with, have a look inside the [examples/](examples) directory. See [score_texts_emojis.py](examples/score_texts_emojis.py) for how to use DeepMoji to extract emoji predictions, [encode_texts.py](examples/encode_texts.py) for how to convert text into 2304-dimensional emotional feature vectors or [finetune_youtube_last.py](examples/finetune_youtube_last.py) for how to use the model for transfer lea",
    "url": "https://github.com/bfelbo/DeepMoji",
    "last_updated": "2025-08-19T09:29:40+00:00"
  },
  {
    "full_name": "asciinema/asciinema",
    "name": "asciinema",
    "description": "Terminal session recorder 📹",
    "language": "Rust",
    "topics": [
      "asciicast",
      "terminal",
      "recorder",
      "recording",
      "cli",
      "rust",
      "asciinema"
    ],
    "readme": "# asciinema\n\n[![Build Status](https://github.com/asciinema/asciinema/actions/workflows/ci.yml/badge.svg)](https://github.com/asciinema/asciinema/actions/workflows/asciinema.yml)\n[![license](http://img.shields.io/badge/license-GNU-blue.svg)](https://raw.githubusercontent.com/asciinema/asciinema/master/LICENSE)\n\n__asciinema__ (aka asciinema CLI or asciinema recorder) is a command-line tool\nfor recording terminal sessions.\n\nUnlike typical _screen_ recording software, which records visual output of a\nscreen into a heavyweight video files (`.mp4`, `.mov`), asciinema recorder runs\n_inside a terminal_, capturing terminal session output into a lightweight\nrecording files in the\n[asciicast](https://docs.asciinema.org/manual/asciicast/v2/) format (`.cast`).\n\nThe recordings can be replayed in a terminal, embedded on a web page with the\n[asciinema player](https://docs.asciinema.org/manual/player/), or published to\nan [asciinema server](https://docs.asciinema.org/manual/server/), such as\n[asciinema.org](https://asciinema.org), for further sharing.\n\n[![asciinema CLI\ndemo](https://asciinema.org/a/85R4jTtjKVRIYXTcKCNq0vzYH.svg)](https://asciinema.org/a/85R4jTtjKVRIYXTcKCNq0vzYH?autoplay=1)\n\nNotable features:\n\n* [recording](https://docs.asciinema.org/manual/cli/usage/#asciinema-rec-filename)\n  and\n  [replaying](https://docs.asciinema.org/manual/cli/usage/#asciinema-play-filename)\n  of sessions inside a terminal,\n* live streaming of terminal sessions, via local HTTP server, and via remote asciinema server,\n* [light-weight recording\n  format](https://docs.asciinema.org/manual/asciicast/v2/), which is highly\n  compressible (down to 15% of the original size e.g. with `zstd` or `gzip`),\n* integration with [asciinema\n  server](https://docs.asciinema.org/manual/server/), e.g.\n  [asciinema.org](https://asciinema.org), for easy recording hosting.\n\nTo record a session run this command in your shell:\n\n```sh\nasciinema rec demo.cast\n```\n\nTo stream a session via built-in HTTP server run:\n\n```sh\na",
    "url": "https://github.com/asciinema/asciinema",
    "last_updated": "2025-09-02T07:22:55+00:00"
  },
  {
    "full_name": "jonocarroll/mathpix",
    "name": "mathpix",
    "description": "Query the mathpix API to convert math images to LaTeX",
    "language": "R",
    "topics": [
      "r",
      "latex"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n![](./tools/logo_blue.png)\n\n# mathpix\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/mathpix)](https://cran.r-project.org/package=mathpix)\n[![packageversion](https://img.shields.io/badge/Package%20version-0.3.0-orange.svg?style=flat-square)](commits/master)\n[![Last-changedate](https://img.shields.io/badge/last%20change-2023--11--06-yellowgreen.svg)](/commits/master)\n\n[![Linux/Mac Travis Build\nStatus](https://img.shields.io/travis/jonocarroll/mathpix/master.svg?label=Mac%20OSX%20%26%20Linux)](https://travis-ci.org/jonocarroll/mathpix)\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/jonocarroll/mathpix?branch=master&svg=true)](https://ci.appveyor.com/project/jonocarroll/mathpix)\n[![codecov](https://codecov.io/gh/jonocarroll/mathpix/branch/master/graph/badge.svg)](https://codecov.io/gh/jonocarroll/mathpix)\n\n[![Downloads](http://cranlogs.r-pkg.org/badges/mathpix)](http://www.r-pkg.org/pkg/mathpix)\n[![GitHub\nforks](https://img.shields.io/github/forks/jonocarroll/mathpix.svg)](https://github.com/jonocarroll/mathpix/network)\n[![GitHub\nstars](https://img.shields.io/github/stars/jonocarroll/mathpix.svg)](https://github.com/jonocarroll/mathpix/stargazers)\n[![Twitter](https://img.shields.io/twitter/url/https/github.com/jonocarroll/mathpix.svg?style=social)](https://twitter.com/intent/tweet?text=Wow:&url=%5Bobject%20Object%5D)\n\n## Installation:\n\n`mathpix` is now on CRAN, so you can install using\n\n``` r\ninstall.packages(\"mathpix\")\n```\n\nIf you’re after newer development features (if there are any); you can\ninstall from GitHub using\n\n``` r\ndevtools::install_github(\"jonocarroll/mathpix\")\n```\n\n## Authentication\n\nThe free API key has been removed due to overuse. Please sign up a",
    "url": "https://github.com/jonocarroll/mathpix",
    "last_updated": "2025-08-30T03:41:12+00:00"
  },
  {
    "full_name": "HenrikBengtsson/R.devices",
    "name": "R.devices",
    "description": "🎨 R package: Unified Handling of Graphics Devices",
    "language": "R",
    "topics": [
      "r",
      "package",
      "graphics"
    ],
    "readme": "\n\n<div id=\"badges\"><!-- pkgdown markup -->\n<a href=\"https://CRAN.R-project.org/web/checks/check_results_R.devices.html\"><img border=\"0\" src=\"https://www.r-pkg.org/badges/version/R.devices\" alt=\"CRAN check status\"/></a> <a href=\"https://github.com/HenrikBengtsson/R.devices/actions?query=workflow%3AR-CMD-check\"><img border=\"0\" src=\"https://github.com/HenrikBengtsson/R.devices/actions/workflows/R-CMD-check.yaml/badge.svg?branch=develop\" alt=\"R CMD check status\"/></a>     <a href=\"https://app.codecov.io/gh/HenrikBengtsson/R.devices\"><img border=\"0\" src=\"https://codecov.io/gh/HenrikBengtsson/R.devices/branch/develop/graph/badge.svg\" alt=\"Coverage Status\"/></a> \n</div>\n\n# R.devices: Unified Handling of Graphics Devices \n\n\n## Installation\nR package R.devices is available on [CRAN](https://cran.r-project.org/package=R.devices) and can be installed in R as:\n```r\ninstall.packages(\"R.devices\")\n```\n\n\n### Pre-release version\n\nTo install the pre-release version that is available in Git branch `develop` on GitHub, use:\n```r\nremotes::install_github(\"HenrikBengtsson/R.devices\", ref=\"develop\")\n```\nThis will install the package from source.  \n\n<!-- pkgdown-drop-below -->\n\n",
    "url": "https://github.com/HenrikBengtsson/R.devices",
    "last_updated": "2025-05-21T04:28:04+00:00"
  },
  {
    "full_name": "benbalter/site-inspector",
    "name": "site-inspector",
    "description": "Ruby Gem to sniff information about a domain's technology and capabilities.",
    "language": "Ruby",
    "topics": [],
    "readme": "# Site Inspector\n\nA Ruby Gem to sniff information about a domain's technology and capabilities.\n\n[![Gem Version](https://badge.fury.io/rb/site-inspector.svg)](http://badge.fury.io/rb/site-inspector) [![Build Status](https://travis-ci.org/benbalter/site-inspector.svg)](https://travis-ci.org/benbalter/site-inspector)\n\n## Demo\n\n[site-inspector.herokuapp.com](https://site-inspector.herokuapp.com) ([source](https://github.com/benbalter/site-inspector-demo))\n\n## Concepts\n\nSite Inspector involves three primary concepts:\n\n* **Domain** - A domain has a host defined by it's TLD + SLD. A domain might be `example.com`. Domain's have certain domain-wide properties like whether it supports non-www requests, or if it enforces HTTPS.\n\n* **Endpoint** - Each domain has four endpoints based on whether you make your request with HTTPS or not, and whether you prefix the host with `www.` or not. So the domain `example.com` may have endpoints at `https://example.com`, `https://www.example.com`, `http://example.com`, and `https://www.example.com`. There may theoretically be a different server responding to each endpoint, so endpoints have certain endpoint-specific properties, like whether it responds or not, or whether it redirects. Each domain has one canonical (primary) endpoint.\n\n* **Checks** - A check is a set of tests performed on an endpoint. A check might look at what headers are returned, what CMS is used, or whether there is a valid HTTPS certificate. There are some built in checks, listed below, or you can define your own. While they're endpoint specific, checks often filter up and inform some of the domain-wide logic (such as if the domain supports HTTPS).\n\n## Usage\n\n### Ruby\n\n```ruby\ndomain = SiteInspector.inspect \"whitehouse.gov\"\ndomain.https?\n#  =>  true\ndomain.www?\n#  =>  true\ndomain.canonical_endpoint.to_s\n#  => \"https://www.whitehouse.gov\"\ndomain.canonical_endpoint.sniffer.cms\n#  =>  { :drupal  =>  {}}\n```\n\n### Command line usage\n\n```\nsite-inspector inspect -- inspects a d",
    "url": "https://github.com/benbalter/site-inspector",
    "last_updated": "2024-05-18T12:08:06+00:00"
  },
  {
    "full_name": "traitecoevo/datastorr",
    "name": "datastorr",
    "description": "Simple data versioning and distribution",
    "language": "R",
    "topics": [
      "r",
      "r-package",
      "rstats"
    ],
    "readme": "# datastorr\n\nSimple data retrieval and versioning using GitHub\n\nThis project is described in a [paper](https://peerj.com/preprints/3401v1) by [Daniel Falster](https://github.com/dfalster/), [Rich FitzJohn](https://github.com/richfitz/), [Matt Pennell](https://github.com/mwpennell/), and [Will Cornwell](https://github.com/wcornwell/). Below we describe the motivation and general idea. Please see the paper for full details.\n\n## The problem\n\nOver the last several years, there has been an increasing recognition that data is a first-class scientific product and a tremendous about of repositories and platforms have been developed to facilitate the storage, sharing, and re-use of data. However we think there is still an important gap in this ecosystem: platforms for data sharing offer limited functions for distributing and interacting with evolving datasets - those that continue to grow with time as more records are added, errors fixed, and new data structures are created. This is particularly the case for small to medium sized datasets that a typical scientific lab, or collection of labs, might produce.\n\nIn addition to enabling data creators to maintain and share a `living` dataset, ideally, such an infrastructure would allow enable data users to:\n\n* Cache downloads, including across R sessions, to make things faster and to work offline\n* Keep track of which versions are downloaded and available remotely\n* Access multiple versions of the data at once; this would be especially helpful if trying to understand why results have changed with the version of the data.\n\n## How datastorr helps\n\nThis package can be used in two ways:\n\n1. Use data stored elsewhere in R efficiently (e.g., work with csv files that are too large to comfortably fit in git).\n2. Create another lightweight package designed to allow easy access to your data.\n\nFor both of these use-cases, `datastorr` will store your data using _GitHub releases_ which do not clog up your repository but allow up to 2GB files to",
    "url": "https://github.com/traitecoevo/datastorr",
    "last_updated": "2025-03-22T08:14:38+00:00"
  },
  {
    "full_name": "rajbanjade/Latent-Dirichlet-Allocation-In-R",
    "name": "Latent-Dirichlet-Allocation-In-R",
    "description": "R implementation of Collapsed Gibbs Sampling based Latent Dirichlet Allocation (LDA)",
    "language": "R",
    "topics": [],
    "readme": "# Latent Dirichlet Allocation (LDA)\n This is an R implementation of \n Latent Dirichlet Allocation (LDA) using (Collapsed) Gibbs Sampling.\n\nPlease go through the code. I hope that the comments in the code itself give you a walk through of it.\nAdditional details maybe available, if not here, at https://umdrive.memphis.edu/rbanjade/public/\n\nTo run it, all you need is to:   \n* provide an input file, each line containing a document and tokens separated by one or more whitespaces or tabs. A sample input        file can be found along with this file. The test file contains texts from Microsoft Research Paraphrase (MSRP) corpus. I don't        think it's a best corpus for topic modeling but I am using it just for demonstration purpose. You may test with some different        input, preferably a bigger collection of documents.\n* set few parameters (optional). The default parameter values will be used otherwise.   \n\nI would suggest you review following topics before reading LDA.  \n   (a) Distributions - Multinomial distribution, Dirichlet distribution  \n   (b) Sampling - Gibbs Sampling  \n   (c) Conjugate distributions  \n\nMain references:  \n     https://www.youtube.com/watch?v=DDq3OVp9dNA (David Blei's talk)  \n     http://u.cs.biu.ac.il/~89-680/darling-lda.pdf  \n\n\n Thanks!\n \n Rajendra\n",
    "url": "https://github.com/rajbanjade/Latent-Dirichlet-Allocation-In-R",
    "last_updated": "2020-12-24T07:43:57+00:00"
  },
  {
    "full_name": "alexbyrnes/Datapiece",
    "name": "Datapiece",
    "description": "Investigative tool for extracting relevant areas from many documents",
    "language": "Scala",
    "topics": [],
    "readme": "## Datapiece\n\nDatapiece does high performance page segmentation for documents with tables, fill-in-the-blanks, or other small areas of interest across many files.  The output is the interest areas cropped exactly based on an estimated location.  This is primarily useful for large transcription efforts with Optical Character Recognition (OCR) but should help document processing for publication, storage, or to make reading the documents easier.\n\nDocuments with data in them make OCR on whole pages difficult. Data tends to come in small strings of characters surrounded by lines and boxes.\n\n\n## Motivation\n\nThere are significant differences between documents where the area of interest is paragraph text, and \"data documents.\"\n\nData documents:\n\n* Have little context to use for correcting errors. Values are usually codes, dates, and numbers, or non-dictionary words such as first and last names.\n* Have markup used to identify the data to the human eye that is difficult for OCR applications to distinguish from characters and symbols.\n* Come in large numbers: disclosure forms, [tax documents](https://archive.org/details/IRS990), election results, and other institutional forms.\n\n\n\n## Installation\n\n### Requirements\n\n     Java 1.7+\n\nDownload a [pre-built jar file](https://github.com/alexbyrnes/Datapiece/blob/master/datapiece.jar?raw=true), or rebuild the archive with [SBT](http://www.scala-sbt.org/release/tutorial/index.html) using `sbt assembly`.  Install according to your preference and OS conventions.  \n\nHowever, the simplest way to get started with input files in place and commands ready is to clone the repo and run the examples:\n\n    git clone https://github.com/alexbyrnes/Datapiece.git\n    cd Datapiece\n    ./run-examples.sh\n    \nYou should see images like the following in the \"out\" directory:\n\n\n![contract dates](https://raw.githubusercontent.com/alexbyrnes/Datapiece/master/out/contract2_advertiser.png)\n---\n\n\n### Usage\n\nPrepare a JSON or CSV file with a bounding box for each ",
    "url": "https://github.com/alexbyrnes/Datapiece",
    "last_updated": "2019-01-20T09:56:28+00:00"
  },
  {
    "full_name": "armstrtw/CppBugs",
    "name": "CppBugs",
    "description": "c++ version of BUGS",
    "language": "C++",
    "topics": [],
    "readme": "### Purpose\n\nCppBugs is a c++ library designed for MCMC sampling.\n\n* CppBugs is now depricated in favor of [stan](http://mc-stan.org).\n\n\n### Features\n\nCppBugs attempts to make writing mcmc models as painless as possible.  It incorporates features\nfrom both WinBugs and PyMC and requires users only to implment an update method which resembles the model section of a WinBUGS script.\n\n* CppBugs is fast.  Typically between 5x to 10x faster than equivalent WinBugs and 3x to 5x faster than PyMC models.\n\n* Common statistical distributions are supported drawing heavily on Boost libraries.  Many more will be implemented\n  to eventually be as feature complete as WinBugs/PyMC. \n\n\n### Usage\n\nStarting with a bugs model:\n```{.bug}\nmodel {\n    for (j in 1:J){\n        y[j] ~ dnorm (theta[j], tau.y[j])\n        theta[j] ~ dnorm (mu.theta, tau.theta)\n        tau.y[j] <- pow(sigma.y[j], -2)\n    }\n    mu.theta ~ dnorm (0, 1.0E-6)\n    tau.theta <- pow(sigma.theta, -2)\n    sigma.theta ~ dunif (0, 1000)\n}\n```\n\nThis mode can be converted to a CppBugs model in two steps.\n\n* define the variable space\n\n* link each variable with its dependencies\n\n```{.cpp}\n\n  const int J = 8;\n  const vec sigma_y({15,10,16,11,9,11,10,18});\n  const vec tau_y = pow(sigma_y,-2);\n  const vec y({28,  8, -3,  7, -1,  1, 18, 12});\n\n  double mu_theta(0);\n  double sigma_theta(1);\n  double tau_theta = pow(sigma_theta,-2);\n  vec theta = randn<vec>(J);\n\n  BoostRng<boost::minstd_rand> rng;\n  MCModel m(rng);\n\n  // noninformative prior on mu\n  m.link<Normal>(mu_theta, 0.0, 1.0E-6);\n\n  // noninformative prior on sigma\n  m.link<Uniform>(sigma_theta, 0, 1000);\n\n  m.link<InvVariance>(tau_theta,sigma_theta);\n  m.link<Normal>(theta,mu_theta,tau_theta);\n  m.link<ObservedNormal>(y, theta, tau_y);\n\n  // things to track\n  std::vector<vec>& theta_hist = m.track<std::vector>(theta);\n\n  m.tune(1e4,100);\n  m.tune_global(1e4,100);\n  m.burn(5e3);\n  m.sample(1e4, 1);\n```\n\nPlease see the test folder for more examples.\n",
    "url": "https://github.com/armstrtw/CppBugs",
    "last_updated": "2025-02-06T16:03:06+00:00"
  },
  {
    "full_name": "RcppCore/RcppParallel",
    "name": "RcppParallel",
    "description": "High-level functions for parallel programming with Rcpp",
    "language": "C++",
    "topics": [],
    "readme": "\n## RcppParallel\n\n<!-- badges: start -->\n[![CRAN](https://www.r-pkg.org/badges/version/RcppParallel)](https://cran.r-project.org/package=RcppParallel)\n[![R-CMD-check](https://github.com/RcppCore/RcppParallel/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/RcppCore/RcppParallel/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\nHigh level functions for parallel programming with Rcpp. The `parallelFor()` function can be used to convert the work of a standard serial \"for\" loop into a parallel one, and the `parallelReduce()` function can be used for accumulating aggregate or other values.\n\nThe high level interface enables safe and robust parallel programming without direct manipulation of operating system threads. On Windows, macOS, and Linux systems, the underlying implementation is based on [Intel TBB](https://github.com/oneapi-src/oneTBB) (Threading Building Blocks). On other platforms, a less-performant fallback implementation based on the [TinyThread](https://tinythreadpp.bitsnbites.eu/) library is used.\n\nFor additional documentation on using RcppParallel see the package website at http://rcppcore.github.io/RcppParallel/.\n\n\n### Intel TBB\n\n`RcppParallel` supports the new interface of Intel TBB, and can be configured to use an external copy of TBB (e.g., with [`oneTBB`](https://github.com/oneapi-src/oneTBB) or the system TBB library), using the `TBB_LIB` and `TBB_INC` environment variables.\n\nTo build the development version of `RcppParallel` with [`oneTBB`](https://github.com/oneapi-src/oneTBB):\n\n- Install [`oneTBB`](https://github.com/oneapi-src/oneTBB).\n\nFor example, installing [`oneTBB`](https://github.com/oneapi-src/oneTBB) on Linux 64-bit (`x86_64`) to `$HOME` directory (change if needed!):\n\n```bash\nTBB_RELEASE=\"https://api.github.com/repos/oneapi-src/oneTBB/releases/latest\"\nTBB_TAG=$(curl --silent $TBB_RELEASE | grep -Po '\"tag_name\": \"\\K.*?(?=\")')\nTBB_VERSION=${TBB_TAG#?}\n\nwget https://github.com/oneapi-src/oneTBB/releases/download/v$TB",
    "url": "https://github.com/RcppCore/RcppParallel",
    "last_updated": "2025-08-27T18:00:47+00:00"
  },
  {
    "full_name": "hadley/data-movies",
    "name": "data-movies",
    "description": "Download data from IMDB movies and parse into useful form",
    "language": "Ruby",
    "topics": [],
    "readme": "The internet movie database, [imdb.com](http://imdb.com/), is a website devoted to collecting movie data supplied by studios and fan.  It claims to be the biggest movie database on the web and is run by amazon.  More about information imdb.com can be found [online](http://imdb.com/help/show_leaf?about), including information about the [data collection process](http://imdb.com/help/show_leaf?infosource).\n\nIMDB makes their [raw data available](http://uk.imdb.com/interfaces/). Unfortunately, the data is divided into many text files and the format of each file differs slightly.  To create one data file containing all the desired information these ruby scripts extract the relevant information and store in a database.  Finally, this data is exported to csv to make it easier to import into data analysis packages.\n\nThe following text files were downloaded and used:\n\n* business.list. Total budget\n* genres.list.  Genres that a movie belongs to (eg. comedy and action)\n* movies.list.  Master list of all movie titles with year of production.\n* mpaa-ratings-reasons.list.  MPAA ratings.\n* ratings.list.  IMDB fan ratings.\n* running-times.list.  Movie length in minutes.\n\nMovies were selected for inclusion if they had a known length and had been rated by at least one IMDB user. The final output contains the following fields:\n\n* title.  Title of the movie.\n* year.  Year of release.\n* budget.  Total budget (if known) in US dollars\n* length.  Length in minutes.\n* rating.  Average IMDB user rating.\n* votes.  Number of IMDB users who rated this movie.\n* r1-10.  Distribution of votes for each rating, to mid point of nearest decile: 0 = no votes, 4.5 = 1-9$\\%$ votes, 14.5 = 11-19$\\%$ of votes, etc.  Due to rounding errors these may not sum to 100.\n* mpaa.  MPAA rating.\n* action, animation, comedy, drama, documentary, romance, short.  Binary variables representing if movie was classified as belonging to that genre.\n",
    "url": "https://github.com/hadley/data-movies",
    "last_updated": "2025-08-22T18:18:25+00:00"
  },
  {
    "full_name": "entelecheia/super-duper-waddle",
    "name": "super-duper-waddle",
    "description": "",
    "language": "Makefile",
    "topics": [],
    "readme": "# Super duper waddle\n\n[![pypi-image]][pypi-url]\n[![license-image]][license-url]\n[![version-image]][release-url]\n[![release-date-image]][release-url]\n[![jupyter-book-image]][docs-url]\n\n<!-- Links: -->\n[pypi-image]: https://img.shields.io/pypi/v/super-duper-waddle\n[license-image]: https://img.shields.io/github/license/entelecheia/super-duper-waddle\n[license-url]: https://github.com/entelecheia/super-duper-waddle/blob/main/LICENSE\n[version-image]: https://img.shields.io/github/v/release/entelecheia/super-duper-waddle?sort=semver\n[release-date-image]: https://img.shields.io/github/release-date/entelecheia/super-duper-waddle\n[release-url]: https://github.com/entelecheia/super-duper-waddle/releases\n[jupyter-book-image]: https://jupyterbook.org/en/stable/_images/badge.svg\n\n[repo-url]: https://github.com/entelecheia/super-duper-waddle\n[pypi-url]: https://pypi.org/project/super-duper-waddle\n[docs-url]: https://entelecheia.github.io/super-duper-waddle\n[changelog]: https://github.com/entelecheia/super-duper-waddle/blob/main/CHANGELOG.md\n[contributing guidelines]: https://github.com/entelecheia/super-duper-waddle/blob/main/CONTRIBUTING.md\n<!-- Links: -->\n\nSuper duper waddle projet\n\n- Documentation: [https://entelecheia.github.io/super-duper-waddle][docs-url]\n- GitHub: [https://github.com/entelecheia/super-duper-waddle][repo-url]\n- PyPI: [https://pypi.org/project/super-duper-waddle][pypi-url]\n\nSuper duper waddle pypi project\n\n## Changelog\n\nSee the [CHANGELOG] for more information.\n\n## Contributing\n\nContributions are welcome! Please see the [contributing guidelines] for more information.\n\n## License\n\nThis project is released under the [MIT License][license-url].\n",
    "url": "https://github.com/entelecheia/super-duper-waddle",
    "last_updated": "2025-01-15T15:28:49+00:00"
  },
  {
    "full_name": "multiversx/mx-sdk-py-transaction-decoder",
    "name": "mx-sdk-py-transaction-decoder",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "[![No Maintenance Intended](http://unmaintained.tech/badge.svg)](http://unmaintained.tech/)\n\nThis package is **deprecated**. All the functionality has been moved to the [sdk-py](https://github.com/multiversx/mx-sdk-py/tree/main) package.\n",
    "url": "https://github.com/multiversx/mx-sdk-py-transaction-decoder",
    "last_updated": "2025-01-15T15:29:14+00:00"
  },
  {
    "full_name": "fhamborg/news-please",
    "name": "news-please",
    "description": "news-please - an integrated web crawler and information extractor for news that just works",
    "language": "Python",
    "topics": [
      "news-crawler",
      "news-extractor",
      "crawler",
      "extractor",
      "news",
      "news-websites",
      "elasticsearch",
      "json",
      "python",
      "nlp",
      "data-gathering",
      "news-archive",
      "news-articles",
      "commoncrawl",
      "extract-articles",
      "extract-information",
      "news-scraper",
      "ccnews",
      "cc-news",
      "roberta"
    ],
    "readme": "# **news-please** #\n\n[![PyPI version](https://img.shields.io/pypi/v/news-please.svg)](https://pypi.org/project/news-please/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4120316.svg)](http://dx.doi.org/10.5281/zenodo.4120316)\n\n<img align=\"right\" height=\"128px\" width=\"128px\" src=\"https://raw.githubusercontent.com/fhamborg/news-please/master/misc/logo/logo-256.png\" />\n\nnews-please is an open source, easy-to-use news crawler that extracts structured information from almost any news website. It can recursively follow internal hyperlinks and read RSS feeds to fetch both most recent and also old, archived articles. You only need to provide the root URL of the news website to crawl it completely. news-please combines the power of multiple state-of-the-art libraries and tools, such as [scrapy](https://scrapy.org/), [Newspaper](https://github.com/AndyTheFactory/newspaper4k), and [readability](https://github.com/buriy/python-readability).\n\nnews-please also allows Python developers to use the crawling and extraction functionality within their own program. Moreover, news-please allows to conveniently [crawl and extract articles](/newsplease/examples/commoncrawl.py) from the (very) large news archive at commoncrawl.org.\n\nIf you want to contribute to news-please, please first read [here](#contributions).\n\n## Announcements\n\n03/23/2021: If you're interested in **sentiment classification** in news articles, check out our large-scale dataset for target-dependent sentiment classification. We also publish an easy-to-use neural model that achieves state-of-the-art performance. Visit the project [here](https://github.com/fhamborg/NewsMTSC).\n\n06/01/2018: If you're interested in **event extraction** from news, you might also want to check out our new project, [Giveme5W1H](https://github.com/fhamborg/Giveme5W1H) - a tool that extracts phrases answering the journalistic five W and one H questions to describe an article's main event, i.e., who did what, when, where, why, and how.\n\n## E",
    "url": "https://github.com/fhamborg/news-please",
    "last_updated": "2025-09-01T11:25:42+00:00"
  },
  {
    "full_name": "thomasp85/tidygraph",
    "name": "tidygraph",
    "description": "A tidy API for graph manipulation",
    "language": "R",
    "topics": [
      "r",
      "network-analysis",
      "igraph",
      "tidyverse",
      "graph-algorithms",
      "graph-manipulation"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tidygraph <img src=\"man/figures/logo.png\" align=\"right\" width=\"140px\"/>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/thomasp85/tidygraph/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/thomasp85/tidygraph/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Release_Badge](http://www.r-pkg.org/badges/version-ago/tidygraph)](https://CRAN.R-project.org/package=tidygraph)\n[![CRAN_Download_Badge](http://cranlogs.r-pkg.org/badges/tidygraph)](https://CRAN.R-project.org/package=tidygraph)\n[![Coverage\nStatus](https://codecov.io/gh/thomasp85/tidygraph/branch/main/graph/badge.svg)](https://app.codecov.io/gh/thomasp85/tidygraph?branch=main)\n<!-- badges: end -->\n\nThis package provides a tidy API for graph/network manipulation. While\nnetwork data itself is not tidy, it can be envisioned as two tidy\ntables, one for node data and one for edge data. `tidygraph` provides a\nway to switch between the two tables and provides `dplyr` verbs for\nmanipulating them. Furthermore it provides access to a lot of graph\nalgorithms with return values that facilitate their use in a tidy\nworkflow.\n\n## An example\n\n``` r\nlibrary(tidygraph)\n\nplay_gnp(10, 0.5) %>% \n  activate(nodes) %>% \n  mutate(degree = centrality_degree()) %>% \n  activate(edges) %>% \n  mutate(centrality = centrality_edge_betweenness()) %>% \n  arrange(centrality)\n#> # A tbl_graph: 10 nodes and 51 edges\n#> #\n#> # A directed simple graph with 1 component\n#> #\n#> # Edge Data: 51 × 3 (active)\n#>     from    to centrality\n#>    <int> <int>      <dbl>\n#>  1     2     7       1.25\n#>  2     6     5       1.33\n#>  3     1     3       1.4 \n#>  4     2    10       1.53\n#>  5     2     8       1.58\n#>  6     8     9       1.65\n#>  7     2     3       1.67\n#>  8     2     5       1.73\n#>  9     3     5       1.73\n#> 10     8     5       1.73\n#> # ℹ 41 more rows\n#> #\n#> # Node Data: 10 × 1\n#>   degree\n#>    <dbl>\n#> 1      6\n#> 2      7\n#> 3      6\n#>",
    "url": "https://github.com/thomasp85/tidygraph",
    "last_updated": "2025-08-28T20:35:11+00:00"
  },
  {
    "full_name": "robustness-gym/robustness-gym",
    "name": "robustness-gym",
    "description": "Robustness Gym is an evaluation toolkit for machine learning.",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n    <img src=\"docs/logo.png\" height=100 alt=\"RG logo\"/>\n    <h1 style=\"font-family: 'IBM Plex Sans'\">Robustness Gym</h1>\n</div>\n\n![GitHub Workflow Status](https://img.shields.io/github/workflow/status/robustness-gym/robustness-gym/CI)\n![GitHub](https://img.shields.io/github/license/robustness-gym/robustness-gym)\n[![Documentation Status](https://readthedocs.org/projects/robustnessgym/badge/?version=latest)](https://robustnessgym.readthedocs.io/en/latest/?badge=latest)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![website](https://img.shields.io/badge/website-live-brightgreen)](https://robustnessgym.com)\n\n[comment]: <> ([![codecov]&#40;https://codecov.io/gh/robustness-gym/robustness-gym/branch/main/graph/badge.svg?token=MOLQYUSYQU&#41;]&#40;https://codecov.io/gh/robustness-gym/robustness-gym&#41;)\n\nRobustness Gym is a Python evaluation toolkit for machine learning models. \n\n[**Getting Started**](#getting-started)\n| [**What is Robustness Gym?**](#what-is-robustness-gym)\n| [**Docs**](https://robustnessgym.readthedocs.io/en/latest/index.html)\n| [**Contributing**](CONTRIBUTING.md)\n| [**About**](#about)\n\n\n### Getting started\n```\npip install robustnessgym\n```\n> Note: some parts of Robustness Gym rely on optional dependencies. \n> If you know which optional dependencies you'd like to install, \n> you can do so using something like `pip install robustnessgym[dev,text]` instead. \n> See `setup.py` for a full list of optional dependencies.\n\n### What is Robustness Gym?\nRobustness Gym is being developed to address challenges in evaluating machine \nlearning models today, with tools to evaluate and visualize the quality of machine \nlearning models. \n\nAlong with [Meerkat](https://github.com/robustness-gym/mosaic), \nwe make it easy for you to load in any kind of data \n(text, images, videos, time-series) and quickly evaluate how well your models are \nperformin",
    "url": "https://github.com/robustness-gym/robustness-gym",
    "last_updated": "2025-08-18T23:36:12+00:00"
  },
  {
    "full_name": "newslynx/siegfried",
    "name": "siegfried",
    "description": "Tools for taming lynx.",
    "language": "Python",
    "topics": [],
    "readme": "![travis-img](https://travis-ci.org/newslynx/siegfried.svg)\nsiegfried\n======================\nTools for taming lynx.\n\n**NOTE**: All functionality of this library has been improved upon and ported to [NewsLynx V2](http://github.com/newslynx/newslynx)\n\n## Install\n```\npip install siegfried\n```\n\n## Test\nrequires `nose`\n```\nnosetests\n```\n\n## Usage\n\nThis module contains various methods that are used throughout `newslnyx`.\nbut the main functions are `unshorten_url`, `is_article_url`, and `prepare_url`:\n\n```python\nfrom siegfried import (\n  unshorten_url, is_article_url, prepare_url\n)\n\nprint unshorten_url('bit.ly/1j3SrUC')\n# http://towcenter.org/blog/tow-fellows-brian-abelson-and-michael-keller-to-study-the-impact-of-journalism/\n\nprint is_article_url(\n  'http://towcenter.org/blog/tow-fellows-brian-abelson-and-michael-keller-to-study-the-impact-of-journalism'\n  )\n# True\n\nprint is_article_url(\n  'http://towcenter.org/blog/tow-fellows-brian-abelson-and-michael-keller-to-study-the-impact-of-journalism',\n  pattern = r'.*towcenter\\.org/blog/.*'\n)\n# True\n\nimport re\npattern = re.compile(r'.*towcenter\\.org/blog/.*')\nprint is_article_url(\n  'http://towcenter.org/blog/tow-fellows-brian-abelson-and-michael-keller-to-study-the-impact-of-journalism',\n  pattern = pattern\n)\n# True\n\nprint prepare_url(\n  'http://towcenter.org/blog/tow-fellows-brian-abelson-and-michael-keller-to-study-the-impact-of-journalism/?q=lfjad&f=lkfdjsal'\n  )\n# http://towcenter.org/blog/tow-fellows-brian-abelson-and-michael-keller-to-study-the-impact-of-journalism\n```\n",
    "url": "https://github.com/newslynx/siegfried",
    "last_updated": "2025-04-12T14:43:52+00:00"
  },
  {
    "full_name": "heike/cityshapes",
    "name": "cityshapes",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# cityshapes",
    "url": "https://github.com/heike/cityshapes",
    "last_updated": "2018-04-10T14:55:40+00:00"
  },
  {
    "full_name": "gojiplus/abbyyR",
    "name": "abbyyR",
    "description": "R Client for the Abbyy Cloud OCR ",
    "language": "HTML",
    "topics": [
      "ocr-engine",
      "ocr",
      "abbyy-cloud-ocr",
      "cran"
    ],
    "readme": "## Access Abbyy Cloud OCR from R\n\n[![Build Status](https://travis-ci.org/soodoku/abbyyR.svg?branch=master)](https://travis-ci.org/soodoku/abbyyR)\n[![Appveyor Build status](https://ci.appveyor.com/api/projects/status/yh856e6cv7uucaj2?svg=true)](https://ci.appveyor.com/project/soodoku/abbyyR)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/abbyyR)](https://cran.r-project.org/package=abbyyR)\n![](http://cranlogs.r-pkg.org/badges/grand-total/abbyyR)\n[![codecov](https://codecov.io/gh/soodoku/abbyyR/branch/master/graph/badge.svg)](https://codecov.io/gh/soodoku/abbyyR)\n[![Research software impact](http://depsy.org/api/package/cran/abbyyR/badge.svg)](http://depsy.org/package/r/abbyyR)\n[![Github Stars](https://img.shields.io/github/stars/soodoku/abbyyR.svg?style=social&label=Github)](https://github.com/soodoku/abbyyR)\n\nEasily OCR images, barcodes, forms, documents with machine readable zones, e.g. passports, right from R. Get the results in a wide variety of formats, from text files to detailed XMLs with information about bounding boxes, etc.\n\nThe package provides access to the [Abbyy Cloud OCR SDK API](http://ocrsdk.com/). Details about results of calls to the API can be [found here](http://ocrsdk.com/documentation/specifications/status-codes/).\n\n### Installation\n\nTo get the latest version on CRAN:\n```r\ninstall.packages(\"abbyyR\")\n```\n\nTo get the current development version from GitHub:\n\n```r\n# install.packages(\"devtools\")\ndevtools::install_github(\"soodoku/abbyyR\", build_vignettes = TRUE)\n```\n\n### Using abbyyR\n\nTo get acquainted with some of the important functions, read the vignettes:\n\n```r\n# Overview of the package\nvignette(\"introduction\", package = \"abbyyR\")\n# some functions are used along with output\nvignette(\"example\", package = \"abbyyR\")\n# how to scrape text from a folder of images\nvignette(\"wiscads\", package = \"abbyyR\")\n```\n\nThe final output quality varies by complexity of the layout to resolution to font face etc. To measure the final quality of ocr, you can",
    "url": "https://github.com/gojiplus/abbyyR",
    "last_updated": "2024-03-30T21:53:59+00:00"
  },
  {
    "full_name": "xuyiqing/interflex",
    "name": "interflex",
    "description": "Multiplicative Interaction Models Diagnostics and Visualization, Producing Flexible Marginal Effect Estimates",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# interflex\n\n<!-- badges: start -->\n\n[![Lifecycle:\nexperimental](https://img.shields.io/badge/lifecycle-stable-green.svg)](https://www.tidyverse.org/lifecycle/#stablel)\n[![License:\nMIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![downloads:\nCRAN](https://cranlogs.r-pkg.org/badges/grand-total/interflex)](https://www.datasciencemeta.com/rpackages)\n<!-- badges: end -->\n\n**interflex** estimates, visualizes, and interprets conditional marginal\neffects and performs diagnostics.\n\n**Authors:** Jens Hainmueller, Jiehan Liu, Licheng Liu, Ziyi Liu,\nJonathan Mummolo, Tianzhu Qin, and [Yiqing Xu](https://yiqingxu.org/)\n(maintainer)\n\n**Date:** Feburary 12, 2025\n\n**Repos:** [Github](https://github.com/xuyiqing/interflex) (1.3.2)\n[CRAN](https://cran.r-project.org/web/packages/interflex/index.html)\n(1.2.6)\n\n**Tutoralis:** See tutorials for cases with\n[continuous](https://yiqingxu.org/packages/interflex/articles/continuous.html)\nand\n[discrete](https://yiqingxu.org/packages/interflex/articles/discrete.html)\noutcomes, as well as a tutorial for double/debiased machine learning\n([DML](https://yiqingxu.org/packages/interflex/articles/dml.html))\nestimators.\n\n**Examples:** R code used in the tutorials can be downloaded from\n[here](examples.R).\n\n**Reference:** [*How Much Should We Trust Estimates from Multiplicative\nInteraction Models? Simple Tools to Improve Empirical\nPractice*](http://bit.ly/HMX2019). Political Analysis, Vol. 27, Iss. 2,\nApril 2019, pp. 163–192.\n\n------------------------------------------------------------------------\n\n## Installation\n\nYou can install the **interflex** package from CRAN:\n\n``` r\ninstall.packages('interflex', type = \"source\", \n                 repos = 'http://cran.us.r-project.org') \n```\n\nOr you can install the up-to-date development version from Github:\n\n``` r\n# if not already installed\ninstall.packages('devtools', repos = 'http://cran.",
    "url": "https://github.com/xuyiqing/interflex",
    "last_updated": "2025-07-08T13:11:16+00:00"
  },
  {
    "full_name": "daroczig/Mastering-Data-Analysis-with-R",
    "name": "Mastering-Data-Analysis-with-R",
    "description": "R code examples for my book",
    "language": "R",
    "topics": [],
    "readme": "## [Mastering Data Analysis with R](http://bit.ly/mastering-R)\n\nThis repository includes the example R source code and data files for the above referenced book published at Packt Publishing in 2015. \n\nThe above R files are identical to the R code examples found in the book except for the leading `>` and `+` characters, which stand for the prompt in the R console. As the book included both the R commands and output, the prompt was also shown in the examples so that the reader can easily distinguish the calls from the returned values:\n\n```r\n> set.seed(42)\n> data.frame(\n+  A = runif(2),\n+  B = sample(letters, 2))\nA B\n1 0.9148060 h\n2 0.9370754 u\n```\n\nIn this repository, both the output and the prompt were intentionally removed here along with arbitrary line-breaks, so that you copy and paste the R expressions to the R console in a more convenient and seamless way.\n\nThe code chunks are grouped by the printed pages of the book. Two hash signs at the beginning of a line stands for a page break, while an extra empty line between the code chunks represents one or more paragraphs in the original book between the examples for easier navigation. Sometimes extra instructions starting with a double hash are also provided on how to run the below expressions.\n\nPlease find more information on the book at http://bit.ly/mastering-R and you can contact me on [Twitter](https://twitter.com/daroczig) and [GitHub](https://github.com/daroczig) in case of any question or feedback.\n\nI hope you will enjoy and find useful this book!\n",
    "url": "https://github.com/daroczig/Mastering-Data-Analysis-with-R",
    "last_updated": "2024-11-04T13:37:42+00:00"
  },
  {
    "full_name": "pystitch/stitch",
    "name": "stitch",
    "description": "Write reproducible reports in Markdown",
    "language": "Python",
    "topics": [],
    "readme": "Stitch\n======\n\n|Build Status|\n\nA `knitr <http://yihui.name/knitr/>`__- `RMarkdown <http://rmarkdown.rstudio.com>`__-like library, in Python.\n\n*Note:* You might want to consider Jan Schulz's `knitpy <https://github.com/janschulz/knitpy/>`__\ninstead. It's probably more mature at this point. However, I wanted\nto see if there was a simpler way of doing things.\n\nThe high-level goal of this type of library (knitr/RMarkdown, knitpy, and stitch) is to make writing\nreproducible reports easier.\n\nDocumentation is available `here <https://pystitch.github.io>`__.\n\nExamples\n========\n\nSee the project's `examples page <https://pystitch.github.io/_downloads/side_by_side.html>`__ for a\nside-by-side comparison of input markdown and stitched HTML.\n\nMore complex examples are linked to from there as well.\n\nInstall\n=======\n\n``stitch`` supports Python 3.5 and above.\nAt the moment ``stitch`` can be installed from pip via\n\n.. code-block:: sh\n\n   pip install knotr\n\nI know, it's confusing.\nI've filed a claim for ``stitch`` on PyPI, but I think the people working that support queue are over-worked.\nOnce that gets processed, I'll put it up on conda-forge as well.\nIf you need a mnemonic, it's \"I want knitr, but `not` the one written in `R`.\"\nAlso I wanted to confuse R users.\nAnd knots are kind of like a buggy version of knits.\n\n``stitch`` requires pandoc>=1.18. This can be installed using your\nsystem package manager, or `pypandoc <https://pypi.python.org/pypi/pypandoc>`__.\n\nDesign\n======\n\nThe goal was to keep ``stitch`` itself extremely simple by reusing\nexisting libraries. A high level overview of our tasks is\n\n1. Command-line Interface\n2. Parse markdown file\n3. Execute code chunks, capturing the output\n4. Collate execution output into the document\n5. Render to final output\n\nFortunately the building blocks are all there.\n\nWe reuse\n\n-  `pandoc <http://pandoc.org>`__ via\n   `pypandoc <https://pypi.python.org/pypi/pypandoc>`__ for parsing\n   markdown and rendering the final output\n-  `jupyter <http",
    "url": "https://github.com/pystitch/stitch",
    "last_updated": "2025-06-16T21:15:39+00:00"
  },
  {
    "full_name": "ropensci/open-science-with-R",
    "name": "open-science-with-R",
    "description": "⛔️ [SUSPENDED] Writing the rOpenSci book on GitHub",
    "language": "TeX",
    "topics": [],
    "readme": "[![Project Status: Suspended – Initial development has started, but there has not yet been a stable, usable release; work has been stopped for the time being but the author(s) intend on resuming work.](http://www.repostatus.org/badges/latest/suspended.svg)](http://www.repostatus.org/#suspended)\n\n# Open Science with R\n\nPublic repo for a upcoming book to be published by Taylor and Francis/CRC Press.  Tentative chapter list:\n\n| CONTENTS  |\n| --------  |\n| 00 Front Matter  |\n| 01 Introduction  |\n| 02 Reproducible research  |\n| __Consuming Data__   |\n| 03 Internet Databases  |\n| 04 Developing Packages for web data  |\n| __Sharing Data__  |\n| 05 Sharing data|\n| 06 Dynamic Documents  |\n| 07 Open Lab Notebooks   |\n| 08 Open Science Workflows  |\n| 09 The rOpenSci Package Suite  |\n\n---\n\n[![](http://ropensci.org/public_images/github_footer.png)](http://ropensci.org)\n",
    "url": "https://github.com/ropensci/open-science-with-R",
    "last_updated": "2024-10-21T15:49:41+00:00"
  },
  {
    "full_name": "mp2893/doctorai",
    "name": "doctorai",
    "description": "Repository for Doctor AI project",
    "language": "Python",
    "topics": [],
    "readme": "Doctor AI\n=========================================\n\nDoctor AI is a automatic diagnosis machine that predicts medical codes that occur in the next visit, while also predicting the time duration until the next visit.\n\n#### Relevant Publications\n\nDoctor AI implements an algorithm introduced in the following:\n\n\tDoctor AI: Predicting Clinical Events via Recurrent Neural Networks  \n\tEdward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun  \n\tarXiv preprint arXiv:1511.05942\n\t\n\tMedical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction  \n\tEdward Choi, Andy Schuetz, Walter F. Stewart, Jimeng Sun  \n\tarXiv preprint arXiv:1602.03686\n\n#### Running Doctor AI\n\n**STEP 1: Installation**  \n\n1. Install [python](https://www.python.org/), [Theano](http://deeplearning.net/software/theano/index.html). We use Python 2.7, Theano 0.7. Theano can be easily installed in Ubuntu as suggested [here](http://deeplearning.net/software/theano/install_ubuntu.html#install-ubuntu)\n\n2. If you plan to use GPU computation, install [CUDA](https://developer.nvidia.com/cuda-downloads)\n\n3. Download/clone the Doctor AI code  \n\n**STEP 2: Preparing training data**  \n\n0. You can use \"process_mimic.py\" to process MIMIC-III dataset and generate a suitable training dataset for Doctor AI. Place the script to the same location where the MIMIC-III CSV files are located, and run the script. Instructions are described inside the script. However, I recommend the readers to read the following steps to understand the structure of the training data and learn how to prepare their own dataset.\n\n1. Doctor AI's training dataset needs to be a Python Pickled list of list of list. Each list corresponds to patients, visits, and medical codes (e.g. diagnosis codes, medication codes, procedure codes, etc.)\nFirst, medical codes need to be converted to an integer. Then a single visit can be seen as a list of integers. Then a patient can be seen as a list",
    "url": "https://github.com/mp2893/doctorai",
    "last_updated": "2025-08-27T22:12:50+00:00"
  },
  {
    "full_name": "r-lidar/lidR",
    "name": "lidR",
    "description": "Airborne LiDAR data manipulation and visualisation for forestry application",
    "language": "R",
    "topics": [
      "point-cloud",
      "lidar",
      "las",
      "laz",
      "r",
      "als",
      "forestry",
      "remote-sensing"
    ],
    "readme": "\nlidR <img src=\"https://raw.githubusercontent.com/r-lidar/lidR/master/man/figures/logo200x231.png\" align=\"right\"/>\n======================================================================================================\n![license](https://img.shields.io/badge/Licence-GPL--3-blue.svg) \n[![R build status](https://github.com/r-lidar/lidR/workflows/R-CMD-check/badge.svg)](https://github.com/r-lidar/lidR/actions)\n[![Codecov test coverage](https://codecov.io/gh/r-lidar/lidR/branch/master/graph/badge.svg)](https://app.codecov.io/gh/r-lidar/lidR?branch=master)\n\n**R package for Airborne LiDAR Data Manipulation and Visualization for Forestry Applications**\n\nThe lidR package provides functions to read and write `.las` and `.laz` files, plot point clouds, compute metrics using an area-based approach, compute digital canopy models, thin LiDAR data, manage a collection of LAS/LAZ files, automatically extract ground inventories, process a collection of tiles using multicore processing, segment individual trees, classify points from geographic data, and provides other tools to manipulate LiDAR data in a **research and development context.**\n\n- 📖 Read [the book](https://r-lidar.github.io/lidRbook/index.html) to get started with the lidR package.\n- 💻 Install `lidR` from R with: `install.packages(\"lidR\")`\n- 💵 [Sponsor `lidR`](https://github.com/sponsors/Jean-Romain). It is free and open source, but requires time and effort to develop and maintain.\n\n`lidR` has been cited by more than 1,000 scientific papers. To cite the package use `citation()` from within R:\n\n```r\ncitation(\"lidR\")\n#> Roussel, J.R., Auty, D., Coops, N. C., Tompalski, P., Goodbody, T. R. H., Sánchez Meador, A., Bourdon, J.F., De Boissieu, F., Achim, A. (2021). lidR : An R package for analysis of Airborne Laser Scanning (ALS) data. Remote Sensing of Environment, 251 (August), 112061. <doi:10.1016/j.rse.2020.112061>.\n#> Jean-Romain Roussel and David Auty (2023). Airborne LiDAR Data Manipulation and Visualization for Forestr",
    "url": "https://github.com/r-lidar/lidR",
    "last_updated": "2025-08-25T16:33:24+00:00"
  },
  {
    "full_name": "mcp-use/mcp-use",
    "name": "mcp-use",
    "description": "mcp-use is the easiest way to interact with mcp servers with custom agents",
    "language": "Python",
    "topics": [
      "agents",
      "ai",
      "mcp",
      "mcp-client",
      "agent",
      "python",
      "model-context-protocol",
      "model-context-protocol-client",
      "model-context-protocol-sdk"
    ],
    "readme": "<div align=\"center\">\n<div align=\"center\" style=\"margin: 0 auto; max-width: 80%;\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"static/logo-gh.jpg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"static/logo-gh.jpg\">\n    <img alt=\"mcp use logo\" src=\"./static/logo-gh.jpg\" width=\"80%\" style=\"margin: 20px auto;\">\n  </picture>\n</div>\n\n\n<h1 align=\"center\">🚀 Create MCP Clients and Agents</h1>\n<p align=\"center\">\n    <a href=\"https://github.com/pietrozullo/mcp-use/stargazers\" alt=\"GitHub stars\">\n        <img src=\"https://img.shields.io/github/stars/pietrozullo/mcp-use?style=social\" /></a>\n    <a href=\"https://pypi.org/project/mcp_use/\" alt=\"PyPI Downloads\">\n        <img src=\"https://static.pepy.tech/badge/mcp-use\" /></a>\n    <a href=\"https://pypi.org/project/mcp_use/\" alt=\"PyPI Version\">\n        <img src=\"https://img.shields.io/pypi/v/mcp_use.svg\"/></a>\n    <a href=\"https://github.com/mcp-use/mcp-use-ts\" alt=\"TypeScript\">\n      <img src=\"https://img.shields.io/badge/TypeScript-mcp--use-3178C6?logo=typescript&logoColor=white\" /></a>\n    <a href=\"https://github.com/pietrozullo/mcp-use/blob/main/LICENSE\" alt=\"License\">\n        <img src=\"https://img.shields.io/github/license/pietrozullo/mcp-use\" /></a>\n    <a href=\"https://docs.mcp-use.com\" alt=\"Documentation\">\n        <img src=\"https://img.shields.io/badge/docs-mcp--use.com-blue\" /></a>\n    <a href=\"https://mcp-use.com\" alt=\"Website\">\n        <img src=\"https://img.shields.io/badge/website-mcp--use.com-blue\" /></a>\n    </p>\n    <p align=\"center\">\n    <a href=\"https://x.com/pietrozullo\" alt=\"Twitter Follow - Pietro\">\n        <img src=\"https://img.shields.io/twitter/follow/Pietro?style=social\" /></a>\n    <a href=\"https://x.com/pederzh\" alt=\"Twitter Follow - Luigi\">\n        <img src=\"https://img.shields.io/twitter/follow/Luigi?style=social\" /></a>\n    <a href=\"https://discord.gg/XkNkSkMz3V\" alt=\"Discord\">\n        <img src=\"https://dcbadge.limes.pink/api/server/XkNkSkMz3V?style=flat\" /></a>\n</p>\n</",
    "url": "https://github.com/mcp-use/mcp-use",
    "last_updated": "2025-09-02T09:45:29+00:00"
  },
  {
    "full_name": "kmunger/Topic_Models",
    "name": "Topic_Models",
    "description": "Presentation for the NYU Data Lab December 2015",
    "language": "R",
    "topics": [],
    "readme": "# Topic_Models\nPresentation for the NYU Politics Data Lab December 2015\n\nThis is a \"Very Applied\" introduction to Topic Models in the social sciences. I introduce the concepts underlying topic models, discuss common pitfalls in their application, and present seminal research using topic models in Political Scinece. \n\nI also provide a hands-on walkthrough of the most famous and widely-used topic model (Latent Dirichlet Allocation) as well as an exciting extension of LDA that's often better suited to answering the kinds of questions that political scientists tend to ask (Structural Topic Model).\n\n",
    "url": "https://github.com/kmunger/Topic_Models",
    "last_updated": "2018-02-02T10:56:10+00:00"
  },
  {
    "full_name": "sunlightlabs/poindexter",
    "name": "poindexter",
    "description": "Creates a datastore for information on 527s, political organizations which disclose donors and expenditures semiannually with the IRS and can raise and spend unlimited amounts of money without filing reports with the Federal Election Commission.",
    "language": "Python",
    "topics": [],
    "readme": "# o_o poindexter o_o\n**A tool for wrangling IRS data on 527s**\n\nPoindexter does the following:\n* Downloads and extracts the IRS' bulk Political Organization Filing and Disclosure data file\n* Cleans this file of database errors, errant DOS and UNIX line endings (there are both), and other cruft\n* Repairs lines broken by unsupported characters in the IRS' database dump\n* Logs all the weirdness it encounters and repairs\n* Writes the results into a series of CSVs, one for each table described in the IRS data documentation [here](http://forms.irs.gov/app/pod/dataDownload/dataDownload)\n\nPoindexter comes complete with the sql statements necessary to make the corresponding tables in a Postgres database.\n\nTo download the bulk data:\n`./prep_files.sh`\n\nTo generate the flatfiles into a directory called 'csvs' -- which should exist in the working directory -- using default settings:\n`./run_this.py & tail -f filemaker.log`\n\nFrom there, you're on your own; SQL scripts are included in `sql/` that will create tables\nin Postgresql one could populate from the flatfiles with a `COPY FROM` command.\n\nPoindexter should log an error when it encounters a row it can't handle.\n",
    "url": "https://github.com/sunlightlabs/poindexter",
    "last_updated": "2024-12-30T22:52:15+00:00"
  },
  {
    "full_name": "holman/bandwidth-friends",
    "name": "bandwidth-friends",
    "description": "A shell script for macOS that makes sure you are being nice to your nice coffeeshop internet neighbors. 💖",
    "language": "Shell",
    "topics": [],
    "readme": "# bandwidth-friends\n\nA shell script for macOS that makes sure you are being nice to your nice coffeeshop internet neighbors. 💖\n\nIf you go over the set limit of bandwidth usage in a thirty second period, your\ncomputer will issue a nice little notice to everyone around you.\n\nSee [this blawg post](https://zachholman.com/posts/remote-work-cafes) for more.\n\n## Install\n\nThis depends on [`bwm-ng`](https://github.com/vgropp/bwm-ng), which is p neat.\nInstall it with:\n\n    brew install bwm-ng\n\n## Run\n\nJust run `./bandwidth-friends` you'll be set.\n\n## u r nice and people like u\n\nlove, [@holman](https://twitter.com)\n",
    "url": "https://github.com/holman/bandwidth-friends",
    "last_updated": "2025-07-20T19:53:53+00:00"
  },
  {
    "full_name": "rstudio/rmarkdown",
    "name": "rmarkdown",
    "description": "Dynamic Documents for R",
    "language": "R",
    "topics": [
      "r",
      "r-package",
      "rmarkdown",
      "pandoc",
      "markdown",
      "literate-programming"
    ],
    "readme": "# rmarkdown <a href=\"https://pkgs.rstudio.com/rmarkdown/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/rstudio/rmarkdown/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/rmarkdown/actions/workflows/R-CMD-check.yaml)\n[![CRAN release](https://www.r-pkg.org/badges/version/rmarkdown)](https://cran.r-project.org/package=rmarkdown)\n[![Codecov test coverage](https://codecov.io/gh/rstudio/rmarkdown/branch/main/graph/badge.svg)](https://app.codecov.io/gh/rstudio/rmarkdown?branch=main)\n<!-- badges: end -->\n\n\nThe **rmarkdown** package helps you create dynamic analysis documents that combine code, rendered output (such as figures), and prose. You bring your data, code, and ideas, and R Markdown renders your content into a polished document that can be used to:\n\n- Do data science interactively within the RStudio IDE,\n\n- Reproduce your analyses,\n\n- Collaborate and share code with others, and\n\n- Communicate your results with others.\n\nR Markdown documents can be rendered to many output formats including HTML documents, PDFs, Word files, slideshows, and more, allowing you to focus on the content while R Markdown takes care of your presentation. \n\n## Books\n\n<a href=\"https://bookdown.org/yihui/rmarkdown/\"><img class=\"book\" src=\"https://bookdown.org/yihui/rmarkdown/images/cover.png\" alt=\"R Markdown: The Definitive Guide\" height=\"400\"></a>\n<a href=\"https://bookdown.org/yihui/rmarkdown-cookbook/\"><img class=\"book\" src=\"https://bookdown.org/yihui/rmarkdown-cookbook/images/cover.png\" alt=\"R Markdown Cookbook\" height=\"400\"></a>\n\nSee more about them in [Get Started](https://pkgs.rstudio.com/rmarkdown/articles/rmarkdown.html).\n\n## Installation\n\nThe easiest way to install the **rmarkdown** package is from within the [RStudio IDE](https://posit.co/download/rstudio-desktop/), but you don't need to explicitly install it or load it, as RStudio automatically does both when needed. A recent vers",
    "url": "https://github.com/rstudio/rmarkdown",
    "last_updated": "2025-09-02T09:41:25+00:00"
  },
  {
    "full_name": "eddelbuettel/littler",
    "name": "littler",
    "description": "A scripting and command-line front-end for GNU R",
    "language": "R",
    "topics": [
      "r",
      "r-package",
      "cran",
      "embedded",
      "examples",
      "littler"
    ],
    "readme": "## littler: A scripting and command-line front-end for GNU R\n\n[![CI](https://github.com/eddelbuettel/littler/workflows/ci/badge.svg)](https://github.com/eddelbuettel/littler/actions?query=workflow%3Aci)\n[![License](https://img.shields.io/badge/license-GPL%20%28%3E=%202%29-brightgreen.svg?style=flat)](https://www.gnu.org/licenses/gpl-2.0.html)\n[![CRAN](https://www.r-pkg.org/badges/version/littler)](https://cran.r-project.org/package=littler)\n[![r-universe](https://eddelbuettel.r-universe.dev/badges/littler)](https://eddelbuettel.r-universe.dev/littler)\n[![Dependencies](https://tinyverse.netlify.app/badge/littler)](https://cran.r-project.org/package=littler)\n[![Downloads](https://cranlogs.r-pkg.org/badges/littler?color=brightgreen)](https://www.r-pkg.org/pkg/littler)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/littler)](https://github.com/eddelbuettel/littler)\n[![Documentation](https://img.shields.io/badge/documentation-is_here-blue)](https://eddelbuettel.github.io/littler/)\n\n### So What Is It For?\n\n```r\n#!/usr/bin/env r              ## for use in scripts\n\nother input | r               ## for use in pipes\n\nr somefile.R                  ## for running files\n\nr -e 'expr'                   ## for evaluating expressions\n\nr --help                      ## to show a quick synopsis\n```\n\n### Examples?\n\nPlenty. See the [examples vignette](https://cran.r-project.org/package=littler/vignettes/littler-examples.html)\nfor a full set of introductory examples. Also\nsee the [examples/ directory](https://github.com/eddelbuettel/littler/tree/master/inst/examples) for a full 28\nexample scripts, as well as maybe the\n[older tests directory](https://github.com/eddelbuettel/littler/tree/master/inst/script-tests)\nboth of which are installed with the package.\n\nSome scripts I use daily or near daily (in alphabetical order):\n\n```\nbuild.r                                ## builds from the current directory\nc4c.r                                  ## submits current director",
    "url": "https://github.com/eddelbuettel/littler",
    "last_updated": "2025-08-26T14:05:06+00:00"
  },
  {
    "full_name": "RBigData/remoter",
    "name": "remoter",
    "description": "Control a remote R session from your local R session.",
    "language": "R",
    "topics": [
      "r",
      "distributed-computing",
      "zeromq",
      "pbdr"
    ],
    "readme": "# remoter\n\n* **Version:** 0.5-0\n* **License:** [BSD 2-Clause](https://opensource.org/licenses/BSD-2-Clause)\n* **Project home**: https://github.com/RBigData/remoter\n* **Bug reports**: https://github.com/RBigData/remoter/issues\n\n\nControl a remote R session from your local R session. The package uses [**pbdZMQ**](https://github.com/snoweye/pbdZMQ) to handle the communication and networking. Encryption is supported if the **sodium** package is (optionally) installed. Details below.\n\n\n\n\n## Installation\n\nYou can install the stable version from CRAN using the usual `install.packages()`:\n\n```r\ninstall.packages(\"remoter\")\n```\n\nIn order to be able to create and connect to secure servers, you need to also install the **sodium** package. The use of **sodium** is optional because it is a non-trivial systems dependency, but it is highly recommended. You can install it manually with a call to `install.packages(\"sodium\")` or by installing **remoter** via:\n\n```r\ninstall.packages(\"remoter\", dependencies=TRUE)\n```\n\nThe development version is maintained on GitHub:\n\n```r\nremotes::install_github(\"RBigData/remoter\")\n```\n\nTo simplify installations on cloud systems, we also have a [Docker container](https://github.com/RBigData/docker) available.\n\n\n\n## Usage\n\nFor setting up a local server, you can do:\n\n```r\nremoter::server()\n```\n\nAnd connect to it interactively via:\n\n```r\nremoter::client()\n```\n\nThere is also the option to pipe commands to the server in batch using the `batch()` function:\n\n```r\n### Passing an R script file\nremoter::batch(file=\"my_rscript_file.r\")\n### Passing in a script manually\nremoter::batch(script=\"1+1\")\n```\n\nFor more details, including working with remote machines, see the package vignette.\n\n\n\n## Acknowledgements\n\nInitial work for the **remoter** package was supported in part by the project *Harnessing Scalable Libraries for Statistical Computing on Modern Architectures and Bringing Statistics to Large Scale Computing* funded by the National Science Foundation Division of",
    "url": "https://github.com/RBigData/remoter",
    "last_updated": "2025-04-05T00:56:22+00:00"
  },
  {
    "full_name": "ulfelder/dart-throwing-chimp",
    "name": "dart-throwing-chimp",
    "description": "Code and data for selected posts on my Dart-Throwing Chimp blog and other work on political instability and forecasting.",
    "language": "R",
    "topics": [],
    "readme": "dart-throwing-chimp\n===================\n\nCode and data for selected posts on my Dart-Throwing Chimp blog.\n",
    "url": "https://github.com/ulfelder/dart-throwing-chimp",
    "last_updated": "2024-05-31T13:12:30+00:00"
  },
  {
    "full_name": "rstudio/rmarkdown-book",
    "name": "rmarkdown-book",
    "description": "R Markdown: The Definitive Guide (published by Chapman & Hall/CRC in July 2018)",
    "language": "RMarkdown",
    "topics": [
      "book",
      "r",
      "rmarkdown"
    ],
    "readme": "# The R Markdown Book\n\nThe comprehensive documentation of R Markdown, published by [Chapman & Hall/CRC](https://www.crcpress.com/p/book/9781138359338).\n",
    "url": "https://github.com/rstudio/rmarkdown-book",
    "last_updated": "2025-08-20T00:12:10+00:00"
  },
  {
    "full_name": "citation-style-language/styles",
    "name": "styles",
    "description": "Official repository for Citation Style Language (CSL) citation styles.",
    "language": "Ruby",
    "topics": [
      "citation-style-language",
      "bibliography",
      "citations",
      "citation-styles",
      "csl",
      "hacktoberfest"
    ],
    "readme": "<p align=\"center\"><a href=\"https://citationstyles.org/\" target=\"_blank\"><img width=\"300\" src=\"https://raw.githubusercontent.com/citation-style-language/logo/master/assets/rgb/%C2%ABCSL%C2%BB.svg\" alt=\"CSL logo\"></a></p>\n\n<h1 align=\"center\">Citation Style Language - Style Repository</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/citation-style-language/styles#licensing\"><img src=\"https://img.shields.io/badge/license-CC%20BY%20SA%203.0-blue.svg\" alt=\"License\"></a>\n  <a href=\"https://github.com/citation-style-language/styles/actions\"><img src=\"https://github.com/citation-style-language/styles/workflows/Merge%20to%20release/badge.svg?event=push\" alt=\"Build Status\"></a>\n</p>\n\nIntroduction\n------------\n\nThe independent open source [Citation Style Language](https://citationstyles.org/) (CSL) project aims to facilitate scholarly communication by automating the formatting of citations and bibliographies.\nThe primary components of the CSL ecosystem are:\n\n* The CSL schema and specification, which describe how the XML-based CSL styles and locale files should be written and interpreted\n* Curated repositories of CSL styles and locale files\n* Third party CSL processors, software libraries for rendering formatted citation and bibliographies from CSL styles, CSL locale files, and item metadata\n\nThis README describes our official curated repository of CSL styles, hosted at https://github.com/citation-style-language/styles/.\nCSL locale files, which provide default localization data for CSL styles (such as translations and date formats), can be found at https://github.com/citation-style-language/locales.\n\nFor more information about CSL and CSL styles, check out https://citationstyles.org/ and the information files in this repository ([Style Requirements](https://github.com/citation-style-language/styles/blob/master/STYLE_REQUIREMENTS.md), [Style Development](https://github.com/citation-style-language/styles/blob/master/STYLE_DEVELOPMENT.md), [Requesting Styles](https://github.c",
    "url": "https://github.com/citation-style-language/styles",
    "last_updated": "2025-09-01T19:10:27+00:00"
  },
  {
    "full_name": "ryankiros/neural-storyteller",
    "name": "neural-storyteller",
    "description": "A recurrent neural network for generating little stories about images",
    "language": "Python",
    "topics": [],
    "readme": "# neural-storyteller\n\nneural-storyteller is a recurrent neural network that generates little stories about images. This repository contains code for generating stories with your own images, as well as instructions for training new models.\n\n<img src=\"https://github.com/ryankiros/neural-storyteller/blob/master/images/ex1.jpg\" height=\"220px\" align=\"left\">\n*We were barely able to catch the breeze at the beach , and it felt as if someone stepped out of my mind . She was in love with him for the first time in months , so she had no intention of escaping . The sun had risen from the ocean , making her feel more alive than normal . She 's beautiful , but the truth is that I do n't know what to do . The sun was just starting to fade away , leaving people scattered around the Atlantic Ocean . I d seen the men in his life , who guided me at the beach once more .*\n\n[Samim](http://samim.io/) has made an awesome blog post with lots of results [here](https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed).\n\nSome more results from an older model trained on Adventure books can be found [here](http://www.cs.toronto.edu/~rkiros/adv_L.html).\n\nThe whole approach contains 4 components:\n* [skip-thought vectors](https://github.com/ryankiros/skip-thoughts)\n* [image-sentence embeddings](https://github.com/ryankiros/visual-semantic-embedding)\n* [conditional neural language models](https://github.com/ryankiros/skip-thoughts/tree/master/decoding)\n* style shifting (described in this project)\n\nThe 'style-shifting' operation is what allows our model to transfer standard image captions to the style of stories from novels. The only source of supervision in our models is from [Microsoft COCO](http://mscoco.org/) captions. That is, we did not collect any new training data to directly predict stories given images.\n\nStyle shifting was inspired by [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576) but the technical details are completely different.\n\n## How does it wor",
    "url": "https://github.com/ryankiros/neural-storyteller",
    "last_updated": "2025-08-21T09:30:39+00:00"
  },
  {
    "full_name": "tidymodels/tidyposterior",
    "name": "tidyposterior",
    "description": "Bayesian comparisons of models using resampled statistics",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tidyposterior <a href='https://tidyposterior.tidymodels.org/'><img src='man/figures/logo.png' align=\"right\" height=\"139\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/tidymodels/tidyposterior/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/tidyposterior/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidymodels/tidyposterior/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/tidyposterior?branch=main)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/tidyposterior)](https://CRAN.r-project.org/package=tidyposterior)\n[![Downloads](http://cranlogs.r-pkg.org/badges/tidyposterior)](https://CRAN.r-project.org/package=tidyposterior)\n![](https://img.shields.io/badge/lifecycle-maturing-blue.svg)\n\n<!-- badges: end -->\n\nThis package can be used to conduct *post hoc* analyses of resampling\nresults generated by models.\n\nFor example, if two models are evaluated with the root mean squared\nerror (RMSE) using 10-fold cross-validation, there are 10 paired\nstatistics. These can be used to make comparisons between models without\ninvolving a test set.\n\nThere is a rich literature on the analysis of model resampling results\nsuch as McLachlan’s [*Discriminant Analysis and Statistical Pattern\nRecognition*](https://books.google.com/books?id=O_qHDLaWpDUC&lpg=PR7&ots=6GJnIREXZM&dq=%22Discriminant%20Analysis%20and%20Statistical%20Pattern%20Recognition%22&lr&pg=PR7#v=onepage&q=%22Discriminant%20Analysis%20and%20Statistical%20Pattern%20Recognition%22&f=false)\nand the references therein. This package follows *the spirit* of\n[Benavoli *et al*\n(2017)](https://people.idsia.ch//~marco/papers/2017jmlr-tests.pdf).\n\ntidyposterior uses Bayesian generalized linear models for this purpose\nand can be considered an upgraded version of the\n[`caret::resamples()`](https://topepo.github.io/caret/model-training-and-tuning.html#explo",
    "url": "https://github.com/tidymodels/tidyposterior",
    "last_updated": "2025-07-31T11:54:11+00:00"
  },
  {
    "full_name": "hmalmedal/ripplerestr",
    "name": "ripplerestr",
    "description": "Ripple REST Client for R",
    "language": "R",
    "topics": [],
    "readme": "ripplerestr\n===========\n\nUse **R** with the [Ripple-REST](https://ripple.com/build/ripple-rest/) API.\n\nIntroduction\n------------\n\n[Ripple](https://ripple.com/) is an Internet protocol for making financial transactions.\n\nThe [Ripple-REST](https://ripple.com/build/ripple-rest/) API provides a simplified, easy-to-use interface to the Ripple Network via a RESTful API.\n\nThe **R** package [`ripplerestr`](https://github.com/hmalmedal/ripplerestr) uses the [`httr`](https://github.com/hadley/httr) package to communicate with Ripple-REST.\n\nPrerequisites\n-------------\n\nBefore you can use the Ripple-REST API, you will need to have three things:\n\n * An installed version of Ripple-REST running locally or remotely. Instructions on installing Ripple-REST can be found in the README.md file in the [Github repository](https://github.com/ripple/ripple-rest).\n * An activated Ripple account.\n * The URL of the server running the Ripple-REST API that you wish to use.\n\nInstallation\n------------\n\nDownload the [latest release](https://github.com/hmalmedal/ripplerestr/releases/latest) and install it.\n\nYou can alternatively install the current version from GitHub:\n\n``` {.r}\nif (!require(\"devtools\")) install.packages(\"devtools\")\ndevtools::install_github(\"hmalmedal/ripplerestr\")\n```\n\nSet up\n------\n\nLoad `ripplerestr` and set the URL.\n\n``` {.r}\nlibrary(ripplerestr)\noptions(\"ripplerestr.url\" = \"http://localhost:5990/\")\n```\n",
    "url": "https://github.com/hmalmedal/ripplerestr",
    "last_updated": "2015-07-26T17:10:38+00:00"
  },
  {
    "full_name": "google-gemini/gemini-fullstack-langgraph-quickstart",
    "name": "gemini-fullstack-langgraph-quickstart",
    "description": "Get started with building Fullstack Agents using Gemini 2.5 and LangGraph",
    "language": "Jupyter Notebook",
    "topics": [
      "gemini",
      "gemini-api"
    ],
    "readme": "# Gemini Fullstack LangGraph Quickstart\n\nThis project demonstrates a fullstack application using a React frontend and a LangGraph-powered backend agent. The agent is designed to perform comprehensive research on a user's query by dynamically generating search terms, querying the web using Google Search, reflecting on the results to identify knowledge gaps, and iteratively refining its search until it can provide a well-supported answer with citations. This application serves as an example of building research-augmented conversational AI using LangGraph and Google's Gemini models.\n\n<img src=\"./app.png\" title=\"Gemini Fullstack LangGraph\" alt=\"Gemini Fullstack LangGraph\" width=\"90%\">\n\n## Features\n\n- 💬 Fullstack application with a React frontend and LangGraph backend.\n- 🧠 Powered by a LangGraph agent for advanced research and conversational AI.\n- 🔍 Dynamic search query generation using Google Gemini models.\n- 🌐 Integrated web research via Google Search API.\n- 🤔 Reflective reasoning to identify knowledge gaps and refine searches.\n- 📄 Generates answers with citations from gathered sources.\n- 🔄 Hot-reloading for both frontend and backend during development.\n\n## Project Structure\n\nThe project is divided into two main directories:\n\n-   `frontend/`: Contains the React application built with Vite.\n-   `backend/`: Contains the LangGraph/FastAPI application, including the research agent logic.\n\n## Getting Started: Development and Local Testing\n\nFollow these steps to get the application running locally for development and testing.\n\n**1. Prerequisites:**\n\n-   Node.js and npm (or yarn/pnpm)\n-   Python 3.11+\n-   **`GEMINI_API_KEY`**: The backend agent requires a Google Gemini API key.\n    1.  Navigate to the `backend/` directory.\n    2.  Create a file named `.env` by copying the `backend/.env.example` file.\n    3.  Open the `.env` file and add your Gemini API key: `GEMINI_API_KEY=\"YOUR_ACTUAL_API_KEY\"`\n\n**2. Install Dependencies:**\n\n**Backend:**\n\n```bash\ncd backend\npip install .\n```",
    "url": "https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart",
    "last_updated": "2025-09-02T10:08:28+00:00"
  },
  {
    "full_name": "NimbleBoxAI/ChainFury",
    "name": "ChainFury",
    "description": "🦋 Production grade chaining engine behind TuneChat. Self host today!",
    "language": "Python",
    "topics": [
      "llm",
      "chat-applications",
      "chatbot-framework",
      "large-language-models",
      "prompt-engineering",
      "hacktoberfest",
      "hacktoberfest2023"
    ],
    "readme": "# 🦋 NimbleBox ChainFury\n\n[![linkcheck](https://img.shields.io/badge/Workflow-Passing-darkgreen)](https://github.com/NimbleBoxAI/ChainFury/actions)\n[![Downloads](https://static.pepy.tech/badge/chainfury)](https://pepy.tech/project/chainfury)\n[![linkcheck](https://img.shields.io/badge/Site-🦋ChainFury-lightblue)](https://chainfury.nbox.ai)\n[![License: Apache](https://img.shields.io/badge/License-Apache%20v2.0-red)](https://github.com/NimbleBoxAI/ChainFury/blob/main/LICENSE) \n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/NimbleBoxAI.svg?style=social&label=Follow%20%40NimbleBoxAI)](https://twitter.com/NimbleBoxAI)\n[![](https://dcbadge.vercel.app/api/server/KhF38hrAJ2?compact=true&style=flat)](https://discord.com/invite/KhF38hrAJ2)\n\n```\n  ___ _         _       ___\n / __| |_  __ _(_)_ _  | __|  _ _ _ _  _ \n| (__| ' \\/ _` | | ' \\ | _| || | '_| || |\n \\___|_||_\\__,_|_|_||_||_| \\_,_|_|  \\_, |\n                                     |__/\ne0 a4 b8 e0 a4 a4 e0 a5 8d e0 a4 af e0 a4\nae e0 a5 87 e0 a4 b5 20 e0 a4 9c e0 a4 af\n            e0 a4 a4 e0 a5 87\n```\n\n🦋 The open source chaining engine behind [Tune Chat](https://chat.tune.app) and [Tune Studio](https://studio.tune.app).\n\n# Read the [Docs](https://nimbleboxai.github.io/ChainFury/index.html)\n\nThe documentation page contains all the information on using `chainfury` and `chainfury_server`.\n\n#### `chainfury`\n\n<img src=\"https://d2e931syjhr5o9.cloudfront.net/tune-research/assets/cf_arch.png\" align=\"center\"/>\n\n#### `chainfury_server`\n\n<img src=\"https://d2e931syjhr5o9.cloudfront.net/tune-research/assets/cfs_arch.png\" align=\"center\"/>\n\n# Looking for Inspirations?\n\nHere's a few example to get your journey started on Software 2.0:\n\n- 📚 Retrieval Augmented Generation (RAG): Load a PDF and ask it questions, read [docs](https://nimbleboxai.github.io/ChainFury/examples/qa-rag.html)\n- 🏞️ Image generation using Stability: Generate your world, read [here](https://nimbleboxai.github.io/ChainFury/examples/stability-apis.html)\n- 🔐 ",
    "url": "https://github.com/NimbleBoxAI/ChainFury",
    "last_updated": "2025-08-13T14:06:00+00:00"
  },
  {
    "full_name": "poteto/hiring-without-whiteboards",
    "name": "hiring-without-whiteboards",
    "description": "⭐️  Companies that don't have a broken hiring process",
    "language": "JavaScript",
    "topics": [
      "interview",
      "hiring",
      "whiteboard",
      "jobs",
      "tech",
      "airtable",
      "hiring-without-whiteboards"
    ],
    "readme": "# Hiring Without Whiteboards\n\nA list of companies (or teams) that don't do \"whiteboard\" interviews. \"Whiteboards\" is used as a metaphor, and is a _symbol_ for the kinds of CS trivia questions that are associated with bad interview practices. Whiteboards are not bad – CS trivia questions are. Using sites like HackerRank/LeetCode _probably_ fall into a similar category.\n\nThe companies and teams listed here use interview techniques and questions that resemble day-to-day work. For example, pairing on a real world problem or a paid/unpaid take home exercise. Read (and contribute to) [our recommendations](RECOMMENDATIONS.md) for ways to conduct better interviews.\n\n### tl;dr\n\n- Discussing a real world problem (with or without whiteboard) is 👍\n- Solving CS trivia, technical puzzles, riddles, brainteasers (with or without whiteboard) is 👎\n\nPlease open a [PR](https://github.com/poteto/hiring-without-whiteboards/pull/new/master) to be added.\n\n### Duds\n\nIf you've been through an interview with one of these companies recently, and your experience suggests otherwise from their description, please open a [PR](https://github.com/poteto/hiring-without-whiteboards/pull/new/master) to remove them from this list.\n\n### I want to search/sort/filter/group by X!\n\nCheck out our [Airtable](https://airtable.com/shr3eGPDm3wGjT2gA)!\n\nAdditions to this document that are properly formatted will automatically be pushed and added to Airtable. Keywords from the description will also be extracted. You can see (and contribute!) to the [API here](https://github.com/poteto/hww-api).\n\nAlso check out [No Whiteboards](https://www.nowhiteboard.org) to search for jobs at these companies.\n\n### Discussion and other reads\n\n<!--lint disable-->\n\n- Discussion\n  - [HackerNews (2017)](https://news.ycombinator.com/item?id=13874026)\n  - [HackerNews (2020)](https://news.ycombinator.com/item?id=23981795)\n- [Finding a better alternative to the whiteboard interview](https://theoutline.com/post/1256/finding-a-better-altern",
    "url": "https://github.com/poteto/hiring-without-whiteboards",
    "last_updated": "2025-09-02T07:58:49+00:00"
  },
  {
    "full_name": "pypi/warehouse",
    "name": "warehouse",
    "description": "The Python Package Index",
    "language": "Python",
    "topics": [
      "package-registry",
      "package-repository",
      "pypi",
      "pypi-source",
      "python"
    ],
    "readme": "\nWarehouse\n=========\n\nWarehouse is the software that powers `PyPI`_.\nSee `our development roadmap`_, `documentation`_, and\n`architectural overview`_.\n\nGetting Started\n---------------\n\nYou can run Warehouse locally in a development environment using\n``docker``. See `Getting started`_\ndocumentation for instructions on how to set it up.\n\nThe canonical deployment of Warehouse is in production at `pypi.org`_.\n\nDiscussion\n----------\n\nYou can find help or get involved on:\n\n- `Github issue tracker`_ for reporting issues\n- IRC: on `Libera`_, channel ``#pypa`` for general packaging discussion\n  and user support, and ``#pypa-dev`` for\n  discussions about development of packaging tools\n- The `PyPA Discord`_ for live discussions\n- The Packaging category on `Discourse`_ for discussing\n  new ideas and community initiatives\n\nTesting\n----------\n\nRead the `running tests and linters section`_ of our documentation to\nlearn how to test your code.\n\nCode of Conduct\n---------------\n\nEveryone interacting in the Warehouse project's codebases, issue trackers, chat\nrooms, and mailing lists is expected to follow the `PSF Code of Conduct`_.\n\n.. _`PyPI`: https://pypi.org/\n.. _`our development roadmap`: https://warehouse.pypa.io/roadmap/\n.. _`architectural overview`: https://warehouse.pypa.io/application/\n.. _`documentation`: https://warehouse.pypa.io\n.. _`Getting started`: https://warehouse.pypa.io/development/getting-started/\n.. _`Github issue tracker`: https://github.com/pypi/warehouse/issues\n.. _`pypi.org`: https://pypi.org/\n.. _`Running tests and linters section`: https://warehouse.pypa.io/development/getting-started/#running-tests-and-linters\n.. _`PSF Code of Conduct`: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md\n.. _`Libera`: https://web.libera.chat/#pypa,#pypa-dev\n.. _`PyPA Discord`: https://discord.gg/pypa\n.. _`Discourse`: https://discuss.python.org/c/packaging/14\n",
    "url": "https://github.com/pypi/warehouse",
    "last_updated": "2025-09-01T00:39:56+00:00"
  },
  {
    "full_name": "twitter/twemoji",
    "name": "twemoji",
    "description": "Emoji for everyone. https://twemoji.twitter.com/",
    "language": "HTML",
    "topics": [
      "emoji",
      "twemoji"
    ],
    "readme": "# Twitter Emoji (Twemoji) [![Build Status](https://travis-ci.org/twitter/twemoji.svg?branch=gh-pages)](https://travis-ci.org/twitter/twemoji)\n\nA simple library that provides standard Unicode [emoji](http://en.wikipedia.org/wiki/Emoji) support across all platforms.\n\n**Twemoji v14.0** adheres to the [Unicode 14.0 spec](https://unicode.org/versions/Unicode14.0.0/) and supports the [Emoji 14.0 spec](https://www.unicode.org/reports/tr51/tr51-21.html). _We do not support custom emoji._\n\nThe Twemoji library offers support for all Unicode-defined emoji which are recommended for general interchange (RGI).\n\n## Usage\n\n### CDN Support\n\n<del>The folks over at [MaxCDN](https://www.maxcdn.com) have graciously provided CDN support.</del>\n\nMaxCDN is shut down right now, so in the meanwhile use a different CDN or download the assets. (See [Maxcdn has shut down, cdn not working anymore. · Issue #580 · twitter/twemoji](https://github.com/twitter/twemoji/issues/580)).\n\nUse the following in the `<head>` tag of your HTML document(s):\n\n```html\n<script src=\"https://unpkg.com/twemoji@latest/dist/twemoji.min.js\" crossorigin=\"anonymous\"></script>\n```\n\nThis guarantees that you will always use the latest version of the library.\n\nIf, instead, you'd like to include the latest version explicitly, you can add the following tag:\n```html\n<script src=\"https://unpkg.com/twemoji@14.0.2/dist/twemoji.min.js\" integrity=\"sha384-ICOlZarapRIX6UjKPcWKEpubjg7lGADN7Y9fYP4DU9zm0aPFhgnP5ef+XFaPyKv+\" crossorigin=\"anonymous\"></script>\n```\n\n### Download\n\nIf instead you want to download a specific version, please look at the `gh-pages` branch, where you will find the built assets for both our latest and older versions.\n\n## API\n\nFollowing are all the methods exposed in the `twemoji` namespace.\n\n### twemoji.parse( ... ) V1\n\nThis is the main parsing utility and has 3 overloads per parsing type.\n\nAlthough there are two kinds of parsing supported by this utility, we recommend you use [DOM parsing](https://github.com/twitter",
    "url": "https://github.com/twitter/twemoji",
    "last_updated": "2025-09-01T15:25:42+00:00"
  },
  {
    "full_name": "drewconway/data_science_box",
    "name": "data_science_box",
    "description": "Simple scripts to setup a fresh data science box using an Ubuntu 12.04.* LTS 64-bit server running on an EC2",
    "language": "Shell",
    "topics": [],
    "readme": "Data Science Box\n================\n\nI often have to spin-up EC2 instances to do various data science(y) things with said instance.  These scripts are the result of having done that many times and needing many of the same tools to be available on those boxes.\n\nRunning these scripts will turn an Ubuntu 12.04.* LTS 64-bit server (tested) running on EC2 into a fully functioning data science box. Along with several base development libraries, the scripts installs and configures:\n\n - [R](http://www.r-project.org/) + [RStudio Server](http://www.rstudio.com/ide/docs/server/getting_started)\n - [shiny-server](http://www.rstudio.com/shiny/) (interactive web apps written in R)\n - Python + a suite of Python scientific computing libraries\n - [IPython](http://ipython.org/) + [IPython notebook](notebook) server ([stable](http://ipython.org/install.html))\n\nInstallation\n============\n\nStick these two scripts in the same directory on your freshly deployed instance and type:\n\n\t$ ./data_science_box.sh\n\nThen follow the on-screen instructions to configure the software.\n\nConfiguration\n=============\n\nThe script takes care of all the on-box configuration, but if you wish to access RStudio Server, shiny-server, and IPython notebook server via a browser you will need to make sure the [Security Groups](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html) for the server you deploy allows in-bound traffic to the follow ports (defaults):\n\n - RStudio: 8787\n - shiny-server: 3838\n - IPython notebook: 8888\n",
    "url": "https://github.com/drewconway/data_science_box",
    "last_updated": "2025-08-13T07:08:56+00:00"
  },
  {
    "full_name": "growthbook/growthbook",
    "name": "growthbook",
    "description": "Open Source Feature Flagging and A/B Testing Platform",
    "language": "TypeScript",
    "topics": [
      "abtesting",
      "statistics",
      "abtest",
      "experimentation",
      "split-testing",
      "mixpanel",
      "snowflake",
      "bigquery",
      "redshift",
      "clickhouse",
      "analytics",
      "ab-testing",
      "feature-flags",
      "feature-flagging",
      "remote-config",
      "continuous-delivery",
      "data-analysis",
      "data-science",
      "data-engineering"
    ],
    "readme": "<p align=\"center\"><a href=\"https://www.growthbook.io\"><img src=\"https://cdn.growthbook.io/growthbook-logo@2x.png\" width=\"400px\" alt=\"GrowthBook - Open Source Feature Flagging and A/B Testing\" /></a></p>\n<p align=\"center\"><b>Open Source Feature Flagging and A/B Testing</b></p>\n<p align=\"center\">\n    <a href=\"https://github.com/growthbook/growthbook/github/actions/workflows/ci.yml\"><img src=\"https://img.shields.io/github/actions/workflow/status/growthbook/growthbook/ci.yml?branch=main\" alt=\"Build Status\" height=\"22\"/></a>\n    <a href=\"https://github.com/growthbook/growthbook/releases\"><img src=\"https://img.shields.io/github/v/release/growthbook/growthbook?color=blue&sort=semver\" alt=\"Release\" height=\"22\"/></a>\n    <a href=\"https://slack.growthbook.io?ref=readme-badge\"><img src=\"https://img.shields.io/badge/slack-join-E01E5A?logo=slack\" alt=\"Join us on Slack\" height=\"22\"/></a>\n</p>\n\nGet up and running in 1 minute with:\n\n```sh\ngit clone https://github.com/growthbook/growthbook.git\ncd growthbook\ndocker compose up -d\n```\n\nThen visit http://localhost:3000\n\n[![GrowthBook Screenshot](/features-screenshot.png)](https://www.growthbook.io)\n\n## Our Philosophy\n\nThe top 1% of companies spend thousands of hours building their own feature flagging and A/B testing platforms in-house.\nThe other 99% are left paying for expensive 3rd party SaaS tools or hacking together unmaintained open source libraries.\n\nWe want to give all companies the flexibility and power of a fully-featured in-house platform without needing to build it themselves.\n\n## Major Features\n\n- 🏁 Feature flags with advanced targeting, gradual rollouts, and experiments\n- 💻 SDKs for [React](https://docs.growthbook.io/lib/react), [Javascript](https://docs.growthbook.io/lib/js), [PHP](https://docs.growthbook.io/lib/php), [Ruby](https://docs.growthbook.io/lib/ruby), [Python](https://docs.growthbook.io/lib/python), [Go](https://docs.growthbook.io/lib/go), [Android](https://docs.growthbook.io/lib/kotlin), [iOS](https://docs.grow",
    "url": "https://github.com/growthbook/growthbook",
    "last_updated": "2025-09-02T06:21:03+00:00"
  },
  {
    "full_name": "gojiplus/adjacent",
    "name": "adjacent",
    "description": "Add related repositories to your readme",
    "language": "",
    "topics": [
      "github-tools",
      "recommender",
      "related-repos"
    ],
    "readme": "# 🤝 Adjacent — Related Repositories Recommender\n\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/gojiplus/adjacent)\n![GitHub Marketplace](https://img.shields.io/badge/GitHub%20Marketplace-adjacent)\n![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)\n[![Used By](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/gojiplus/adjacent/main/docs/adjacent.json)](https://github.com/search?q=gojiplus/adjacent+path%3A.github%2Fworkflows+language%3AYAML&type=code)\n\n**Adjacent** is a GitHub Action that discovers and inserts a list of **related repositories** into your README based on shared GitHub topics.\n\nPerfect for discovery, organization, and letting your users explore similar tools you’ve built.\n\n---\n\n## 🚀 Features\n\n- 🔎 Finds related repositories by topic similarity\n- 🧠 Ranks and inserts up to 5 adjacent repos into your `README.md`\n- 🔄 Runs on a schedule or manual trigger\n- 💬 Ideal for portfolios, developer tools, and curated ecosystems\n\n---\n\n## 📦 Usage\n\nHere's a repository that uses this GitHub Action: https://github.com/notnews/fox_news_transcripts/\n\n### 1. **Add to your workflow**\n\nSave the following to `.github/workflows/adjacent.yml`:\n\n```yaml\nname: Find Adjacent Repositories\n\non:\n  schedule:\n    - cron: '0 5 * * 0'   # Every Sunday at 5am UTC\n  workflow_dispatch:\n\njobs:\n  recommend-repos:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Adjacent Repositories Recommender\n        uses: gojiplus/adjacent@v1.3\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}  # ✅ Pass the required token\n\n      - name: Commit and push changes\n        run: |\n          git config --global user.name \"github-actions\"\n          git config --global user.email \"actions@github.com\"\n          git add README.md\n          git commit -m \"Update adjacent repositories [automated]\" || echo \"No changes to commit\"\n          git push\n\n```\n\n## 🔗 Adjacent Repositorie",
    "url": "https://github.com/gojiplus/adjacent",
    "last_updated": "2025-07-09T01:31:35+00:00"
  },
  {
    "full_name": "commoncrawl/news-crawl",
    "name": "news-crawl",
    "description": "News crawling with StormCrawler - stores content as WARC",
    "language": "Java",
    "topics": [
      "crawler",
      "news",
      "warc",
      "web-crawler",
      "apache-storm",
      "common-crawl",
      "commoncrawl",
      "storm-crawler"
    ],
    "readme": "# NEWS-CRAWL\n\nCrawler for news based on [StormCrawler](https://stormcrawler.net/). Produces WARC files to be stored as part of the [Common Crawl](https://commoncrawl.org/). The data is hosted as [AWS Open Data Set](https://registry.opendata.aws/) – if you want to use the data and not the crawler software please read [the announcement of the news dataset](https://commoncrawl.org/2016/10/news-dataset-available/).\n\n\nPrerequisites\n-------------\n\n* Java 8\n* Install Elasticsearch 7.5.0 (ev. also Kibana)\n* Install Apache Storm 1.2.4\n* Start Elasticsearch and Storm\n* Build ES indices by running `bin/ES_IndexInit.sh`\n\nCrawler Seeds\n-------------\n\nThe crawler relies on [RSS](https://en.wikipedia.org/wiki/RSS)/[Atom](https://en.wikipedia.org/wiki/Atom_(Web_standard)) feeds and [news sitemaps](https://en.wikipedia.org/wiki/Sitemaps#Google_News_Sitemaps) to find links to news articles on news sites. A small collection of example seeds (feeds and sitemaps) is provided in [./seeds/](./seeds/). Adding support for news sites which do not provide a news feed or sitemap is an open issue, see [#41](//github.com/commoncrawl/news-crawl/issues/41).\n\n\nConfiguration\n-------------\n\nThe default configuration should work out-of-the-box. The only thing to do is to configure the user agent properties send in the HTTP request header. Open the file `conf/crawler-conf.yaml` in an editor and fill in the values for `http.agent.name` and all further properties starting with the `http.agent.` prefix.\n\n\nRun the crawl\n-------------\n\nGenerate an uberjar:\n``` sh\nmvn clean package\n```\n\nAnd run ...\n``` sh\nstorm jar target/crawler-1.18.1.jar org.commoncrawl.stormcrawler.news.CrawlTopology -conf $PWD/conf/es-conf.yaml -conf $PWD/conf/crawler-conf.yaml $PWD/seeds/ feeds.txt\n```\n\nThis will launch the crawl topology. It will also \"inject\" all URLs found in the file `./seeds/feeds.txt` in the status index. The URLs point to news feeds and sitemaps from which links to news articles are extracted and fetched. The to",
    "url": "https://github.com/commoncrawl/news-crawl",
    "last_updated": "2025-08-27T19:42:01+00:00"
  },
  {
    "full_name": "RBigData/pbdCS",
    "name": "pbdCS",
    "description": "A set of utilities for interactively using pbdR.",
    "language": "R",
    "topics": [],
    "readme": "# pbdCS \n\n* **Version:** 0.2-1\n* **License:** [BSD 2-Clause](http://opensource.org/licenses/BSD-2-Clause)\n* **Author:** Drew Schmidt and Wei-Chen Chen\n* **Project home**: https://github.com/RBigData/pbdCS\n* **Bug reports**: https://github.com/RBigData/pbdCS/issues\n\nA client/server framework for the pbdR packages. The client is actually the same as the client from the **remoter** package.\n\n\n## Installation\n\n<!-- You can install the stable version from CRAN using the usual `install.packages()`:\n\n```r\ninstall.packages(\"pbdCS\")\n```\n\nIn order to be able to create and connect to secure servers, you need to also install the **sodium** package.  The use of **sodium** is optional because it is a non-trivial systems dependency, but it is highly recommended.  You can install it manually with a call to `install.packages(\"sodium\")` or by installing **remoter** via:\n\n```r\ninstall.packages(\"pbdCS\", dependencies=TRUE)\n``` -->\n\nThe development version is maintained on GitHub, and can easily be installed by any of the packages that offer installations from GitHub:\n\n```r\nremotes::install_github(\"RBigData/pbdCS\")\n```\n\nTo simplify installations on cloud systems, we also have a [Docker container](https://github.com/RBigData/pbdr-cs) available.\n\n\n\n\n## Usage\n\nLaunch the batch servers:\n\n```bash\nmpirun -np 2 Rscript -e \"pbdCS::pbdserver()\"\n```\n\nConnect the client to the servers by running in an interactive session:\n\n```r\npbdCS::pbdclient()\n```\n\nFor more information, see the **remoter** and **pbdCS** package vignettes.\n\n\n\n## Acknowledgements\n\nThe development for this package was supported by the project *Harnessing Scalable Libraries for Statistical Computing on Modern Architectures and Bringing Statistics to Large Scale Computing* funded by the National Science Foundation Division of Mathematical Sciences under Grant No. 1418195.\n\nAny opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Na",
    "url": "https://github.com/RBigData/pbdCS",
    "last_updated": "2025-04-05T01:10:51+00:00"
  },
  {
    "full_name": "jrnold/r4ds-exercise-solutions",
    "name": "r4ds-exercise-solutions",
    "description": "Exercise solutions to \"R for Data Science\"",
    "language": "R",
    "topics": [
      "data-science",
      "exercise-solutions",
      "tidyverse",
      "r",
      "ggplot2",
      "dplyr",
      "r4ds",
      "tidyr",
      "rmarkdown",
      "bookdown"
    ],
    "readme": "[![Lifecycle: superseded](https://img.shields.io/badge/lifecycle-superseded-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#superseded)\n\n# Exercise Solutions to R for Data Science\n\nThese are solutions to the **1st edition** of R for Data Science. The solutions to the 2nd edition of [R for Data Science](https://r4ds.hadley.nz/) are available at [R for Data Science (2e) - Solutions to Exercises](https://mine-cetinkaya-rundel.github.io/r4ds-solutions/).\n\nThis repository contains the code and text behind the [Solutions for R for Data Science](https://jrnold.github.io/r4ds-exercise-solutions/), which, as its name suggests, has solutions to the the exercises in [R for Data Science](https://r4ds.had.co.nz/) by Garrett Grolemund and Hadley Wickham.\n\nThe R packages used in this book can be installed via\n```r\ndevtools::install_github(\"jrnold/r4ds-exercise-solutions\")\n```\n\n## Contributing\n\nWork on this repo has effectively stopped since the 2nd edition of R for Data Science has been published. Please direct your contributions to [R for Data Science (2e) - Solutions to Exercises](https://mine-cetinkaya-rundel.github.io/r4ds-solutions/).\n\n## Build\n\nThe site is built using the [bookdown](https://bookdown.org/yihui/bookdown/) package and pandoc.\n",
    "url": "https://github.com/jrnold/r4ds-exercise-solutions",
    "last_updated": "2025-08-18T21:32:29+00:00"
  },
  {
    "full_name": "berkeley-scf/spark-cloudwg-2015",
    "name": "spark-cloudwg-2015",
    "description": "Materials for BRC/D-Lab Cloud Working group session on Spark on AWS and Savio",
    "language": "Python",
    "topics": [],
    "readme": "# spark-cloudwg-2015\nMaterials for BRC/D-Lab Cloud Working group session on Spark on AWS and Savio\n\nThe file pres.pdf has an overview. The files example-aws.sh and example-savio.sh show how to get Spark up and running on AWS and Savio. The files example-analysis.py and example-sql.py show how to use the Python interface for Spark (PySpark) to run analyses, including using Spark SQL (example-sql.py). \n",
    "url": "https://github.com/berkeley-scf/spark-cloudwg-2015",
    "last_updated": "2019-03-25T03:44:47+00:00"
  },
  {
    "full_name": "sferik/t-ruby",
    "name": "t-ruby",
    "description": "A command-line power tool for Twitter.",
    "language": "Ruby",
    "topics": [],
    "readme": "# [![Application icon](https://github.com/sferik/t/raw/master/icon/t.png)][icon]\n[icon]: https://github.com/sferik/t/raw/master/icon/t.png\n\n# Twitter CLI\n[![Gem Version](https://img.shields.io/gem/v/t.svg)][gem]\n[![Build Status](https://img.shields.io/travis/sferik/t.svg)][travis]\n[![Dependency Status](https://img.shields.io/gemnasium/sferik/t.svg)][gemnasium]\n[![tip for next commit](https://tip4commit.com/projects/102.svg)](https://tip4commit.com/github/sferik/t)\n\n[gem]: https://rubygems.org/gems/t\n[travis]: https://travis-ci.org/sferik/t\n[gemnasium]: https://gemnasium.com/sferik/t\n\n#### A command-line power tool for Twitter.\nThe CLI takes syntactic cues from the [Twitter SMS commands][sms], but it\noffers vastly more commands and capabilities than are available via SMS.\n\n[sms]: https://support.twitter.com/articles/14020-twitter-sms-command\n\n## Dependencies\nFirst, make sure you have Ruby installed.\n\n**On a Mac**, open `/Applications/Utilities/Terminal.app` and type:\n\n    ruby -v\n\nIf the output looks something like this, you're in good shape:\n\n    ruby 3.4.3 (2025-04-14 revision d0b7e5b6a0) +PRISM [arm64-darwin24]\n\nIf the output looks more like this, you need to [install Ruby][ruby]:\n\n[ruby]: https://www.ruby-lang.org/en/downloads/\n\n    ruby: command not found\n\n**On Linux**, for Debian-based systems, open a terminal and type:\n\n    sudo apt-get install ruby-dev\n\nor for Red Hat-based distros like Fedora and CentOS, type:\n\n    sudo yum install ruby-devel\n\n(if necessary, adapt for your package manager)\n\n**On Windows**, you can install Ruby with [RubyInstaller][rubyinstaller].\n\n[rubyinstaller]: http://rubyinstaller.org/downloads/\n\n## Installation\nOnce you've verified that Ruby is installed:\n\n    gem install t\n\n## Configuration\nTwitter API v1.1 requires OAuth for all of its functionality, so you'll need a\nregistered Twitter application. If you've never registered a Twitter\napplication before, it's easy! Just sign-in using your Twitter account and then\nfill out the short fo",
    "url": "https://github.com/sferik/t-ruby",
    "last_updated": "2025-09-02T09:31:20+00:00"
  },
  {
    "full_name": "soodoku/daughters",
    "name": "daughters",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "### Replication Materials For \"Revisiting a Natural Experiment: Do Legislators With Daughters Vote More Liberally on Women's Issues?\"\n\nAn intriguing natural experiment arises from the fact that legislators are randomly assigned some combination of sons or daughters. The pioneering work of Washington (2008) shows that legislators with daughters cast more liberal roll call votes on women's issues. Costa et al. (2019) find that this pattern subsides in more recent congresses and speculate that increasing party polarization might diminish the ``daughters effect.'' The present paper delves more deeply into patterns of change over time by looking at eight congresses prior to the four studied by Washington (2008) as well as eight subsequent congresses, including three not included in Costa et al. (2019). Contrary to the party polarization hypothesis, we find no daughters effect leading up to the period that Washington studied and no effect thereafter. The cohort of members whom Washington studied exhibit consistently positive effects over time, while other legislators exhibit non-positive effects.The daughters effect appears to be a statistical aberration.\n\n### Manuscript\n\n* [Manuscript](ms/ms.pdf)\n* [Supporting Information](ms/si.pdf)\n\n### Data\n\n1. [AAUW Data 97th--116th Congresses](https://doi.org/10.7910/DVN/HD5VHI)\n2. [Costa et al. (2019) Replication Archive](data/costa_et_al/)\n3. [Washington (2008) Replication Archive](data/washington)\n4. [Congressional Member ID Data](data/member_id/)\n5. [Voteview Data](data/voteview_congress_members.csv)\n6. [Female Members of Congress](data/female%20members%20of%20congress.csv)\n7. [Data on Children of MCs](data/Child%20Info%20Master%20List%20Dotters.csv)\n8. [US Census Bureau Regions and Divisions](data/us_census_bureau_regions_and_divisions.csv) via [Chris](https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv)\n9. [Literature Review](data/dotters_lit.csv)\n\n### Scripts\n",
    "url": "https://github.com/soodoku/daughters",
    "last_updated": "2025-04-15T22:34:58+00:00"
  },
  {
    "full_name": "raphael-susewind/india-religion-politics",
    "name": "india-religion-politics",
    "description": "Data on religion and politics in India",
    "language": "Perl",
    "topics": [],
    "readme": "# Data on religion and politics in India\n\nThis repository provides highly localized statistics on religion and politics in India under an open license. I aim to cover Uttar Pradesh as comprehensively as possible, and the rest of India during general elections (see [roadmap](https://github.com/raphael-susewind/india-religion-politics/tree/master/ROADMAP.md)) and/or if other people contribute. A (potentially incomplete) list of academic usecases for this data is on [Google Scholar](https://scholar.google.com/scholar?oi=bibs&hl=de&cites=11938760322875868825); there is also a separate folder with [examples](https://github.com/raphael-susewind/india-religion-politics/tree/master/examples) to replicate. \n\nFortunately, recent transparency initiatives by the Election Commission of India in general and the Chief Electoral Officer of UP in particular now allow researchers to shift the central unit of quantitative political analyses from the constituency level to that of polling booths, stations, and villages (earlier, such data had to be interpolated or estimated). Often, this data is not very user-friendly, though (think garbled, scanned PDFs). The purpose of this repository is to curate this data in a more accessible format and to share the scraping and cleanup code for reference. This official data is then supplemented with estimates of religious demography based on the religious connotations of electors' names in the voter lists (see below).\n \nFrom 2013 to 2015, the whole dataset was located on my [personal website](https://www.raphael-susewind.de), and the [blog there](https://www.raphael-susewind.de/blog/category/quantitativemethods) continues to provide bits and pieces of advice on how to use it, as do my various [publications](https://writing.raphael-susewind.de). This created unnecessary hurdles for collaboration, though, and created its unique challenges in terms of long-term availability. After pondering various options, I decided to move to GitHub entirely. Techni",
    "url": "https://github.com/raphael-susewind/india-religion-politics",
    "last_updated": "2025-08-08T12:46:39+00:00"
  },
  {
    "full_name": "nrjones8/website-screenshotter",
    "name": "website-screenshotter",
    "description": "Hourly screenshots of the front pages of news websites, starting Jan 1, 2019",
    "language": "Python",
    "topics": [],
    "readme": "## News Homepage Archive\nWelcome! This project aims to provide a visual representation of how different media organizations cover various topics. Screenshots of the homepages of five different news organizations are taken once per hour, and made public thereafter. As an example, here are the homepages of cnn.com and foxnews.com the morning after the summary of Robert Mueller's report was released:\n\ncnn.com             |  foxnews.com\n:-------------------------:|:-------------------------:\n![](sample-images/cnn.com_morning_after_mueller.png)  |  ![](sample-images/foxnews.com_morning_after_mueller.png)\n\n\nIf you would like to access this archive, read on!\n\n## How to access screenshots\nThe screenshots are available as `.png`s, and can be accessed directly via a URL based on the time the screenshot was taken and the website. All follow the same format of:\n```\nhttps://d1k37mkoj29puy.cloudfront.net/{website_name}/{4 digit year}/{1-2 digit month}/{1-2 digit day}/{1-2 digit hour}/2/screenshot.png\n```\n*Note that all dates/times are in UTC.* For example, to see what cnn.com was showing on March 24, 2019 at 3pm EDT, you would use the following URL (note the conversion of 3pm EDT to 7pm UTC):\n```\nhttps://d1k37mkoj29puy.cloudfront.net/cnn.com/2019/3/24/7/2/screenshot.png\n```\n\nThe \"2\" is the minute that the screenshot was taken; it happens to always be \"2\" because the cronjob taking screenshots is set up to run on the second minute of each hour.\n\n## How much data is there?\nScreenshots are available at every hour starting from January 1, 2019. Currently, the only websites being tracked are:\n\n1. nytimes.com\n2. washingtonpost.com\n3. cnn.com\n4. wsj.com\n5. foxnews.com\n\n## What if I come across an error or a blank screenshot?\nSome screenshots are flakey, and occasionally the screenshotting process fails. If you'd like to report those issues, please open a [Github issue](https://github.com/nrjones8/website-screenshotter/issues) - this repo will make note of specific time / website issues ",
    "url": "https://github.com/nrjones8/website-screenshotter",
    "last_updated": "2022-10-02T09:33:31+00:00"
  },
  {
    "full_name": "hrbrmstr/cloc",
    "name": "cloc",
    "description": "🔢 R package to the perl cloc script (which counts blank lines, comment lines, and physical lines of source code in source files/trees/archives)",
    "language": "Perl",
    "topics": [
      "r",
      "rstats",
      "cloc",
      "count-lines-of-code"
    ],
    "readme": "\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-2%25-lightgrey.svg)\n\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-3.6.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# cloc\n\nCount Lines of Code, Comments and Whitespace in Source Files and\nArchives\n\n## Description\n\nCounts blank lines, comment lines, and physical lines of source code in\nsource files/trees/archives. An R wrapper to the ‘Perl’ command-line\nutility <https://github.com/AlDanial/cloc>.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n- `cloc_by_file`: Count lines of code, comments and whitespace in source\n  files/archives by file\n- `cloc_call`: Call cloc.pl directly with granular control over options\n- `cloc_cran`: Count lines of code (etc) from source packages on CRAN\n- `cloc_git`: Count lines of code, comments and whitespace in a git tree\n- `cloc_pkg_md`: Run cloc_pkg() on active package and format it as a\n  markdown table for knitting into reports\n- `cloc_pkg`: Count lines of code, comments and whitespace in a package\n- `cloc_recognized_languages`: Return a data frame of cloc recognized\n  languages and associated extensions\n- `cloc_remove_comments`: Strip comments and white space from a single\n  source file\n\n## Installation\n\n``` r\nremotes::install_github(\"hrbrmstr/cloc\")\n```\n\nNOTE: To use the ‘remotes’ install options you will need to have the\n[{remotes} package](https://github.com/r-lib/remotes) installed.\n\n## Usage\n\n``` r\nlibrary(cloc)\nlibrary(tibble) # for printing\n\n# current version\npackageVersion(\"cloc\")\n## [1] '0.3.5'\n```\n\nBasic usage\n\n``` r\n# by dir\ncloc(system.file(\"extdata\", package=\"cloc\"))\n## # A tibble: 3 × 10\n##   s",
    "url": "https://github.com/hrbrmstr/cloc",
    "last_updated": "2025-07-05T19:38:39+00:00"
  },
  {
    "full_name": "drivendataorg/cookiecutter-data-science",
    "name": "cookiecutter-data-science",
    "description": "A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.",
    "language": "Python",
    "topics": [
      "cookiecutter-data-science",
      "cookiecutter",
      "cookiecutter-template",
      "data-science",
      "machine-learning",
      "ai"
    ],
    "readme": "# Cookiecutter Data Science\n\n_A logical, reasonably standardized but flexible project structure for doing and sharing data science work._\n\n[![PyPI - Version](https://img.shields.io/pypi/v/cookiecutter-data-science)](https://pypi.org/project/cookiecutter-data-science/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/cookiecutter-data-science)](https://pypi.org/project/cookiecutter-data-science/)\n<a target=\"_blank\" href=\"https://cookiecutter-data-science.drivendata.org/\">\n    <img src=\"https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter\" />\n</a>\n[![tests](https://github.com/drivendataorg/cookiecutter-data-science/actions/workflows/tests.yml/badge.svg)](https://github.com/drivendataorg/cookiecutter-data-science/actions/workflows/tests.yml)\n\n**Cookiecutter Data Science (CCDS)** is a tool for setting up a data science project template that incorporates best practices. To learn more about CCDS's philosophy, visit the [project homepage](https://cookiecutter-data-science.drivendata.org/).\n\n> ℹ️ Cookiecutter Data Science v2 has changed from v1. It now requires installing the new cookiecutter-data-science Python package, which extends the functionality of the [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/README.html) templating utility. Use the provided `ccds` command-line program instead of `cookiecutter`.\n\n## Installation\n\nCookiecutter Data Science v2 requires Python 3.9+. Since this is a cross-project utility application, we recommend installing it with [pipx](https://pypa.github.io/pipx/). Installation command options:\n\n```bash\n# With pipx from PyPI (recommended)\npipx install cookiecutter-data-science\n\n# With pip from PyPI\npip install cookiecutter-data-science\n\n# With conda from conda-forge (coming soon)\n# conda install cookiecutter-data-science -c conda-forge\n```\n\n## Starting a new project\n\nTo start a new project, run:\n\n```bash\nccds\n```\n\n### The resulting directory structure\n\nThe directory structure of your new pr",
    "url": "https://github.com/drivendataorg/cookiecutter-data-science",
    "last_updated": "2025-09-02T01:52:32+00:00"
  },
  {
    "full_name": "gradio-app/gradio",
    "name": "gradio",
    "description": "Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!",
    "language": "Python",
    "topics": [
      "machine-learning",
      "models",
      "ui",
      "ui-components",
      "interface",
      "python",
      "data-science",
      "data-visualization",
      "deep-learning",
      "data-analysis",
      "gradio",
      "gradio-interface",
      "python-notebook",
      "deploy",
      "hacktoberfest"
    ],
    "readme": "<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align=\"center\">\n<a href=\"https://gradio.app\">\n<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=350>\n</a>\n</div>\n\n<div align=\"center\">\n<span>\n<a href=\"https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light\" alt=\"Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt\" style=\"width: 150px; height: 54px;\" width=\"150\" height=\"54\" /></a>\n<a href=\"https://trendshift.io/repositories/2145\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2145\" alt=\"gradio-app%2Fgradio | Trendshift\" style=\"width: 150px; height: 55px;\" width=\"150\" height=\"55\"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [中文](readme_files/zh-cn#readme)\n\n</d",
    "url": "https://github.com/gradio-app/gradio",
    "last_updated": "2025-09-02T09:48:26+00:00"
  },
  {
    "full_name": "yahoo/TensorFlowOnSpark",
    "name": "TensorFlowOnSpark",
    "description": "TensorFlowOnSpark brings TensorFlow programs to Apache Spark clusters.",
    "language": "Python",
    "topics": [
      "tensorflow",
      "spark",
      "yahoo",
      "machine-learning",
      "cluster",
      "featured",
      "python",
      "scala"
    ],
    "readme": "<!--\nCopyright 2019 Yahoo Inc.\nLicensed under the terms of the Apache 2.0 license.\nPlease see LICENSE file in the project root for terms.\n-->\n# TensorFlowOnSpark\n> _TensorFlowOnSpark brings scalable deep learning to Apache Hadoop and Apache Spark\nclusters._\n\n[![Build Status](https://cd.screwdriver.cd/pipelines/6384/badge)](https://cd.screwdriver.cd/pipelines/6384)\n[![Package](https://img.shields.io/badge/package-pypi-blue.svg)](https://pypi.org/project/tensorflowonspark/)\n[![Downloads](https://img.shields.io/pypi/dm/tensorflowonspark.svg)](https://img.shields.io/pypi/dm/tensorflowonspark.svg)\n[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://yahoo.github.io/TensorFlowOnSpark/)\n\nBy combining salient features from the [TensorFlow](https://www.tensorflow.org) deep learning framework with [Apache Spark](http://spark.apache.org) and [Apache Hadoop](http://hadoop.apache.org), TensorFlowOnSpark enables distributed\ndeep learning on a cluster of GPU and CPU servers.\n\nIt enables both distributed TensorFlow training and\ninferencing on Spark clusters, with a goal to minimize the amount\nof code changes required to run existing TensorFlow programs on a\nshared grid.  Its Spark-compatible API helps manage the TensorFlow\ncluster with the following steps:\n\n1. **Startup** - launches the Tensorflow main function on the executors, along with listeners for data/control messages.\n1. **Data ingestion**\n   - **InputMode.TENSORFLOW** - leverages TensorFlow's built-in APIs to read data files directly from HDFS.\n   - **InputMode.SPARK** - sends Spark RDD data to the TensorFlow nodes via a `TFNode.DataFeed` class.  Note that we leverage the [Hadoop Input/Output Format](https://github.com/tensorflow/ecosystem/tree/master/hadoop) to access TFRecords on HDFS.\n1. **Shutdown** - shuts down the Tensorflow workers and PS nodes on the executors.\n\n## Table of Contents\n\n- [Background](#background)\n- [Install](#install)\n- [Usage](#usage)\n- [API](#api)\n- [Contribute](#c",
    "url": "https://github.com/yahoo/TensorFlowOnSpark",
    "last_updated": "2025-08-27T12:11:18+00:00"
  },
  {
    "full_name": "paulhendricks/detector",
    "name": "detector",
    "description": "Detect data containing  Personally Identifiable Information (PII) in R",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\ndetector\n========\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/detector)](http://cran.r-project.org/package=detector) [![Downloads from the RStudio CRAN mirror](http://cranlogs.r-pkg.org/badges/detector)](http://cran.rstudio.com/package=detector) [![Build Status](https://travis-ci.org/paulhendricks/detector.png?branch=master)](https://travis-ci.org/paulhendricks/detector) [![Build status](https://ci.appveyor.com/api/projects/status/gu5ggnr1i2muw5r3/branch/master?svg=true)](https://ci.appveyor.com/project/paulhendricks/detector/branch/master) [![codecov.io](http://codecov.io/github/paulhendricks/detector/coverage.svg?branch=master)](http://codecov.io/github/paulhendricks/detector?branch=master) [![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)\n\n`detector` makes detecting data containing [Personally Identifiable Information](https://en.wikipedia.org/wiki/Personally_identifiable_information) (PII) quick, easy, and scalable. It provides high-level functions that can take vectors and data.frames and return important summary statistics in a convenient data.frame. Once complete, `detector` will be able to detect the following types of PII:\n\n-   Full name\n-   Home address\n-   E-mail address\n-   National identification number\n-   Passport number\n-   Social Security number\n-   IP address\n-   Vehicle registration plate number\n-   Driver's license number\n-   Credit card number\n-   Date of birth\n-   Birthplace\n-   Telephone number\n-   Latitude and longtiude\n\nState of the Union\n------------------\n\n### Complete!\n\n-   E-mail address\n-   Telephone number\n-   National identification number\n\n### Needs more work...\n\n-   Credit card number\n\n### Haven't even started :(\n\n-   Full name\n-   Date of birth\n-   Home address\n-   IP address\n-   Vehicle registration ",
    "url": "https://github.com/paulhendricks/detector",
    "last_updated": "2025-01-26T17:21:09+00:00"
  },
  {
    "full_name": "notnews/fox_news_transcripts",
    "name": "fox_news_transcripts",
    "description": "Fox News Transcripts 2003--2025",
    "language": "Jupyter Notebook",
    "topics": [
      "fox-news",
      "transcripts",
      "cable-news",
      "news"
    ],
    "readme": "## Fox News Transcripts 2003--2025\n\nWe scraped Fox News transcripts from [here](https://www.foxnews.com/transcript). In all, we scraped around ~24k transcripts.\n\nI scraped the data again in 2025, and the breakdown is as follows:\n\n|   year |   count |\n|-------:|--------:|\n|   2003 |     450 |\n|   2004 |     365 |\n|   2005 |     431 |\n|   2006 |     411 |\n|   2007 |     304 |\n|   2008 |     418 |\\n\n|   2009 |     425 |\\n\n|   2010 |     314 |\\n\n|   2011 |     523 |\\n\n|   2012 |    1019 |\\n\n|   2013 |     777 |\\n\n|   2014 |     866 |\\n\n|   2015 |     890 |\\n\n|   2016 |     821 |\\n\n|   2017 |    1259 |\\n\n|   2018 |    1752 |\\n\n|   2019 |    5865 |\\n\n|   2020 |    5995 |\\n\n|   2021 |    5400 |\\n\n|   2022 |    6782 |\\n\n|   2023 |    9585 |\\n\n|   2024 |    8256 |\\n\n|   2025 |    1474 |\n\nThe final dataset, including the HTML files, is posted on a [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/Q2KIES)\n\n## Scripts\n\n1. [Get Transcript URLs](01_get_transcript_urls.ipynb)\n2. [Download Transcript HTMLs](02_download_transcripts.ipynb)\n3. [Get Text from HTMLs](03_transcript_to_text.ipynb)\n4. [Upload to Dataverse](04_upload_to_dataverse.ipynb)\n\n## 🔗 Adjacent Repositories\n\n- [notnews/msnbc_transcripts](https://github.com/notnews/msnbc_transcripts) — MSNBC Transcripts: 2003--2022\n- [notnews/cnn_transcripts](https://github.com/notnews/cnn_transcripts) — CNN Transcripts 2000--2025\n- [notnews/stanford_tv_news](https://github.com/notnews/stanford_tv_news) — Stanford Cable TV News Dataset\n- [notnews/nbc_transcripts](https://github.com/notnews/nbc_transcripts) — NBC transcripts 2011--2014\n- [notnews/archive_news_cc](https://github.com/notnews/archive_news_cc) — Closed Caption Transcripts of News Videos from archive.org 2014--2023\n",
    "url": "https://github.com/notnews/fox_news_transcripts",
    "last_updated": "2025-08-16T16:50:35+00:00"
  },
  {
    "full_name": "r-lib/ellipsis",
    "name": "ellipsis",
    "description": "Tools for Working with ...",
    "language": "R",
    "topics": [
      "r",
      "ellipsis",
      "dot-dot-dot"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# ellipsis\n\n<!-- badges: start -->\n\n[![Lifecycle:\nmaturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/ellipsis)](https://cran.r-project.org/package=ellipsis)\n[![Travis build\nstatus](https://travis-ci.org/r-lib/ellipsis.svg?branch=master)](https://travis-ci.org/r-lib/ellipsis)\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/ellipsis/branch/master/graph/badge.svg)](https://codecov.io/gh/r-lib/ellipsis?branch=master)\n<!-- badges: end -->\n\nAdding `...` to a function is a powerful technique because it allows you\nto accept any number of additional arguments. Unfortunately it comes\nwith a big downside: any misspelled or extraneous arguments will be\nsilently ignored. This package provides tools for making `...` safer:\n\n  - `check_dots_used()` errors if any components of `...` are not\n    evaluated. This allows an S3 generic to state that it expects every\n    input to be evaluated.\n\n  - `check_dots_unnamed()` errors if any components of `...` are named.\n    This allows you to collect arbitrary unnamed arguments, warning if\n    the user misspells a named argument.\n\n  - `check_dots_empty()` errors if `...` is used. This allows you to use\n    `...` to force the user to supply full argument names, while still\n    warning if an argument name is misspelled.\n\nThanks to [Jenny Bryan](https://github.com/jennybc) for the idea, and\n[Lionel Henry](https://github.com/lionel-) for the heart of the\nimplementation.\n\n## Installation\n\nInstall the released version from CRAN:\n\n``` r\ninstall.packages(\"ellipsis\")\n```\n\nOr the development version from GitHub:\n\n``` r\ndevtools::install_github(\"r-lib/ellipsis\")\n```\n\n## Example\n\n`mean()` is a little dangerous because you might expect it to work like\n`sum()`:\n\n``` r\nsum(1, 2, 3, 4)\n#> [1] 10\nmean(1, 2, 3, 4)\n#> [1] 1\n```\n\nThis silently returns the incorrect ",
    "url": "https://github.com/r-lib/ellipsis",
    "last_updated": "2024-07-29T23:31:32+00:00"
  },
  {
    "full_name": "schliebs/trollR",
    "name": "trollR",
    "description": "LSE Hackathon Challenge: Detecting Online Trolling Behaviour ",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\ntrollR - Online Troll Detection using R\n=======================================\n\nLSE Hackathon Challenge: Detecting Online Trolling Behaviour\n\nData source: <https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/>\n\nData description\n\nA large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n-   toxic\n-   severe\\_toxic\n-   obscene\n-   threat\n-   insult\n-   identity\\_hate\n\nUsage\n=====\n\nTo install the package use\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"schliebs/trollR\")\nlibrary(trollR)\nlibrary(xgboost)\n```\n\n``` r\npredict_troll(\"Hello World - this is an example of trollR - Identifying trolling comments using R\")\n#> [1] 0.0722369\n\n# take some text\ntext <- c(\n  \"I would like to point out that your comment was substandard!\",\n  \"YOU SHOULD DIE!!!!\",\n  \"YOU SHOULD DIE\",\n  \"you should die!!!!\",\n  \"you should die\",\n  \"Go rot in hell\",\n  \"I can also write something non-toxic -- really\",\n  \"COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\",\n  \"bloody hell, i forgot my purse at the pub yesterday\"\n)\n\n# and find how likely it is to be trolling?\ndata_frame(text = text, troll = predict_troll(text)) %>% arrange(-troll)\n#> # A tibble: 9 x 2\n#>   text                                                          troll\n#>   <chr>                                                         <dbl>\n#> 1 COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK                 0.972 \n#> 2 bloody hell, i forgot my purse at the pub yesterday          0.958 \n#> 3 Go rot in hell                                               0.796 \n#> 4 you should die!!!!                                           0.729 \n#> 5 YOU SHOULD DIE!!!!                                           0.714 \n#> 6 YOU SHOULD DIE                                               0.667 \n#> 7 you should die                                               0.543 \n#> 8 I would like to point out",
    "url": "https://github.com/schliebs/trollR",
    "last_updated": "2023-11-18T12:19:24+00:00"
  },
  {
    "full_name": "quanteda/spacyr",
    "name": "spacyr",
    "description": "R wrapper to spaCy NLP",
    "language": "R",
    "topics": [
      "nlp",
      "r",
      "spacy",
      "speech-tagging",
      "extract-entities"
    ],
    "readme": "# [![spacyr: an R wrapper for spaCy](https://cdn.rawgit.com/quanteda/spacyr/master/images/spacyr_logo_small.svg)](https://spacyr.quanteda.io)\n\n<!-- badges: start -->\n\n[![CRAN\nVersion](https://www.r-pkg.org/badges/version/spacyr)](https://CRAN.R-project.org/package=spacyr)\n[![](https://img.shields.io/badge/devel%20version-1.3.0-royalblue.svg)](https://github.com/quanteda/spacyr)\n[![R-CMD-check](https://github.com/quanteda/spacyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/quanteda/spacyr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/quanteda/spacyr/branch/master/graph/badge.svg)](https://app.codecov.io/gh/quanteda/spacyr?branch=master)\n[![Downloads](https://cranlogs.r-pkg.org/badges/spacyr)](https://CRAN.R-project.org/package=spacyr)\n[![Total\nDownloads](https://cranlogs.r-pkg.org/badges/grand-total/spacyr?color=orange)](https://CRAN.R-project.org/package=spacyr)\n<!-- badges: end -->\n\nAn R wrapper to the spaCy “industrial strength natural language\nprocessing” Python library from <https://spacy.io>.\n\n## Installing the package\n\n1.  Install the **spacyr** R package:\n\n    - From CRAN:\n\n    ``` r\n    install.packages(\"spacyr\")\n    ```\n\n    - From GitHub:\n\n      To install the latest package from source, you can simply run the\n      following.\n\n    ``` r\n    remotes::install_github(\"quanteda/spacyr\")\n    ```\n\n2.  Install spaCy and requirements\n\n    Simply run:\n\n    ``` r\n    library(spacyr)\n    spacy_install()\n    ```\n\n    If you want to install a specific version, simply add it to the\n    install command:\n\n    ``` r\n    library(spacyr)\n    spacy_install(version = \"apple\")\n    ```\n\n    Check the helpful version tool on <https://spacy.io/usage> and to\n    see what is available.\n\n3.  (optional) Add more language models\n\n    If left unchanged, `spacy_install()` adds the default\n    “en_core_web_sm” model. You can add more language models with\n    `spacy_download_langmodel()`. For instance, to install a small and\n    e",
    "url": "https://github.com/quanteda/spacyr",
    "last_updated": "2025-06-17T12:25:34+00:00"
  },
  {
    "full_name": "ropensci/skimr",
    "name": "skimr",
    "description": "A frictionless, pipeable approach to dealing with summary statistics",
    "language": "HTML",
    "topics": [
      "unconf17",
      "r",
      "summary-statistics",
      "ropensci",
      "unconf",
      "r-package",
      "rstats",
      "peer-reviewed"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# skimr <a href='https://docs.ropensci.org/skimr/'>\n\n<img src='https://docs.ropensci.org/skimr/reference/figures/logo.png'\nalign=\"right\" height=\"139\" /></a>\n\n<!-- badges: start -->\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)\n[![R-CMD-check](https://github.com/ropensci/skimr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ropensci/skimr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/ropensci/skimr/graph/badge.svg)](https://app.codecov.io/gh/ropensci/skimr)\n[![This is an ROpenSci Peer reviewed\npackage](https://badges.ropensci.org/175_status.svg)](https://github.com/ropensci/software-review/issues/175)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/skimr)](https://cran.r-project.org/package=skimr)\n[![cran\nchecks](https://badges.cranchecks.info/worst/skimr.svg)](https://badges.cranchecks.info/worst/skimr.svg)\n<!-- badges: end -->\n\n`skimr` provides a frictionless approach to summary statistics which\nconforms to the [principle of least\nsurprise](https://en.wikipedia.org/wiki/Principle_of_least_astonishment),\ndisplaying summary statistics the user can skim quickly to understand\ntheir data. It handles different data types and returns a `skim_df`\nobject which can be included in a pipeline or displayed nicely for the\nhuman reader.\n\n**Note: `skimr` version 2 has major changes when skimr is used\nprogrammatically. Upgraders should review this document, the release\nnotes and vignettes carefully.**\n\n## Installation\n\nThe current released version of `skimr` can be installed from CRAN. If\nyou wish to install the current build of the next release you can do so\nusing the following:\n\n    # install.packages(\"devtools\")\n    devtools::install_github(\"ropensci/skimr\")\n\nThe APIs for this branch should be co",
    "url": "https://github.com/ropensci/skimr",
    "last_updated": "2025-08-26T14:16:37+00:00"
  },
  {
    "full_name": "rasbt/deep-learning-book",
    "name": "deep-learning-book",
    "description": "Repository for \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\"",
    "language": "Jupyter Notebook",
    "topics": [
      "deep-learning",
      "neural-network",
      "machine-learning",
      "python",
      "tensorflow",
      "artificial-intelligence",
      "data-science",
      "pytorch"
    ],
    "readme": "![Python 3.6](https://img.shields.io/badge/Python-3.6-blue.svg)\n![License](https://img.shields.io/badge/Code%20License-MIT-blue.svg)\n[![Mailing List](https://img.shields.io/badge/-Mailing%20List-lightgrey.svg)](https://groups.google.com/forum/#!forum/ann-and-dl-book)\n\n# Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\n\nRepository for the book *Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python*.\n\n---\n\n**Deep learning is not just the talk of the town among tech folks. Deep learning allows us to tackle complex problems, training artificial neural networks to recognize complex patterns for image and speech recognition. In this book, we'll continue where we left off in [*Python Machine Learning*](https://github.com/rasbt/python-machine-learning-book) and implement deep learning algorithms in [PyTorch](https://pytorch.org).**\n\n---\n\n- This repository will contain the instructions, code examples, and solutions for the *Hands-On* and *Exercise* portions of each chapter.\n\n- PDF and ebook versions of the book will be available from [Leanpub](https://leanpub.com/ann-and-deeplearning).\n\n[![Deep Learning Book](images/ann-and-deeplearning-cover.jpg)](https://leanpub.com/ann-and-deeplearning)\n\n\nISBN-10: [TBA]  \nISBN-13: [TBA]  \nPaperback: est. 2018  \n\n---\n\n## Manuscripts / Early Access Drafts\n\n- 01 - Introduction\n\n- 02 - The Perceptron \n\n- 03 - Optimizing Cost Functions with Gradient Descent\n\n- 04 - Logistic Regression and Softmax Regression\n\n- 05 - From Softmax Regression to Multilayer Perceptrons\n\n- 06 - Cross Validation and Performance Metrics\n\n- 07 - Regularization in Neural Networks\n\n- 08 - Learning Rates and Weight Initialization\n\n- 09 - Convolutional Neural Networks\n\n- 10 - Recurrent Neural Networks\n\n- 11 - Autoencoders\n\n- 12 - General Adverserial Neural Networks\n\n- 13 - Deep Generative Models\n\n- 14 - Reinforcement Learning\n\n#### Supporting Material\n\n- Appendi",
    "url": "https://github.com/rasbt/deep-learning-book",
    "last_updated": "2025-08-29T01:45:35+00:00"
  },
  {
    "full_name": "hrbrmstr/sanipy",
    "name": "sanipy",
    "description": "☮️Bring some sanity to R when working with Python via reticulate",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "reticulate",
      "python"
    ],
    "readme": "\n# sanipy\n\nBring some sanity to R when working with Python via reticulate\n\n## Description\n\nA set of utilities and helpers that make using Python via `reiculate` a\nbit saner.\n\nSee\n<https://rud.is/b/2018/08/04/digging-into-mbox-details-a-tale-of-tm-reticulate/>\nfor some functionality or look at the examples in the manual pages.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\nStucture inspection helpers:\n\n  - `pystr`: Compactly Display the Structure of a Python object\n  - `pynames`: (All) The Names of a Python Object\n\nStrng helpers:\n\n  - `get_chr`: Retrieve a value from a Python ‘dict’ as a character\n  - `utf8_decode`: Decode a Python structure and retrieve a UTF-8\n    encoded value from it\n\nDocumentation helpers:\n\n  - `py_doc`: Use built-in ‘pydoc’ to get the Python HTML manual page\n    for a given object\n  - `readthedocs`: Use “Read the Docs” to get help on a Python\n    module/object\n\nGeneral Utility:\n\n  - `%|0|%`: Default value for an object length being 0\n\n## Installation\n\n``` r\ndevtools::install_git(\"https://gitlab.com/hrbrmstr/sanipy.git\")\n# or\ndevtools::install_github(\"hrbrmstr/sanipy\")\n```\n\n## Usage\n\n``` r\nlibrary(sanipy)\n\n# current verison\npackageVersion(\"sanipy\")\n```\n\n    ## [1] '0.1.0'\n",
    "url": "https://github.com/hrbrmstr/sanipy",
    "last_updated": "2025-03-22T11:04:21+00:00"
  },
  {
    "full_name": "PUNCH-Cyber/stoq",
    "name": "stoq",
    "description": "An open source framework for enterprise level automated analysis.",
    "language": "Python",
    "topics": [
      "yara",
      "malware-analysis",
      "malware-analyzer",
      "malware-detection",
      "framework",
      "security-automation",
      "malware-research",
      "automation-framework"
    ],
    "readme": "<p align=\"center\">\n<img src=\"http://stoq.punchcyber.com/i/stoq.png\" width=\"300\"><br />\n</p>\n\n[![Join the community on Spectrum](https://withspectrum.github.io/badge/badge.svg)](https://spectrum.chat/stoq)\n\n[![Build Status](https://travis-ci.org/PUNCH-Cyber/stoq.svg?branch=master)](https://travis-ci.org/PUNCH-Cyber/stoq)\n[![Coverage Status](https://coveralls.io/repos/github/PUNCH-Cyber/stoq/badge.svg?branch=master)](https://coveralls.io/github/PUNCH-Cyber/stoq?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/stoq-framework/badge/?version=latest)](https://stoq-framework.readthedocs.io/en/latest/?badge=latest)\n[![Docker Build](https://img.shields.io/docker/build/punchcyber/stoq.svg)](https://hub.docker.com/r/punchcyber/stoq/)\n[![pypi](https://img.shields.io/pypi/v/stoq-framework.svg)](https://pypi.org/project/stoq-framework/)\n[![License](https://img.shields.io/pypi/l/stoq-framework.svg)](https://pypi.org/project/stoq-framework/)\n\n# Get Started\n\n- [Documentation](https://stoq-framework.readthedocs.io/)\n- [Installation](https://stoq-framework.readthedocs.io/en/latest/installation.html)\n- [Plugin Repository](https://github.com/PUNCH-Cyber/stoq-plugins-public)\n- [Plugin Documentation](https://stoq-framework.readthedocs.io/en/latest/dev/plugin_overview.html)\n\n# Overview\n\nstoQ is an automation framework that helps to simplify the mundane and repetitive\ntasks an analyst is required to do. It enables analysts and DevSecOps teams to\nquickly transition between different data sources, databases, decoders/encoders,\nand numerous other tasks using enriched and consistent data structures. stoQ was\ndesigned to be enterprise ready and scalable, while also being lean enough for\nindividual security researchers.\n\n## Why use stoQ?\n\n- Extremely lightweight and designed with simplicity in mind.\n- Fully supports AsyncIO.\n- A wide range of [publicly available plugins](https://github.com/PUNCH-Cyber/stoq-plugins-public).\n- stoQ makes no assumptions about your workflow. A",
    "url": "https://github.com/PUNCH-Cyber/stoq",
    "last_updated": "2025-08-19T15:20:40+00:00"
  },
  {
    "full_name": "gojiplus/allstar",
    "name": "allstar",
    "description": "Tally stats across all the public repos. in your orgs. and under your username",
    "language": "Python",
    "topics": [
      "analytics",
      "github",
      "stats"
    ],
    "readme": "## 📊 GitHub Stats Aggregator\n\nGitHub Action to aggregate and summarize statistics from public repositories across multiple GitHub organizations and user accounts.\n\n### 🚀 Overview\n\nGitHub Stats Aggregator collects statistics such as total repositories, stars, forks, and open issues from specified GitHub user accounts and organizations. The aggregated statistics are saved in both JSON and Markdown formats, perfect for integrating into README files or dashboards.\n\n### ✨ Features\n\n1. Summarizes data across multiple GitHub organizations and personal accounts.\n\n2. Generates a Markdown summary report (stats.md).\n\n3. Generates a JSON data file (stats.json) for further use or visualization.\n\n### 🔧 Usage\n\nStep 1: Create a Workflow File\n\nCreate a file .github/workflows/aggregate-stats.yml:\n\n```yaml\nname: Aggregate GitHub Stats\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n\njobs:\n  aggregate:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    env:\n      GITHUB_USER: \"your-github-username\"\n      ORG_LIST: \"USER, org1, org2, org3\"\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - run: python -m pip install requests\n\n      - run: python src/main.py\n\n      - name: Commit and push\n        run: |\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n          git add stats.json stats.md\n          git commit -m \"Update aggregated GitHub stats\" || echo \"No changes\"\n          git push\n```\n\nReplace your-github-username and org1, org2, org3 with your GitHub username and any organizations you want to include.\n\nStep 2: Set up Python Script\n\nEnsure the script (src/main.py) from this repository is present in your repository, along with any required files:\n```\nrepo-root/\n├── .github/\n│   └── workflows/\n│       └── aggregate-stats.yml\n└── src/\n    └── main.py\n```\n\nStep 3: Workflow Execution\n\nThe workflow runs daily (adjust",
    "url": "https://github.com/gojiplus/allstar",
    "last_updated": "2025-08-31T01:37:25+00:00"
  },
  {
    "full_name": "jedisct1/libsodium",
    "name": "libsodium",
    "description": "A modern, portable, easy to use crypto library.",
    "language": "C",
    "topics": [
      "crypto",
      "cryptography",
      "c",
      "zig-package"
    ],
    "readme": "[![GitHub CI](https://github.com/jedisct1/libsodium/workflows/CI/badge.svg)](https://github.com/jedisct1/libsodium/actions)\n[![Windows build status](https://ci.appveyor.com/api/projects/status/fu8s2elx25il98hj?svg=true)](https://ci.appveyor.com/project/jedisct1/libsodium)\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/2397/badge.svg)](https://scan.coverity.com/projects/2397)\n[![Azure build status](https://jedisct1.visualstudio.com/Libsodium/_apis/build/status/jedisct1.libsodium?branchName=stable)](https://jedisct1.visualstudio.com/Libsodium/_build/latest?definitionId=3&branchName=stable)\n[![CodeQL scan](https://github.com/jedisct1/libsodium/workflows/CodeQL%20scan/badge.svg)](https://github.com/jedisct1/libsodium/actions)\n\n# ![libsodium](https://raw.github.com/jedisct1/libsodium/master/logo.png)\n\nSodium is an easy-to-use software library that provides a wide range of cryptographic operations including encryption, decryption, digital signatures, and secure password hashing.\n\nIt is a portable, cross-compilable, installable, and packageable fork of [NaCl](http://nacl.cr.yp.to/). While maintaining API compatibility, libsodium extends functionality to improve usability and simplify the development of secure applications.\n\n---\n\n## Key Features\n\n- **Encryption & Decryption:** Securely encrypt and decrypt data with modern algorithms.\n- **Digital Signatures:** Create and verify signatures to ensure data authenticity.\n- **Cross-Platform Compatibility:** Supported on Windows (MinGW and Visual Studio, both x86 and x64), iOS, Android, JavaScript, and WebAssembly.\n- **User-Friendly API:** Designed to provide all core cryptographic operations while remaining easy to integrate into your projects.\n\n---\n\n## Documentation\n\nDetailed documentation is available online. It is generated from the [libsodium-doc](https://github.com/jedisct1/libsodium-doc) repository and requires JavaScript for full functionality:\n\n- [libsodium Documentation](https://doc.libsodium.org)\n\n---",
    "url": "https://github.com/jedisct1/libsodium",
    "last_updated": "2025-09-01T23:06:12+00:00"
  },
  {
    "full_name": "remy/mit-license",
    "name": "mit-license",
    "description": "Hosted MIT License with details controlled through this repo",
    "language": "CSS",
    "topics": [],
    "readme": "# A permalink for your MIT License\n\nI always forget to add a `LICENSE` file to my projects, so I wanted to link to a single resource that would always be up to date and would always have my details online.\n\nWhy keep this to myself, there are three ways to create your _own_ MIT license page:\n\n1.  Use the generator tool (easiest)\n2.  Make a request to the API (details below)\n3.  Fork this project and send a pull request\n\nNow I can always include <https://rem.mit-license.org> in all my projects which links `rem` (the CNAME) against my copyright holder name `Remy Sharp` - all stored in the `users` directory.\n\n## Requesting your own MIT license page\n\nThe simplest way to create your own MIT license page is to use the self-service generator found [here](https://richienb.github.io/mit-license-generator).\n\nYou can fork this project, send me a pull request and wait for me to pull (which I'll do as quickly as possible) or if the user is still available you can do it yourself from the command line:\n\n```bash\ncurl -d'{ \"copyright\": \"Remy Sharp\" }' https://rem.mit-license.org\n```\n\nIf the `rem` user isn't taken already, then this will create the new user file on the fly and the URL will be immediately available.\n\nYou can send a full JSON file to the API, not _just_ the copyright, so this works too:\n\n```bash\ncurl -d'{ \"copyright\": \"Remy Sharp\", \"url\": \"https://remysharp.com\", \"email\": \"me@mysite.com\", \"format\": \"txt\" }' https://rem.mit-license.org\n```\n\nWhilst the command above sends the data as a string which will later be parsed, you can explicitly specify a JSON `Content-Type`:\n\n```bash\ncurl -H 'Content-Type: application/json' -d'{ \"copyright\": \"Remy Sharp\", \"url\": \"https://remysharp.com\", \"email\": \"me@mysite.com\", \"format\": \"txt\" }' https://rem.mit-license.org\n```\n\nYou can also encode the data as URL query parameters like so:\n\n```bash\ncurl -X POST \"https://rem.mit-license.org/?copyright=Remy%20Sharp&url=http%3A%2F%2Fremysharp.com&email=me%40mysite.com&format=txt\"\n```\n\nIf there ar",
    "url": "https://github.com/remy/mit-license",
    "last_updated": "2025-08-26T17:59:20+00:00"
  },
  {
    "full_name": "paultopia/lawvec",
    "name": "lawvec",
    "description": "working on creating word vectors for legal text.  in progress experiment/learning project",
    "language": "HTML",
    "topics": [],
    "readme": "",
    "url": "https://github.com/paultopia/lawvec",
    "last_updated": "2023-01-23T22:22:38+00:00"
  },
  {
    "full_name": "josephmate/coding_interview_practice",
    "name": "coding_interview_practice",
    "description": "",
    "language": "Java",
    "topics": [],
    "readme": "\nThis is my journey, preparing myself for coding interviews. My goals are:\n- refresh my C++\n- learn some new technologies in C++\n  - unit tests\n  - C++11 features\n- brush up on my problem solving skills\n- practice a bit of big-O\n\n1. equals_without_comparisons: first C++ program in a while\n2. find_max: simple standard library review and first unit test ever in C++ (done lots in Java)\n3. linked_list: implement a linked list in C++ to review classes, header files, unit tests\n4. stack_using_link_node: implement a stack using only a link_node\n5. nested_list: doubley linked list structure which also has a child\n6. linked_list_cycle: efficiently determine if a linked list has a cycle\n7. bst: implement a binary search tree using C (not C++)\n  1. inorder and preorder traversal\n  2. eliminating recusion from a recusive method\n8. heap: implement a heap using Java and gradle build environment\n9. first_non_repeat_char: get practice with reading strings in C/C++\n10. remove_chars: remove the required characters from the input string\n  1. efficiently build the new string\n11. reverse_words: reverse the words separated by spaces in place (without\n\t\tallocating new memory)\n12. str_int_convert: convert between string and integer\n13. binary_search: binary search implemented recursively and interatively\n14. permute_str: output all permutations of a string\n15. string_combos: output all the combinations of a string\n16. telephone_words: output all possible character combinations of a phone number\n17. busy_waiting: describe techniques used to prevent busy waiting\n18. producer_consumer: implementation of the producer consumer example\n19. dining_philosophers: make sure no one starves\n20. simple_sql: insert a row into a table\n21. employee_db: select practice\n22. max_no_agg: implement max without using aggregate functions (ie: group by)\n23. three_valued_logic: where clause practice\n24. eighth_of_a_circle: practice graphics and bits manipulations by drawing\n\t\teigth of a circle\n24. rectangle_overla",
    "url": "https://github.com/josephmate/coding_interview_practice",
    "last_updated": "2022-04-17T16:56:01+00:00"
  },
  {
    "full_name": "airbnb/aerosolve",
    "name": "aerosolve",
    "description": "A machine learning package built for humans.",
    "language": "Scala",
    "topics": [],
    "readme": "\naerosolve\n=========\n\nMachine learning **for humans**.\n\n[![Build Status](https://travis-ci.org/airbnb/aerosolve.svg)](https://travis-ci.org/airbnb/aerosolve)\n[ ![Download](https://api.bintray.com/packages/airbnb/aerosolve/aerosolve-core/images/download.svg) ](https://bintray.com/airbnb/aerosolve/aerosolve-core/_latestVersion)\n[ ![Download](https://api.bintray.com/packages/airbnb/aerosolve/aerosolve-training/images/download.svg) ](https://bintray.com/airbnb/aerosolve/aerosolve-training/_latestVersion)\n\nWhat is it?\n-----------\n\nA machine learning library designed from the ground up to be human friendly.\nIt is different from other machine learning libraries in the following ways:\n\n  * A [thrift based feature representation](https://github.com/airbnb/aerosolve/tree/master/core/src/main/thrift) that enables pairwise ranking loss and single context multiple item representation.\n  * A [feature transform language](https://github.com/airbnb/aerosolve/tree/master/core/src/main/java/com/airbnb/aerosolve/core/transforms) gives the user a lot of control over the features\n  * Human friendly [debuggable models](https://github.com/airbnb/aerosolve/tree/master/core/src/main/java/com/airbnb/aerosolve/core/models)\n  * Separate lightweight [Java inference code](https://github.com/airbnb/aerosolve/tree/master/core/src/main/java/com/airbnb/aerosolve/core)\n  * Scala code for [training](https://github.com/airbnb/aerosolve/tree/master/training/src/main/scala/com/airbnb/aerosolve/training)\n  * Simple [image content analysis code](https://github.com/airbnb/aerosolve/tree/master/core/src/main/java/com/airbnb/aerosolve/core/images) suitable for ordering or ranking images\n\nThis library is meant to be used with sparse, interpretable features such as those that commonly occur in search\n(search keywords, filters) or pricing (number of rooms, location, price). It is not as interpretable with problems with very dense\nnon-human interpretable features such as raw pixels or audio samples.\n\nThere are a f",
    "url": "https://github.com/airbnb/aerosolve",
    "last_updated": "2025-08-28T16:53:38+00:00"
  },
  {
    "full_name": "mitre/sparklyr.nested",
    "name": "sparklyr.nested",
    "description": "A sparklyr extension for nested data",
    "language": "R",
    "topics": [],
    "readme": "\n[![Build\nStatus](https://travis-ci.org/mitre/sparklyr.nested.svg?branch=master)](https://travis-ci.org/mitre/sparklyr.nested)\n[![CRAN Status\nBadge](http://www.r-pkg.org/badges/version/sparklyr.nested)](https://cran.r-project.org/package=sparklyr.nested)\n![downloads](http://cranlogs.r-pkg.org/badges/grand-total/sparklyr.nested)\n\nA package to extend the capabilities available in the `sparklyr` package\nwith support for working with nested data.\n\n## Installation & Documentation\n\nTo install:\n\n``` r\ninstall.packages(\"sparklyr.nested\")\n```\n\nOr to get the development version:\n\n``` r\ndevtools::install_github(\"mitre/sparklyr.nested\")\n```\n\nNote that per the `sparklyr` installation instructions, you will need to\ninstall Spark if you have not already done so or are not using a cluster\nwhere it is already installed.\n\nFull documentation is available here:\n<https://mitre.github.io/sparklyr.nested/>\n\n## Nested Operations\n\nThe `sparklyr` package makes working with Spark in R easy. The goal of\nthis package is to extend `sparklyr` so that working with nested data is\neasy. The flagship functions are `sdf_select`, `sdf_explode`,\n`sdf_unnest`, and `sfd_schema_viewer`.\n\n### Schema Viewer\n\nSuppose I have data about aircraft phase of flight (e.g., climb, cruise,\ndescent). The data is somewhat complex, storing radar data points marked\nas the start and end points of a given phase. Furthermore, the data is\nstructured such that for a given flight, there are several phases\n(disjoint in time) in a nested array.\n\nThis is a data set that is not very natural for more R use cases (though\nthe `tidyr` package helps close this gap) but is fairly typical for\nHadoop storage (e.g., using Avro or Parquet). The schema viewer (coupled\nwith a json schema getter `sdf_schema_json`) makes understanding the\nstructure of the data simple.\n\nSuppose that `spark_data` is a Spark data frame. The structure may be\nunderstood by expanding/collapsing the schema via\n\n``` r\nspark_data %>%\n  sdf_schema_viewer()\n```\n\n![schema v",
    "url": "https://github.com/mitre/sparklyr.nested",
    "last_updated": "2025-03-22T11:14:46+00:00"
  },
  {
    "full_name": "TransformerLensOrg/TransformerLens",
    "name": "TransformerLens",
    "description": "A library for mechanistic interpretability of GPT-style language models",
    "language": "Python",
    "topics": [],
    "readme": "# TransformerLens\n\n<!-- Status Icons -->\n[![Pypi](https://img.shields.io/pypi/v/transformer-lens?color=blue)](https://pypi.org/project/transformer-lens/)\n![Pypi Total Downloads](https://img.shields.io/pepy/dt/transformer_lens?color=blue) ![PyPI -\nLicense](https://img.shields.io/pypi/l/transformer_lens?color=blue) [![Release\nCD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/release.yml/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/release.yml)\n[![Tests\nCD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/checks.yml/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/checks.yml)\n[![Docs\nCD](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/TransformerLensOrg/TransformerLens/actions/workflows/pages/pages-build-deployment)\n\nA Library for Mechanistic Interpretability of Generative Language Models. Maintained by [Bryce Meyer](https://github.com/bryce13950) and created by [Neel Nanda](https://neelnanda.io/about)\n\n[![Read the Docs\nHere](https://img.shields.io/badge/-Read%20the%20Docs%20Here-blue?style=for-the-badge&logo=Read-the-Docs&logoColor=white&link=https://TransformerLensOrg.github.io/TransformerLens/)](https://TransformerLensOrg.github.io/TransformerLens/)\n\nThis is a library for doing [mechanistic\ninterpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models. The\ngoal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms\nthe model learned during training from its weights.\n\nTransformerLens lets you load in 50+ different open source language models, and exposes the internal\nactivations of the model to you. You can cache any internal activation in the model, and add in\nfunctions to edit, remove or replace these activations as the model runs.\n\n## Quick Start\n\n### Install\n\n```shell\npip install transforme",
    "url": "https://github.com/TransformerLensOrg/TransformerLens",
    "last_updated": "2025-09-02T02:23:51+00:00"
  },
  {
    "full_name": "jarrodmillman/scientific-review",
    "name": "scientific-review",
    "description": "See http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3328792/",
    "language": "",
    "topics": [],
    "readme": "Getting the repo::\n\n    git clone git@github.com:jarrodmillman/scientific-review.git\n\nCommiting changes::\n\n    git commit -m 'commit message'\n\nPushing changes back up::\n\n    git push origin master\n\nCreating a simple PDF from the article::\n\n   rst2pdf article.rst\n\n\nNotes from Jarrod's 5/9/2011 email:\n\nOnce you've created your own fork, here are the main git commands you will need:\n\n### create your working repo\n$ git clone git@github.com:binarybottle/scientific-review.git\n$ cd scientific-review\n\n### add satra and my github repos\n$ git remote add satra git@github.com:satra/scientific-review.git\n$ git remote add jarrod git@github.com:jarrodmillman/scientific-review.git\n\n### get my newest revision and merge it with your master branch\n$ git fetch jarrod\n$ git merge jarrod/master\n\n### get satra's newest revision and merge it with your master branch\n$ git fetch satra\n$ git merge satra/master\n\n### commit your changes and push to your github repo\n$ git commit\n$ git push\n\nArno (binarybottle), Brian (stnava), and Satra (satra) should have\npermission to push to my github repo as well:\n$ git push jarrod\n",
    "url": "https://github.com/jarrodmillman/scientific-review",
    "last_updated": "2015-07-16T22:11:46+00:00"
  },
  {
    "full_name": "facebook/Ax",
    "name": "Ax",
    "description": "Adaptive Experimentation Platform",
    "language": "Python",
    "topics": [],
    "readme": "<img width=\"300\" src=\"https://ax.dev/img/ax_logo_lockup.svg\" alt=\"Ax Logo\" />\n\n<hr/>\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n[![Build Status](https://img.shields.io/pypi/v/ax-platform.svg)](https://pypi.org/project/ax-platform/)\n[![Build Status](https://img.shields.io/pypi/pyversions/ax-platform.svg)](https://pypi.org/project/ax-platform/)\n[![Build Status](https://img.shields.io/pypi/wheel/ax-platform.svg)](https://pypi.org/project/ax-platform/)\n[![Build Status](https://github.com/facebook/Ax/workflows/Build%20and%20Test%20Workflow/badge.svg)](https://github.com/facebook/Ax/actions?query=workflow%3A%22Build+and+Test+Workflow%22)\n[![codecov](https://codecov.io/gh/facebook/Ax/branch/main/graph/badge.svg)](https://codecov.io/gh/facebook/Ax)\n[![Build Status](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\nAx is an accessible, general-purpose platform for understanding, managing,\ndeploying, and automating adaptive experiments.\n\nAdaptive experimentation is the machine-learning guided process of iteratively\nexploring a (possibly infinite) parameter space in order to identify optimal\nconfigurations in a resource-efficient manner. Ax currently supports Bayesian\noptimization and bandit optimization as exploration strategies. Bayesian\noptimization in Ax is powered by\n[BoTorch](https://github.com/facebookexternal/botorch), a modern library for\nBayesian optimization research built on PyTorch.\n\nFor full documentation and tutorials, see the [Ax website](https://ax.dev)\n\n## Why Ax?\n\n- **Expressive API**: Ax has an expressive API that can address many real-world\n  optimization tasks. It handles complex search spaces, multiple objectives,\n  constraints on both parameters and outcomes, and noisy observations. It\n  supports suggesting multiple designs to evaluate in parallel (both\n  synchronously and asynchronously) and the ability to early-stop evaluations.\n\n- **St",
    "url": "https://github.com/facebook/Ax",
    "last_updated": "2025-08-29T19:06:38+00:00"
  },
  {
    "full_name": "vertica/ddR",
    "name": "ddR",
    "description": "Standard API for Distributed Data Structures in R",
    "language": "R",
    "topics": [],
    "readme": "---\ntitle: \"ddR README\"\nauthor: \"Edward Ma, Indrajit Roy, Michael Lawrence\"\ndate: \"2015-10-22\"\n---\n\nThe 'ddR' package aims to provide an unified R interface for writing\nparallel and distributed applications.  Our goal is to ensure that R\nprograms written using the 'ddR' API work across different distributed\nbackends, therefore, reducing the effort required by users to\nunderstand and program on different backends.  Currently 'ddR'\nprograms can be executed on R's default 'parallel' package as well as\nthe open source HP Distributed R.  We plan to add support for\nSparkR. This package is an outcome of feedback and collaboration\nacross different companies and R-core members!\n\nThrough funding provided by the\n[R-consortium](https://www.r-consortium.org/projects) this package is under\nactive development for the summer of 2016. Check out the [mailing\nlist](https://lists.r-consortium.org/mailman/listinfo/rconsortium-wg-ddr)\nto see the latest discussions.\n\n'ddR' is an API, and includes a default execution engine, to express\nand execute distributed applications. Users can declare distributed\nobjects (i.e., `dlist`, `dframe`, `darray`), and execute parallel\noperations on these data structures using R-style `apply`\nfunctions. It also allows different backends (that support ddR, and\nhave ddR \"drivers\" written for them) to be dynamically activated in\nthe R user's environment to execute applications\n\nPlease refer to the user guide under vignettes/ for a detailed description on how to use the package.\n\n### Some quick examples\n\n\n```r\nlibrary(ddR)\n```\nBy default, the `parallel` backend is used with all the cores present on the machine. You can switch backends or specify the number of cores to use with the `useBackend` function. For example, you can specify that the `parallel` backend should be used with only 4 cores by executing `useBackend(parallel, executors=4)`.\n\nInitializing a distributed list (dlist):\n\n\n```r\na <- dmapply(function(x) { x }, rep(3,5))\ncollect(a)\n```\n\n```\n## [[1]]\n## ",
    "url": "https://github.com/vertica/ddR",
    "last_updated": "2024-06-20T23:05:55+00:00"
  },
  {
    "full_name": "probml/pml-book",
    "name": "pml-book",
    "description": "\"Probabilistic Machine Learning\" - a book series by Kevin Murphy",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "\n# \"Probabilistic machine learning\": a book series by Kevin Murphy\n\n\n  <p>&nbsp;</p>\n  \n## Book 0: \"Machine Learning: A Probabilistic Perspective\" (2012)\n\nSee [this link](https://probml.github.io/pml-book/book0.html)\n\n<!--\nSee [this link](https://probml.github.io/pml-book/pml0/book0.html)\n-->\n\n## Book 1: \"Probabilistic Machine Learning: An Introduction\" (2022)\n\nSee [this link](https://probml.github.io/pml-book/book1.html)\n\n\n## Book 2: \"Probabilistic Machine Learning: Advanced Topics\" (2023)\n\nSee [this link](https://probml.github.io/pml-book/book2.html)\n\n\n\n",
    "url": "https://github.com/probml/pml-book",
    "last_updated": "2025-09-01T19:12:03+00:00"
  },
  {
    "full_name": "OpenGenderTracking/globalnamedata",
    "name": "globalnamedata",
    "description": "Tools to download and process name data from various sources.",
    "language": "R",
    "topics": [],
    "readme": "## What is this?\n\nMost data on names and gender is ill-suited for any serious analytical purpose. Websites which collect data on birth names mainly offer searches, top ten lists and suggestions for parents. Most available data on the web comes either from commercial sources or from summary data.\n\nWe have collected birth record data from the United States and the United Kingdom across a number of years for all births in the two countries and are releasing the collected and cleaned up data here. We have also generated a simple gender classified based on incidence of gender by name. You can use this data for any purpose compatible with the [license](https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md).\n\nAnd, unlike any other open record for name data, we've provided the scripts necessary to check our work! You don't need to trust us in order to trust the data.\n\nYou can read about some uses of this data along with code examples at the Bocoup [blog](http://weblog.bocoup.com/global-name-data/).\n\n## Setup\n\nThe easiest way to set up Global Name Data is to install it as an R package with [devtools](https://github.com/hadley/devtools). With `devtools` installed you can install the package directly from github with `install_github(\"globalnamedata\", \"OpenGenderTracking\")`. Dependencies will be automatically installed.\n\nOnce installed, the package will make available datasets (in compressed `.RData` format) for each source of name data as well as functions to process and check that data against available records.\n\n## Not an R user?\n\nIf you're mainly interest in the data, pre and post classified name data is available in the [assets directory](https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets). If you install the package these will not be included in the install as `.csv` files but will be included as compressed binaries (the data are identical).\n\n## Contributing\n\nWe love pull requests. While not required, please try to adhere to [Goo",
    "url": "https://github.com/OpenGenderTracking/globalnamedata",
    "last_updated": "2025-05-03T03:50:37+00:00"
  },
  {
    "full_name": "kbenoit/ITAUR",
    "name": "ITAUR",
    "description": "Introduction to Text Analysis Using R",
    "language": "HTML",
    "topics": [],
    "readme": "## Introduction to Text Analysis Using R\n\n\n#### A Three-Day tutorial\n\n[Kenneth Benoit](kbenoit@lse.ac.uk), Department of Methodology, LSE  \n\n**Date:** Updated for newer versions (> 1.0.0) of **quanteda** in March 2018\n**quanteda version:** 1.1.0 (CRAN)  \n\nThis repository contains the workshop materials for a one-day version of a workshop [Introduction to Text Analysis Using R](link here) taught by Kenneth Benoit.  This workshop and the materials it contains are funded by by the European Research Council grant ERC-2011-StG 283794-QUANTESS: *Quantitative Analysis of Text for Social Science*.\n\n### Instructions for using this resource ###\n\nYou have three options for downloading the course material found on this page:  \n\n1.  You can download the materials by clicking on each link.  \n\n2.  You can \"clone\" repository, using the buttons found to the right side of your browser window as you view this repository.  This is the button labelled \"Clone in Desktop\".  If you do not have a git client installed on your system, you will need to [get one here](https://git-scm.com/download/gui) and also to make sure that [git is installed](https://git-scm.com/downloads).  This is preferred, since you can refresh your clone as new content gets pushed to the course repository.  (And new material will get actively pushed to the course repository at least once per day as this course takes place.)\n\n3.  Statically, you can choose the button on the right marked \"Download zip\" which will download the entire repository as a zip file.\n\nYou can also subscribe to the repository if you have [a GitHub account](https://github.com), which will send you updates each time new changes are pushed to the repository.\n\n\n### Objectives\n\nThis workshop covers how to perform common text analysis and natural language processing tasks using R.  Contrary to a belief popular among some data scientists, when used properly, R is a fast and powerful tool for managing even very large text analysis tasks.  \n\nThe course com",
    "url": "https://github.com/kbenoit/ITAUR",
    "last_updated": "2025-05-21T13:44:44+00:00"
  },
  {
    "full_name": "amueller/scipy_2015_sklearn_tutorial",
    "name": "scipy_2015_sklearn_tutorial",
    "description": "Scikit-Learn tutorial material for Scipy 2015",
    "language": "Python",
    "topics": [],
    "readme": "SciPy 2015 Scikit-learn Tutorial\n================================\n\nYou can find the video recordings on youtube:\n\n- [Part 1](https://www.youtube.com/watch?v=80fZrVMurPM)\n- [Part 2](https://www.youtube.com/watch?v=Ud-FsEWegmA)\n\n\nBased on the SciPy [2013 tutorial](https://github.com/jakevdp/sklearn_scipy2013) by [Gael Varoquaux](http://gael-varoquaux.info), [Olivier Grisel](http://ogrisel.com) and [Jake VanderPlas](http://jakevdp.github.com\n).\n\n\nInstructors\n-----------\n- [Kyle Kastner](https://kastnerkyle.github.io/)  [@kastnerkyle](https://twitter.com/kastnerkyle)- Université de Montréal\n- [Andreas Mueller](http://amuller.github.io) [@t3kcit](https://twitter.com/t3kcit) - NYU Center for Data Science\n\n\nThis repository will contain files and other info associated with our Scipy\n2015 scikit-learn tutorial.\n\nParts 1 to 5 make up the morning session, while\nparts 6 to 9 will be presented in the afternoon.\n\nInstallation Notes\n------------------\n\nThis tutorial will require recent installations of *numpy*, *scipy*,\n*matplotlib*, *scikit-learn* and *ipython* with ipython\nnotebook.\n\nThe last one is important, you should be able to type:\n\n    ipython notebook\n\nin your terminal window and see the notebook panel load in your web browser.\nTry opening and running a notebook from the material to see check that it works.\n\nFor users who do not yet have these  packages installed, a relatively\npainless way to install all the requirements is to use a package such as\n[Anaconda CE](http://store.continuum.io/ \"Anaconda CE\"), which can be\ndownloaded and installed for free.\nPython2.7 and 3.4 should both work fine for this tutorial.\n\nAfter getting the material, you should run ``python check_env.py`` to verify\nyour environment.\n\nDownloading the Tutorial Materials\n----------------------------------\nI would highly recommend using git, not only for this tutorial, but for the\ngeneral betterment of your life.  Once git is installed, you can clone the\nmaterial in this tutorial by using the git address",
    "url": "https://github.com/amueller/scipy_2015_sklearn_tutorial",
    "last_updated": "2025-06-15T08:14:21+00:00"
  },
  {
    "full_name": "pachadotdev/analogsea",
    "name": "analogsea",
    "description": "Digital Ocean R client",
    "language": "R",
    "topics": [
      "cloud-computing",
      "rstats",
      "droplet",
      "ssh"
    ],
    "readme": "analogsea\n=========\n\n[![codecov.io](https://codecov.io/github/pachadotdev/analogsea/coverage.svg?branch=master)](https://codecov.io/github/pachadotdev/analogsea?branch=master)\n[![rstudio mirror downloads](https://cranlogs.r-pkg.org/badges/analogsea)](https://github.com/r-hub/cranlogs.app)\n[![cran version](https://www.r-pkg.org/badges/version/analogsea)](https://cran.r-project.org/package=analogsea)\n[![R-CMD-check](https://github.com/pachadotdev/analogsea/actions/workflows/R-CMD-check.yml/badge.svg)](https://github.com/pachadotdev/analogsea/actions/workflows/R-CMD-check.yml)\n[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)\n\n<img src=\"https://raw.githubusercontent.com/pachadotdev/analogsea/main/inst/analogsea.svg\" width=150 align=\"center\" alt=\"sticker\"/>\n\n**Use our link https://m.do.co/c/1d5a471e5f54, you'll get 100 usd in credits to try DO!**\n\n`analogsea` is an R client for version 2 of the Digital Ocean API.  See `?droplet_functions` after loading analogsea. It allows you to programatically create and destroy droplets (remote computers), and install various R related tools:\n\n* R (done)\n* RStudio Server (done)\n* RStudio Shiny Server (done)\n* OpenCPU (not yet)\n* Use packrat to move a project to a droplet (not yet)\n\nIn addition, it allows you to use a readily available image with RStudio Server, Shiny Server and fully tidyverse from [DigitalOcean Marketplace](https://marketplace.digitalocean.com/apps/rstudio).\n\nDocs: https://pacha.dev/analogsea/. \n\n## Use cases\n\n- Andrew Heiss: [Create a cheap, disposable supercomputer with R, DigitalOcean, and future](https://www.andrewheiss.com/blog/2018/07/30/disposable-supercomputer-future/)\n\n## Install\n\nStable version from CRAN\n\n```r\ninstall.packages(\"analogsea\")\n```\n\nDevelopment version from GitHub\n\n```r\nremotes::install_github(\"pachadotdev/analogsea\")\n```\n\n```r\nlibrary(\"analogsea\")\n```\n\n## Create a DO account\n\nIf you don't already have ",
    "url": "https://github.com/pachadotdev/analogsea",
    "last_updated": "2025-03-22T11:23:26+00:00"
  },
  {
    "full_name": "STAT545-UBC/STAT545-UBC-original-website",
    "name": "STAT545-UBC-original-website",
    "description": "⚠️ Old repository for website of STAT 545 @ University of British Columbia",
    "language": "HTML",
    "topics": [],
    "readme": "<a alt = \"Project Status: Moved - The project has been moved to a new location, and the version at that location should be considered authoritative.\" href=\"http://www.repostatus.org/#moved\"><img src=\"http://www.repostatus.org/badges/latest/moved.svg\" height = 20 /></a>\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" height = 20 /></a>\n\nThe old repository for the website of STAT 545 @ University of British Columbia, a course in data wrangling, exploration, and analysis with R.\n\nThis repo is no longer in use and has been archived (as of mid-September 2019).\n\nMost STAT 545 content is now available from a new website at the usual domain:\n\n[stat545.com](https://stat545.com)\n\nThe source for the new website is here:\n\n<https://github.com/rstudio-education/stat545>\n\nSpecifics relating to ongoing runs of STAT 545 at UBC are here:\n\n<https://stat545.stat.ubc.ca>\n",
    "url": "https://github.com/STAT545-UBC/STAT545-UBC-original-website",
    "last_updated": "2025-04-22T15:15:26+00:00"
  },
  {
    "full_name": "hrbrmstr/ndjson",
    "name": "ndjson",
    "description": ":hotsprings: Wicked-Fast Streaming 'JSON' ('ndjson') Reader in R",
    "language": "C++",
    "topics": [
      "r",
      "ndjson",
      "rstats",
      "json",
      "r-cyber"
    ],
    "readme": "\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-100%25-lightgrey.svg)\n\n[![cran\nchecks](https://cranchecks.info/badges/worst/ndjson)](https://cranchecks.info/pkgs/ndjson)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/ndjson)](https://www.r-pkg.org/pkg/ndjson)\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-3.2.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# ndjson\n\nWicked-Fast Streaming ‘JSON’ (‘ndjson’) Reader\n\n## Description\n\nStreaming ‘JSON’ (‘ndjson’) has one ‘JSON’ record per-line and many\nmodern ‘ndjson’ files contain large numbers of records. These constructs\nmay not be columnar in nature, but it is often useful to read in these\nfiles and “flatten” the structure out to enable working with the data in\nan R ‘data.frame’-like context. Functions are provided that make it\npossible to read in plain ‘ndjson’ files or compressed (‘gz’) ‘ndjson’\nfiles and either validate the format of the records or create “flat”\n‘data.table’ structures from them.\n\nPretty much an Rcpp/C++17 wrapper for <https://github.com/nlohmann/json>\n\nThe goal is to create a completely “flat” `data.frame`-like structure\nfrom ndjson records in plain text ndjson files or gzip’d ndjson files.\n\n### Installation guidance for Linux/BSD-ish systems\n\nCRAN has binaries for Windows and macOS. To build this on UNIX-like\nsystems, you need at least g++4.9 or clang++. This is a forced\nrequirement by the ndjson library.\n\nThe least painful way to do this is to install gcc \\>= 4.9 (and you\nshould install `ccache` while you’re at it) and mmodfiy `~/.R/Makevars`\nthusly:\n\n    # Use whatever version of (g++ >=4.9 or clang++) that you downloaded\n    VER=-4.9\n    CC=ccache gcc$(VER)\n    CXX=ccache g++$(VER)\n    SHLIB_CXXLD=g++$(VER)\n    FC=ccache gfortran\n    F77=cc",
    "url": "https://github.com/hrbrmstr/ndjson",
    "last_updated": "2025-03-22T11:21:33+00:00"
  },
  {
    "full_name": "kdeldycke/awesome-falsehood",
    "name": "awesome-falsehood",
    "description": "😱 Falsehoods Programmers Believe in",
    "language": "",
    "topics": [
      "falsehoods",
      "programming",
      "awesome-list",
      "phone-number",
      "geography",
      "postal-addresses",
      "identity",
      "date",
      "time",
      "email",
      "network",
      "awesome",
      "resources",
      "myths",
      "edge-cases",
      "internationalization",
      "phone-numbers",
      "typography",
      "engineering"
    ],
    "readme": "<!--lint disable awesome-heading-->\n\n<p align=\"center\">\n  <a href=\"https://github.com/kdeldycke/awesome-falsehood/\">\n    <img src=\"https://github.com/kdeldycke/awesome-falsehood/raw/main/assets/awesome-falsehood-header.jpg\" alt=\"Awesome Falsehood header image\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/sponsors/kdeldycke\">\n    <strong>Yᴏᴜʀ Pʀᴏᴅᴜᴄᴛ ʜᴇʀᴇ!</strong>\n    <br/>\n    <sup>Add a link to your company or project here: purchase a GitHub sponsorship.</sup>\n  </a>\n</p>\n\n---\n\n<p align=\"center\">\n  <a href=\"https://github.com/kdeldycke/awesome-falsehood#readme.md\" hreflang=\"en\"><img src=\"https://img.shields.io/badge/lang-English-blue?style=flat-square\" lang=\"en\" alt=\"English\"></a>\n  <a href=\"https://github.com/kdeldycke/awesome-falsehood/blob/main/readme.zh.md\" hreflang=\"zh\"><img src=\"https://img.shields.io/badge/lang-中文-blue?style=flat-square\" lang=\"zh\" alt=\"中文\"></a>\n</p>\n\n<p align=\"center\">\n  <i>The logic of the world is prior to all truth and falsehood.</i><br>\n  — Ludwig Wittgenstein<sup id=\"intro-quote-ref\"><a href=\"#intro-quote-def\">[1]</a></sup>\n</p>\n\nA curated [![Awesome](https://awesome.re/badge-flat.svg)](https://github.com/sindresorhus/awesome) list of falsehoods programmers believe in. A *falsehood* is an ***idea* that you initially believed was true**, but in reality, it is **proven to be false**.\n\nE.g. of an *idea*: valid email address exactly has one `@` character. So, you will use this rule to implement your email-field validation logic. Right? Wrong! The *reality* is: emails can have multiple `@` chars. Therefore your implementation should allow this. The initial *idea* is a falsehood you believed in.\n\nThe *falsehood* articles listed below will have a comprehensive list of those false-beliefs that you should be aware of, to help you become a better programmer.\n\n## Contents\n\n<!-- mdformat-toc start --slug=github --no-anchors --maxlevel=6 --minlevel=2 -->\n\n- [Meta](#meta)\n- [Arts](#arts)\n- [Business](#business)\n- [Cryptocurrency](#",
    "url": "https://github.com/kdeldycke/awesome-falsehood",
    "last_updated": "2025-09-02T08:24:35+00:00"
  },
  {
    "full_name": "sckott/pubpatternsapi",
    "name": "pubpatternsapi",
    "description": "PubPatterns REST API (DEFUNCT)",
    "language": "Ruby",
    "topics": [
      "api",
      "sinatra",
      "publishing",
      "doi"
    ],
    "readme": "Pubpatterns API\n===============\n\nAPI is down. No longer maintained.\n\n<!-- this is what you want: <https://ftdoi.org>\n\nsee also: <https://github.com/sckott/pubpatterns>\n\nFiguring out URLs for full text version of articles is a huge PITA.\n\nThere's <https://doi.org> for resolving DOIs to their full URLs on the web, and\nthere's <https://oadoi.org/> for the same but to OA versions.\n\nHowever, there's no good tool for figuring out links to versions for text mining:\ntypically either pdf or xml.\n\nThere is Crossref's TDM (text and data mining) bit, where publishers voluntarily\ndeposit full text links into Crossref's metadata. However, very few publishers\ndo this; some that do don't deposit correct metadata; some deposit but don't update\nwhen they change their URL structure (and publishers change URL stucture __A LOT__).\n\nThis API builds on work at <https://github.com/sckott/pubpatterns> - which\nis simply rules for building URLs.\n\nThis API allows you to give a DOI and get back full text URLs for PDF/XML/etc. if\navailable.  And if they aren't available chip in and make it work.\n\n## Under the hood\n\n* API: Ruby/Sinatra\n* Caching: Redis - only on the `/api/doi/` route\n    * expires: 86400 seconds, or 24 hrs\n* Server: Caddy\n  * https\n* Authentication: none\n\n## setup\n\n* static files in <https://github.com/sckott/pubpatterns/tree/master/src> define patterns\n* we use these patterns to generate urls depending on the publisher, which can be determined from the DOI or given by the user\n* patterns are simply read from disk from the `src/` dir - simple, no database\n\n## API\n\n* root path `/` - redirects to `/heartbeat/`\n* `/heartbeat/` - list routes\n* `/members/` - list all members with known patterns\n* `/api/members/:member/` - list a single member\n* `/prefixes/:prefix/` - some publishers are inside of bigger publishers & don't have own Crossref member number, but do have their own prefix\n* `/doi/` - get full text links and other metadata\n* `/fetch/` - redirect to the full text url\n\n```r\n{\n",
    "url": "https://github.com/sckott/pubpatternsapi",
    "last_updated": "2023-01-28T12:18:28+00:00"
  },
  {
    "full_name": "unitedstates/congress-legislators",
    "name": "congress-legislators",
    "description": "Members of the United States Congress, 1789-Present, in YAML/JSON/CSV, as well as committees, presidents, and vice presidents.",
    "language": "Python",
    "topics": [],
    "readme": "congress-legislators\n====================\n\nMembers of the United States Congress (1789-Present), congressional committees (1973-Present), committee membership (current only), and presidents and vice presidents of the United States in YAML, JSON, and CSV format.\n\n[![Build Status](https://circleci.com/gh/unitedstates/congress-legislators.svg?style=shield)](https://circleci.com/gh/unitedstates/congress-legislators)\n\nOverview\n--------\n\nThis project provides the following data files:\n\nFile | Download | Description\n---- | -------- | -----------\n`legislators-current` | [YAML](https://unitedstates.github.io/congress-legislators/legislators-current.yaml) [JSON](https://unitedstates.github.io/congress-legislators/legislators-current.json) [CSV](https://unitedstates.github.io/congress-legislators/legislators-current.csv) | Currently serving Members of Congress.\n`legislators-historical` | [YAML](https://unitedstates.github.io/congress-legislators/legislators-historical.yaml) [JSON](https://unitedstates.github.io/congress-legislators/legislators-historical.json) [CSV](https://unitedstates.github.io/congress-legislators/legislators-historical.csv) | Historical Members of Congress (i.e. all Members of Congress except those in the current file).\n`legislators-social-media` | [YAML](https://unitedstates.github.io/congress-legislators/legislators-social-media.yaml) [JSON](https://unitedstates.github.io/congress-legislators/legislators-social-media.json) | Current social media accounts for Members of Congress. Official accounts only (no campaign or personal accounts).\n`committees-current` | [YAML](https://unitedstates.github.io/congress-legislators/committees-current.yaml) [JSON](https://unitedstates.github.io/congress-legislators/committees-current.json) | Current committees of the Congress, with subcommittees.\n`committee-membership-current` | [YAML](https://unitedstates.github.io/congress-legislators/committee-membership-current.yaml) [JSON](https://unitedstates.github.io/congress-le",
    "url": "https://github.com/unitedstates/congress-legislators",
    "last_updated": "2025-09-01T20:56:37+00:00"
  },
  {
    "full_name": "tflearn/tflearn",
    "name": "tflearn",
    "description": "Deep learning library featuring a higher-level API for TensorFlow.",
    "language": "Python",
    "topics": [
      "tflearn",
      "tensorflow",
      "neural-network",
      "deep-learning",
      "machine-learning",
      "data-science"
    ],
    "readme": "[![Build Status](https://travis-ci.org/tflearn/tflearn.svg?branch=master)](https://travis-ci.org/tflearn/tflearn)\n[![PyPI version](https://badge.fury.io/py/tflearn.svg)](https://badge.fury.io/py/tflearn)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Join the chat at https://gitter.im/einsteinsci/betterbeginnings](https://badges.gitter.im/tflearn/tflearn.svg)](https://gitter.im/tflearn/tflearn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n# TFLearn: Deep learning library featuring a higher-level API for TensorFlow.\n\nTFlearn is a modular and transparent deep learning library built on top of Tensorflow.  It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.\n\nTFLearn features include:\n\n- Easy-to-use and understand high-level API for implementing deep neural networks, with tutorial and examples.\n- Fast prototyping through highly modular built-in neural network layers, regularizers, optimizers, metrics...\n- Full transparency over Tensorflow. All functions are built over tensors and can be used independently of TFLearn.\n- Powerful helper functions to train any TensorFlow graph, with support of multiple inputs, outputs and optimizers.\n- Easy and beautiful graph visualization, with details about weights, gradients, activations and more...\n- Effortless device placement for using multiple CPU/GPU.\n\nThe high-level API currently supports most of recent deep learning models, such as Convolutions, LSTM, BiRNN, BatchNorm, PReLU, Residual networks, Generative networks... In the future, TFLearn is also intended to stay up-to-date with latest deep learning techniques.\n\nNote: Latest TFLearn (v0.5) is only compatible with TensorFlow v2.0 and over.\n\n## Overview\n```python\n# Classification\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 784])\nnet = tflearn.fully_conne",
    "url": "https://github.com/tflearn/tflearn",
    "last_updated": "2025-08-27T12:09:06+00:00"
  },
  {
    "full_name": "unitedstates/data-releases",
    "name": "data-releases",
    "description": "A listing of public data releases by federal agencies",
    "language": "",
    "topics": [],
    "readme": "# data-releases\nA listing of public data releases by federal agencies\n",
    "url": "https://github.com/unitedstates/data-releases",
    "last_updated": "2025-06-28T16:03:00+00:00"
  },
  {
    "full_name": "keyvanakbary/learning-notes",
    "name": "learning-notes",
    "description": "Notes on books I read, talks I watch, articles I study, and papers I love",
    "language": "SCSS",
    "topics": [
      "book-notes"
    ],
    "readme": "# Learning Notes\n\nTaking notes on books I read, talks I watch, articles I study, and papers I love – recalling them right afterward by creating short summaries – helps a lot in my learning process. Here you'll find some of those little pieces.\n\nIf you are looking for an easy way to consume these notes, please check out [keyvanakbary.github.io/learning-notes/](https://keyvanakbary.github.io/learning-notes/).\n\n### Books\n\n* [99 Bottles of OOP](books/99-bottles-of-oop.md) by **Sandi Metz** and **Katrina Owen**, 2016.\n* [An Elegant Puzzle: Systems of Engineering Management](books/an-elegant-puzzle.md) by **Will Larson**, 2019.\n* [A Guide to the Good Life: The Ancient Art of Stoic Joy](books/a-guide-to-the-good-life.md) by **William B. Irvine**, 2008.\n* [Designing Data-Intensive Applications](books/designing-data-intensive-applications.md) by **Martin Kleppmann**, 2015.\n* [Distributed Systems Observability](books/distributed-systems-observability.md) by **Cindy Sridharan**, 2018.\n* [Effective Java](books/effective-java.md) by **Joshua Bloch**, 2001.\n* [Elements of Programming Style](books/elements-of-programming-style.md) by **Brian W. Kernighan** and **P.J. Plauger**, 1988.\n* [El cerebro del niño explicado a los padres](books/el-cerebro-del-nino-explicado-a-los-padres.md) by **Álvaro de Bilbao**, 2015, \n* [Escaping the Build Trap](books/escaping-the-build-trap.md) by **Melissa Perri**, 2019.\n* [How to Win Friends and Influence People](books/how-to-win-friends-and-influence-people.md) by **Dale Carnegie**, 1936.\n* [Kanban: Successful Evolutionary Change for Your Technology Business](books/kanban.md) by **David J. Anderson**, 2010.\n* [Peopleware: Productive Projects and Teams](books/peopleware.md) by **Tom DeMarco** and **Timothy R. Lister**, 1999.\n* [Personal Kanban: Mapping Work, Navigating Life](books/personal-kanban.md) by **Jim Benson** and **Tonianne DeMaria Barry**, 2011.\n* [Radical Focus: Achieving Your Most Important Goals with Objectives and Key Results](books/ra",
    "url": "https://github.com/keyvanakbary/learning-notes",
    "last_updated": "2025-08-29T20:13:18+00:00"
  },
  {
    "full_name": "shap/shap",
    "name": "shap",
    "description": "A game theoretic approach to explain the output of any machine learning model.",
    "language": "Jupyter Notebook",
    "topics": [
      "interpretability",
      "machine-learning",
      "deep-learning",
      "gradient-boosting",
      "shap",
      "shapley",
      "explainability"
    ],
    "readme": "\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/shap/shap/master/docs/artwork/shap_header.svg\" width=\"800\" />\n</p>\n\n---\n[![PyPI](https://img.shields.io/pypi/v/shap)](https://pypi.org/project/shap/)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/shap)](https://anaconda.org/conda-forge/shap)\n![License](https://img.shields.io/github/license/shap/shap)\n![Tests](https://github.com/shap/shap/actions/workflows/run_tests.yml/badge.svg)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/shap/shap/master)\n[![Documentation Status](https://readthedocs.org/projects/shap/badge/?version=latest)](https://shap.readthedocs.io/en/latest/?badge=latest)\n![Downloads](https://img.shields.io/pypi/dm/shap)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/shap)](https://pypi.org/pypi/shap/)\n\n\n**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](#citations) for details and citations).\n\n<!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).-->\n\n\n\n## Install\n\nSHAP can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n\n<pre>\npip install shap\n<i>or</i>\nconda install -c conda-forge shap\n</pre>\n\n## Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n\nWhile SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see o",
    "url": "https://github.com/shap/shap",
    "last_updated": "2025-09-02T07:07:27+00:00"
  },
  {
    "full_name": "gaborcsardi/secret",
    "name": "secret",
    "description": ":closed_lock_with_key: Secure sharing of sensitive information in R packages",
    "language": "R",
    "topics": [],
    "readme": "\n\n\n<!-- badges: start -->\n[![R build status](https://github.com/gaborcsardi/secret/workflows/R-CMD-check/badge.svg)](https://github.com/gaborcsardi/secret/actions)\n[![](http://www.r-pkg.org/badges/version/secret)](http://www.r-pkg.org/pkg/secret)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/secret)](http://www.r-pkg.org/pkg/secret)\n[![Coverage Status](https://img.shields.io/codecov/c/github/gaborcsardi/secret/master.svg)](https://codecov.io/github/gaborcsardi/secret?branch=master)\n<!-- badges: end -->\n\nAllow sharing sensitive information, for example passwords, 'API' keys,\netc., in R packages, using public key cryptography.\n\n## Disclaimer\n\n1. Although the package authors did what they could to make sure that\n   the package is secure and cryptographically sound, they are not\n   security experts.\n\n2. Memory areas for secrets, user passwords, passpharases, private keys and\n   other sensitive information, are not securely cleaned after use!\n   Technically, the local R process and other processes on the same\n   computer, may have access to them. Never use this package on a public\n   computer or any system that you don't trust. (Actually, never typing in\n   any password on a public computer is good security practice, in general.)\n\n3. Use this package at your own risk!\n\n## Installation\n\nInstall the package from CRAN:\n\n```{r, eval = FALSE}\ninstall.packages(\"secret\")\n```\n\n## Usage\n\n\n\n### Load the package:\n\n\n```r\nlibrary(secret)\n```\n\n### Set up your keys:\n\nEnsure you know the location of your public and private keys. In Linux this is usually the folder `~/.ssh`, so on Windows you may want to choose the same folder.\n\nBy default, the package looks for your private key at \n\n1. `~/.ssh/id_rsa`\n1. `~/.ssh/id_rsa.pem`.\n\nYou can change this default by setting an environment variable `USER_KEY`:\n\n\n```r\n# This is optional - only do this if you want to change the default location\nSys.setenv(USER_KEY = \"path/to/private/key\")\n```\n\nTest that the package can read your",
    "url": "https://github.com/gaborcsardi/secret",
    "last_updated": "2025-03-22T08:13:51+00:00"
  },
  {
    "full_name": "dmcgarry/kaggle_cooking",
    "name": "kaggle_cooking",
    "description": "Quick solution to Kaggle's \"What's Cooking\" competition.",
    "language": "Python",
    "topics": [],
    "readme": "# What's Cooking?\nQuick solution to Kaggle's \"What's Cooking\" competition: https://www.kaggle.com/c/whats-cooking.  I entered this competition to build a very quick code sample and was pleasantly surprised to place in 4th place at the time of publishing the code to github.\n\n## To Run\nTo train the model simply execute the src/train.py script. For example:\n./train.py > ../data/results.txt",
    "url": "https://github.com/dmcgarry/kaggle_cooking",
    "last_updated": "2024-09-18T00:52:23+00:00"
  },
  {
    "full_name": "tensorflow/privacy",
    "name": "privacy",
    "description": "Library for training machine learning models with privacy for training data",
    "language": "Python",
    "topics": [
      "machine-learning",
      "privacy"
    ],
    "readme": "# TensorFlow Privacy\n\nThis repository contains the source code for TensorFlow Privacy, a Python\nlibrary that includes implementations of TensorFlow optimizers for training\nmachine learning models with differential privacy. The library comes with\ntutorials and analysis tools for computing the privacy guarantees provided.\n\nThe TensorFlow Privacy library is under continual development, always welcoming\ncontributions. In particular, we always welcome help towards resolving the\nissues currently open.\n\n## Latest Updates\n\n2024-02-14: As of version 0.9.0, the TensorFlow Privacy github repository will\nbe published as two separate PyPI packages. The first will inherit the name\ntensorflow-privacy and contain the parts related to training of DP models. The\nsecond, tensorflow-empirical-privacy, will contain the parts related to testing\nfor empirical privacy.\n\n2023-02-21: A new implementation of efficient per-example gradient clipping is\nnow available for\n[DP keras models](https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/keras_models)\nconsisting only of Dense and Embedding layers. The models use the fast gradient\ncalculation results of [this paper](https://arxiv.org/abs/1510.01799). The\nimplementation should allow for doing DP training without any meaningful memory\nor runtime overhead. It also removes the need for tuning the number of\nmicrobatches as it clips the gradient with respect to each example.\n\n## Setting up TensorFlow Privacy\n\n### Dependencies\n\nThis library uses [TensorFlow](https://www.tensorflow.org/) to define machine\nlearning models. Therefore, installing TensorFlow (>= 1.14) is a pre-requisite.\nYou can find instructions [here](https://www.tensorflow.org/install/). For\nbetter performance, it is also recommended to install TensorFlow with GPU\nsupport (detailed instructions on how to do this are available in the TensorFlow\ninstallation documentation).\n\n### Installing TensorFlow Privacy\n\nIf you only want to use TensorFlow Privacy as a library",
    "url": "https://github.com/tensorflow/privacy",
    "last_updated": "2025-08-31T22:12:46+00:00"
  },
  {
    "full_name": "notnews/tv_schedules",
    "name": "tv_schedules",
    "description": "70+ years of data on Network television. Attributes of shows, race & gender of cast members, directors, producers, presenters, etc.",
    "language": "Python",
    "topics": [
      "tv-schedules",
      "tv-programs",
      "gender",
      "race",
      "wikipedia",
      "female-producers"
    ],
    "readme": "## What and Who is on Network Television?\n\nUsing data on TV schedules and metadata on TV programs from Wikipedia, we explore the popularity of different genres of shows over time, and the race and gender composition of directors, producers, creators, presenters, cast members, music composers, etc. \n\nA small preview of what we find: \n\nThe percentage of shows that are on crime has gone from 0 to about 10% over the last 70 years. \n\n![percentage of crime shows over time](figs/crime_over_time.png)\n\nThe percentage of black cast members, presenters, directors, and producers has remained less than 5%, scraping 0 some years. Trends for gender are slightly more hopeful. The percentage of female cast members has risen from around 30% to about 40% in recent years. The percentage of female directors has risen at a steady clip from 0 to about 10%, while the most pleasing trend has been among female producers, which has risen from 0 to about 35%.\n\n![percentage of female producers over time](figs/female_producers_over_time.png)\n\nThe rest of the document is arranged as follows:\n\n1. [Scripts](#scripts) --- how did we get the data and run the analyses\n2. [Data](#data) --- final data used for the analyses\n3. [Results](#results) --- tables and figures\n\n-----------------------------\n\n### Scripts\n\nFor running the Python scripts at your end, see the [readme](scripts/README.md)\n\n1. **Get the data**\n    - [Get TV Schedules](scripts/tv_schedules.py)\n    - [Get Meta Data for TV Programs](scripts/tv_schedules_meta.py)\n\n2. **Parse and Augment the data**\n    - [Parse Names and Roles Data And Add Race and Gender via Python](scripts/names_role.py)\n    - [Impute Race and Gender using R](scripts/gender_race.R)\n\n3. **Analyze the data**\n    - [Race and Gender of Different Roles Over Time](scripts/race_gender_over_time.py)\n    - [Pretty Graphs](scripts/figs.R)\n\n### Data\n\nData are from [TV Schedules Data on Wikipedia](https://en.wikipedia.org/wiki/Category:United_States_television_schedules) and gray box",
    "url": "https://github.com/notnews/tv_schedules",
    "last_updated": "2025-04-16T22:08:21+00:00"
  },
  {
    "full_name": "dnsviz/dnsviz",
    "name": "dnsviz",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# ![DNSViz](doc/images/logo-220x100.png)\n\n\n## Table of Contents\n* [Installation](#installation)\n* [Usage](#usage)\n* [Pre-Deployment DNS Testing](#pre-deployment-dns-testing)\n* [Docker Container](#docker-container)\n\n\n## Description\n\nDNSViz is a tool suite for analysis and visualization of Domain Name System\n(DNS) behavior, including its security extensions (DNSSEC).  This tool suite\npowers the Web-based analysis available at https://dnsviz.net/\n\n\n## Installation\n\nDNSViz is available in package repositories for popular operating systems, such\nas Debian, Ubuntu, Fedora, Gentoo, and FreeBSD.  It is also available in the\nExtra Packages for Linux (EPEL) repository for Red Hat Enterprise Linux\n(RHEL) 8 and 9 and CentOS 8 and 9. (See\n[notes](#rhel-89-or-centos-stream-89-notes) for installation on RHEL and\nCentos.)  In each case, it can be installed using the package installation\ncommands typical for that operating system.  DNSViz can also be installed on\nMac OS X using Homebrew or MacPorts.\n\nThe remainer of this section covers other methods of installation, including a\nlist of [dependencies](#dependencies), installation to a\n[virtual environment](#installation-in-a-virtual-environment), and\n[notes for installing on RHEL 8 or 9 or CentOS Stream 8 or 9,](#rhel-89-or-centos-stream-89-notes)).\n\nInstructions for running in a Docker container are also available\n[later in this document](#docker-container).\n\n\n### Dependencies\n\n* Python (2.7, 3.5 - 3.12) - https://www.python.org/\n  (Note that python 2.7 support will be removed in a future release.)\n\n* dnspython (1.13.0 or later) - https://www.dnspython.org/\n\n* pygraphviz (1.3 or later) - https://pygraphviz.github.io/\n\n* cryptography (36.0.0 or later) - https://cryptography.io/\n\nNote that earlier versions of the software listed above might also work with\nDNSViz, but are not supported.  For example, versions of cryptography as early\nas 2.6 seem to work.  Also note that while DNSViz itself still works with\nPython 2.7, some versions of ",
    "url": "https://github.com/dnsviz/dnsviz",
    "last_updated": "2025-09-01T01:31:29+00:00"
  },
  {
    "full_name": "kibitzr/kibitzr",
    "name": "kibitzr",
    "description": "Personal Web Assistant",
    "language": "Python",
    "topics": [
      "ifttt",
      "python",
      "self-hosted",
      "web-monitor",
      "assistant"
    ],
    "readme": "===============================\nKibitzr\n===============================\n\n.. image:: https://badges.gitter.im/kibitzr/Lobby.svg\n   :alt: Join the chat at https://gitter.im/kibitzr/Lobby\n   :target: https://gitter.im/kibitzr/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n\n\n.. image:: https://img.shields.io/pypi/v/kibitzr.svg\n        :target: https://pypi.python.org/pypi/kibitzr\n\n.. image:: https://github.com/kibitzr/kibitzr/actions/workflows/test.yml/badge.svg\n        :target: https://github.com/kibitzr/kibitzr/actions/workflows/test.yml\n\n.. image:: https://readthedocs.org/projects/kibitzr/badge/?version=latest\n        :target: https://kibitzr.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n.. image:: https://www.codefactor.io/repository/github/kibitzr/kibitzr/badge\n        :target: https://www.codefactor.io/repository/github/kibitzr/kibitzr\n        :alt: CodeFactor\n\n.. image:: https://coveralls.io/repos/github/kibitzr/kibitzr/badge.svg?branch=master\n        :target: https://coveralls.io/github/kibitzr/kibitzr?branch=master\n        :alt: Coveralls\n\n.. image:: https://pepy.tech/badge/kibitzr\n        :target: https://pepy.tech/project/kibitzr\n        :alt: Downloads\n\n\nKibitzr is like a secret twin brother who does routine tasks and asks for nothing in return. Try and see it for yourself.\n\n* Problems? Ask in issues_, or gitter_\n* Documentation: https://kibitzr.readthedocs.io.\n* Free software: MIT license\n\n.. _gitter: https://gitter.im/kibitzr/Lobby\n.. _issues: https://github.com/kibitzr/kibitzr/issues/\n",
    "url": "https://github.com/kibitzr/kibitzr",
    "last_updated": "2025-08-30T08:45:00+00:00"
  },
  {
    "full_name": "leeper/make-example",
    "name": "make-example",
    "description": "An example of using make for a data analysis project",
    "language": "R",
    "topics": [
      "make",
      "reproducible-research",
      "data-analysis",
      "manuscript"
    ],
    "readme": "# Example of `make` for data analysis\n\nThis repository shows how to use `make` for a data analysis project.\n\nThe paper (PDF) is generated from a LaTeX source file, a table, and a figure.\n\n`make` uses a set of instructions in `makefile` to generate the table and figure using R code and a datafile, and then generate the PDF from the manuscript.\n\n`make` is clever because it deconstructs each part of the analysis so that only parts that have changed need to be rerun. If the data change, everything is rerun. If figure-generating code changes, only that code and the manuscript are rerun. If only the manuscript changes, only `pdflatex` is rerun. It's smart like that.\n\nBasically it works on a directed acyclic graph (DAG) model, represented by this network graph:\n\n\n\n\n```\n## Registered S3 methods overwritten by 'ggplot2':\n##   method         from \n##   [.quosures     rlang\n##   c.quosures     rlang\n##   print.quosures rlang\n```\n\n![plot of chunk dag](fig-dag-1.svg)\n\nThe R file `analysis.R` shows what is going on in `makefile` using possibly more familiar R syntax. The `README.Rmd` file contains the code to construct the above graph from an arbitrary makefile.\n\nZach Jones has [a good tutorial](http://zmjones.com/make/) about all of this.\n",
    "url": "https://github.com/leeper/make-example",
    "last_updated": "2025-01-14T16:17:59+00:00"
  },
  {
    "full_name": "bnosac/crfsuite",
    "name": "crfsuite",
    "description": "Labelling Sequential Data in Natural Language Processing with R - using CRFsuite",
    "language": "C",
    "topics": [
      "crf",
      "crfsuite",
      "conditional-random-fields",
      "nlp",
      "ner",
      "chunking",
      "data-science",
      "natural-language-processing",
      "r",
      "r-package",
      "intent-classification"
    ],
    "readme": "# Labelling Sequential Data in Natural Language Processing\n\nThis repository contains an R package which wraps the CRFsuite C/C++ library (https://github.com/chokkan/crfsuite), allowing the following:\n\n- Fit a **Conditional Random Field** model (1st-order linear-chain Markov) \n- Use the model to get predictions alongside the model on new data\n- The focus of the implementation is in the area of Natural Language Processing where this R package allows you to easily build and apply models for **named entity recognition, text chunking, part of speech tagging, intent recognition or classification** of any category you have in mind.\n\nFor users unfamiliar with Conditional Random Field (CRF) models, you can read this excellent tutorial https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf\n\n## Installation\n\n- The package is on CRAN, so just install it with the command `install.packages(\"crfsuite\")`\n- For installing the development version of this package: `devtools::install_github(\"bnosac/crfsuite\", build_vignettes = TRUE)`\n\n## Model building and tagging\n\nFor detailed documentation on how to build your own CRF tagger for doing NER / Chunking. Look to the vignette.\n\n```r\nlibrary(crfsuite)\nvignette(\"crfsuite-nlp\", package = \"crfsuite\")\n```\n\n#### Short example\n\n```r\nlibrary(crfsuite)\n\n## Get example training data + enrich with token and part of speech 2 words before/after each token\nx <- ner_download_modeldata(\"conll2002-nl\")\nx <- crf_cbind_attributes(x, \n                          terms = c(\"token\", \"pos\"), by = c(\"doc_id\", \"sentence_id\"), \n                          from = -2, to = 2, ngram_max = 3, sep = \"-\")\n\n## Split in train/test set\ncrf_train <- subset(x, data == \"ned.train\")\ncrf_test <- subset(x, data == \"testa\")\n\n## Build the crf model\nattributes <- grep(\"token|pos\", colnames(x), value=TRUE)\nmodel <- crf(y = crf_train$label, \n             x = crf_train[, attributes], \n             group = crf_train$doc_id, \n             method = \"lbfgs\", options = list(max_ite",
    "url": "https://github.com/bnosac/crfsuite",
    "last_updated": "2025-04-08T01:43:25+00:00"
  },
  {
    "full_name": "robbarry/nyc-property",
    "name": "nyc-property",
    "description": "A tool to help piece together useful New York City property sale data",
    "language": "R",
    "topics": [],
    "readme": "#\u0000 \u0000n\u0000y\u0000c\u0000-\u0000p\u0000r\u0000o\u0000p\u0000e\u0000r\u0000t\u0000y\u0000\r\u0000\nA tool to help piece together useful New York City property sale data.\n\n## The Problem\n\nNew York City's Department of Finance publishes fairly detailed, transaction-level data going back to 2004 on all real estate sales in the city. The problem is, this data is spread across at least 60 Excel spreadsheets and is missing critical pieces of information.\n\n## Solution\n\nThis set of scripts downloads all the Excel files, stiches them together using `readxl`, merges them with square footage data and writes them to a friendly CSV file. It also takes care of errors it encounters in the source data, such as weirdly encoded characters that crop up in the address field.\n\n![Sale data](https://raw.githubusercontent.com/robbarry/nyc-property/master/img/example-1.png) \n\n## Usage Instructions\n\n1. Run `download-sale-data.r` to download sale data from the Department of Finance from 2004-2015.\n2. Make sure you decompress `data/dof/property-info/bblsqft.csv.gz` (or follow the instructions below to re-create it).\n3. Run `merge-sales-sqft.r` to create `data/output/sale-data.csv` which will have sale data merged with gross square footage data.\n\n## Data\n\nThe sales data is posted on the New York City Department of Finance's website. For inexplicable reasons, the DOF splits its files into two categories: annualized sale data and a weird \"rolling\" sales file that captures every sale in the previous 12-month period. As a result, there's a bit of overlap that needs to be dealt with.\n\nThe sales data is also lacking some key information! For instance, among the 150,424 R4 (condo with elevator) sales between 2004 and the end of August 2015, just 15 had any square footage data. This makes any kind of practical analysis nearly impossible. To at least partially address the problem, I've parsed the city' valuation/assessment archives into a file here called `bblsqft.csv.gz` (bbl stands for borough/block/lot). There may be other data in the valuation/assessment archives bu",
    "url": "https://github.com/robbarry/nyc-property",
    "last_updated": "2016-12-06T23:31:19+00:00"
  },
  {
    "full_name": "salesforce/pytorch-qrnn",
    "name": "pytorch-qrnn",
    "description": "PyTorch implementation of the Quasi-Recurrent Neural Network - up to 16 times faster than NVIDIA's cuDNN LSTM",
    "language": "Python",
    "topics": [],
    "readme": "# Quasi-Recurrent Neural Network (QRNN) for PyTorch\n\nUpdated to support multi-GPU environments via `DataParallel` - see the the `multigpu_dataparallel.py` example.\n\nThis repository contains a PyTorch implementation of [Salesforce Research](https://einstein.ai/)'s [Quasi-Recurrent Neural Networks](https://arxiv.org/abs/1611.01576) paper.\n\nThe QRNN provides similar accuracy to the LSTM but can be betwen 2 and 17 times faster than the highly optimized NVIDIA cuDNN LSTM implementation depending on the use case.\n\nTo install, simply run:\n\n`pip install cupy pynvrtc git+https://github.com/salesforce/pytorch-qrnn`\n\nIf you use this code or our results in your research, please cite:\n\n```\n@article{bradbury2016quasi,\n  title={{Quasi-Recurrent Neural Networks}},\n  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},\n  journal={International Conference on Learning Representations (ICLR 2017)},\n  year={2017}\n}\n```\n\n## Software Requirements\n\nThis codebase requires Python 3, [PyTorch](http://pytorch.org/), [pynvrtc](https://github.com/NVIDIA/pynvrtc) (NVIDIA's Python Bindings to NVRTC), and [CuPy](https://cupy.chainer.org/).\nWhile the codebase contains a CPU implementation of the QRNN, the GPU QRNN implementation is used by default if possible.\nRequirements are provided in `requirements.txt`.\n\n## Example Usage\n\nWe've updated the previously released Salesforce Research [AWD-LSTM language modeling](https://github.com/salesforce/awd-lstm-lm) codebase to support use of the `AWD-QRNN`.\nWith the same number of parameters as the LSTM and less well tuned hyper parameters, the QRNN model trains over twice as quickly and achieves nearly equivalent state-of-the-art language modeling results.\nFor full details, refer to the [AWD-LSTM-LM repository](https://github.com/salesforce/awd-lstm-lm).\n\n## Usage\n\nThe QRNN API is meant to be drop-in compatible with the [LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM) for many standard use cases.\nA",
    "url": "https://github.com/salesforce/pytorch-qrnn",
    "last_updated": "2025-08-27T07:10:44+00:00"
  },
  {
    "full_name": "mimno/RMallet",
    "name": "RMallet",
    "description": "R package wrapping Mallet",
    "language": "R",
    "topics": [],
    "readme": "mallet\n======\n[![R](https://github.com/mimno/RMallet/actions/workflows/r.yml/badge.svg?branch=master)](https://github.com/mimno/RMallet/actions/workflows/r.yml)\n[![codecov](https://codecov.io/gh/mimno/RMallet/branch/master/graph/badge.svg)](https://codecov.io/gh/mimno/RMallet)\n[![Downloads](http://cranlogs.r-pkg.org/badges/grand-total/mallet)](https://cran.r-project.org/package=pxweb)\n[![Downloads](http://cranlogs.r-pkg.org/badges/mallet)](https://cran.r-project.org/package=pxweb)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mallet)](https://cran.r-project.org/package=mallet)\n\nmallet is an R package to interface with the Java machine learning tool Mallet. More information about Mallet can be found [here](http://mallet.cs.umass.edu/).\n\nFor installation and usage, see the [introduction vignette](https://htmlpreview.github.io/?https://raw.githubusercontent.com/mimno/RMallet/master/mallet/vignettes/mallet.html). \n\nYou are welcome to help out by:\n\n  * [submit suggestions and bug reports](https://github.com/mimno/RMallet/issues) (provide the output of `sessionInfo()` and `packageVersion(\"mallet\")`)\n  * [send a pull request](https://github.com/mimno/RMallet/)\n\n\n\n",
    "url": "https://github.com/mimno/RMallet",
    "last_updated": "2025-02-28T04:34:19+00:00"
  },
  {
    "full_name": "derekgreene/topic-ensemble",
    "name": "topic-ensemble",
    "description": "Ensemble topic modeling with matrix factorization",
    "language": "Python",
    "topics": [],
    "readme": "topic-ensemble\n===============\n\nThis repository contains a Python reference implementation of methods for ensemble topic modeling with Non-negative Matrix Factorization (NMF).\n\nDetails of these methods are described in the following paper [[Link]](https://doi.org/10.1016/j.eswa.2017.08.047): \n\n\tBelford, M., Mac Namee, B., & Greene, D. (2018). Stability of topic modeling via matrix factorization. \n\tExpert Systems with Applications, 91, 159-169.\n\nDraft pre-print: [https://arxiv.org/abs/1702.07186](https://arxiv.org/abs/1702.07186)\n\nAdditional pre-processed datasets for use with this package [can be downloaded here](http://mlg.ucd.ie/files/datasets/stability-topic-datasets.zip) (179MB).\n\n### Dependencies\nTested with Python 3.5, and requiring the following packages, which are available via PIP:\n\n* Required: [numpy >= 1.8.0](http://www.numpy.org/)\n* Required: [scikit-learn >= 0.14](http://scikit-learn.org/stable/)\n* Required for utility tools: [prettytable >= 0.7.2](https://code.google.com/p/prettytable/)\n\n### Basic Usage\n#### Step 1. \nBefore applying topic modeling to a corpus, the first step is to pre-process the corpus and store it in a suitable format. The script 'parse-directory.py' can be used to parse a directory of plain text documents. Here, we parse all .txt files in the directory or sub-directories of 'data/sample-text'. \n\n\tpython parse-directory.py data/sample-text/ -o sample --tfidf --norm\n\nThe output will be sample.pkl, stored as a Joblib binary file. The identifiers of the documents in the dataset correspond to the original text input filenames.\n\nAlternatively, if all of your documents are stored in a text file, with one document per line, the script 'parse-file.py' can be used:\n\n\tpython parse-file.py data/sample.txt -o sample --tfidf --norm\n\n#### Step 2. \nNext, we generate a set of \"base\" topic models, which represent the members of the ensemble. We provide two different ways to do this.\n\nFirstly, we can generate a specified number of base topic models us",
    "url": "https://github.com/derekgreene/topic-ensemble",
    "last_updated": "2025-07-18T18:12:58+00:00"
  },
  {
    "full_name": "glynnbird/countriesgeojson",
    "name": "countriesgeojson",
    "description": "Countries of the world as GeoJSON",
    "language": "",
    "topics": [
      "geojson",
      "geography",
      "geospatial",
      "data",
      "json"
    ],
    "readme": "# Countries GEOJSON\n\nList of countries as GeoJSON. Slightly modified version of files found here (https://github.com/johan/world.geo.json)\n\n",
    "url": "https://github.com/glynnbird/countriesgeojson",
    "last_updated": "2025-08-07T08:22:10+00:00"
  },
  {
    "full_name": "gorilla-llm/gorilla-cli",
    "name": "gorilla-cli",
    "description": "LLMs for your CLI",
    "language": "Python",
    "topics": [
      "aws",
      "bash",
      "cli",
      "k8s",
      "llm",
      "productivity",
      "terminal",
      "gcp",
      "iterm2",
      "kubernetes-cli",
      "wsl",
      "zsh",
      "shell",
      "kubernetes"
    ],
    "readme": "# Gorilla CLI\n\n<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png\" width=20% height=20%>\n\nGorilla CLI powers your command-line interactions with a user-centric tool. Simply state your objective, and Gorilla CLI will generate potential commands for execution. Gorilla today supports ~1500 APIs, including Kubernetes, AWS, GCP,  Azure, GitHub, Conda, Curl, Sed, and many more. No more recalling intricate CLI arguments! 🦍\n\nDeveloped by UC Berkeley as a research prototype, Gorilla-CLI prioritizes user control and confidentiality:\n - Commands are executed solely with your explicit approval.\n - While we utilize queries and error logs (stderr) for model enhancement, we NEVER collect output data (stdout).\n\n![gorilla-cli](https://github.com/gorilla-llm/gorilla-cli/assets/30296397/f448c04b-e2a1-4560-b040-37f9840c356d)\n\n## Getting Started\n\nYou can readily install Gorilla CLI via pip. \n\n```bash\npip install gorilla-cli\n```\n\n## Usage\n\nActivate Gorilla CLI with `gorilla` followed by your task in plain English.\n\nFor instance, to generate a file with 100 random characters, type:\n\n```bash\n$ gorilla generate 100 random characters into a file called test.txt\n```\n\nor if you prefer, you can use quotes to avoid issues with string parsing:\n\n```bash\n$ gorilla \"generate 100 random characters into a file called test.txt\"\n```\n\nGorilla CLI will then generate candidate commands. Use the arrow keys to navigate through the options, then press enter to execute the chosen command. \n\n```bash\n🦍  Welcome to Gorilla. Use arrows to select\n » cat /dev/urandom | env LC_ALL=C tr -dc 'a-zA-Z0-9' | head -c 100 > test.txt \n   echo $(head /dev/urandom | LC_CTYPE=C tr -dc 'a-zA-Z0-9' | dd bs=100 count=1) > test.txt\n   dd if=/dev/urandom bs=1 count=100 of=test.txt\n```\n\nSome more examples\n\n```bash\n$ gorilla list all my GCP instances\n» gcloud compute instances list --format=\"table(name,zone,status)\"\n  gcloud compute instances list --format table\n  gcloud compute instances list --format",
    "url": "https://github.com/gorilla-llm/gorilla-cli",
    "last_updated": "2025-09-01T07:32:38+00:00"
  },
  {
    "full_name": "openaddresses/machine",
    "name": "machine",
    "description": "Scripts for running OpenAddresses on a complete data set and publishing the results.",
    "language": "Python",
    "topics": [],
    "readme": "<h1 align=\"center\">OA Machine [Deprecated]</h1>\n\n\nLegacy scripts for running OpenAddresses on a complete data set and publishing\nthe results. Uses [OpenAddresses](https://github.com/openaddresses/openaddresses)\ndata sources to work.\n\nStatus\n------\n\nREADME: This code powers the legacy https://results.openaddresses.io/ site. Current development efforts should be focused on the batch service\nat https://batch.openaddresses.io/data. This service is powered by a fork of the machine code found https://github.com/openaddresses/batch-machine. Changes made to this code **will not** affect the newer, batch service.\n\nThis code is being used to process the complete OA dataset on a weekly and on-demand\nbasis, with output visible at [results.openaddresses.io](https://results.openaddresses.io).\n\n[![Build Status](https://travis-ci.org/openaddresses/machine.svg?branch=master)](https://travis-ci.org/openaddresses/machine/branches)\n\nUsage\n-----\nNOTE: machine's use for CI has been deprecated and it is currenty serving a static version of sources processed up to August, 2020.\nAll CI and weekly source functionality is disabled. For current data please see the [batch service](https://batch.openaddresses.io)\n\nMachine is an integral of the OpenAddresses project. When new sources\n[are added in Github](https://github.com/openaddresses/openaddresses#contributing-addresses),\nthey are automatically processed and status output is displayed in Github’s\npull request UI. A successful set of checks looks like this:\n\n![Github status display](docs/github-status.png)\n\nMore information about Machine’s output can be seen by following the Details link\n[to a job page like this](http://results.openaddresses.io/jobs/b044ce9c-caa0-46fb-a7e4-842beeae3f52).\n\nMachine also runs its own weekly batch process to generate the downloadable\nfiles, maps, and other resources available via [results.openaddresses.io](https://results.openaddresses.io).\n\n![OpenAddresses worldwide coverage map](https://data.openaddresses.io/ren",
    "url": "https://github.com/openaddresses/machine",
    "last_updated": "2024-09-19T01:25:20+00:00"
  },
  {
    "full_name": "justingrimmer/TAD",
    "name": "TAD",
    "description": "This is the public repository for my quarter long text as data course",
    "language": "TeX",
    "topics": [],
    "readme": "# TAD\n",
    "url": "https://github.com/justingrimmer/TAD",
    "last_updated": "2024-12-04T08:28:08+00:00"
  },
  {
    "full_name": "soodoku/interpretation_gap",
    "name": "interpretation_gap",
    "description": "Replication Materials For \"A Gap in Our Understanding? Reconsidering the Evidence for Partisan Knowledge Gaps\"",
    "language": "Stata",
    "topics": [
      "knowledge",
      "partisanship",
      "partisan-gap"
    ],
    "readme": "\n## 🔗 Adjacent Repositories\n\n- [soodoku/partisan-gaps](https://github.com/soodoku/partisan-gaps) — How do (biased) guessing encouraging features and guessing agnostic coding techniques affect the partisan gap?\n- [soodoku/adult](https://github.com/soodoku/adult) — Consumption of Pornography Online Using Passively Observed Browsing Data\n- [soodoku/nireland](https://github.com/soodoku/nireland) — Replication Data And Scripts for How Can You Think That?: Deliberation and the Learning of Opposing Arguments\n- [soodoku/nga](https://github.com/soodoku/nga) — Scraping National Governor's Association (from 2012)\n- [soodoku/kirkuk](https://github.com/soodoku/kirkuk) — Data and scripts behind the paper \"What Future For Kirkuk?\"\n",
    "url": "https://github.com/soodoku/interpretation_gap",
    "last_updated": "2025-05-04T05:03:27+00:00"
  },
  {
    "full_name": "derekeder/csv-to-html-table",
    "name": "csv-to-html-table",
    "description": ":arrow_down_small: Display any CSV (comma separated values) file as a searchable, filterable, pretty HTML table",
    "language": "CSS",
    "topics": [],
    "readme": "# CSV to HTML Table\n\nDisplay any CSV file as a searchable, filterable, pretty [HTML table](https://www.scaler.com/topics/html/tables-in-html/). Done in 100% JavaScript.\n\nCheck out the working demo: https://csv-to-html-table.netlify.app/\n\n![Screen Shot 2021-03-26 at 4 53 39 PM](https://user-images.githubusercontent.com/919583/112696463-d7ddd000-8e53-11eb-8f0e-084794450943.png)\n\n## Usage\n\n#### 1. Clone this repository (in the command line)\n\n``` bash\ngit clone git@github.com:derekeder/csv-to-html-table.git\ncd csv-to-html-table\n```\n\n#### 2. Add your CSV file to the `data/` folder\n\n#### 3. In `index.html` set your options in the `CsvToHtmlTable.init()` function\n\n``` html\n<script>\n  CsvToHtmlTable.init({\n    csv_path: 'data/Health Clinics in Chicago.csv', \n    element: 'table-container', \n    allow_download: true,\n    csv_options: {separator: ',', delimiter: '\"'},\n    datatables_options: {\"paging\": false}\n  });\n</script>\n```\n\n##### Available options\n\n* `csv_path` Path to your CSV file.\n* `element` The HTML element to render your table to. Defaults to `table-container`\n* `allow_download` if true, shows a link to download the CSV file. Defaults to `false`\n* `csv_options` jQuery CSV configuration. Use this if you want to use a custom `delimiter` or `separator` in your input file. See [their documentation](https://code.google.com/p/jquery-csv/wiki/API#$.csv.toArrays%28%29).\n* `datatables_options` DataTables configuration. See [their documentation](http://datatables.net/reference/option/).\n* `custom_formatting` **New!** A list of column indexes and custom functions to format your data (see below)\n\n\n##### Custom formatting\nIf you want to do custom formatting for one or more column, you can pass in an array of arrays containing the index of the column and a custom function for formatting it. You can pass in multiple formatters and they will be executed in order.\n\nThe custom functions must take in one parameter (the value in the cell) and return a HTML string:\n\nExample:\n\n``` html",
    "url": "https://github.com/derekeder/csv-to-html-table",
    "last_updated": "2025-09-01T06:32:12+00:00"
  },
  {
    "full_name": "lmullen/geochecker",
    "name": "geochecker",
    "description": "An R package to check the accuracy of geocoded coordinates using a Shiny gadget",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\ngeochecker\n----------\n\nAn R package that lets you check the accuracy of geocoded coordinates using a [Shiny gadget](http://shiny.rstudio.com/articles/gadgets.html).\n\n**Author:** [Lincoln Mullen](http://lincolnmullen.com)<br> **License:** [MIT](http://opensource.org/licenses/MIT)\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/geochecker)](http://cran.r-project.org/package=geochecker) [![Travis-CI Build Status](https://travis-ci.org/lmullen/geochecker.svg?branch=master)](https://travis-ci.org/lmullen/geochecker)\n\n### Installation\n\nTo get the development version from GitHub, use [devtools](https://github.com/hadley/devtools).\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"lmullen/geochecker\")\n```\n\n### Use\n\nPass the `geocheck()` function a data frame containing latitudes and longitudes that have been geocoded. A Shiny gadget will open that will step you through each point and allow you to mark it as correct or to change the coordinates by clicking on the map. Be sure to save the data frame which is returned.\n\n``` r\nlibrary(geochecker)\ncorrected <- geocheck(geocoded_cities, zoom = 8)\n```\n\n![Screenshot of geochecker](inst/geochecker-screenshot.png)\n\n### Contributing\n\nPlease note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.\n",
    "url": "https://github.com/lmullen/geochecker",
    "last_updated": "2023-08-02T00:49:36+00:00"
  },
  {
    "full_name": "pricecc/CPS.Tail.Adjustment",
    "name": "CPS.Tail.Adjustment",
    "description": "Adjustment to the CPS using the WTID",
    "language": "R",
    "topics": [],
    "readme": "# CPS.Tail.Adjustment\nAdjustment to the CPS using the WTID\n\nMore description to come\n",
    "url": "https://github.com/pricecc/CPS.Tail.Adjustment",
    "last_updated": "2015-06-09T11:09:36+00:00"
  },
  {
    "full_name": "soodoku/nga",
    "name": "nga",
    "description": "Scraping National Governor's Association (from 2012)",
    "language": "R",
    "topics": [],
    "readme": "## Data from the National Governor's Association\n\n**Note:** Scraped in 2012\n\n### Scripts and Data\n\n* [Scrape and parse the site](scripts/01_scrape_gov.py). It produces [data/governor.csv](data/governor.csv)\n* [Clean and Augment the data](scripts/02_clean_gov.R). It adds [state codes](data/state_num.csv).\n\n### License\n\nScripts are released under the [MIT License](https://opensource.org/licenses/MIT).\n",
    "url": "https://github.com/soodoku/nga",
    "last_updated": "2025-03-22T11:13:32+00:00"
  },
  {
    "full_name": "nutterb/pixiedust",
    "name": "pixiedust",
    "description": "Tables So Beautifully Fine-Tuned You Will Believe It's Magic.",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# pixiedust\n\nAfter tidying up your analyses with the `broom` package, go ahead and\ngrab the `pixiedust`. Customize your table output and write it to\nmarkdown, HTML, LaTeX, or even just the console. `pixiedust` makes it\neasy to customize the appearance of your tables in all of these formats\nby adding any number of “sprinkles”, much in the same way you can add\nlayers to a `ggplot`.\n\n``` r\nfit <- lm(mpg ~ qsec + factor(am) + wt + factor(gear), data = mtcars)\nlibrary(pixiedust)\ndust(fit) %>% \n  sprinkle(col = 2:4, round = 3) %>% \n  sprinkle(col = 5, fn = quote(pvalString(value))) %>% \n  sprinkle_colnames(term = \"Term\", \n                    estimate = \"Estimate\", \n                     std.error = \"SE\",\n                     statistic = \"T-statistic\", \n                     p.value = \"P-value\") %>%\n  sprinkle_print_method(\"console\")\n#>            Term Estimate    SE T-statistic P-value\n#> 1   (Intercept)    9.365 8.373       1.118    0.27\n#> 2          qsec    1.245 0.383       3.252   0.003\n#> 3   factor(am)1    3.151 1.941       1.624    0.12\n#> 4            wt   -3.926 0.743      -5.286 < 0.001\n#> 5 factor(gear)4   -0.268 1.655      -0.162    0.87\n#> 6 factor(gear)5    -0.27 2.063      -0.131     0.9\n```\n\n### Customizing with Sprinkles\n\nTables can be customized by row, column, or even by a single cell by\nadding sprinkles to the `dust` object. The table below shows the\ncurrently planned and implemented sprinkles. In the “implemented”\ncolumn, an ‘x’ indicates a customization that has been implemented,\nwhile a blank cell suggests that the customization is planned but has\nnot yet been implemented. In the remaining columns, an ‘x’ indicates\nthat the sprinkle is already implemented for the output format; an ‘o’\nindicates that implementation is planned but not yet completed; and a\nblank cell indicates that the sprinkle will not be implemented (usually\nbecause the output format doesn’t support the option).\n\n",
    "url": "https://github.com/nutterb/pixiedust",
    "last_updated": "2025-06-18T08:03:39+00:00"
  },
  {
    "full_name": "AlexLamson/DataWrangler",
    "name": "DataWrangler",
    "description": "Make quick and dirty data mining made easier in Sublime Text",
    "language": "Python",
    "topics": [
      "sublime-text-plugin",
      "data-munging",
      "data-cleaning",
      "data-cleansing",
      "data-wrangling",
      "text-manipulation"
    ],
    "readme": "# DataWrangler - Sublime Plugin\nClean and analyze text data more easily\n\n![\"Screenshot\"](https://raw.github.com/AlexLamson/DataWrangler/master/screenshots/demo.gif \"Screenshot\")\n*I used [this](https://www.screentogif.com/) to capture the screen*\n\n\n## Motivation\nWhile cleaning data from a large survey, I wanted to ask questions about the data so I could clean it better.\nFor example, many people would type what city they were from, but some would misspell it. By looking at the most common responses, I could quickly find the mispelled words and fix them.\n\n## Installation\nInstall via Package Control by searching for `DataWrangler`.\n\nIf you don't have Package Control, you can install it [here](https://packagecontrol.io/installation).\n\n\n## Usage\nTo run a command, open the command palette (ctrl+shift+P on Windows) and type the name of the function you want.\n\nIf a command expects data to be formatted in rows and columns of data, tabs are the preferred separator (it also tries to be clever if you are using another separator, ex. commas). This was chosen because it makes it very compatible with copy-pasting from Google Sheets and Excel spreadsheets.\n\n\n## Commands\nAll commands are non-destructive, and the results will appear in a new tab.\n\n- Line/word frequency\n- Flatten a list of lists\n- Vertically align all columns\n- Delete each column that contains a cursor\n",
    "url": "https://github.com/AlexLamson/DataWrangler",
    "last_updated": "2025-08-11T06:24:23+00:00"
  },
  {
    "full_name": "markledwich2/Recfluence",
    "name": "Recfluence",
    "description": "An analysis of YouTube's political influence through recommendations. ",
    "language": "HTML",
    "topics": [
      "youtube",
      "visualization"
    ],
    "readme": "# Recfluence\n\n[Recfluence](https://www.recfluence.net) is an analysis of YouTube's political influence through recommendations. This is important, because the algorithm that determines suggested videos has become an influential but underappreciated part of politics. A [Pew survey](https://www.journalism.org/2017/09/07/news-use-across-social-media-platforms-2017/) found that 18% of US adults consume news through YouTube, and — according to YouTube Chief Product Officer [Neal Mohan](https://www.cnet.com/news/youtube-ces-2018-neal-mohan/) — 70% of the watch time is from YouTube’s suggested videos.\n\n\n## Data Collection Process \nA list of political channels is manually created, categorized and improved over time. An automated process run daily. It collects:\n- **Channels** information & stats (e.g. subscribers, country) \n  - from the YouTube API.\n- **Videos** information & stats (e.g. views, likes, title), captions\n  - by scraping the website from a US IP address.\n  - since 1st Jan 2018\n  - updated daily stats for video's younger than 120 days\n- **Recommendations** (i.e. from video A to video B)\n  -  by scraping the website from a US IP address\n  -  for video's younger than 120 days old (max 10)\n  -  if no young video's, take the latest video's recommendations.\n\nThis is then analysed to provide statistics and data which is freely available for researches and media. \n\n### Channels Included\nChannels were included if the met the following criteria\n- 10k+ subscribers. If subscriber data is missing/lower, still include if video's average above 10k views\n- Significant focus (more than 30% of content) on US political or cultural news/commentary. I considered cultural commentary was anything from the [ISideWith social issues list](https://www.isidewith.com/en-us/polls)\n\nThere is no definitive list of YouTube channels, so a variety of techniques were used. \n- Reviewed the following lists:\n    - https://www.adfontesmedia.com/\n    - https://blog.feedspot.com/political_youtube_channe",
    "url": "https://github.com/markledwich2/Recfluence",
    "last_updated": "2025-08-18T13:50:04+00:00"
  },
  {
    "full_name": "khanhnamle1994/statistical-learning",
    "name": "statistical-learning",
    "description": "Lecture Slides and R Sessions for Trevor Hastie and Rob Tibshinari's \"Statistical Learning\" Stanford course",
    "language": "R",
    "topics": [
      "r",
      "statistical-learning",
      "data-science",
      "data-mining",
      "regression"
    ],
    "readme": "## ABOUT THIS COURSE\n\n![CoverPhoto](stat-learning.jpeg)\n\nThis is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).\n\nThis is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data analysis. Computing is done in R. There are lectures devoted to R, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.\n\nThe lectures cover all the material in [An Introduction to Statistical Learning, with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) by James, Witten, Hastie and Tibshirani (Springer, 2013). The pdf for this book is available for free on the book website.\n",
    "url": "https://github.com/khanhnamle1994/statistical-learning",
    "last_updated": "2025-08-28T10:40:06+00:00"
  },
  {
    "full_name": "otakumesi/llm-write",
    "name": "llm-write",
    "description": "CLI for Automated article writing",
    "language": "Python",
    "topics": [],
    "readme": "# LLM-Write (powered by GPT-3) :writing_hand:\n\n__LLM make writing an article brazing fast :fire:.__\n\nPyPi:\n- https://pypi.org/project/llmwrite/\n\n![LLM-Write Demo](https://github.com/otakumesi/llm-write/blob/main/demo.gif?raw=true \"デモ\")\n\n## :telescope: Overview\n- This app is built with GPT-3.\n- LLM-Write is the CLI tool for Intractive Automated Article Writing.  \n- You can create articles, just answer the questions.   \n- Since the language is specified in the LLM prompt and LLM is allowed to generate the text, it could theoretically be used in a variety of languages.  \n    - However, the supported languages in the shell messages are only English and Japanese.\n\n\n## :runner: Installation\n```sh\npip install llmwrite\n```\n\n## :computer: Usage\n\n```sh\n# Set Environment \"OPENAI_API_KEY\"\nexport OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxx\n\n# Run\nllmwrite\n? Language? <Select Language>\n# ...(Lots of questions come in to the interactive.)\n```\n",
    "url": "https://github.com/otakumesi/llm-write",
    "last_updated": "2025-01-15T15:29:13+00:00"
  },
  {
    "full_name": "JosepER/Book_How_to_weight_a_survey",
    "name": "Book_How_to_weight_a_survey",
    "description": "This is an introductory guide to survey weighting. It provides a step-by-step walkthrough of the main procedures and explains the statistical principles behind them. The guide includes R code to implement all stages of survey weighting and reproduces the weighting procedures of the 7th European Social Survey in the UK. ",
    "language": "HTML",
    "topics": [
      "survey",
      "weigths",
      "weighting",
      "data-analysis",
      "social-sciences",
      "sampling",
      "calibration"
    ],
    "readme": "",
    "url": "https://github.com/JosepER/Book_How_to_weight_a_survey",
    "last_updated": "2025-06-04T15:54:32+00:00"
  },
  {
    "full_name": "gensx-inc/gensx",
    "name": "gensx",
    "description": "The TypeScript framework for agents & workflows with react-like components. Lightning fast dev loop. Easy to learn. Easy to extend.",
    "language": "TypeScript",
    "topics": [
      "generative-ai",
      "llm-framework",
      "llms",
      "workflow"
    ],
    "readme": "# GenSX ⚡️\n\n[![npm version](https://badge.fury.io/js/gensx.svg)](https://badge.fury.io/js/gensx)\n[![Website](https://img.shields.io/badge/Visit-gensx.com-orange)](https://gensx.com)\n[![Discord](https://img.shields.io/badge/Join-Discord-5865F2)](https://discord.gg/wRmwfz5tCy)\n[![X](https://img.shields.io/badge/Follow-X-black)](https://x.com/gensx_inc)\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n[GenSX](https://gensx.com/) is a simple TypeScript framework for building complex LLM applications. It's a workflow engine designed for building agents, chatbots, and long-running workflows.\n\n## Why GenSX?\n\n- 🎯 **Pure Functions**: Components are pure TypeScript functions that are easily testable, reusable, and sharable\n- 🌴 **Natural Composition**: Building workflows is as simple as composing functions together\n- 🔒 **Type-safe**: Full TypeScript support with no DSLs or special syntax - just standard language features\n- 🚀 **Built for Scale**: Start simple and evolve to complex patterns like agents and reflection without changing your programming model\n- 📊 **Automatic Tracing**: Real-time tracing of all component inputs/outputs, tool calls, and LLM calls making debugging and observability easy\n- ☁️ **One-Click Deployment**: Deploy workflows as REST APIs with a single command, optimized for long-running LLM workloads up to 60 minutes\n- 💾 **Built-in Storage**: Zero-config blob storage, SQL databases, and vector search for building stateful agents and workflows\n\nCheck out the [documentation](https://gensx.com/docs) to learn more about building LLM applications with GenSX.\n\n## Building a workflow\n\nMost LLM frameworks are graph oriented--you express your workflow with nodes, edges, and a global state object. GenSX takes a different approach--you compose your workflow with components, and GenSX handles the execution for you.\n\nComponents in GenSX look a lot like functions. You create them by passing in a function an",
    "url": "https://github.com/gensx-inc/gensx",
    "last_updated": "2025-08-15T22:29:54+00:00"
  },
  {
    "full_name": "Meidozuki/VBAO",
    "name": "VBAO",
    "description": "A library aiming for building an MVVM (or MVFM) project",
    "language": "Python",
    "topics": [],
    "readme": "# VBAO\n\n> A lite library aiming for building an MVVM (or MVFM) project.\n\n<center>\n\n[中文ReadMe](README-cn.md)\n\n</center>\n\n## Install and start\n\n### Python\n\n```shell\nconda activate YOURENV\npip install vbao-mvvm\n```\n\n```python\nimport vbao\n```\n\n## What is MVVM? (briefly)\n\n> There are three core components in the MVVM pattern: the model, the view, and the view model. Each serves a distinct purpose. The diagram below shows the relationships between the three components.\n> ![image mentioned above](img/mvvm-pattern.png)\n> In addition to understanding the responsibilities of each component, it's also important to understand how they interact. At a high level, the view \"knows about\" the view model, and the view model \"knows about\" the model, but the model is unaware of the view model, and the view model is unaware of the view. Therefore, the view model isolates the view from the model, and allows the model to evolve independently of the view. -- [MVVM from Microsoft](https://learn.microsoft.com/en-us/dotnet/architecture/maui/mvvm)\n\n## How does it work?\n\nWe implement the data-binding, command-binding and property-change-notification to form an MVVM system.\n\n![](img/binding.dot.jpg)\n\ndata-binding & command-binding\n\n![](img/changing.dot.jpg)\n\nproperty-change-notification\n\nWhen we make it, the interaction among them is as follows.\n\n![](img/action.dot.jpg)\n\n## Filetree\n\n├── example  \n│   ├── app  \n│   ├── model  \n│   ├── view  \n│   ├── viewmodel  \n│   └── window  \n├── extern  \n│   └── pybind11  \n├── Lib_VBao  \n│   ├── python  \n│   │   ├── vbao  \n│   └── VBAO  \n├── LICENSE  \n└── README.md\n\n\n## Special Thanks\n\n[Zhejiang-University-GKC/SJDXQcourseware: The courseware of SJDXQ (github.com)](https://github.com/Zhejiang-University-GKC/SJDXQcourseware)\n",
    "url": "https://github.com/Meidozuki/VBAO",
    "last_updated": "2025-01-15T15:28:36+00:00"
  },
  {
    "full_name": "r-hub/pkgsearch",
    "name": "pkgsearch",
    "description": "Search R packages on CRAN",
    "language": "R",
    "topics": [
      "cran",
      "package",
      "search-engine",
      "r",
      "ranking",
      "rstats",
      "r-package"
    ],
    "readme": "# Search and Query CRAN R Packages\n\n<!-- badges: start -->\n[![lifecycle](https://lifecycle.r-lib.org/articles/figures/lifecycle-stable.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable-1)\n[![R-CMD-check](https://github.com/r-hub/pkgsearch/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-hub/pkgsearch/actions/workflows/R-CMD-check.yaml)\n[![CRAN status](https://www.r-pkg.org/badges/version/pkgsearch)](https://cran.r-project.org/package=pkgsearch)\n[![CRAN RStudio mirror downloads](https://cranlogs.r-pkg.org/badges/pkgsearch)](https://www.r-pkg.org/pkg/pkgsearch)\n[![Codecov test coverage](https://codecov.io/gh/r-hub/pkgsearch/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-hub/pkgsearch?branch=main)\n<!-- badges: end -->\n\n`pkgsearch` uses R-hub web services that munge CRAN metadata and let you\naccess it through several lenses.\n\n-   [Installation](#installation)\n-   [Usage](#usage)\n    -   [Search relevant packages](#search-relevant-packages)\n    -   [Do it all *clicking*](#do-it-all-clicking)\n    -   [Get package metadata](#get-package-metadata)\n    -   [Discover packages](#discover-packages)\n    -   [Keep up with CRAN](#keep-up-with-cran)\n-   [Search features](#search-features)\n    -   [More details](#more-details)\n    -   [Pagination](#pagination)\n    -   [Stemming](#stemming)\n    -   [Ranking](#ranking)\n    -   [Preferring Phrases](#preferring-phrases)\n    -   [British vs American English](#british-vs-american-english)\n    -   [Ascii Folding](#ascii-folding)\n-   [Configuration](#configuration)\n    -   [Options](#options)\n-   [More info](#more-info)\n-   [License](#license)\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n## Installation\n\nInstall the latest pkgsearch release from CRAN:\n\n``` r\ninstall.packages(\"pkgsearch\")\n```\n\nThe development version is on GitHub:\n\n``` r\npak::pak(\"r-hub/pkgsearch\")\n```\n\n## Usage\n\n### Search relevant packages\n\nDo you need to find packages solving a particular problem, e.g.\n",
    "url": "https://github.com/r-hub/pkgsearch",
    "last_updated": "2025-08-25T16:29:04+00:00"
  },
  {
    "full_name": "visgl/luma.gl",
    "name": "luma.gl",
    "description": "High-performance Toolkit for WebGL-based Data Visualization",
    "language": "TypeScript",
    "topics": [
      "webgl",
      "data-visualization",
      "uber"
    ],
    "readme": "<p align=\"right\">\n  <a href=\"https://npmjs.org/package/@luma.gl/core\">\n    <img src=\"https://img.shields.io/npm/v/@luma.gl/core.svg?style=flat-square\" alt=\"version\" />\n  </a>\n  <a href=\"https://github.com/visgl/luma.gl/actions?query=workflow%3Atest+branch%3Amaster\">\n    <img src=\"https://github.com/visgl/luma.gl/workflows/test/badge.svg?branch=master\" alt=\"build\" />\n  </a>\n  <a href=\"https://npmjs.org/package/@luma.gl.core\">\n    <img src=\"https://img.shields.io/npm/dm/@luma.gl/core.svg?style=flat-square\" alt=\"downloads\" />\n  </a>\n  <a href='https://coveralls.io/github/visgl/luma.gl?branch=master'>\n    <img src='https://img.shields.io/coveralls/visgl/luma.gl.svg?style=flat-square' alt='Coverage Status' />\n  </a>\n</p>\n\n<h1 align=\"center\">luma.gl | <a href=\"https://luma.gl\">Docs</a></h1>\n\n<h5 align=\"center\">luma.gl: High-performance Toolkit for WebGL-based Data Visualization</h5>\n\n## Overview\n\nluma.gl is a GPU toolkit for the Web focused primarily on data visualization use cases. luma.gl aims to provide support for GPU programmers that need to work directly with shaders and want a low abstraction API that remains conceptually close to the WebGPU and WebGL APIs. Some features of luma.gl include:\n\n- A robust GLSL shader module system.\n- A convenient object-oriented API wrapping most WebGL objects\n- Higher-level engine constructs to manage the animation loop, drawing and resource management\n\nUnlike other common WebGL APIs, the developer can choose to use the parts of luma.gl that support their use case and leave the others behind.\n\nWhile generic enough to be used for general 3D rendering, luma.gl's mandate is primarily to support GPU needs of data visualization frameworks in the vis.gl suite, such as:\n\n- [kepler.gl](https://github.com/keplergl/kepler.gl) a powerful open source geospatial analysis tool for large-scale data sets\n- [deck.gl](https://github.com/visgl/deck.gl) a WebGL-powered framework for visual exploratory data analysis of large data sets\n- [streetscape.gl](",
    "url": "https://github.com/visgl/luma.gl",
    "last_updated": "2025-09-02T05:42:32+00:00"
  },
  {
    "full_name": "leeper/slopegraph",
    "name": "slopegraph",
    "description": "Edward Tufte-Inspired Slopegraphs",
    "language": "R",
    "topics": [
      "slopegraph",
      "cran",
      "r",
      "tufte",
      "ggplot2",
      "dataviz",
      "data-visualization"
    ],
    "readme": "# Tufte-Inspired Slopegraphs in R\n\nThis repository holds some working code for creating \"slopegraphs\" in R.\n\n*This is very much a work in progress. Once it's more stable, I will release the package to CRAN.*\n\nPull requests welcome. Please report any issues on the [issues page](https://github.com/leeper/slopegraph/issues).\n\nThe package currently includes one mainfunction, `slopegraph()`, which produces a slopegraph from an observation-by-period data frame. Everything is more or less drawn automatically, but is highly customizable in terms of line and text colors, font sizes and styles, axes, titles, and plotting behind and in front of the slopegraph lines. An underlying function, `segmentize()` produces the data structure used for the actual plotting. And a new function, `ggslopegraph()` does the same as `slopegraph()` but using ggplot2 graphics.\n\n## Examples\n\nThe current output of the `slopegraph()` function (for the examples included in documentation) are shown below. \n\nTufte's most famous slopegraph example is probably the [\"cancer survival graph,\"](http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0003nk) depicting 5, 10, 15, and 20 year survival rates for various cancers. The first example mimics this result but draws it to the correct scale (unlike Tufte's original):\n\n\n```r\nlibrary(\"slopegraph\")\ndata(cancer)\nslopegraph(cancer, col.lines = 'gray', col.lab = \"black\", \n           xlim = c(-.5,5.5), cex.lab = 0.5, cex.num = 0.5,\n           xlabels = c('5 Year','10 Year','15 Year','20 Year'))\n```\n\n![Cancer Survival](https://rawgithub.com/leeper/slopegraph/master/inst/examples/cancer-survival-1.svg)\n\n\nThe second example, also from Tufte, shows changes in gross domestic product for a small set of countries over two points in time:\n\n\n```r\ndata(gdp)\nslopegraph(gdp, col.lines = 'gray', col.lab = \"black\", xlabels = c('1970','1979'),  \n           main = 'Current Receipts of Goverment as a Percentage of Gross Domestic Product')\n```\n\n![GDP](https://rawgithub.com/lee",
    "url": "https://github.com/leeper/slopegraph",
    "last_updated": "2025-08-29T00:35:14+00:00"
  },
  {
    "full_name": "hrbrmstr/domain-collections",
    "name": "domain-collections",
    "description": "Collections of categorized internet domain lists for internet & cybersecurity researchers",
    "language": "Shell",
    "topics": [
      "dataset",
      "internet-domains",
      "research"
    ],
    "readme": "# Categorized Internet Domain Lists\n\nListings, GH submodules and/or directories+processing scripts that hold domain lists (official or crowdsourced). \n\n## Purpose\n\nFirst up: this is not a \"useless IoC\" domain list dumping ground. There are plenty of other internet sharing platforms for tossing around lists of \"bad domains\".\n\nThe intent of this resource is to have a curated set of domain lists internet/cybersecurity researchers can use when analyzing attributes associated with internet resources. That means the domain list must have some \"meaning\". Two of the lists used to bootstrap the repository are moderately good examples of this. One is a curated & maintained list of _university_ domains; the other is a U.S. GSA maintained set of _U.S. government_ domains.\n\nThe intent, however, is not to \"duplicate the wheel\".\n\nContributors are welcome and encouraged to maintain curated domain lists here.\n\nHowever:\n\n- If a an approved domain list is being maintained on GitHub, a _submodule_ should be added for it, not the _contents_ of the repository.\n- If an approved resource exists on the internet and is small enough to include, then the code to retrieve that list should be present and be able to run from a daily job.\n- If an approved resource exists on the internet and is not small enough to include, retrieval code should still be proivded.\n\nThe goal is also not to define a \"unified schema\" for domains. That, as a \"nice to have\", might come later or may be built by others around this project. There are free and useful domain lists currently all over the place or able to be created pretty quickly and adding friction to the curation & inclusion by forcing a schema would likely provide enough inertia to make this effort fizzle out. For now, just enough metadata is being required to enable researchers to work with the included resources.\n\n## Contributing\n\nMake sure you can [sign your commits](https://help.github.com/articles/signing-commits-using-gpg/) before doing anything.\n\n###",
    "url": "https://github.com/hrbrmstr/domain-collections",
    "last_updated": "2025-03-22T11:08:53+00:00"
  },
  {
    "full_name": "jeroen/agent",
    "name": "agent",
    "description": "Store sensitive data such as API tokens",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "sensitive-data",
      "agent",
      "key-value-store",
      "security",
      "security-tools",
      "data-security",
      "r-package"
    ],
    "readme": "# agent\n\n##### *Encrypted Key-Value Store for Sensitive Data*\n\n[![Project Status: Concept – Minimal or no implementation has been done yet, or the repository is only intended to be a limited example, demo, or proof-of-concept.](http://www.repostatus.org/badges/latest/concept.svg)](http://www.repostatus.org/#concept)\n[![Build Status](https://travis-ci.org/ropensci/agent.svg?branch=master)](https://travis-ci.org/ropensci/agent)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/ropensci/agent?branch=master&svg=true)](https://ci.appveyor.com/project/jeroen/agent)\n[![Coverage Status](https://codecov.io/github/ropensci/agent/coverage.svg?branch=master)](https://codecov.io/github/ropensci/agent?branch=master)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/agent)](http://cran.r-project.org/package=agent)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/agent)](http://cran.r-project.org/web/packages/agent/index.html)\n\n> Cross platform solution for securely storing sensitive data. This \n  can either be used directly by the user or by other packages for storing \n  e.g. web tokens or other secrets. The degree of security is detemined by \n  the strength of the password as set by the user.\n\n## Hello World\n\nThe agent works like a simple key-value store. The value can be any object that can be serialized by R such as a token or data frame. This API can either be called by the user or by other packages.\n\n```r\nlibrary(agent)\nagent_set(\"my_secret_token\", \"ABCXYZ\")\nagent_get(\"my_secret_token\")\n## \"ABCXYZ\"\nagent_has(\"my_secret_token\")\n## TRUE\nagent_del(\"my_secret_token\")\n```\n\nIt is up to the user to protect the keystore with a password:\n\n\n```r\nupdate_password()\n```\n\nThe user will automatically be prompted for a password when the keystore needs to be unlocked, for example when a package needs to retrieve a secured token.\n",
    "url": "https://github.com/jeroen/agent",
    "last_updated": "2025-03-03T19:51:15+00:00"
  },
  {
    "full_name": "Yorko/mlcourse.ai",
    "name": "mlcourse.ai",
    "description": "Open Machine Learning Course",
    "language": "Python",
    "topics": [
      "machine-learning",
      "data-analysis",
      "data-science",
      "pandas",
      "algorithms",
      "numpy",
      "scipy",
      "matplotlib",
      "seaborn",
      "plotly",
      "scikit-learn",
      "kaggle-inclass",
      "vowpal-wabbit",
      "python",
      "ipynb",
      "docker",
      "math"
    ],
    "readme": "<div align=\"center\">\n\n![ODS stickers](https://github.com/Yorko/mlcourse.ai/blob/main/img/ods_stickers.jpg)\n\n**[mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course**\n\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-green)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n[![Slack](https://img.shields.io/badge/slack-ods.ai-orange)](https://opendatascience.slack.com/archives/C91N8TL83/p1567408586359500)\n[![Donate](https://img.shields.io/badge/support-patreon-red)](https://www.patreon.com/ods_mlcourse)\n[![Donate](https://img.shields.io/badge/support-ko--fi-red)](https://ko-fi.com/mlcourse_ai)\n\n</div>\n\n[mlcourse.ai](https://mlcourse.ai) is an open Machine Learning course by [OpenDataScience (ods.ai)](https://ods.ai/), led by [Yury Kashnitsky (yorko)](https://yorko.github.io/). Having both a Ph.D. degree in applied math and a Kaggle Competitions Master tier, Yury aimed at designing an ML course with a perfect balance between theory and practice. Thus, the course meets you with math formulae in lectures, and a lot of practice in a form of assignments and  Kaggle Inclass competitions. Currently, the course is in a **self-paced mode**. Here we guide you through the self-paced [mlcourse.ai](https://mlcourse.ai).\n\n__Bonus:__\nAdditionally, you can purchase a Bonus Assignments pack with the best non-demo versions of [mlcourse.ai](https://mlcourse.ai/) assignments. Select the [\"Bonus Assignments\" tier](https://www.patreon.com/ods_mlcourse). Refer to the details of the deal on the main page [mlcourse.ai](https://mlcourse.ai/).\n\nMirrors (:uk:-only): [mlcourse.ai](https://mlcourse.ai) (main site), [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse) (same notebooks as Kaggle Notebooks)\n\n### Self-paced passing\nYou are guided through 10 weeks of [mlcourse.ai](https://mlcourse.ai). For each week, from Pandas to Gradient Boosting, instructions are given on which articles to read, lectures to watch, what assignments to accom",
    "url": "https://github.com/Yorko/mlcourse.ai",
    "last_updated": "2025-09-01T17:53:05+00:00"
  },
  {
    "full_name": "pablobarbera/scholarnetwork",
    "name": "scholarnetwork",
    "description": "Extract and Visualize Google Scholar Collaboration Networks",
    "language": "R",
    "topics": [],
    "readme": "# Extract and Visualize Google Scholar Collaboration Networks\n\n**scholarnetwork** is an R package that provides functions to extracts publication information from Google Scholar, create network of collaborators based on co-authored projects, and visualize these networks using a force-directed layout algorithm.\n\n## Installation ##\n\nAn initial release of this package is available in this repository (eventually maybe also on CRAN), and can be installed directly using Hadley Wickham's [devtools](http://cran.r-project.org/web/packages/devtools/index.html) package:\n\n```\nif(!require(\"devtools\")) install.packages(\"devtools\")\nlibrary(\"devtools\")\ninstall_github(\"pablobarbera/scholarnetwork\")\n```\n\n## Examples ##\n\nFor now, the package consists of two functions, `extractNetwork` and `plotNetwork`, which correspond to the data collection and data visualization steps.\n\n`extractNetwork` wraps the `get_publications` function from the scholar package, which extracts the list of publications on a Google Scholar profile, cleans it, and then parses the results into a format that is more suitable for network analysis: \n\n- a data frame of __weighted edges__, where each edge is a collaboration in a publication, and the weight is one divided by number of co-authors; and \n\n- a data frame with __node-level information__, which includes the group resulting from running a walktrap community detection algorithm. \n\n```r\nd <- extractNetwork(id=\"jGLKJUoAAAAJ\", n=500)\nstr(d)\n```\n```\nList of 2\n $ nodes:'data.frame':\t40 obs. of  3 variables:\n  ..$ label : chr [1:40] \"A Boydstun\" \"A Valeriani\" \"A Venetz\" \"C Roca Cuberes\" ...\n  ..$ degree: num [1:40] 0.75 1.69 0.75 0.667 1.69 ...\n  ..$ group : num [1:40] 11 7 10 8 7 1 3 4 5 13 ...\n $ edges:'data.frame':\t106 obs. of  3 variables:\n  ..$ node1 : chr [1:106] \"P Barberá\" \"C Vaccari\" \"K Ackermann\" \"P Barberá\" ...\n  ..$ node2 : chr [1:106] \"A Boydstun\" \"A Valeriani\" \"A Venetz\" \"A Venetz\" ...\n  ..$ weight: num [1:106] 0.25 0.31 0.25 0.25 0.25 ...\n```\n\n`plotNetw",
    "url": "https://github.com/pablobarbera/scholarnetwork",
    "last_updated": "2025-06-21T14:57:01+00:00"
  },
  {
    "full_name": "seankross/usbudget",
    "name": "usbudget",
    "description": ":us: R data package featuring United States federal budget data",
    "language": "R",
    "topics": [],
    "readme": "# usbudget\n\nThis R data package contains United States federal budget data for years since \n1962 and projections until 2020. Adapted from\n[The White House Federal Budget Data Release](https://github.com/WhiteHouse/2016-budget-data).\n\n## To install:\n```\nlibrary(devtools)\ninstall_github(\"seankross/usbudget\")\n```",
    "url": "https://github.com/seankross/usbudget",
    "last_updated": "2018-11-12T19:51:33+00:00"
  },
  {
    "full_name": "libscie/retractcheck",
    "name": "retractcheck",
    "description": "Check DOIs in a paper for retractions.",
    "language": "R",
    "topics": [],
    "readme": "# `retractcheck` <img src=\"tools/images/retractcheck_hex.png\" align=\"right\" height=\"64\" />\n\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-2-orange.svg?style=flat-square)](#contributors) \n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n[![Build Status](https://travis-ci.org/libscie/retractcheck.svg?branch=master)](https://travis-ci.org/libscie/retractcheck)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/retractcheck)](https://cran.r-project.org/package=retractcheck)\n[![CRAN\\_Downloads\\_Total](http://cranlogs.r-pkg.org/badges/grand-total/retractcheck?color=brightgreen)](https://cran.r-project.org/package=retractcheck)\n\nCheck DOIs in a paper for being retracted by running your manuscript through `retractcheck` in R or using the [Shiny app](https://frederikaust.shinyapps.io/retractcheck_shinyapp/) in your browser. This R package builds on the API of [Open retractions](http://openretractions.com) ([also an open source project](https://github.com/fathomlabs/open-retractions)). \n\nThe original inspiration for this package can be found [in a tweet by @PaolaPalma](https://twitter.com/PaoloAPalma/status/976545221268815872) and the origin of the name in [this tweet by @MarkHoffarth](https://twitter.com/MarkHoffarth/status/976548240672870405) :fire: \n\nRetracted papers :books: keep getting cited (see for example [here](https://osf.io/cszpy)) and this package aims to help reduce this effect. [Zotero provides](https://www.zotero.org/blog/retracted-item-notifications/) retraction checking *into* your reference manager, so be sure to check that out too :+1:\n\n## Installation\n\n```R\ninstall.packages('retractcheck')\n```\n\nPlease note that the dependency on `textreadr` may fail if you don't have the necessary software for that. If installation fails, try `install.packages('textreadr')` and see what errors it gives.\n\n## Limitations\n\nIf the `retractcheck` package does not return any hits for retracted",
    "url": "https://github.com/libscie/retractcheck",
    "last_updated": "2025-06-15T13:29:38+00:00"
  },
  {
    "full_name": "nytimes/gunsales",
    "name": "gunsales",
    "description": "Statistical analysis of monthly background checks of gun purchases",
    "language": "R",
    "topics": [],
    "readme": "# Analysis of NICS gun purchase background checks\n\n[![Build Status](https://travis-ci.org/NYTimes/gunsales.svg)](https://travis-ci.org/NYTimes/gunsales) [![License](http://img.shields.io/badge/license-Apache%20%28=%202%29-brightgreen.svg?style=flat)](http://www.apache.org/licenses/LICENSE-2.0) [![CRAN](http://www.r-pkg.org/badges/version/gunsales)](https://cran.r-project.org/package=gunsales) [![Downloads](http://cranlogs.r-pkg.org/badges/gunsales?color=brightgreen)](http://www.r-pkg.org/pkg/gunsales)\n\nStatistical analysis of monthly background checks of gun purchases for the New York Times story [What Drives Gun Sales: Terrorism,\nObama and Calls for Restrictions](http://www.nytimes.com/interactive/2015/12/10/us/gun-sales-terrorism-obama-restrictions.html?).\n\n### Pre-requisites\n\nThis package depends on the R package [seasonal](https://cran.r-project.org/package=seasonal/vignettes/seas.pdf) for the seasonal adjustments, which itself uses a program called [X-13ARIMA-SEATS](https://www.census.gov/srd/www/x13as/).\nWindows, OS X and Linux binaries for this program are installed by the R package [x13binary](https://github.com/x13org/x13binary).\n\nBoth packages are now on CRAN and can be installed along with the other dependencies via\n\n```r\n> install.packages(\"gunsales\")\n```\n\n\n### Running the main function\n\nOnce the package has loaded, run this in an R shell:\n\n```r\n> library(gunsales)\n> df <- analysis()\n```\n\nto create a single dataframe containing the results. The dataframe can be\nvisualized via\n\n```r\n> plot_gunsales(df)    \n> ggplot_gunsales(df)\n```\n\nto create, respectively, plots via R base or\n[ggplot2](https://github.com/hadley/ggplot2). Options to save the output in the `out/` folder exist. The resulting [ggplot2](https://github.com/hadley/ggplot2) charts are shown below:\n\n![Total Estimated Gun Sales](https://raw.githubusercontent.com/NYTimes/gunsales/master/out/ggplot_total.png)\n\n![Total Estimated Gun Sales, Seasonally Adjusted](https://raw.githubusercontent.com/NYTime",
    "url": "https://github.com/nytimes/gunsales",
    "last_updated": "2025-03-03T05:17:53+00:00"
  },
  {
    "full_name": "christophergandrud/networkD3",
    "name": "networkD3",
    "description": "D3 JavaScript Network Graphs from R",
    "language": "R",
    "topics": [
      "networks",
      "d3js",
      "rstats"
    ],
    "readme": "D3 JavaScript Network Graphs from R\n===================================\n\nDevelopment version: 0.4.9000 [![CRAN\nVersion](http://www.r-pkg.org/badges/version/networkD3)](https://CRAN.R-project.org/package=networkD3) [![R-CMD-check](https://github.com/christophergandrud/networkD3/workflows/R-CMD-check/badge.svg)](https://github.com/christophergandrud/networkD3/actions) ![CRAN Monthly\nDownloads](http://cranlogs.r-pkg.org/badges/last-month/networkD3) ![CRAN\nTotal Downloads](http://cranlogs.r-pkg.org/badges/grand-total/networkD3)\n\nThis README includes information on set up and a number of basic\nexamples. For more information see the package's [main\npage](http://christophergandrud.github.io/networkD3/).\n\nUsage\n-----\n\nHere's an example of `simpleNetwork`:\n\n    library(networkD3)\n\n    # Create fake data\n    src <- c(\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\")\n    target <- c(\"B\", \"C\", \"D\", \"J\", \"E\", \"F\", \"G\", \"H\", \"I\")\n    networkData <- data.frame(src, target)\n\n    # Plot\n    simpleNetwork(networkData)\n\nHere's `forceNetwork`:\n\n    # Load data\n    data(MisLinks)\n    data(MisNodes)\n\n    # Plot\n    forceNetwork(Links = MisLinks, Nodes = MisNodes, Source = \"source\",\n                 Target = \"target\", Value = \"value\", NodeID = \"name\",\n                 Group = \"group\", opacity = 0.7,\n                 colourScale = JS(\"d3.scaleOrdinal(d3.schemeCategory20);\"))\n\nHere's `sankeyNetwork` using a downloaded JSON data file:\n\n    # Recreate Bostock Sankey diagram: https://bost.ocks.org/mike/sankey/\n    # Load energy projection data\n    URL <- paste0(\"https://cdn.rawgit.com/christophergandrud/networkD3/\",\n                  \"master/JSONdata/energy.json\")\n    Energy <- jsonlite::fromJSON(URL)\n\n    # Plot\n    sankeyNetwork(Links = Energy$links, Nodes = Energy$nodes, Source = \"source\",\n                 Target = \"target\", Value = \"value\", NodeID = \"name\",\n                 units = \"TWh\", fontSize = 12, nodeWidth = 30)\n\n### Interacting with igraph\n\nYou can use [igraph](https://igraph.org/r/) to",
    "url": "https://github.com/christophergandrud/networkD3",
    "last_updated": "2025-07-24T14:33:20+00:00"
  },
  {
    "full_name": "paultopia/scrapecontracts",
    "name": "scrapecontracts",
    "description": "WIP scraper to build dataset of contracts from web",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "switching to python, holding clojure stuff around in case I want it later\n",
    "url": "https://github.com/paultopia/scrapecontracts",
    "last_updated": "2022-12-28T18:34:44+00:00"
  },
  {
    "full_name": "hiroalchem/napari-SAM4IS",
    "name": "napari-SAM4IS",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# napari-SAM4IS\n\n[![License Apache Software License 2.0](https://img.shields.io/pypi/l/napari-SAM4IS.svg?color=green)](https://github.com/hiroalchem/napari-SAM4IS/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/napari-SAM4IS.svg?color=green)](https://pypi.org/project/napari-SAM4IS)\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-SAM4IS.svg?color=green)](https://python.org)\n[![tests](https://github.com/hiroalchem/napari-SAM4IS/workflows/tests/badge.svg)](https://github.com/hiroalchem/napari-SAM4IS/actions)\n[![codecov](https://codecov.io/gh/hiroalchem/napari-SAM4IS/branch/main/graph/badge.svg)](https://codecov.io/gh/hiroalchem/napari-SAM4IS)\n[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-SAM4IS)](https://napari-hub.org/plugins/napari-SAM4IS)\n\n\n### napari plugin for instance and semantic segmentation annotation using Segment Anything Model (SAM)\n\nThis is a plugin for [napari](https://napari.org/), a multi-dimensional image viewer for Python, that allows for instance and semantic segmentation annotation. This plugin provides an easy-to-use interface for annotating images with the option to output annotations as COCO format.\n\n----------------------------------\n\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template.\n\n<!--\nDon't miss the full getting started guide to set up your new package:\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\n\nand review the napari docs for plugin developers:\nhttps://napari.org/stable/plugins/index.html\n-->\n\n## Installation\n\n### Step 1: Install napari-SAM4IS\n\nYou can install `napari-SAM4IS` via [pip]:\n\n```bash\npip install napari-SAM4IS\n```\n\nOr via conda:\n\n```bash\nconda install -c conda-forge napari-SAM4IS\n```\n\nTo install the latest development version:\n\n```bash\npip install git+https://github.com/hiroalchem/napari-SAM4IS.git\n```\n\n### Step 2: Install Segment Anything Model (Optional - for local mod",
    "url": "https://github.com/hiroalchem/napari-SAM4IS",
    "last_updated": "2025-08-18T04:29:15+00:00"
  },
  {
    "full_name": "pkremp/polls",
    "name": "polls",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "This is a [Stan](http://mc-stan.org) implementation of Drew Linzer's dynamic Bayesian election forecasting model, with some tweaks to incorporate national poll data, pollster house effects, correlated priors on state-by-state election results and comovement of public opinion across states. \n\nThe model is presented briefly at the end of [`report.html`](http://pkremp.github.io/report.html).\n\n`runmodel.R` downloads poll data from the HuffPost Pollster API, processes the data, and runs the Stan model in `state and national polls.stan`.\n\n`report.Rmd` is a Rmarkdown document used to automatically generate the graphs/tables/maps in [`report.html`](http://pkremp.github.io/report.html) and relies on `graphs.R`.\n\n# Reproducing the analysis\n\n1. Clone this repository.\n2. Install the required R packages (listed below)\n3. Remove or comment out the lines in `graphs.R`, `runmodel.R` and `report.Rmd`\nthat read `setwd(\"~/GitHub/polls\")`.\n4. Optionally, modify line 2 of `runmodel.R` to use a number of cores that is\nless than all available (e.g., `options(mc.cores = parallel::detectCores()\n- 1)` to leave one core free for multitasking\n5. In an R session, run `source(\"runmodel.R\")`\n\n## Required R packages\n\n```r\ncurl\ndplyr\nDT\nggplot2\nggrepel\nknitr\nlubridate\nmapproj\nmaps\nmvtnorm\npurrr\nreshape2\nrmarkdown\nrstan\nshinystan\nstringr\ntidyr\n```\n",
    "url": "https://github.com/pkremp/polls",
    "last_updated": "2024-10-02T03:46:24+00:00"
  },
  {
    "full_name": "mediacloud/web-tools",
    "name": "web-tools",
    "description": "The shared repository for Media Cloud web apps (Explorer, Source Manager, Topic Mapper)",
    "language": "JavaScript",
    "topics": [
      "media-analytics",
      "reactjs",
      "flask",
      "python",
      "media-cloud",
      "civic-tech"
    ],
    "readme": "Media Cloud Web Tools\n=====================\n\nThis is a shared repository for all the front-facing [Media Cloud](https://mediacloud.org) web tools.\nThis includes:\n * [Explorer](https://explorer.mediacloud.org)\n * [Source Manager](https://sources.mediacloud.org)\n * [Topic Mapper](https://topics.mediacloud.org)\n * [Tools](https://tools.mediacloud.org)\n\n**Check out the `doc` folder for more documentation.**\n\nDev Installation\n----------------\n\nGit:\n * `git submodule update --init --recursive`\n\nPython:\n * Follow the instructions in `doc/python-versions.md` to setup Python the way we do\n * Once you've got Python setup, install the requirements by running `pip install -r requirements.txt`\n\nNode and yarn:\n * On Windows, make sure to create an environment variable: `set NODE_ENV=dev`\n * make sure your node installation is up-to-date (we work with v8.2.1 right now)\n * `yarn install` to install all the package dependencies (as specified in the `package.json`)\n\nMongoDB:\n[Install MongoDb](https://docs.mongodb.com/manual/administration/install-community/):\n* `brew tap mongodb/brew`\n* `brew install mongodb-community@4.2`\nIf you get a connection refused error, make sure you've started the server by running `brew services start mongodb-community@4.2`\n\nRedis:\n[Install Redis](http://redis.io/)  We develop on OS X and install via the [HomeBrew package manager](http://brew.sh): `brew install redis`\n\nMemCache:\nOn OSX, make sure to run `brew install libmemcached` otherwise you'll get an error about pylibmc failing to install (http://brew.sh)\n\nMulti-platform setup:\nComing soon\n\nConfiguration\n-------------\n\nCopy `config/app.config.template` to `config/app.config` and fill in the required info there.\n\nRunning the Apps\n----------------\n\nYou need to open two terminal windows and run one thing in each (so the hot-reloading can work):\n * `redis-server` to start redis (if it's not running already)\n * `yarn run topics-dev` or `yarn run sources-dev`\n * `python run.py`\n    - if you get flask errors, ",
    "url": "https://github.com/mediacloud/web-tools",
    "last_updated": "2025-04-26T14:23:46+00:00"
  },
  {
    "full_name": "ropensci/stplanr",
    "name": "stplanr",
    "description": "Sustainable transport planning with R",
    "language": "R",
    "topics": [
      "r",
      "transport",
      "spatial",
      "rstats",
      "r-package",
      "peer-reviewed",
      "transport-planning",
      "walking",
      "cycling",
      "pubic-transport",
      "origin-destination",
      "desire-lines",
      "routes",
      "routing",
      "route-network",
      "transportation",
      "cycle"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# stplanr <a href='https://docs.ropensci.org/stplanr/'><img src='man/figures/logo.png' align=\"right\" height=215/></a>\n\n<!-- [![Build Status](https://travis-ci.org/ropensci/stplanr.svg?branch=master)](https://travis-ci.org/ropensci/stplanr) -->\n\n[![rstudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/stplanr)](https://github.com/r-hub/cranlogs.app)\n[![](https://cranlogs.r-pkg.org/badges/grand-total/stplanr)](https://cran.r-project.org/package=stplanr)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/stplanr)](https://cran.r-project.org/package=stplanr)\n[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![](https://badges.ropensci.org/10_status.svg)](https://github.com/ropensci/software-review/issues/10)\n[![R-CMD-check](https://github.com/ropensci/stplanr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ropensci/stplanr/actions/workflows/R-CMD-check.yaml)\n\n**stplanr** is a package for sustainable transport planning with R.\n\nIt provides functions for solving common problems in transport planning\nand modelling, such as how to best get from point A to point B. The\noverall aim is to provide a reproducible, transparent and accessible\ntoolkit to help people better understand transport systems and inform\npolicy, as outlined in a\n[paper](https://journal.r-project.org/archive/2018/RJ-2018-053/index.html)\nabout the package, and the potential for open source software in\ntransport planning in general, published in the [R\nJournal](https://journal.r-project.org/).\n\nThe initial work on the project was funded by the Department of\nTransport\n([DfT](https://www.gov.uk/government/organisations/department-for-transport))\nas part of the development of the Propensity to Cycle Tool (PCT), a web\napplication to explore current travel patterns and cycling potential at\nzone, desire line, route and route network level",
    "url": "https://github.com/ropensci/stplanr",
    "last_updated": "2025-09-01T02:03:11+00:00"
  },
  {
    "full_name": "delight-im/FreeGeoDB",
    "name": "FreeGeoDB",
    "description": "Free database of geographic place names and corresponding geospatial data",
    "language": "PHP",
    "topics": [],
    "readme": "# FreeGeoDB\n\nFree database of geographic place names and corresponding geospatial data\n\n## Entities\n\n * airports\n * cities\n * countries (admin-0)\n * lakes\n * ports\n * railroads\n * regions (admin-1)\n * roads\n * time zones\n\n## Formats\n\n * [CSV](Distribution/CSV/)\n * [JSON](Distribution/JSON/)\n * [MySQL](Distribution/MySQL/)\n\n## Representation of geographic coordinates\n\nAll geographic coordinates are stored in the [WKT format](https://en.wikipedia.org/wiki/Well-known_text).\n\nIn WKT, the pairs of coordinates are always written as `x y`.\n\nSuch tuples, potentially along with other tuples, are then wrapped by one of 18 geometric objects, e.g. `POINT`, `MULTILINESTRING` or `MULTIPOLYGON`.\n\nThe most basic example would be `POINT(x y)`.\n\nAccordingly, the order of latitude and longitude in those tuples is always `long lat`, as in `POINT(long lat)`.\n\n## Authoritative data source\n\nThe single authoritative source for all data is in [`Source/json`](Source/json).\n\nIf you want to make any changes, please apply them in that folder only.\n\nAll other data files are generated from the files in that authoritative set.\n\n## Contributing\n\nAll contributions are welcome! If you wish to contribute, please create an issue first so that your feature, problem or question can be discussed.\n\n## License\n\n```\nCopyright (c) delight.im <info@delight.im>\n\nExcept where otherwise noted, all content is licensed under a\nCreative Commons Attribution 4.0 International License.\n\nYou should have received a copy of the license along with this\nwork. If not, see <http://creativecommons.org/licenses/by/4.0/>.\n```\n",
    "url": "https://github.com/delight-im/FreeGeoDB",
    "last_updated": "2025-07-29T22:10:01+00:00"
  },
  {
    "full_name": "ContinuumIO/topik",
    "name": "topik",
    "description": "A Topic Modeling toolbox",
    "language": "Python",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/ContinuumIO/topik.svg?branch=master)](https://travis-ci.org/ContinuumIO/topik)\n[![Coverage Status](https://coveralls.io/repos/ContinuumIO/topik/badge.svg?branch=master&service=github)](https://coveralls.io/github/ContinuumIO/topik?branch=master)\n[![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/ContinuumIO/topik/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/ContinuumIO/topik/?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/topik/badge/?version=latest)](http://topik.readthedocs.org/en/latest/?badge=latest)\n\n# Topik\n\nA Topic Modeling toolbox.\n\n\n## Introduction\n\nThe aim of `topik` is to provide a full suite and high-level interface for anyone interested in applying topic modeling.\nFor that purpose, `topik` includes many utilities beyond statistical modeling algorithms and wraps all of its\nfeatures into an easy callable function and a command line interface.\n\n`Topik` is built on top of existing natural language and topic modeling libraries and primarily provides a wrapper around them, for a quick and easy exploratory analysis of your text data sets.\n\nPlease see our [complete documentation at ReadTheDocs](http://topik.readthedocs.org/en/latest/).\n\n## LICENSE\n\nNew BSD. See [License File](https://github.com/ContinuumIO/topik/blob/master/LICENSE.txt).\n\n",
    "url": "https://github.com/ContinuumIO/topik",
    "last_updated": "2025-02-14T15:52:05+00:00"
  },
  {
    "full_name": "dwillis/mvp",
    "name": "mvp",
    "description": "Most Viewed (at) WashingtonPost.com",
    "language": "Ruby",
    "topics": [],
    "readme": "# MVP\nThe Most Viewed (at) WashingtonPost.com\n\nWashingtonPost.com publishes JSON feeds of its most-viewed stories on both web and mobile platforms. Using a Ruby wrapper for these endpoints called [PostHaste](https://github.com/dwillis/post_haste), this simple Ruby script grabs the top 100 most viewed pieces each hour and writes the results to day-specific CSV files.\n\nThe goal is to gain at least some insights into the data that the Post's own audience development folks are seeing, particularly in terms of which stories are popular on different platforms. All of the fields in the CSV file are directly from the original JSON except for the `rank` column, which I populate using the order of the piece in the listing.\n",
    "url": "https://github.com/dwillis/mvp",
    "last_updated": "2021-01-13T19:47:25+00:00"
  },
  {
    "full_name": "yixuan/showtext",
    "name": "showtext",
    "description": "Using Fonts More Easily in R Graphs",
    "language": "C",
    "topics": [
      "font",
      "graphics",
      "r-graphics",
      "graphics-device"
    ],
    "readme": "## showtext<img src=\"https://statr.me/images/sticker-showtext.png\" alt=\"showtext\" height=\"150px\" align=\"right\" />\n\n### What's this package all about?\n\n**showtext** makes it easy to use various types of fonts (TrueType, OpenType,\nType 1, web fonts, etc.) in R plots. The motivation to develop this package\nis that using non-standard fonts in R plots (especially for PDF device)\nis not straightforward, for example, when creating PDF with Chinese characters.\nThis is because most of the standard fonts used by `pdf()` do not contain\nChinese character glyphs, and users could hardly use system fonts in R.\n\nThe [extrafont](https://github.com/wch/extrafont) package developed by\n[Winston Chang](https://github.com/wch) is one nice solution to this problem,\nwhich mainly focuses on using TrueType fonts (`.ttf`) in PDF graphics device.\nNow **showtext** is able to support more font formats and more graphics devices,\nand avoids using external software such as Ghostscript.\n\n### A quick example\n\n```r\nlibrary(showtext)\n## Loading Google fonts (https://fonts.google.com/)\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Covered By Your Grace\", \"grace\")\nfont_add_google(\"Rock Salt\", \"rock\")\n\n## Automatically use showtext to render text for future devices\nshowtext_auto()\n\n## Tell showtext the resolution of the device,\n## only needed for bitmap graphics. Default is 96\nshowtext_opts(dpi = 96)\n\nset.seed(123)\nx = rnorm(10)\ny = 1 + x + rnorm(10, sd = 0.2)\ny[1] = 5\nmod = lm(y ~ x)\n\nop = par(cex.lab = 2, cex.axis = 1.5, cex.main = 2)\nplot(x, y, pch = 16, col = \"steelblue\",\n     xlab = \"X variable\", ylab = \"Y variable\", family = \"gochi\")\ngrid()\ntitle(\"Draw Plots Before You Fit A Regression\", family = \"bell\")\ntext(-0.5, 4.5, \"This is the outlier\", cex = 2, col = \"steelblue\",\n     family = \"grace\")\nabline(coef(mod))\nabline(1, 1, col = \"red\")\npar(family = \"rock\")\ntext(1, 1, expression(paste(\"True model: \", y == x + 1)),\n     cex = 1.5, col = \"red\", srt = 20)\n",
    "url": "https://github.com/yixuan/showtext",
    "last_updated": "2025-08-02T20:11:24+00:00"
  },
  {
    "full_name": "abhworthington/gifsinbeamer",
    "name": "gifsinbeamer",
    "description": "A quick and dangerously sparse tutorial for putting gifs in your Beamer presentations.",
    "language": "TeX",
    "topics": [],
    "readme": "# gifsinbeamer: A quick and dangerously sparse tutorial for putting gifs in your Beamer presentations.\n\nThis README should allow you to embed GIFs in beamer slides quickly. To learn more about the process and configuration options, see the `gifsinbeamer.pdf` file. \n\n# Why? How?\n\nThe PDF format doesn't support GIFs, and its handling of video is pretty bad in general. To circumvent these problems, we are going to:\n\n1. Convert a GIF to a series of PNG files using the ImageMagick program.\n2. Use the `animate` LaTeX package to embed those images in our beamer slides.\n\nThe resulting PDF should show moving GIFs, but *only* in some viewers, such as Adobe Acrobat Reader. Other viewers such as Foxit or OSX Preview do not appear to support animations.\n\n# Requirements\n\nYou need three things to embed GIFs in beamer slides:\n\n1. A full LaTeX distribution which includes the `animate` package.\n2. ImageMagick\n3. A PDF reader which supports animations (e.g., Adobe Acrobat Reader)\n\nHere are some instructions to install ImageMagick on OSX and Windows\n\n#### OSX\n\nThe simplest way to install ImageMagick on OSX is to use the Homebrew package manager. If you don't already use Homebrew, install it by following the instructions on this website: \n\nhttps://brew.sh/\n\nOnce Homebrew is installed, open the Terminal and execute these lines of code:\n\n```bash\nbrew update && brew upgrade\nbrew install imagemagick\n```\n\nIf you already have Imagemagick and Homebrew, you may want to update: \n\n```bash\nbrew reinstall imagemagick\n```\n\nYou're ready!\n\n#### Windows\n\nGood luck!\n\n# Usage\n\n### OSX\n\n#### Step 1\n\nDownload the file called `prep_gif.sh` from the Github respository to the directory where your slides are saved.\n\n#### Step 2\n\nOpen a Terminal, move to the directory where your slides are saved, and make the script executable. For example:\n\n```bash\ncd ~/Desktop/slides\nchmod u+x prep_gif.sh\n```\n\n#### Step 3\n\nExecute the script by pointing it to the GIF you want to use:\n\n```bash\n./pref_gif.sh figures/cat.gif\n```",
    "url": "https://github.com/abhworthington/gifsinbeamer",
    "last_updated": "2020-02-12T16:01:11+00:00"
  },
  {
    "full_name": "OpenDataDE/State-zip-code-GeoJSON",
    "name": "State-zip-code-GeoJSON",
    "description": "Zip code boundaries for each of the 50 states",
    "language": "",
    "topics": [],
    "readme": "# State-zip-code-GeoJSON\n\nZip code boundaries for each of the 50 states\n\nThis is a collection of zip code boundrary files for each of the 50 states, plus DC. Thanks to Github user\n[jgoodall for his us-maps example](https://github.com/jgoodall/us-maps)! His example was how I found where to\ndownload the shape files.\n\nEach file from the Census is of the format: \n\ntl_2010_STATE_FIPS_CODE_zcta510.zip\n\nWhere STATE_FIPS_CODE is the two-digit [Federal Information Processing Standard state code]\n(https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code). \n\nEach file was then unzipped, converted to GeoJSON with [ogr2ogr](http://www.gdal.org/), and minified\nwith [json-minify](https://www.npmjs.com/package/json-minify). Here's an example for Delaware:\n\n#### Zip codes\n* In a browser, visit: https://www.census.gov/cgi-bin/geo/shapefiles2010/main\n* Choose 'Zip Code Tabulation Areas' and 'Submit'\n* Choose 'Delware' for '5-Digit ZIP Code Tabulation Area (2010)' and click 'Download'\n\nThen unzip the files, and run:\n\n```\nogr2ogr -f \"GeoJSON\" de_delaware_zip_codes_geo.json tl_2010_10_zcta510.shp\n\njson-minify de_delaware_zip_codes_geo.json > de_delaware_zip_codes_geo.min.json\n\n```\n\n\n",
    "url": "https://github.com/OpenDataDE/State-zip-code-GeoJSON",
    "last_updated": "2025-08-30T23:01:11+00:00"
  },
  {
    "full_name": "usrflo/registered-domain-libs",
    "name": "registered-domain-libs",
    "description": "Detect the registered domain for a given domain name in C, Perl and PHP, based on Mozillas effective TLD listing",
    "language": "PHP",
    "topics": [
      "domainname-parser"
    ],
    "readme": "# Detection of registered domain names by reg-dom libs\n\nThe reg-dom libs are available in PHP, Perl and C.\n\nThey include recent representations of the effective TLD list available at\nhttps://publicsuffix.org/list/effective_tld_names.dat\nand help to convert an arbitrary domain name to the registered domain name.\nUpon changes of `effective_tld_names.dat` this git repository is automatically updated once a day.\nThe library supports Unicode (IDN notation) for domain name input and output. ACE notation is not supported currently.\n\n### Sample Use\nSigning domains in DKIM signatures can be used on the level of registered domains\nto rate senders who use e.g. a.spamdomain.tld, b.spamdomain.tld, ... under\nthe most common identifier - the registered domain - finally.\nProject page: https://www.agitos.de/registered-domain-libs/\nUpdated library downloads (besides Github): https://services.agitos.de/regdom-lib-downloads/\n\n### Pseudo code\nregisteredDomain = getRegisteredDomain(ingoingDomain);\n\n### Return values\n1) NULL if ingoingDomain is a TLD\n2) the registered domain name if TLD is known\n3) just <domain>.<tld> if <tld> is unknown\n   This case was added to support new TLDs in outdated reg-dom libs\n   by a certain likelihood. This fallback method is implemented in the\n   last conversion step and can be simply commented out.\n\n---\n\nIf you like to regenerate the effective TLD tree structure by yourself\nyou can use the script `generateEffectiveTLDs.php` with the following parameters:\n\n```\nphp generateEffectiveTLDs.php php  > PHP/effectiveTLDs.inc.php\nphp generateEffectiveTLDs.php perl > Perl/effectiveTLDs.pm\nphp generateEffectiveTLDs.php c    > C/tld-canon.h\n```\n\n### License\n```\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to you under the Apache License, Version 2.0\n# (the \"License\"); you may n",
    "url": "https://github.com/usrflo/registered-domain-libs",
    "last_updated": "2025-08-29T23:45:08+00:00"
  },
  {
    "full_name": "OpenGenderTracking/beauvoir",
    "name": "beauvoir",
    "description": "Guess a person's gender by their first name. Caveats apply.",
    "language": "Ruby",
    "topics": [],
    "readme": "Beauvoir\n========\n\nBeauvoir is a gem for guessing a person's gender by their first name. Caveats apply (see below)\n\nBeauvoir uses more precise data sourced from [Open Gender Tracker](http://opengendertracking.org/)'s [Global Name Data](https://github.com/OpenGenderTracking/globalnamedata). Beauvoir lets you set avg and lower bounds and choose countries from which to draw data (so far US, UK only, more to come soon).\n\nCaution\n-------\nThis is pre-alpha software. The API will change, I guarantee it.\n\nCaveats\n-------\n\nIt's important to note that many people identify as neither a man nor a woman. It's important, too, to note that many people who do identify as male or female have names for which most other people with that name identify as a different gender. All of these people deserve not to be misgendered.\n\nNevertheless, automatically classifying people by apparent gender can be a very useful tool to perform censuses of communities or publications to detect and quantify perhaps-invisible bias. VIDA is a pioneer in performing theses censuses, but their \"Count\" is limited by a manual methodology that depends hundreds of person-hours of labor. There is a place for more automated counts and Beauvoir can help, but if you plan to publish a count like this, you should be careful. Beauvoir's confidence thresholds are set very high by default on purpose, you shouldn't lower them unless you take other steps to make sure that you're very unlikely to misgender someone; you should also be prepared to be responsive and respectful if you do. You should include your methodology, prominently. You might also consider emphasizing aggregate numbers over your mapping of individual people's names to genders.\n\nUsage\n-----\n\nBasic case:\n````\nrequire 'beauvoir'\nb = Beauvoir.new\n\nb.guess(\"John\")\n=> :male\nb.guess(\"Mary\")\n=> :female\nb.guess(\"Sam\")\n=> :unknown\n\nb.estimated_female_value(\"Sam\")\n=> 0.0103360994972961\nb.estimated_male_value(\"Sam\")\n=> 0.9896639005027039\n````\n\nSet other options.\n````\nre",
    "url": "https://github.com/OpenGenderTracking/beauvoir",
    "last_updated": "2019-05-07T15:03:30+00:00"
  },
  {
    "full_name": "clinicalml/anchorExplorer",
    "name": "anchorExplorer",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "anchorExplorer\n==============\n\nHi! This document accompanies the initial release of the anchor interface described in: ``Using Anchors to Estimate Clinical State without Labeled Data'' by Y. Halpern, Y.D. Choi, S. Horng, D. Sontag. To appear in the American Medical Informatics Association (AMIA) Annual Symposium, Nov. 2014. \n\nA more detailed description and a quickstart guide can be found in Docs/README.pdf\n\nPlease direct questions/comments to Yoni Halpern: halpern@cs.nyu.edu.\n\n\n\n",
    "url": "https://github.com/clinicalml/anchorExplorer",
    "last_updated": "2022-03-25T08:29:10+00:00"
  },
  {
    "full_name": "h2oai/rsparkling",
    "name": "rsparkling",
    "description": "RSparkling: Use H2O Sparkling Water from R  (Spark + R + Machine Learning)",
    "language": "R",
    "topics": [
      "h2o",
      "spark",
      "machine-learning",
      "sparklyr",
      "deep-learning",
      "data-science",
      "big-data",
      "r",
      "water"
    ],
    "readme": "# Rsparkling has been moved to [Sparkling Water REPO](https://github.com/h2oai/sparkling-water/tree/master/r)\n\nPlease submit issues, questions and PRs in the new location. The current repo is not maintained.\n\nThe repository has been moved for several reasons, mainly to improve the integrations with Sparkling Water and \nfor the stability reasons.\n",
    "url": "https://github.com/h2oai/rsparkling",
    "last_updated": "2025-03-22T11:19:54+00:00"
  },
  {
    "full_name": "jeroenjanssens/data-science-at-the-command-line",
    "name": "data-science-at-the-command-line",
    "description": "Data Science at the Command Line",
    "language": "HTML",
    "topics": [
      "data-science",
      "linux",
      "unix",
      "cli",
      "bash",
      "book",
      "gnuplot",
      "ggplot2",
      "r",
      "python",
      "bookdown",
      "zsh",
      "jq",
      "curl",
      "cowsay",
      "terminal",
      "shell",
      "oreilly",
      "oreilly-books"
    ],
    "readme": "# Data Science at the Command Line\n\n[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/license-CC%20BY--NC--ND%204.0-orange.svg?style=flat-square)](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n\n<a href=\"https://datascienceatthecommandline.com/\">\n<img src=\"https://datascienceatthecommandline.com/static/img/cover-readme.png\" width=\"226px\" /></a>\n\nThis repository contains the full text, data, and scripts used in the second edition of the book *Data Science at the Command Line* by Jeroen Janssens. The book is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\n\nYou can read the book for free at https://datascienceatthecommandline.com.\n",
    "url": "https://github.com/jeroenjanssens/data-science-at-the-command-line",
    "last_updated": "2025-09-01T09:43:58+00:00"
  },
  {
    "full_name": "ApolloAuto/apollo",
    "name": "apollo",
    "description": "An open autonomous driving platform",
    "language": "C++",
    "topics": [
      "apollo",
      "autonomous-vehicles",
      "autonomous-driving",
      "autonomy",
      "self-driving-car"
    ],
    "readme": "![](docs/02_Quick%20Start/demo_guide/images/Apollo_logo.png)\n\n[![Build Status](http://180.76.142.62:8111/app/rest/builds/buildType:Apollo_Build/statusIcon)](http://180.76.142.62:8111/viewType.html?buildTypeId=Apollo_Build&guest=1)\n[![Simulation Status](https://azure.apollo.auto/dailybuildstatus.svg)](https://azure.apollo.auto/daily-build/public)\n\n```\n\nWe choose to go to the moon in this decade and do the other things,\n\nnot because they are easy, but because they are hard.\n\n-- John F. Kennedy, 1962\n\n```\n\nWelcome to Apollo's GitHub page!\n\n[Apollo](http://apollo.auto) is a high performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles.\n\nFor business and partnership, please visit [our website](http://apollo.auto).\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Prerequisites](#prerequisites)\n3. [Individual Versions](#individual-versions)\n4. [Architecture](#architecture)\n5. [Installation](#installation)\n6. [Quick Starts](#quick-starts)\n7. [Documents](#documents)\n\n## Introduction\n\nApollo is loaded with new modules and features but needs to be calibrated and configured perfectly before you take it for a spin. Please review the prerequisites and installation steps in detail to ensure that you are well equipped to build and launch Apollo. You could also check out Apollo's architecture overview for a greater understanding of Apollo's core technology and platforms.\n\n## Prerequisites\n\n**[New 2024-11]** The Apollo platform (stable version) is now upgraded with\nsoftware packages and library dependencies of newer versions including:\n\n1. CUDA upgraded to version 11.8 to support Nvidia Ada Lovelace (40x0 series) GPUs,\n   with NVIDIA driver >= 520.61.05\n2. LibTorch (only for arm64, both CPU and GPU version) bumped to version 1.11.0 accordingly, and for x86_64, still version 1.7.0.\n\nWe do not expect a disruption to your current work, but to ease your life of\nmigration, you would need to:\n\n1. Update NVIDIA driver on",
    "url": "https://github.com/ApolloAuto/apollo",
    "last_updated": "2025-09-02T07:28:42+00:00"
  },
  {
    "full_name": "JohannesFriedrich/SpaceX",
    "name": "SpaceX",
    "description": "An R API wrapper for the SpaceX API",
    "language": "R",
    "topics": [
      "spacex",
      "spacex-api",
      "r",
      "api-wrapper"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# SpaceX - An R API wrapper for the SpaceX project\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n\n## Installation\n\nUntil now the package is not on CRAN but you can download it via GitHub\nwith the following command:\n\n``` r\nif (!require(\"devtools\"))\n  install.packages(\"devtools\")\ndevtools::install_github(\"JohannesFriedrich/SpaceX\")\n```\n\n## Introduction\n\nThe **R**-package *SpaceX* is an API wrapper for data collected by\n<https://api.spacexdata.com/v3/> (version v3). You can request available\ndata with different functions:\n\n| Function name             | Description                            | Example                                            |\n|---------------------------|----------------------------------------|----------------------------------------------------|\n| get\\_SpaceX\\_capsules()   | request information on capsules        | get\\_SpaceX\\_capsules(“upcoming”)                  |\n| get\\_SpaceX\\_cores()      | request information on cores           | get\\_SpaceX\\_cores(core\\_serial = “B1037”)         |\n| get\\_SpaceX\\_dragons()    | request information on dragon capsules | get\\_SpaceX\\_dragons(“dragon1”)                    |\n| get\\_SpaceX\\_history()    | request information on SpaceX history  | get\\_SpaceX\\_history()                             |\n| get\\_SpaceX\\_info()       | request common information on SpaceX   | get\\_SpaceX\\_info()                                |\n| get\\_SpaceX\\_landpads()   | request information on landpads        | get\\_SpaceX\\_landpads(id = “LZ-4”)                 |\n| get\\_SpaceX\\_launches()   | request information on launches        | get\\_SpaceX\\_launches(launch\\_year = 2020)         |\n| get\\_SpaceX\\_launchpads() | request information on launchpads      | get\\_SpaceX\\_launchpads(site\\_id = “ksc\\_lc\\_39a”) |\n| get\\_Sp",
    "url": "https://github.com/JohannesFriedrich/SpaceX",
    "last_updated": "2025-03-22T10:50:20+00:00"
  },
  {
    "full_name": "maxmind/MaxMind-DB",
    "name": "MaxMind-DB",
    "description": "Spec and test data for the MaxMind DB file format",
    "language": "Go",
    "topics": [
      "geoip",
      "mmdb",
      "geoip2",
      "maxmind"
    ],
    "readme": "MaxMind DB is a binary file format that stores data indexed by IP address\nsubnets (IPv4 or IPv6).\n\nThis repository contains the spec for that format as well as test databases.\n\n# Copyright and License\n\nThis software is Copyright (c) 2013 - 2025 by MaxMind, Inc.\n\nThis is free software, licensed under the [Apache License, Version\n2.0](LICENSE-APACHE) or the [MIT License](LICENSE-MIT), at your option.\n",
    "url": "https://github.com/maxmind/MaxMind-DB",
    "last_updated": "2025-08-21T17:49:13+00:00"
  },
  {
    "full_name": "GSA/data",
    "name": "data",
    "description": "Assorted data from the General Services Administration.",
    "language": "HTML",
    "topics": [
      "domains",
      "technology",
      "standards",
      "enterprise",
      "data"
    ],
    "readme": "# GSA Data\n\nA home for miscellaneous data published by the [General Services Administration](http://gsa.gov). If you use any of this data for something, please do let us know by [opening an issue](https://github.com/gsa/data/issues) and telling us about it!\n\n## Datasets\n\n* ~[.gov domains](dotgov-domains/#readme)~ - Moved to https://github.com/cisagov/dotgov-data.\n* [.gov federal websites](dotgov-websites/#readme) - various files of trivially discoverable Federal .gov hostnames\n* [GSA Enterprise Architecture](enterprise-architecture/)\n\n## Copyright information\n\nThis data is in the public domain under [17 U.S.C. 105](https://www.law.cornell.edu/uscode/text/17/105).\n",
    "url": "https://github.com/GSA/data",
    "last_updated": "2025-09-01T15:03:55+00:00"
  },
  {
    "full_name": "JeffSackmann/tennis_pointbypoint",
    "name": "tennis_pointbypoint",
    "description": "Sequential point-by-point data for tens of thousands of pro matches",
    "language": "",
    "topics": [],
    "readme": "# tennis_pointbypoint\nSequential point-by-point data for tens of thousands of pro matches\n\nEach row in these files represents one match, and contains the following:\n- date\n- tournament name\n- tour (ATP, CH[allenger], FU[tures], WTA, or ITF [women])\n- draw (Main [draw] or Qual[ifying])\n- server1 (the player who served first)\n- server2\n- winner (1 or 2, corresponding to one of the previous two columns)\n- pbp (see below)\n- score\n- adf_flag (1 if the point sequence notes any aces or double faults, 0 if not; see below)\n\nThe pbp field:\n\nEach point is described with one character:\n- S (server won)\n- R (returner won)\n- A (ace)\n- D (double fault)\n\nSets are delimited with the '.' character, games are delimited with the ';' character, and the '/' character indicates changes of serve during a tiebreak.\n\nAces and double faults are not always noted in the original source data, which is why there is an adf_flag column. A '0' in that column indicates only that there are no aces or double faults recorded in the match, not whether aces or dfs *would've* been recorded had their been any. Matches without either are fairly rare, but they do happen.\n\nThe files themselves are separated by level, gender, and draw (same as the 'tour' and 'draw' columns in each sheet). The '_current' files contain 2015 matches, and the '_archive' files contain earlier matches, most of which are from 2012-14.\n\nCompiling, parsing, and cleaning up this rather messy dataset has been an enormous task, and I've done what I can to present only relatively clean data. For instance, all match scores have been validated to eliminate obvious errors in the source data. One casualty in this approach is the exclusion of retirements, the scores of which are not 'valid'. However, there are surely many errors remaining, including mis-classified matches and mis-parsed matches which result in valid but incorrect scores. There are probably also some duplicate matches.\n\nThe combination of incomplete source data, my probably imper",
    "url": "https://github.com/JeffSackmann/tennis_pointbypoint",
    "last_updated": "2025-08-18T16:50:00+00:00"
  },
  {
    "full_name": "stanford-policylab/polling-errors",
    "name": "polling-errors",
    "description": "Replication materials for \"Disentangling Bias and Variance in Election Polls\"",
    "language": "R",
    "topics": [],
    "readme": "# polling-errors\n\nThis repo contains data and code for the paper \"Disentangling Bias and Variance in Election Polls\" by Houshmand Shirani-Mehr, David Rothschild, Sharad Goel, and Andrew Gelman.\n\nThe original paper can be accessed [here](https://5harad.com/papers/polling-errors.pdf).\n\nTo reproduce the results, the R scripts should be run from `src/` directory.\n\n## Data Description\n\n1. polls_main_dataset.tsv\n    - The primary dataset used for analysis in the paper. The dataset contains 4,221 polls completed during the final three weeks of 608 state-level presidential, senatorial, and gubernatorial elections between 1998 and 2014. This includes:\n      - 4,154 state-level polls for elections in 1998–2013 that were collected and made available by FiveThirtyEight, all of which were completed during the final three weeks of the campaigns.\n      - 67 state-level 2014 polls posted on Pollster.com, where for consistency with the FiveThirtyEight data, we consider only those completed in the last three weeks of the campaigns.\n\n2. polls_auxiliary_dataset.tsv\n    - This dataset contains 10,444 polls for state-level presidential, senatorial, and gubernatorial elections between 2004 and 2012. 7,040 of these polls are completed less than 100 days before the election, and are used for analyzing RMSE of polls over time in the paper. All polls for this secondary dataset were obtained from Pollster.com and RealClearPolitics.com.\n\n## Recommended order to run the scripts\n\n1. data_exploration.R\n    - Prelimiary analysis of the data\n    - Generates figures 1, 2, and 3 in the paper\n\n2. model_prepare_input.R\n    - Prepares the input data for the Stan model\n    - Writes the prepared data as RData files into `data/`\n\n3. model_fit_data.R\n    - Fits the Bayesian model to data using Stan\n    - Writes the fit objects into `data/`\n\n4. model_analyze_results.R\n    - Analysis of the Bayesian model results\n    - Generates figures 4, 5, and 6 in the paper",
    "url": "https://github.com/stanford-policylab/polling-errors",
    "last_updated": "2025-05-13T23:59:21+00:00"
  },
  {
    "full_name": "debnolan/DynDocs",
    "name": "DynDocs",
    "description": "Repository for Creating Dynamic Statistics Documents",
    "language": "HTML",
    "topics": [],
    "readme": "# DynDocs\nRepository for Creating Dynamic Statistics Documents\n",
    "url": "https://github.com/debnolan/DynDocs",
    "last_updated": "2015-09-21T17:56:06+00:00"
  },
  {
    "full_name": "gojiplus/diversion",
    "name": "diversion",
    "description": "Go from Point A to B with a 'nicer' path",
    "language": "Python",
    "topics": [],
    "readme": "# Diversion 🛤️\n\nFind scenic and interesting routes between two locations by discovering points of interest along alternative paths. Instead of just getting from A to B, find routes that pass by cafes, parks, museums, and other places that match your preferences.\n\nhttps://diversion.streamlit.app/\n\n## Features\n\n- **Smart Route Discovery**: Finds alternative routes within your time constraints\n- **POI-Based Scoring**: Discovers restaurants, parks, museums, and cultural sites along routes\n- **AI-Powered Recommendations**: Uses OpenAI to explain why each route is interesting\n- **Interactive Maps**: Visual route comparison with POI markers\n- **Customizable Preferences**: Weight routes by scenic areas, food, culture, and walkability\n\n## Quick Start\n\n### Local Development\n\n1. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n2. **Set up API keys**:\n   - Get a Google Maps API key from [Google Cloud Console](https://console.cloud.google.com/google/maps-apis)\n   - Get an OpenAI API key from [OpenAI Platform](https://platform.openai.com/api-keys)\n   - Copy `.env.example` to `.env` and add your keys\n\n3. **Run the app**:\n   ```bash\n   streamlit run app.py\n   ```\n\n### Streamlit Community Cloud Deployment\n\nFor public deployment without exposing your API keys, the app accepts API keys through the UI:\n\n1. **Deploy to Streamlit Community Cloud**:\n   - Connect your GitHub repository to [Streamlit Community Cloud](https://streamlit.io/cloud)\n   - No environment variables needed - users enter their own API keys\n\n2. **User provides keys**:\n   - Users enter their Google Maps and OpenAI API keys in the sidebar\n   - Keys are stored only for the current session\n   - No keys are saved or logged\n\nThis approach protects your API costs while allowing public deployment.\n",
    "url": "https://github.com/gojiplus/diversion",
    "last_updated": "2025-09-01T21:58:44+00:00"
  },
  {
    "full_name": "Flagsmith/flagsmith",
    "name": "flagsmith",
    "description": "Flagsmith is an open source feature flagging and remote config service. Self-host or use our hosted version at https://app.flagsmith.com.",
    "language": "Python",
    "topics": [
      "feature-flags",
      "continuous-integration",
      "remote-control",
      "ci",
      "cd",
      "feature-toggles",
      "feature-flag",
      "remote-config",
      "python",
      "feature-flagging",
      "feature-flaggers",
      "flagsmith",
      "react",
      "multivariate-testing",
      "docker",
      "feature-management",
      "self-hosted",
      "hacktoberfest"
    ],
    "readme": "[![Feature Flag, Remote Config and A/B Testing platform, Flagsmith](static-files/flagsmith-cover.png)](https://www.flagsmith.com/)\n\n[![Stars](https://img.shields.io/github/stars/flagsmith/flagsmith)](https://github.com/Flagsmith/flagsmith/stargazers)\n[![Docker Pulls](https://img.shields.io/docker/pulls/flagsmith/flagsmith)](https://hub.docker.com/u/flagsmith)\n[![Docker Image Size](https://img.shields.io/docker/image-size/flagsmith/flagsmith)](https://hub.docker.com/r/flagsmith/flagsmith)\n[![Join the Discord chat](https://img.shields.io/discord/517647859495993347)](https://discord.gg/hFhxNtXzgm)\n[![Coverage](https://codecov.io/gh/Flagsmith/flagsmith/branch/main/graph/badge.svg?token=IyGii7VSdc)](https://codecov.io/gh/Flagsmith/flagsmith)\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n<a href=\"https://depot.dev?utm_source=Flagsmith\"><img src=\"https://depot.dev/badges/built-with-depot.svg\" alt=\"Built with Depot\" height=\"20\"></a>\n\n<p align=\"center\">\n  <a href=\"https://www.flagsmith.com/demo\">\n  <img width=\"75%\" height=\"75%\" src=\"static-files/ReadMe_Demo.gif\" alt=\"Try our interactive demo\">\n</p>\n<p align=\"center\">\n  <a href=\"https://www.flagsmith.com/demo\">Try our interactive demo </a>\n</p>\n\n\n# [Flagsmith](https://flagsmith.com/) is an Open-Source Feature Flagging Tool to Ship Faster & Control Releases \n\nChange the way your team releases software. Roll out, segment, and optimise—with granular control. Stay secure with on-premise and private cloud hosting.\n\n* Feature flags: Release features behind the safety of a feature flag\n* Make changes remotely: Easily toggle individual features on and off, and make changes without deploying new code\n* A/B testing: Use segments to run A/B and multivariate tests on new features\n* Segments: Release features to beta testers, collect feedback, and iterate \n* Organisation management: Stay organised with orgs, projects, and roles for team members\n* SDKs & frameworks: ",
    "url": "https://github.com/Flagsmith/flagsmith",
    "last_updated": "2025-09-02T05:49:11+00:00"
  },
  {
    "full_name": "hadley/mastering-shiny",
    "name": "mastering-shiny",
    "description": "Mastering Shiny: a book",
    "language": "R",
    "topics": [
      "r",
      "shiny",
      "book"
    ],
    "readme": "<!-- badges: start -->\n[![Build Status](https://github.com/hadley/mastering-shiny/workflows/.github/workflows/build-book.yaml/badge.svg)](https://github.com/hadley/mastering-shiny/actions?workflow=.github/workflows/build-book.yaml)\n<!-- badges: end -->\n\nThis is the repo for the book _Mastering Shiny_ by Hadley Wickham. It is licensed under the Creative Commons [Attribution-NonCommercial-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nc-nd/4.0/). \n\nBuilt with [bookdown](https://bookdown.org/yihui/bookdown/).\n\n## Images\n\nThere are three directories for images:\n\n* `diagrams/` contains omnigraffle diagrams. Source of truth is `.graffle` \n  files. Can delete all subdirectories.\n  \n* `screenshots/` contains programmatic screenshots. Source of truth is \n  book code. Can delete all subdirectories.\n  \n* `images/` contains images created some other way. Images are source of\n  truth and should not be deleted.\n",
    "url": "https://github.com/hadley/mastering-shiny",
    "last_updated": "2025-08-22T18:18:12+00:00"
  },
  {
    "full_name": "timelessnesses/typed_env",
    "name": "typed_env",
    "description": "A python module that help you have a type safety on enviroment variable",
    "language": "Python",
    "topics": [
      "typing",
      "enviroment",
      "environment-variables",
      "python3"
    ],
    "readme": "# typed-env\n\nA python module that help you have a type safety on enviroment variable (on runtime)\n\n## Documentation\n\n```python\ntimelessnesses.TypedEnv\n```\n\nA parent class that will help you ensure type safetiness on your enviroment variable.  \nUsage:\n\n```python\nfrom timelessnesses import typed_env\nimport datetime\n\nclass MyDotEnv(typed_env.TypedEnv):\n    # define your enviroment variable name and it's type (default value is optional and typing.Optional is also supported)\n    AMONG_US: bool = True\n    DISCORD_TOKEN: str\n    DATETIME: datetime.datetime = datetime.datetime.now()\n    NICE_DICTIONARY: typing.Dict[str, int] = {\"a\": 1, \"b\": 2}\n    DAMN_LIST: typing.List[int] = [1, 2, 3]\n    BALLS_KIND: BallsEnum # oh no! TypedEnv doesn't support my custom class!\n    # don't worry you can implement your own converter!\n\na = MyDotEnv()\na.add_validator(BallsEnum, lambda x: BallsEnum(int(x)))\na.get_env(typed_env.Method.dotenv, dotenv=\".env\") # you have options to either get only from dotenv or os.environ or both!\n\"\"\"\na.get_env(typed_env.Method.all, dotenv=\"path_to_.env\") # this fetch all the variable from both dotenv and os.environ\na.get_env(typed_env.Method.env) # this fetch all the variable from os.environ\na.get_env(typed_env.Method.dotenv, dotenv=\"path_to_.env\") # this fetch all the variable from dotenv\nNOTE: for dotenv/all method you have to supply dotenv argument\n\"\"\"\na.raise_error_on_unknown_env(False) # if this set to true any excessive enviroment variable will raise an error (default is True)\na.load() # let it do the work!\n```\n\n`TypedEnv` supports normal types like `str` or `int` and also `typing.Dict` and `typing.List` etc. But it also supports custom type by adding a validator, you are also allowed to overwrite the default validator by using `TypedEnv.add_validator` method.  \nCheck out more examples at `tests` folder!\n",
    "url": "https://github.com/timelessnesses/typed_env",
    "last_updated": "2025-01-15T15:28:47+00:00"
  },
  {
    "full_name": "johnmyleswhite/SimpleAintEasy",
    "name": "SimpleAintEasy",
    "description": "A compendium of the pitfalls and problems that arise when using standard statistical methods",
    "language": "R",
    "topics": [],
    "readme": "# Simple Ain't Easy: Real-World Problems with Basic Summary Statistics\n\nIn applied statistical work, the use of even the most basic summary statistics,\nlike means, medians and modes, can be seriously problematic. When forced to\nchoose a single summary statistic, many considerations come into practice.\n\nThis repo attempts to describe some of the non-obvious properties possessed by\nstandard statistical methods so that users can make informed choices about\nmethods.\n\n# Reading\n\nIf you just want to read the book, download it\n[here](https://github.com/johnmyleswhite/SimpleAintEasy/raw/master/book.pdf).\n\nNote that the formatting of the book is quite simple. This is intentional: it's\neasier to iterate on changes if you don't have to worry about typesetting until\nthe copy is finalized.\n\n# Contributing\n\nThe reason I chose to announce a book of examples isn't just pedagogical:\nby writing fully independent examples, it's possible to write a book as a\ncommunity working in parallel. If 30 people each contributed 10 examples over\nthe next month, we'd have a full-length book containing 300 examples in our\nhands. In practice, things are complicated by the need to make sure that\nexamples aren't redundant or low quality, but it's still possible to make this\nbook a large-scale community project.\n\nAs such, I hope you'll consider contributing. To contribute, just submit a\nnew example. If your example only requires text, you only need to write\na short LaTeX-flavored Markdown document. If you need images, please include\nR code that generates your images.\n\nIf you contribute, make sure you're happy with the following terms:\n\n* All code will be released under the\n  [MIT license](http://opensource.org/licenses/MIT)\n* All text will be released under the\n  [CC-BY 4.0](http://creativecommons.org/licenses/by/4.0/)\n* The finished book will be published with all proceeds going to\n  a to-be-determined charity\n\n# Layout\n\nThe main text for the book lives in the `text` directory. Each example is\ncategor",
    "url": "https://github.com/johnmyleswhite/SimpleAintEasy",
    "last_updated": "2025-03-23T00:55:32+00:00"
  },
  {
    "full_name": "trinker/wakefield",
    "name": "wakefield",
    "description": "Generate random data sets",
    "language": "R",
    "topics": [
      "data-generation",
      "wakefield",
      "r"
    ],
    "readme": "wakefield   \n============\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/0.1.0/active.svg)](https://www.repostatus.org/#active)\n[![Build\nStatus](https://travis-ci.org/trinker/wakefield.svg?branch=master)](https://travis-ci.org/trinker/wakefield)\n[![Coverage\nStatus](https://s3.amazonaws.com/assets.coveralls.io/badges/coveralls_0.svg)](https://coveralls.io/github/trinker/wakefield)\n[![DOI](https://zenodo.org/badge/5398/trinker/wakefield.svg)](https://dx.doi.org/10.5281/zenodo.17172)\n[![](https://cranlogs.r-pkg.org/badges/wakefield)](https://cran.r-project.org/package=wakefield)\n\n**wakefield** is designed to quickly generate random data sets. The user\npasses `n` (number of rows) and predefined vectors to the `r_data_frame`\nfunction to produce a `dplyr::tbl_df` object.\n\n![](tools/wakefield_logo/r_wakefield.png)\n\nTable of Contents\n============\n\n-   [Installation](#installation)\n-   [Contact](#contact)\n-   [Demonstration](#demonstration)\n    -   [Getting Started](#getting-started)\n    -   [Random Missing Observations](#random-missing-observations)\n    -   [Repeated Measures & Time Series](#repeated-measures-time-series)\n        -   [Related Series](#related-series)\n            -   [Some Examples With Variation](#some-examples-with-variation)\n            -   [Adjust Correlations](#adjust-correlations)\n            -   [Visualize the Relationship](#visualize-the-relationship)\n    -   [Expanded Dummy Coding](#expanded-dummy-coding)\n    -   [Visualizing Column Types](#visualizing-column-types)\n\n\nInstallation\n============\n\nTo download the development version of **wakefield**:\n\nDownload the [zip\nball](https://github.com/trinker/wakefield/zipball/master) or [tar\nball](https://github.com/trinker/wakefield/tarball/master), decompress\nand run `R CMD INSTALL` on it, or use the **pacman** package to install\nthe development version:\n\n    if (!require(\"pacman\")) install.packages(\"pacman\")\n  ",
    "url": "https://github.com/trinker/wakefield",
    "last_updated": "2025-08-25T16:30:41+00:00"
  },
  {
    "full_name": "mon231/apkpatcher",
    "name": "apkpatcher",
    "description": "Apkmod - automatically patch an apk to load frida script at runtime",
    "language": "Python",
    "topics": [
      "android",
      "frida",
      "frida-gadget",
      "patch",
      "patcher",
      "python3"
    ],
    "readme": "# apkmod - an apk patcher\n[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fmon231%2Fapkpatcher&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23AC3838&title=hits&edge_flat=false)](https://hits.seeyoufarm.com) <br />\nCross-Platform tool used to inject frida script & gadget to an APK <br />\nThis project started as a fork of [apkpatcher](https://github.com/badadaf/apkpatcher) <br />\n<br />\n*NOTE* that you should use this tool for debugging / educational purposes only!\n\n## Installation\nThe `apkmod` tool uses the [`buildapp`](https://github.com/mon231/buildapp) project <br />\nInstall both using a pypi package, then fetch the tools required for buildapp:\n> pip install apkmod --upgrade && buildapp_fetch_tools\n\n## Automation\nYou can use `apkmod` in your github workflow process! <br />\nSimply add a `build.yml` at your repo `.github/workflows` folder:\n```yml\nname: build patched app\non: [push]\n\njobs:\n  build:\n    runs-on: windows-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: mon231/apkpatcher@master\n        with:\n          original-apk: 'original.apk'\n          output-apk: 'patched.apk'\n          frida-script: 'frida.js'\n```\n\n## Patching process\nThis tool gets an android app installation file (`.apk`) and a [frida js-script](https://frida.re/docs/javascript-api/) <br />\nThen builds a new apk with frida-gadget & script runner ready to be installed on non-rooted android devices!\n\n## Requirements\nThe tool uses [`buildapp`](https://github.com/mon231/buildapp) package, <br />\nTherefore you have to provide it's [requirements](https://github.com/mon231/buildapp/#requirements) <br />\nOr run the requirements fetcher tool `buildapp_fetch_tools` after the `pip install` command\n",
    "url": "https://github.com/mon231/apkpatcher",
    "last_updated": "2025-06-16T21:44:16+00:00"
  },
  {
    "full_name": "probml/dynamax",
    "name": "dynamax",
    "description": "A Python package for probabilistic state space modeling with JAX",
    "language": "Python",
    "topics": [
      "state-space-models",
      "hidden-markov-models",
      "jax",
      "python",
      "kalman-filter"
    ],
    "readme": "# Welcome to DYNAMAX!\n\n![Logo](https://raw.githubusercontent.com/probml/dynamax/main/logo/logo.gif)\n\n![Test Status](https://github.com/probml/dynamax/actions/workflows/run_tests.yml/badge.svg?branch=main)\n![Docstrings](https://github.com/probml/dynamax/actions/workflows/interrogate.yml/badge.svg)\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.07069/status.svg)](https://doi.org/10.21105/joss.07069)\n\nDynamax is a library for probabilistic state space models (SSMs) written\nin [JAX](https://github.com/google/jax). It has code for inference\n(state estimation) and learning (parameter estimation) in a variety of\nSSMs, including:\n\n-   Hidden Markov Models (HMMs)\n-   Linear Gaussian State Space Models (aka Linear Dynamical Systems)\n-   Nonlinear Gaussian State Space Models\n-   Generalized Gaussian State Space Models (with non-Gaussian emission\n    models)\n\nThe library consists of a set of core, functionally pure, low-level\ninference algorithms, as well as a set of model classes which provide a\nmore user-friendly, object-oriented interface. It is compatible with\nother libraries in the JAX ecosystem, such as\n[optax](https://github.com/deepmind/optax) (used for estimating\nparameters using stochastic gradient descent), and\n[Blackjax](https://github.com/blackjax-devs/blackjax) (used for\ncomputing the parameter posterior using Hamiltonian Monte Carlo (HMC) or\nsequential Monte Carlo (SMC)).\n\n## Documentation\n\nFor a highlevel summary, see [this JOSS 2024 article](https://joss.theoj.org/papers/10.21105/joss.07069).\n\nFor tutorials and API documentation, see: https://probml.github.io/dynamax/.\n\nFor an extension of dynamax that supports structural time series models, \nsee https://github.com/probml/sts-jax.\n\nFor an illustration of how to use dynamax inside of [bayeux](https://jax-ml.github.io/bayeux/) to perform Bayesian inference\nfor the parameters of an SSM, see https://jax-ml.github.io/bayeux/examples/dynamax_and_bayeux/.\n\n## Installation and Testing\n\nTo install the latest releas",
    "url": "https://github.com/probml/dynamax",
    "last_updated": "2025-09-02T02:11:48+00:00"
  },
  {
    "full_name": "drewconway/mturk_coder_quality",
    "name": "mturk_coder_quality",
    "description": "Code, data, and web frameworks for experiments to enhance coder quality in Mechanical Turk jobs",
    "language": "Python",
    "topics": [],
    "readme": "Experiments Assessing the Viability of Crowd-sourcing for Political Text Coding\n===============================================================================\n\n>  - *Last updated*: March 11, 2013\n>  - *Author*: Drew Conway\n>  - *Contact*: drew.conway@nyu.edu\n\nThe following repository is a collection of code, data, and web frameworks for experiments to test the viability of using Mechanical Turk for political text coding.  These experiments will generate data to be used in a forthcoming paper treatment of this study.\n\nThese results will additionally be reported in a larger, co-authored, paper on using crowd-source to code political text.  A working version of this paper, entitled \"Crowd-sourced data coding for the social sciences: massive non-expert coding of political texts\" paper is [available here](https://files.nyu.edu/ml127/public/Forthcoming/Crowd%20sourced%20data%20coding%20Harvard%201.1.pdf)\n\n**NOTE**: These files have been exposed primarily to provide replication resources for these experiments. These experiments rely on formatted web pages hosted remotely on Amazon's S3 storage service.  As such, this repository is not an \"out-of-the-box\" resources for generating these experiments.  The files contained in the `html`, `js`,`css`, and `data` folders must be hosted, and appropriately deployed, in order to replicate these experiments exactly.  Also, since these are experiments even a perfect duplication of the experiments may not generate exactly the same results.\n\n## Experimental Design\n\n**NOTE**: For details on the specific coding tasks please see the [worker instruction page](http://s3.amazonaws.com/aws.drewconway.com/mt/experiments/cmp/html/instructions.html).\n\nThis set of experiments seeks to test if text codings from Mechanical Turk workers are statistically indistinguishable from codings generated by experts.  Additionally, these experiments test how variations in the mechanism allowing coders into a task affects the overall quality of codings.  To test",
    "url": "https://github.com/drewconway/mturk_coder_quality",
    "last_updated": "2025-02-06T04:32:04+00:00"
  },
  {
    "full_name": "ageron/handson-ml",
    "name": "handson-ml",
    "description": "⛔️ DEPRECATED – See https://github.com/ageron/handson-ml3 instead.",
    "language": "Jupyter Notebook",
    "topics": [
      "tensorflow",
      "scikit-learn",
      "machine-learning",
      "python",
      "deep-learning",
      "neural-network",
      "ml",
      "distributed",
      "jupyter-notebook",
      "deprecated"
    ],
    "readme": "Machine Learning Notebooks\n==========================\n\n# ⚠ THE <a href=\"https://github.com/ageron/handson-ml3\">THIRD EDITION OF MY BOOK</a> IS NOW AVAILABLE.\n\nThis project is for the first edition, which is now outdated.\n\n<details>\n\nThis project aims at teaching you the fundamentals of Machine Learning in\npython. It contains the example code and solutions to the exercises in my O'Reilly book [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/):\n\n[![book](http://akamaicovers.oreilly.com/images/9781491962282/cat.gif)](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n\n\n## Quick Start\n\n### Want to play with these notebooks online without having to install anything?\nUse any of the following services.\n\n**WARNING**: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.\n\n* **Recommended**: open this repository in [Colaboratory](https://colab.research.google.com/github/ageron/handson-ml/blob/master/):\n<a href=\"https://colab.research.google.com/github/ageron/handson-ml/blob/master/\"><img src=\"https://colab.research.google.com/img/colab_favicon.ico\" width=\"90\" /></a>\n\n* Or open it in [Binder](https://mybinder.org/v2/gh/ageron/handson-ml/master):\n<a href=\"https://mybinder.org/v2/gh/ageron/handson-ml/master\"><img src=\"https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png\" width=\"90\" /></a>\n\n  * _Note_: Most of the time, Binder starts up quickly and works great, but when handson-ml is updated, Binder creates a new environment from scratch, and this can take quite some time.\n\n* Or open it in [Deepnote](https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/master/index.ipynb):\n<a href=\"https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/m",
    "url": "https://github.com/ageron/handson-ml",
    "last_updated": "2025-09-02T09:56:38+00:00"
  },
  {
    "full_name": "orca3/MiniAutoML",
    "name": "MiniAutoML",
    "description": "Source code for \"Enginneering Deep Learning Platforms\"",
    "language": "Java",
    "topics": [],
    "readme": "# MiniAutoML\n\n![cover page](cover.png)\n\nThis repository contains source code examples for `Engineering Deep Learning Systems`. [Purchase a copy here](http://mng.bz/GGgN). \n\nTo demonstrate design principles introduced in the book, we built this mini system (5 micro-services) with Java (web API) and Python (model training code). The system can run on Mac or Linux if you have Docker and Kubernetes (optional) installed, and it addresses the core requirements of a machine learning system: dataset management, model training and model serving.  \n\nLearning a deep learning system could be intimidating. The complexity of web services setup, intricacies of business logic, and variety of libraries and framework installations prevent us from getting to the core piece of a deep learning system. \nTo enable you to easily access the core implementation of the system, we did several things to keep this demo system simple:\n1. Use gRPC to build the web API to avoid boilerplate code.\n2. Offer [shell scripts](/scripts/) to run the system and interact with services. \n3. Code is simple and short. Each service of the system is built with less than a few thousand lines of code. We only keep the bare minimum code that is sufficient to demonstrate the design principles.  \n\n## System Overview\nOur mini deep learning system consists of four services and one storage system, they are:\n\n- **[Dataset management service](https://github.com/orca3/MiniAutoML/tree/main/data-management)**, designed for storing and fetching dataset. \n- **[Model training service](https://github.com/orca3/MiniAutoML/tree/main/training-service)**, designed for running model training code.\n- **[Metadata store service](https://github.com/orca3/MiniAutoML/tree/main/metadata-store)**, designed for storing model metadata, such as model name, model version and model algorithm.\n- **[Prediction service](https://github.com/orca3/MiniAutoML/tree/main/prediction-service)**, designed to execute models to process customer’s prediction req",
    "url": "https://github.com/orca3/MiniAutoML",
    "last_updated": "2025-08-15T07:46:03+00:00"
  },
  {
    "full_name": "nealjean/predicting-poverty",
    "name": "predicting-poverty",
    "description": "Combining satellite imagery and machine learning to predict poverty",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Combining satellite imagery and machine learning to predict poverty\n\nThe data and code in this repository allows users to generate figures appearing in the main text of the paper ***Combining satellite imagery and machine learning to predict poverty*** (except for Figure 2, which is constructed from specific satellite images). Paper figures may differ aesthetically due to post-processing.\n\nCode was written in R 3.2.4 and Python 2.7.\n\nUsers of these data should cite Jean, Burke, et al. (2016). If you find an error or have a question, please submit an issue.\n\n## Links to related projects\n\nWe are no longer maintaining this project, but will link to related projects as we learn of them.\n\nPytorch implementation: https://github.com/jmather625/predicting-poverty-replication\n\n## Description of folders\n\n- **data**: Input and output data stored here\n- **figures**: Notebooks used to generate Figs. 3-5\n- **scripts**: Scripts used to process data and produce Fig. 1\n- **model**: Store parameters for trained convolutional neural network\n\n## Packages required\n\n**R**\n- R.utils\n- magrittr\n- foreign\n- raster\n- readstata13\n- plyr\n- RColorBrewer\n- sp\n- lattice\n- ggplot2\n- grid\n- gridExtra\n\nThe user can run the following command to automatically install the R packages\n```\ninstall.packages(c('R.utils', 'magrittr', 'foreign', 'raster', 'readstata13', 'plyr', 'RColorBrewer', 'sp', 'lattice', 'ggplot2', 'grid', 'gridExtra'), dependencies = T)\n```\n**Python**\n- NumPy\n- Pandas\n- SciPy\n- scikit-learn\n- Seaborn\n- Geospatial Data Abstraction Library (GDAL)\n- Caffe\n\nCaffe and pycaffe\n- See [Caffe Installation](https://github.com/BVLC/caffe/wiki/Installation)\n\nWe recommend using the open data science platform [Anaconda](https://www.continuum.io/downloads).\n\n## Instructions for processing survey data\n\nDue to data access agreements, users need to independently download data files from the World Bank's Living Standards Measurement Surveys and the Demographic and Health Surveys websites. These two dat",
    "url": "https://github.com/nealjean/predicting-poverty",
    "last_updated": "2025-08-11T07:56:05+00:00"
  },
  {
    "full_name": "ajrgodfrey/WriteR",
    "name": "WriteR",
    "description": "An application for editing R markdown documents that is accessible for blind people using screen reading software.",
    "language": "HTML",
    "topics": [],
    "readme": "# The WriteR family of applications \n\n\nWriteR  was created so that blind users could draft and compile R markdown documents. It is needed because while sighted users do this in RStudio, this software does not work well enough with the screen reading software blind people use.\n\nWriteR also works well enough with regular markdown documents.\n\n\nThe advent of Quarto as the next generation of R markdown in 2022 inspired more work on WriteR. First, the syntax of Quarto is different, and second, Quarto documents can be processed outside of R. The third app in the family is a cut-back version for editing R scripts as a substitute for the novice user who does not want to dive into R markdown or Quarto at the outset.\n\n![OpenSource](https://img.shields.io/badge/OpenSource-Yes-green)\n![OSc](https://img.shields.io/badge/OS-Multi-blue)\n![License](https://img.shields.io/badge/License-GPL2-yellow)\n\n\n## Latest news\n\n### ScriptR 2023.0\n\nThe first version of the app allows editing of an R script. Processing into HTML is done using the `rmarkdown` package in R. The result is an HTML page that the user must refresh.\n\n\n### WriteR 2022.2\n\nThe primary reason for this new version is to create  a second application for writing quarto documents. Work is needed to pull all common content out of the primary app file into child files (modules)\n\nWork on this version also had to deal with wxPython developments, including deprocated functionality.\n\n\n\n### WriteR 2022.1\n\nIn March 2022, AJRG worked on making an executable and corresponding installer for Windows users.\n\n- the executable is built using pyinstaller, with both a folder and a single file version.\n- Nullsoft is the tool being used to create the Windows installer.\n- a version number was added to the distribution. This duplicates information held in the Help menu.\n\n\n## System requirements for WriteR\n\n\n### R and R markdown processing\n\nWriteR is intended for use with R markdown files. Users must have R and Pandoc installed and several R packages",
    "url": "https://github.com/ajrgodfrey/WriteR",
    "last_updated": "2025-03-05T08:22:10+00:00"
  },
  {
    "full_name": "fchollet/deep-learning-with-python-notebooks",
    "name": "deep-learning-with-python-notebooks",
    "description": "Jupyter notebooks for the code samples of the book \"Deep Learning with Python\"",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Companion notebooks for Deep Learning with Python\n\nThis repository contains Jupyter notebooks implementing the code samples found in the book [Deep Learning with Python, third edition (2025)](https://www.manning.com/books/deep-learning-with-python-third-edition?a_aid=keras&a_bid=76564dff)\nby Francois Chollet and Matthew Watson. In addition, you will also find the legacy notebooks for the [second edition (2021)](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff)\nand the [first edition (2017)](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n\nFor readability, these notebooks only contain runnable code blocks and section titles, and omit everything else in the book: text paragraphs, figures, and pseudocode.\n**If you want to be able to follow what's going on, I recommend reading the notebooks side by side with your copy of the book.**\n\n## Running the code\n\nWe recommend running these notebooks on [Colab](https://colab.google), which\nprovides a hosted runtime with all the dependencies you will need. You can also,\nrun these notebooks locally, either by setting up your own Jupyter environment,\nor using Colab's instructions for\n[running locally](https://research.google.com/colaboratory/local-runtimes.html).\n\nBy default, all notebooks will run on Colab's free tier GPU runtime, which\nis sufficient to run all code in this book. Chapter 8-18 chapters will benefit\nfrom a faster GPU if you have a Colab Pro subscription. You can change your\nruntime type using **Runtime -> Change runtime type** in Colab's dropdown menus.\n\n## Choosing a backend\n\nThe code for third edition is written using Keras 3. As such, it can be run with\nJAX, TensorFlow or PyTorch as a backend. To set the backend, update the backend\nin the cell at the top of the colab that looks like this:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n```\n\nThis must be done only once per session before importing Keras. If you are\nin t",
    "url": "https://github.com/fchollet/deep-learning-with-python-notebooks",
    "last_updated": "2025-09-01T15:09:06+00:00"
  },
  {
    "full_name": "ucbvislab/d3-deconstructor",
    "name": "d3-deconstructor",
    "description": "A Google Chrome plugin for extracting data from D3 visualizations.",
    "language": "JavaScript",
    "topics": [],
    "readme": "D3 Deconstructor\n=======\n\nFirst release - October 2014\n\nThe D3 Deconstructor is a Google Chrome extension for extracting data from [D3.js](http://d3js.org) visualizations.  D3 _binds data_ to DOM elements when building a visualization.  Deconstructor extracts this data and the visual mark attributes (such as position, width, height, and color) for each element in a D3 visualization.  Then, elements are grouped by the type of data they are bound to.\n\nThe D3 Deconstructor was developed in the [VisLab](http://vis.berkeley.edu) at UC Berkeley.  We also used the results of deconstruction to enable restyling of D3 visualizations.  You can find the paper here: http://vis.berkeley.edu/papers/d3decon\n\n### Usage\n\nTo extract data from a D3 visualization the user right clicks on the visualization and selects \"Deconstruct Visualization\" in the context menu.  Deconstructor then creates a window showing the data tables for each group of elements.  Then, you can save visualization data as JSON or CSV.\n\nIn addition to data and mark attributes, Deconstructor extracts the mappings between the data and marks in the visualizations.  These mappings are saved when saving as JSON only.  JSON output is an array of \"schema\" objects which have several properties:\n\n* **data** - The data table for the visualization, represented as an object whose keys are the data column names and the value for each key is the array of data values in the column.\n* **attrs** - The mark attribute table, represented using an object similar to *data*.\n* **mappings** - A list of mappings found for the group of marks.  Each mapping is an object with several properties:\n  * *type* - The type of mapping; we extract mappings which are linear and one-to-one correspondences between data and attributes.\n  * *data* - Either a single data field name or an array of data field names for the mapping.\n  * *attr* - The mapped attribute.\n  * *params* - A set of parameters that describe the mapping.\n* **ids** - A list containing a ",
    "url": "https://github.com/ucbvislab/d3-deconstructor",
    "last_updated": "2025-08-31T14:46:03+00:00"
  },
  {
    "full_name": "goose3/goose3",
    "name": "goose3",
    "description": "A Python 3 compatible version of goose http://goose3.readthedocs.io/en/latest/index.html",
    "language": "HTML",
    "topics": [],
    "readme": "Goose3 - Article Extractor\n===============================================\n\n.. image:: https://img.shields.io/badge/license-Apache_2.0-blue.svg\n    :target: https://opensource.org/licenses/Apache-2.0/\n    :alt: License\n.. image:: https://github.com/goose3/goose3/workflows/Python%20package/badge.svg?branch=master\n    :target: https://github.com/goose3/goose3/actions?query=workflow%3A%22Python+package%22\n    :alt: Build Status\n.. image:: https://img.shields.io/github/release/goose3/goose3.svg\n    :target: https://github.com/goose3/goose3/releases\n    :alt: GitHub release\n.. image:: https://codecov.io/gh/goose3/goose3/branch/master/graph/badge.svg?token=PoWLaCLbW1\n    :target: https://codecov.io/gh/goose3/goose3\n    :alt: Test Coverage\n.. image:: https://badge.fury.io/py/goose3.svg\n    :target: https://badge.fury.io/py/goose3\n    :alt: PyPi Release\n.. image:: http://pepy.tech/badge/goose3\n    :target: https://pepy.tech/project/goose3\n    :alt: Downloads\n\nIntro\n--------------------------------------------------------------------------------\n\nGoose was originally an article extractor written in Java that has most\nrecently (Aug2011) been converted to a `scala project <https://github.com/GravityLabs/goose>`_.\n\nThis is a complete rewrite in Python. The aim of the software is to\ntake any news article or article-type web page and not only extract what\nis the main body of the article but also all meta data and most probable\nimage candidate.\n\nGoose will try to extract the following information:\n\n-  Main text of an article\n-  Main image of article\n-  Any YouTube/Vimeo movies embedded in article\n-  Meta Description\n-  Meta tags\n\nThe Python version was originally rewritten by:\n\n-  Xavier Grangier\n\nLicensing\n--------------------------------------------------------------------------------\n\nIf you find Goose useful or have issues please drop me a line. I'd love\nto hear how you're using it or what features should be improved.\n\nGoose is licensed by Gravity.com under the Apache 2.0 lice",
    "url": "https://github.com/goose3/goose3",
    "last_updated": "2025-08-23T08:03:20+00:00"
  },
  {
    "full_name": "ideonate/jupyter-innotater",
    "name": "jupyter-innotater",
    "description": "Inline data annotator for Jupyter notebooks",
    "language": "Python",
    "topics": [
      "jupyter",
      "jupyter-widgets",
      "jupyter-notebooks",
      "binder",
      "binder-ready"
    ],
    "readme": "# jupyter-innotater\n\nAnnotate data including image bounding boxes inline within your [Jupyter notebook](https://jupyter.org/) in Python. Innotater's flexible API allows easy selection of interactive controls to suit your datasets exactly. \n\nNow works with JupyterLab (2.0+ recommended)\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ideonate/jupyter-innotater/master)\n\n[![Documentation Status](https://readthedocs.org/projects/jupyter-innotater/badge/?version=latest)](https://jupyter-innotater.readthedocs.io/en/latest/?badge=latest)\n\n## 1 - Overview\n\nIn a data science or machine learning project, you may prepare and study images or other data within a Jupyter notebook then need to annotate the data to augment the training or fix errors in your source data.\n\nSince you are already working within a Jupyter notebook, the Innotater works inline allowing you to interact with your data and annotate it quickly and easily, syncing straight back to your input data arrays or matrices.\n\nWithin Jupyter, you can easily home in on problem input data - perhaps only misclassified images - so you can step through and adjust bounding boxes just for those items. \n\nThe Innotater widget is designed with a flexible API making it quick and easy to get started exploring your dataset, guessing how to work with your data without explicit configuration where possible.\n\nThe project is currently in ALPHA development phase, and I appreciate all feedback on any problems including details on how the current code works or fails to work for the structure of your particular projects.\n\n**Full documentation is [now on ReadTheDocs](https://jupyter-innotater.readthedocs.io/)**\n\n## 2 - Examples\n\nYou can easily combine Innotater's interactive components to suit your project. Here are some examples.\n\n### Images and Bounding Boxes\n\nLoad some images from filenames in an array, initialise empty bounding boxes.\n\nThen set up Innotater to display the images so you can draw updated boundin",
    "url": "https://github.com/ideonate/jupyter-innotater",
    "last_updated": "2025-08-21T21:55:59+00:00"
  },
  {
    "full_name": "rstudio/spark.rstudio.com",
    "name": "spark.rstudio.com",
    "description": "Official repo for:",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Instructions\n\n```r\nlibrary(convertsite)\n\nov <- list()\nov$project$`output-dir` <- \"docs\"\n\nconvert_to_quarto(setup_override = ov)\n\nquarto::quarto_serve()\n```",
    "url": "https://github.com/rstudio/spark.rstudio.com",
    "last_updated": "2025-04-04T07:40:36+00:00"
  },
  {
    "full_name": "jtleek/advdatasci15",
    "name": "advdatasci15",
    "description": "Advanced Data Science @ JHU Biostats",
    "language": "HTML",
    "topics": [],
    "readme": "Biostatistics 711 and 712: Advanced Data Science\n============\n\nClass github repository for 711 and 712; doctoral classes in the Department of Biostatistics at Johns Hopkins. You may use the material according to the [Creative Commons Share-Alike 2.0 (CC-SA 2.0)](http://creativecommons.org/licenses/by/2.0/) license. \n\nYou can view the course website here: http://www.jtleek.com/advdatasci\n",
    "url": "https://github.com/jtleek/advdatasci15",
    "last_updated": "2024-02-14T15:34:37+00:00"
  },
  {
    "full_name": "notnews/news-ai-choose",
    "name": "news-ai-choose",
    "description": "News A.I. Choose",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# News A.I. Choose\n\nThe goal of New A.I. Choose is to train machine learning models to accurately predict the category of a news article based on the article body and other context clues. Additionally, this project intends to provide a front-end web interface for users to interact with stories by requesting more positive or more negative new stories. Users will also be able to assist the model by verifying if the recommendation is accurate - this will be used in future training to fine tune the models.\n\n# Architecture\n\n![Overview of News You Choose](images/Overview_News_You_Choose.png)\n\n![Overview of News You Choose Lambda](images/Detailed_View_News_You_Choose_Lambda.png)\n\n![Overview of News You Choose Database](images/Database_ERD.png)\n\n![Overview of News You Choose Model](images/Detailed_View_Model_Structure.jpg)\n\n# ECR, Lambda and Deployments\n\nWe are using ECR along with a lambda function to run some web scraping tasks for this project. Read a quick [tutorial on this here](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/).\n\n## ECR repositories:\n\n- `news-you-choose`\n  - Contains the scraper for daily news articles\n  - Used in Lambda function `news-you-choose-daily-scraper`\n- `news-you-choose-inference`\n  - Contains the model inference/api endpoint\n  - Expects either\n    - An S3 [event notification](https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-content-structure.html)\n    - Or a JSON string with a \"text\" key: `{\"text\": \"I like this article\"}`\n\n## Deployment to Lambda\n\nDeployments to Lambda are currently manual, while we research how to do this manually.\n\n1. alter the files in `scraper/` or `inference/` directories\n   - Note that you can test this locally as described in the [tutorial](https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/).\n2. run `make latest` to login, build and push the latest version of the image to the ECR repository as `news-you-choose:latest`\n3. Go to the Lambda function ",
    "url": "https://github.com/notnews/news-ai-choose",
    "last_updated": "2025-03-16T21:13:28+00:00"
  },
  {
    "full_name": "pbreheny/biglasso",
    "name": "biglasso",
    "description": "biglasso: Extending Lasso Model Fitting to Big Data in R",
    "language": "C++",
    "topics": [
      "lasso",
      "r",
      "out-of-core",
      "bigdata",
      "parallel-computing"
    ],
    "readme": "<!-- badges: start -->\n[![GitHub version](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pbreheny/biglasso/master/.version.json&style=flat&logo=github)](https://github.com/pbreheny/biglasso)\n[![CRAN version](https://img.shields.io/cran/v/biglasso?logo=R)](https://cran.r-project.org/package=biglasso)\n[![downloads](https://cranlogs.r-pkg.org/badges/biglasso)](https://cran.r-project.org/package=biglasso)\n[![R-CMD-check](https://github.com/pbreheny/biglasso/workflows/R-CMD-check/badge.svg)](https://github.com/pbreheny/biglasso/actions)\n[![codecov](https://codecov.io/gh/pbreheny/biglasso/graph/badge.svg?token=V2IDV3BG5O)](https://codecov.io/gh/pbreheny/biglasso)\n<!-- badges: end -->\n\n# [biglasso: Extend Lasso Model Fitting to Big Data in R](https://pbreheny.github.io/biglasso/index.html)\n\n`biglasso` extends lasso and elastic-net linear and logistic regression models for ultrahigh-dimensional, multi-gigabyte data sets that cannot be loaded into memory. It utilizes memory-mapped files to store the massive data on the disk and only read those into memory whenever necessary during model fitting. Moreover, some advanced feature screening rules are proposed and implemented to accelerate the model fitting. **As a result, this package is much more memory- and computation-efficient and highly scalable as compared to existing lasso-fitting packages such as [glmnet](https://CRAN.R-project.org/package=glmnet) and [ncvreg](https://CRAN.R-project.org/package=ncvreg)**. Bechmarking experiments using both simulated and real data sets show that `biglasso` is not only 1.5x to 4x times faster than existing packages, but also at least 2x more memory-efficient. More importantly, to the best of our knowledge, `biglasso` is the first R package that enables users to fit lasso models with data sets that are larger than available RAM, thus allowing for powerful big data analysis on an ordinary laptop.\n\n## Installation\n\nTo install the latest stable release version from CRAN:",
    "url": "https://github.com/pbreheny/biglasso",
    "last_updated": "2025-07-28T16:15:09+00:00"
  },
  {
    "full_name": "cwickham/nass",
    "name": "nass",
    "description": "R package for National Automotive Sampling System data",
    "language": "SAS",
    "topics": [],
    "readme": "# nass\n\nData from: https://www.nhtsa.gov/research-data/national-automotive-sampling-system-nass\n\nIn development: **Currently only CDS from 2015**\n\n## Crashworthiness Data System (CDS)\nhttps://www.nhtsa.gov/national-automotive-sampling-system-nass/crashworthiness-data-system\n\n> NASS CDS has detailed data on a representative, random sample of thousands of minor, serious, and fatal crashes. Field research teams located at Primary Sampling Units (PSU's) across the country study about 5,000 crashes a year involving passenger cars, light trucks, vans, and utility vehicles. Trained crash investigators obtain data from crash sites, studying evidence such as skid marks, fluid spills, broken glass, and bent guard rails. They locate the vehicles involved, photograph them, measure the crash damage, and identify interior locations that were struck by the occupants. These researchers follow up on their on-site investigations by interviewing crash victims and reviewing medical records to determine the nature and severity of injuries.\n\n## General Estimates System (GES)\nhttps://www.nhtsa.gov/national-automotive-sampling-system-nass/nass-general-estimates-system\n\n> In order for a crash to be eligible for the GES sample a police accident report (PAR) must be completed, it must involve at least one motor vehicle traveling on a traffic way, and the result must be property damage, injury, or death.\n>\n> These accident reports are chosen from 60 areas that reflect the geography, roadway mileage, population, and traffic density of the U.S. GES data collectors make weekly visits to approximately 400 police jurisdictions in the 60 areas across the United States, where they randomly sample about 50,000 PARs each year. The collectors obtain copies of the PARs and send them to a central contractor for coding. No other data are collected beyond the selected PARs.\n\n## To Do List\n\n* Tidy `cds_acc_desc` to have one record per case (e.g. concatenate multi-row description)\n* Improve documentation\n* Ad",
    "url": "https://github.com/cwickham/nass",
    "last_updated": "2025-03-22T11:19:57+00:00"
  },
  {
    "full_name": "kosukeimai/qss",
    "name": "qss",
    "description": "Supplementary Materials for ``Quantitative Social Science: An Introduction''",
    "language": "R",
    "topics": [],
    "readme": "# QSS (Quantitative Social Science) [![Build Status](https://travis-ci.org/kosukeimai/qss.svg?branch=master)](https://travis-ci.org/kosukeimai/qss)\nSupplementary Materials for the book,\n**[Quantitative Social Science: An Introduction](http://press.princeton.edu/titles/11025.html)**,\npublished by Princeton University Press in 2017.  See the [book website](http://qss.princeton.press/).  It is\nalso available for purchase at vendors like\n[Amazon](https://www.amazon.com/Quantitative-Social-Science-Kosuke-Imai/dp/0691175462). Also included are materials for **[Quantitative Social Science: An Introduction in tidyverse](https://qss.princeton.press/)**, published by Princeton University Press in 2022. All tidyverse versions contain \"-tidy\" in their file names. \n\nThe book is based on the teaching philosophy summarized in the talk I\ngave at the Nuffield Foundation's\n[Q-Step Programme](http://www.nuffieldfoundation.org/q-step) in 2015: \n[slides](http://imai.princeton.edu/talk/files/Q-Step15.pdf) \n\nThis repository contains the data sets and **R** scripts (available in .R, .Rmd, and .pdf formats) for all of the chapters:\n\n1. [Introduction](INTRO)\n2. [Causality](CAUSALITY)\n3. [Measurement](MEASUREMENT)\n4. [Prediction](PREDICTION)\n5. [Discovery](DISCOVERY)\n6. [Probability](PROBABILITY)\n7. [Uncertainty](UNCERTAINTY)\n\nIn addition, the repository contains:\n\n1. Errata ([QSS](errata/QSSerrata.pdf), [QSStidy](errata/QSS_tidyverse_errata.pdf))\n2. [Sample course syllabi](syllabus)\n\n\n## R package `qss`\n\nThe data and code in this repository are also available as an\n[R package `qss`](https://github.com/kosukeimai/qss-package) \n(see [the package website](https://kosukeimai.github.io/qss-package/)). The code is in \nthe form of vignettes. To install this package, use the following command:\n\n    install.packages(\"devtools\") # if you have not installed devtools package already\n    devtools::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\n    \nOnce the `qss` package is installed, y",
    "url": "https://github.com/kosukeimai/qss",
    "last_updated": "2025-09-02T07:18:50+00:00"
  },
  {
    "full_name": "paultopia/scrape-ebook",
    "name": "scrape-ebook",
    "description": "scrape ebook collection of pdfs and merge.",
    "language": "Clojure",
    "topics": [],
    "readme": "Some people decide, annoyingly, to publish books online via open access licenses, but as a bunch of individual chapter-by-chapter pdf files.\n\nExample: O'Reilly makes [Managing Projects with GNU Make, by Robert Mecklenburg](http://www.oreilly.com/openbook/make3/book/index.html) available under a creative commons license, but in a bunch of individual PDF files.\n\nIf you want the whole book, you have to download them individually by hand, and then merge them by hand.  Well, no more. \n\nInstead, download a release of this, stick it on your $PATH, have a recent version of java to run it, and then just do \n\n`bookscraper http://www.oreilly.com/openbook/make3/book/index.html gnu-make-book.pdf` \n\nand the entire book will appear, as if by magic.\n\nNotes: \n\n1.  Be patient.  It will take a few seconds to fetch all the PDFs and merge them.\n\n2.  You might get a bunch of warnings about not closing pdf files.  Ignore them. \n\nThis works in the simplest possible way: it loads the page you give it, extracts every link that ends in \".pdf\", in order, saves all those pdf files in a temp directory, merges them using pdfbox, and then deletes the temp file. It's less than 50 lines of code, including whitespace.\n",
    "url": "https://github.com/paultopia/scrape-ebook",
    "last_updated": "2020-02-16T16:58:40+00:00"
  },
  {
    "full_name": "mscharkow/newsclassifier",
    "name": "newsclassifier",
    "description": "A tool for manual and automatic content analysis",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Welcome to NewsClassifier\n\nNewsClassifier is a web application for analyzing potentially large quantities of electronic documents, i.e. web pages or news feeds. NewsClassifier provides a framework for automatically retrieving news articles from RSS feeds, extracting relevant text from messy websites and archive them for later analyses. You can use NewsClassifier as a frontend for manual and automatic (using keywords or regular expressions) coding. It supports unlimited users, news sources and categories. You can run automatic reliability tests and export all results as standard CSV files.\n\n## Installation\n\nNewsClassifier is a Rails application, so installation is fairly standard. \n\n### Requirements\n\n* Ruby 1.9.2 or later (I recommend using RVM)\n* Redis/Resque\n* MySQL/PostgreSQL/Sqlite\n\nYou also need to have Python 2.6+ installed for the BTE tool.\n\n### Setup\n\nInstall the necessary bundles, create the db, go:\n\n\t$ bundle install\n\t$ vim config/database.yml\n\t$ vim config/config.yml\n\t\n\t$ rake db:create\n\n## Getting started\n\nYou can manage multiple projects with NewsClassifier, each with its own users, sources, documents and categories. The easiest way to set up a project is the quick_create method:\n\n\t$ rails console\n\tirb> Project.quick_create(title, email, subdomain)\n\nThen start the web server and resque via foreman:\n\n\t$ foreman start\n\t\nAfter creating a project and starting the server, you can access it via a subdomain, i.e. http://projectname.localhost.local You might need to update your hosts file for local testing and setup your web server in production.\n\nYou can login with the email supplied above and the password you received.\n\n## License\n\nNewsClassifier is Free Software and licensed under the Affero General Public License 3 (AGPL).\n\n### Disclaimer\n\nNewsClassifier was written as part of my PhD thesis, so it has not been widely tested. Please report bugs or fix them and send pull requests.\n\nThank you.\n\n&copy; 2008-2012 by Michael Scharkow <michael@underused.org>",
    "url": "https://github.com/mscharkow/newsclassifier",
    "last_updated": "2023-01-27T22:58:11+00:00"
  },
  {
    "full_name": "atlanhq/camelot",
    "name": "camelot",
    "description": "Camelot: PDF Table Extraction for Humans",
    "language": "Python",
    "topics": [
      "pdf",
      "table",
      "extract",
      "for-humans"
    ],
    "readme": "<p align=\"center\">\n   <img src=\"https://raw.githubusercontent.com/camelot-dev/camelot/master/docs/_static/camelot.png\" width=\"200\">\n</p>\n\n# Camelot: PDF Table Extraction for Humans\n\n[![Build Status](https://travis-ci.org/camelot-dev/camelot.svg?branch=master)](https://travis-ci.org/camelot-dev/camelot) [![Documentation Status](https://readthedocs.org/projects/camelot-py/badge/?version=master)](https://camelot-py.readthedocs.io/en/master/)\n [![codecov.io](https://codecov.io/github/camelot-dev/camelot/badge.svg?branch=master&service=github)](https://codecov.io/github/camelot-dev/camelot?branch=master)\n [![image](https://img.shields.io/pypi/v/camelot-py.svg)](https://pypi.org/project/camelot-py/) [![image](https://img.shields.io/pypi/l/camelot-py.svg)](https://pypi.org/project/camelot-py/) [![image](https://img.shields.io/pypi/pyversions/camelot-py.svg)](https://pypi.org/project/camelot-py/) [![Gitter chat](https://badges.gitter.im/camelot-dev/Lobby.png)](https://gitter.im/camelot-dev/Lobby)\n[![image](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\n\n**Camelot** is a Python library that makes it easy for *anyone* to extract tables from PDF files!\n\n**Note:** You can also check out [Excalibur](https://github.com/camelot-dev/excalibur), which is a web interface for Camelot!\n\n---\n\n**Here's how you can extract tables from PDF files.** Check out the PDF used in this example [here](https://github.com/camelot-dev/camelot/blob/master/docs/_static/pdf/foo.pdf).\n\n<pre>\n>>> import camelot\n>>> tables = camelot.read_pdf('foo.pdf')\n>>> tables\n&lt;TableList n=1&gt;\n>>> tables.export('foo.csv', f='csv', compress=True) # json, excel, html, sqlite\n>>> tables[0]\n&lt;Table shape=(7, 7)&gt;\n>>> tables[0].parsing_report\n{\n    'accuracy': 99.02,\n    'whitespace': 12.24,\n    'order': 1,\n    'page': 1\n}\n>>> tables[0].to_csv('foo.csv') # to_json, to_excel, to_html, to_sqlite\n>>> tables[0].df # get a pandas DataFrame!\n</pre>\n\n| Cycle Name | KI (1/km) | ",
    "url": "https://github.com/atlanhq/camelot",
    "last_updated": "2025-08-27T10:49:40+00:00"
  },
  {
    "full_name": "eddelbuettel/drat",
    "name": "drat",
    "description": "Drat R Archive Template",
    "language": "HTML",
    "topics": [
      "r",
      "cran",
      "repository",
      "repository-tools",
      "r-package"
    ],
    "readme": "## drat: Drat R Archive Template\n\n[![CI](https://github.com/eddelbuettel/drat/workflows/ci/badge.svg)](https://github.com/eddelbuettel/drat/actions?query=workflow%3Aci)\n[![License](https://img.shields.io/badge/license-GPL%20%28%3E=%202%29-brightgreen.svg?style=flat)](https://www.gnu.org/licenses/gpl-2.0.html)\n[![CRAN](https://www.r-pkg.org/badges/version/drat)](https://cran.r-project.org/package=drat)\n[![r-universe](https://eddelbuettel.r-universe.dev/badges/drat)](https://eddelbuettel.r-universe.dev/drat)\n[![Dependencies](https://tinyverse.netlify.app/badge/drat)](https://cran.r-project.org/package=drat)\n[![Downloads](https://cranlogs.r-pkg.org/badges/drat?color=brightgreen)](https://www.r-pkg.org/pkg/drat)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/drat)](https://github.com/eddelbuettel/drat)\n[![Documentation](https://img.shields.io/badge/documentation-is_here-blue)](https://eddelbuettel.github.io/drat/)\n[![Fork](https://img.shields.io/badge/fork-this_instead-orange)](https://github.com/drat-base/drat)\n\n> **drat**\n>\n> What cute people say when they are pissed off\n>\n> _\"Oh Drat, i lost my wallet\"_\n>\n> [Urban Dictionary](https://www.urbandictionary.com/define.php?term=drat)\n\n### Nota Bene\n\nDo **not** fork _this_ repo as a quick start towards creating your `drat`, fork [this\nrepo](https://github.com/drat-base/drat) instead. See below for more.\n\n### Background\n\nThe R package ecosystem is one of the cornerstones of the success seen by R.\nAs of July 2020, over 16000 packages are on [CRAN](https://cran.r-project.org),\nwith about one thousand more at [BioConductor](https://www.bioconductor.org).\n\nSupport for multiple repositories is built deeply into R; mostly via the\n(default) package `utils`. The\n[`update.packages`](https://www.rdocumentation.org/packages/utils/functions/update.packages)\nfunction (along with several others from the `utils` package) can be used with\nease for these three default repositories as well as many others. But it\nseeme",
    "url": "https://github.com/eddelbuettel/drat",
    "last_updated": "2025-06-18T13:07:03+00:00"
  },
  {
    "full_name": "Spandan-Madan/DeepLearningProject",
    "name": "DeepLearningProject",
    "description": "An in-depth machine learning tutorial introducing readers to a whole machine learning pipeline from scratch.",
    "language": "HTML",
    "topics": [
      "machine-learning",
      "deep-learning",
      "neural-networks",
      "tutorial"
    ],
    "readme": "![harvard-logo](http://logonoid.com/images/harvard-logo.png)\n# An end to end tutorial of a machine learning pipeline\n\nThis tutorial tries to do what most Most Machine Learning tutorials available online do not. It is not a 30 minute tutorial which teaches you how to \"Train your own neural network\" or \"Learn deep learning in under 30 minutes\". It's a full pipeline which you would need to do if you actually work with machine learning - introducing you to all the parts, and all the implementation decisions and details that need to be made. The dataset is not one of the standard sets like MNIST or CIFAR, you will make you very own dataset. Then you will go through a couple conventional machine learning algorithms, before finally getting to deep learning!\n\nIn the fall of 2016, I was a Teaching Fellow (Harvard's version of TA) for the graduate class on \"Advanced Topics in Data Science (CS209/109)\" at Harvard University. I was in-charge of designing the class project given to the students, and this tutorial has been built on top of the project I designed for the class.\n\n# UPDATE 24th October 2018\nThe tutorial has now been re-written in PyTorch thanks to Anshul Basia (https://github.com/AnshulBasia)\n\nYou can access the HTML here: https://spandan-madan.github.io/DeepLearningProject/PyTorch_version/Deep_Learning_Project-Pytorch.html\nand the IPython Notebook with the code in PyTorch here:https://github.com/Spandan-Madan/DeepLearningProject/blob/master/PyTorch_version/Deep_Learning_Project-Pytorch.ipynb\n\n\n\n# Citing if you use the work here\nIf you would like to use this work, please cite the work using the doi -\n[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.830003.svg)](http://dx.doi.org/10.5281/zenodo.830003)\n\n# Reading/Viewing the Tutorial\nTo view the project as an HTML file, visit - https://spandan-madan.github.io/DeepLearningProject/\n\n# The Code\nIf you would like to access to Code, please go through the ipython notebook `Deep_Learning_Project.ipynb`\n\n# SETUP\n\nPython\n- ",
    "url": "https://github.com/Spandan-Madan/DeepLearningProject",
    "last_updated": "2025-08-28T02:55:51+00:00"
  },
  {
    "full_name": "jakevdp/PythonDataScienceHandbook",
    "name": "PythonDataScienceHandbook",
    "description": "Python Data Science Handbook: full text in Jupyter Notebooks",
    "language": "Jupyter Notebook",
    "topics": [
      "scikit-learn",
      "numpy",
      "python",
      "jupyter-notebook",
      "matplotlib",
      "pandas"
    ],
    "readme": "# Python Data Science Handbook\n\n[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb)\n\nThis repository contains the entire [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do), in the form of (free!) Jupyter notebooks.\n\n![cover image](notebooks/figures/PDSH-cover.png)\n\n## How to Use this Book\n\n- Read the book in its entirety online at https://jakevdp.github.io/PythonDataScienceHandbook/\n\n- Run the code using the Jupyter notebooks available in this repository's [notebooks](notebooks) directory.\n\n- Launch executable versions of these notebooks using [Google Colab](http://colab.research.google.com): [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb)\n\n- Launch a live notebook server with these notebooks using [binder](https://beta.mybinder.org/): [![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb)\n\n- Buy the printed book through [O'Reilly Media](http://shop.oreilly.com/product/0636920034919.do)\n\n## About\n\nThe book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.\n\nThe book introduces the core libraries essential for working with data in Python: particularly [IPython](http://ipython.org), [NumPy](http://numpy.org), [Pandas](http://pandas.pydata.org), [Matplotlib](http://matplotlib.org), [Scikit-Learn](http://scikit-learn.org), and related packages.\nFamiliarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project,\n[",
    "url": "https://github.com/jakevdp/PythonDataScienceHandbook",
    "last_updated": "2025-09-02T09:24:58+00:00"
  },
  {
    "full_name": "google/dopamine",
    "name": "dopamine",
    "description": "Dopamine is a research framework for fast prototyping of reinforcement learning algorithms. ",
    "language": "Jupyter Notebook",
    "topics": [
      "rl",
      "ml",
      "ai",
      "google",
      "tensorflow"
    ],
    "readme": "# Dopamine\n[Getting Started](#getting-started) |\n[Docs][docs] |\n[Baseline Results][baselines] |\n[Changelist](https://google.github.io/dopamine/docs/changelist)\n\n<div align=\"center\">\n  <img src=\"https://google.github.io/dopamine/images/dopamine_logo.png\"><br><br>\n</div>\n\nDopamine is a research framework for fast prototyping of reinforcement learning\nalgorithms. It aims to fill the need for a small, easily grokked codebase in\nwhich users can freely experiment with wild ideas (speculative research).\n\nOur design principles are:\n\n* _Easy experimentation_: Make it easy for new users to run benchmark\n                          experiments.\n* _Flexible development_: Make it easy for new users to try out research ideas.\n* _Compact and reliable_: Provide implementations for a few, battle-tested\n                          algorithms.\n* _Reproducible_: Facilitate reproducibility in results. In particular, our\n                  setup follows the recommendations given by\n                  [Machado et al. (2018)][machado].\n\nDopamine supports the following agents, implemented with jax:\n\n* DQN ([Mnih et al., 2015][dqn])\n* C51 ([Bellemare et al., 2017][c51])\n* Rainbow ([Hessel et al., 2018][rainbow])\n* IQN ([Dabney et al., 2018][iqn])\n* SAC ([Haarnoja et al., 2018][sac])\n* PPO ([Schulman et al., 2017][ppo])\n\nFor more information on the available agents, see the [docs](https://google.github.io/dopamine/docs).\n\nMany of these agents also have a tensorflow (legacy) implementation, though\nnewly added agents are likely to be jax-only.\n\nThis is not an official Google product.\n\n## Getting Started\n\n\nWe provide docker containers for using Dopamine.\nInstructions can be found [here](https://google.github.io/dopamine/docker/).\n\nAlternatively, Dopamine can be installed from source (preferred) or installed\nwith pip. For either of these methods, continue reading at prerequisites.\n\n### Prerequisites\n\nDopamine supports Atari environments and Mujoco environments. Install the\nenvironments you intend to us",
    "url": "https://github.com/google/dopamine",
    "last_updated": "2025-09-01T00:46:55+00:00"
  },
  {
    "full_name": "hrbrmstr/exiv",
    "name": "exiv",
    "description": ":camera: Read and Write 'Exif' Image/Media Tags with R",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "exiv2-library",
      "exiv2",
      "exif",
      "r-cyber"
    ],
    "readme": "\n# exiv\n\n## Description\n\nRead and Write ‘Exif’, ‘ID3v1’ and ‘ID3v2’ Image/Media Tags\n\n### README FIRST\\!\\!\\!\n\nThis package shld work on macOS and Linux systems that have the\n[`exiv2`](http://www.exiv2.org/) and\n[`taglib`](https://github.com/taglib/taglib) packages installed:\n\n  - macOS: `brew install taglib exiv2`\n  - Ubuntu/Debian `sudo apt-get install libexiv2-dev exiv2 libtag1-dev`\n\nFor the time being, they need to be easily findable. It’ll be more\nrobust when the pkg is out of Alpha status.\n\nONLY “Standard” Exif TAGS ARE SUPPORTED FOR THE MOMENT.\n\nValue Exif types currently supported:\n\n  - `ascii`\n  - `long`\n  - `short`\n  - `rational`\n  - `srational`\n  - `comment`\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `read_exif`: Retrieve Exif data from an image file\n  - `set_exif`: Set Exif tag data on an image file\n  - `exif_tags`: Return a data frame of all possible Exif tags\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/exiv\")\n```\n\n## Usage\n\n``` r\nlibrary(exiv)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"exiv\")\n```\n\n    ## [1] '0.1.0'\n\n### Tags, you say?\n\nWe got your tags, *right here*:\n\n``` r\nexif_tags(filter_type=\"ascii\")\n```\n\n    ## # A tibble: 49 x 4\n    ##                              key  type\n    ##                            <chr> <chr>\n    ##  1 Exif.Image.ProcessingSoftware ascii\n    ##  2       Exif.Image.DocumentName ascii\n    ##  3   Exif.Image.ImageDescription ascii\n    ##  4               Exif.Image.Make ascii\n    ##  5              Exif.Image.Model ascii\n    ##  6           Exif.Image.Software ascii\n    ##  7           Exif.Image.DateTime ascii\n    ##  8             Exif.Image.Artist ascii\n    ##  9       Exif.Image.HostComputer ascii\n    ## 10           Exif.Image.InkNames ascii\n    ## # ... with 39 more rows, and 2 more variables: description <chr>, rname <chr>\n\n### Read exif data\n\nThis shld be an empty data frame:\n\n``` r\nr_logo <- system.file(\"extdata\", \"Rlogo.png\", package=\"exiv\")\n\nread_exif(r_log",
    "url": "https://github.com/hrbrmstr/exiv",
    "last_updated": "2025-03-22T11:13:06+00:00"
  },
  {
    "full_name": "zackchase/mxnet-the-straight-dope",
    "name": "mxnet-the-straight-dope",
    "description": "An interactive book on deep learning. Much easy, so MXNet. Wow. [Straight Dope is growing up] ---> Much of this content has been incorporated into the new Dive into Deep Learning Book available at https://d2l.ai/.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Deep Learning - The Straight Dope (*Deprecated* Please see d2l.ai)\n## This content has been moved to Dive into the Deep Learning Book freely available at https://d2l.ai/.\n\n\n\n## Abstract\nThis repo contains an\nincremental sequence of notebooks designed to teach deep learning, MXNet, and\nthe ``gluon`` interface. Our goal is to leverage the strengths of Jupyter\nnotebooks to present prose, graphics, equations, and code together in one place.\nIf we're successful, the result will be a resource that could be simultaneously\na book, course material, a prop for live tutorials, and a resource for\nplagiarising (with our blessing) useful code. To our knowledge there's no source\nout there that teaches either (1) the full breadth of concepts in modern deep\nlearning or (2) interleaves an engaging textbook with runnable code. We'll find\nout by the end of this venture whether or not that void exists for a good\nreason.\n\nAnother unique aspect of this book is its authorship process. We are\ndeveloping this resource fully in the public view and are making it available\nfor free in its entirety. While the book has a few primary authors to set the\ntone and shape the content, we welcome contributions from the community and hope\nto coauthor chapters and entire sections with experts and community members.\nAlready we've received contributions spanning typo corrections through full\nworking examples.\n\n## Implementation with Apache MXNet\nThroughout this book,\nwe rely upon MXNet to teach core concepts, advanced topics, and a full\ncomplement of applications. MXNet is widely used in production environments\nowing to its strong reputation for speed. Now with ``gluon``, MXNet's new\nimperative interface (alpha), doing research in MXNet is easy.\n\n## Dependencies\nTo run these notebooks, you'll want to build MXNet from source. Fortunately,\nthis is easy (especially on Linux) if you follow [these\ninstructions](http://mxnet.io/get_started/install.html). You'll also want to\n[install Jupyter](http://jupyter.read",
    "url": "https://github.com/zackchase/mxnet-the-straight-dope",
    "last_updated": "2025-08-21T21:55:02+00:00"
  },
  {
    "full_name": "delta-rho/trelliscope",
    "name": "trelliscope",
    "description": "Detailed Visualization of Large Complex Data in R",
    "language": "R",
    "topics": [],
    "readme": "# Trelliscope: Detailed Visualization of Large Complex Data in R\n\n[![Join the chat at https://gitter.im/delta-rho/users](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/delta-rho/users?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://travis-ci.org/delta-rho/trelliscope.svg?branch=master)](https://travis-ci.org/delta-rho/trelliscope)\n[![CRAN](http://www.r-pkg.org/badges/version/trelliscope)](https://cran.r-project.org/package=trelliscope)\n\nTrelliscope is an R package to be used in conjunction with [datadr](https://github.com/delta-rho/datadr) and [RHIPE](https://github.com/delta-rho/RHIPE) to provide a framework for detailed visualization of large complex data.\n\n## Installation\n\n```r\n# from CRAN:\ninstall.packages(\"trelliscope\")\n\n# from github:\ndevtools::install_github(\"delta-rho/datadr\")\ndevtools::install_github(\"delta-rho/trelliscope\")\n```\n\n## Tutorial\n\nTo get started, see the package documentation and function reference located [here](http://deltarho.org/docs-trelliscope/).\n\n## License\n\nThis software is currently under the BSD license.\n\n## Acknowledgement\n\nTrelliscope development is sponsored by the DARPA XDATA program.\n",
    "url": "https://github.com/delta-rho/trelliscope",
    "last_updated": "2024-06-08T23:24:46+00:00"
  },
  {
    "full_name": "dennis6p/adaptive-cards-py",
    "name": "adaptive-cards-py",
    "description": "A user-friendly python library for building adaptive cards easily on code level ",
    "language": "Python",
    "topics": [
      "adaptive-cards",
      "adaptivecards",
      "python",
      "ui",
      "bot",
      "message"
    ],
    "readme": "# Adaptive Cards <!-- no toc --> \n\n- [Adaptive Cards ](#adaptive-cards-)\n  - [About](#about)\n  - [Features](#features)\n  - [Dependencies](#dependencies)\n  - [Installation](#installation)\n  - [Library structure](#library-structure)\n  - [Usage](#usage)\n    - [Create a card](#create-a-card)\n      - [A simple card](#a-simple-card)\n      - [A more complex card](#a-more-complex-card)\n    - [Update card components](#update-card-components)\n    - [Validate a card](#validate-a-card)\n    - [Send card to MS Teams](#send-card-to-ms-teams)\n  - [Examples](#examples)\n  - [Feature Roadmap](#feature-roadmap)\n  - [Contribution](#contribution)\n  - [Glossary](#glossary)\n\n[![PyPI version](https://badge.fury.io/py/adaptive-cards-py.svg)](https://pypi.org/project/adaptive-cards-py/)\n\nA thin Python wrapper for creating [**Adaptive Cards**](https://adaptivecards.io/) easily on code level. The deep integration of Python's `typing` package alongside the famous `pydantic` library prevents you from creating invalid schemas and guides you while setting up the code for generating visually appealing cards.\n\nIf you are interested in the general concepts of adaptive cards and want to dig a bit deeper, have a look into the [**official documentation**](https://learn.microsoft.com/en-us/adaptive-cards/) or get used to the [**schema**](https://adaptivecards.io/explorer/) first.\n\n💡 **Please note**\n<br>This library is work in progress. Missing parts are planned to be added from time to time.\n\n## About\n\nThis library is intended to provide a clear and simple interface for creating adaptive cards with only a few lines of code in a more robust way. The heavy usage of Python's `typing` mechanisms and the `pydantic` library should prevent one from creating invalid schemes and structures. Instead, creating and sending cards should be intuitive and supported by the typing system.\n\nFor a comprehensive introduction into the main ideas and patterns of adaptive cards, head over to the [**official documentation**](htt",
    "url": "https://github.com/dennis6p/adaptive-cards-py",
    "last_updated": "2025-08-07T07:41:04+00:00"
  },
  {
    "full_name": "iv-org/invidious",
    "name": "invidious",
    "description": "Invidious is an alternative front-end to YouTube",
    "language": "Crystal",
    "topics": [
      "invidious",
      "watch",
      "agplv3",
      "youtube",
      "video",
      "youtube-video",
      "libre",
      "hacktoberfest"
    ],
    "readme": "<div align=\"center\">\n  <img src=\"assets/invidious-colored-vector.svg\" width=\"192\" height=\"192\" alt=\"Invidious logo\">\n  <h1>Invidious</h1>\n\n  <a href=\"https://www.gnu.org/licenses/agpl-3.0.en.html\">\n    <img alt=\"License: AGPLv3\" src=\"https://shields.io/badge/License-AGPL%20v3-blue.svg\">\n  </a>\n  <a href=\"https://github.com/iv-org/invidious/actions\">\n    <img alt=\"Build Status\" src=\"https://github.com/iv-org/invidious/workflows/Invidious%20CI/badge.svg\">\n  </a>\n  <a href=\"https://github.com/iv-org/invidious/commits/master\">\n    <img alt=\"GitHub commits\" src=\"https://img.shields.io/github/commit-activity/y/iv-org/invidious?color=red&label=commits\">\n  </a>\n  <a href=\"https://github.com/iv-org/invidious/issues\">\n    <img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues/iv-org/invidious?color=important\">\n  </a>\n  <a href=\"https://github.com/iv-org/invidious/pulls\">\n    <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/iv-org/invidious?color=blueviolet\">\n  </a>\n  <a href=\"https://hosted.weblate.org/engage/invidious/\">\n    <img alt=\"Translation Status\" src=\"https://hosted.weblate.org/widgets/invidious/-/translations/svg-badge.svg\">\n  </a>\n\n  <a href=\"https://github.com/humanetech-community/awesome-humane-tech\">\n    <img alt=\"Awesome Humane Tech\" src=\"https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/main/humane-tech-badge.svg?sanitize=true\">\n  </a>\n\n  <h3>An open source alternative front-end to YouTube</h3>\n\n  <a href=\"https://invidious.io/\">Website</a>\n  &nbsp;•&nbsp;\n  <a href=\"https://instances.invidious.io/\">Instances list</a>\n  &nbsp;•&nbsp;\n  <a href=\"https://docs.invidious.io/faq/\">FAQ</a>\n  &nbsp;•&nbsp;\n  <a href=\"https://docs.invidious.io/\">Documentation</a>\n  &nbsp;•&nbsp;\n  <a href=\"#contribute\">Contribute</a>\n  &nbsp;•&nbsp;\n  <a href=\"https://invidious.io/donate/\">Donate</a>\n\n  <h5>Chat with us:</h5>\n  <a href=\"https://matrix.to/#/#invidious:matrix.org\">\n    <img alt=\"Matrix\" src=\"https://img.sh",
    "url": "https://github.com/iv-org/invidious",
    "last_updated": "2025-09-02T09:57:15+00:00"
  },
  {
    "full_name": "18F/open-data-maker",
    "name": "open-data-maker",
    "description": "make it easy to turn a lot of potentially large csv files into easily accessible open data",
    "language": "Ruby",
    "topics": [],
    "readme": "# Open Data Maker\n[![Build Status](https://circleci.com/gh/18F/open-data-maker/tree/dev.svg?style=svg)](https://circleci.com/gh/18F/open-data-maker/tree/dev)\n\nThe goal of this project is to make it easy to turn a lot of potentially large\ncsv files into open data via an API and the ability for people to download\nsmaller csv files with a subset of the data.\n\nPreliminary research suggests that open data users (journalists and others)\nactually know how to work with spreadsheets really well, but a lot of the\ndata sets that we have in government are huge.\n\nThe first version of this project will allow us to host a website for an\nagency with a specific set of csv files, which are deployed with the app.\nThis will allows us to deploy more quickly since there will be a lower risk\nsecurity profile than if an agency could upload the CSV files (which might\nbe a nice longer term feature).\n\n\n## Install and Run the App (as a developer)\n\nSee our [Installation Guide](INSTALL.md)\n\n## How this works\n\nBy default, data will be loaded from /sample-data when you run `rake import`\n\n* [cities100.csv](sample-data/cities100.csv) - dataset of 100 most populous cities in the US\n* [data.yaml](sample-data/data.yaml) - configuration for\n  * index name *city-data*\n  * api endpoint name *cities*\n  * how columns are mapped to fields in json output\n  * data types\n  * unique columns *name*  \n\nWhen you run the app, you can query the dataset via json API, like: /cities?name=Chicago\n\n* http://localhost:3000/cities?name=Chicago\n* http://localhost:3000/cities?name=Chicago&state=IL\n* http://localhost:3000/cities?state=NY,MA\n* http://localhost:3000/cities?state=CA&fields=name,size\n\nTo use your own data, you can set a different directory, for example:\n\n```\nexport DATA_PATH='./data'\n```\n\n1. Put csv files into /data\n1. Import files from /data: ```rake import``` (or restart the app)\n   1. There can be multiple files (must end in .csv)\n   1. Optional [data.yaml](sample-data/data.yaml) file that specifies  index name",
    "url": "https://github.com/18F/open-data-maker",
    "last_updated": "2025-03-02T02:55:49+00:00"
  },
  {
    "full_name": "dennisbakhuis/pigeonXT",
    "name": "pigeonXT",
    "description": "🐦 Quickly annotate data from the comfort of your Jupyter notebook",
    "language": "Python",
    "topics": [],
    "readme": "# 🐦 pigeonXT - Quickly annotate data in Jupyter Lab\nPigeonXT is an extention to the original [Pigeon](https://github.com/agermanidis/pigeon), created by [Anastasis Germanidis](https://pypi.org/user/agermanidis/).\nPigeonXT is a simple widget that lets you quickly annotate a dataset of\nunlabeled examples from the comfort of your Jupyter notebook.\n\nPigeonXT currently support the following annotation tasks:\n- binary / multi-class classification\n- multi-label classification\n- regression tasks\n- captioning tasks\n\nAnything that can be displayed on Jupyter\n(text, images, audio, graphs, etc.) can be displayed by pigeon\nby providing the appropriate `display_fn` argument.\n\nAdditionally, custom hooks can be attached to each row update (`example_process_fn`),\nor when the annotating task is complete(`final_process_fn`).\n\nThere is a full blog post on the usage of PigeonXT on [Towards Data Science](https://towardsdatascience.com/quickly-label-data-in-jupyter-lab-999e7e455e9e).\n\n### Contributors\n- Anastasis Germanidis\n- Dennis Bakhuis\n- Ritesh Agrawal\n- Deepak Tunuguntla\n- Bram van Es\n\n## Installation\nPigeonXT obviously needs a Jupyter Lab environment. Futhermore, it requires ipywidgets.\nThe widget itself can be installed using pip:\n```bash\n    pip install pigeonXT-jupyter\n```\n\nCurrently, it is much easier to install due to Jupyterlab 3:\nTo run the provided examples in a new environment using Conda:\n```bash\n    conda create --name pigeon python=3.9\n    conda activate pigeon\n    pip install numpy pandas jupyterlab ipywidgets pigeonXT-jupyter\n```\n\nFor an older Jupyterlab or any other trouble, please try the old method:\n```bash\n    conda create --name pigeon python=3.7\n    conda activate pigeon\n    conda install nodejs\n    pip install numpy pandas jupyterlab ipywidgets\n    jupyter nbextension enable --py widgetsnbextension\n    jupyter labextension install @jupyter-widgets/jupyterlab-manager\n\n    pip install pigeonXT-jupyter\n```\n\nStarting Jupyter Lab environment:\n```bash\n    jupyter lab",
    "url": "https://github.com/dennisbakhuis/pigeonXT",
    "last_updated": "2025-09-02T08:43:59+00:00"
  },
  {
    "full_name": "jamesfeigenbaum/extract_bib",
    "name": "extract_bib",
    "description": "Limit .bib file based on citations in a .tex file",
    "language": "R",
    "topics": [],
    "readme": "# extract_bib\n## feigenbaum\n## 24feb2017\n\n### Introduction\n\n`extract_bib` is a simple `R` script to create minimal a `bibtex` library file for an article. Some journals require `tex` and `bib` files with submissions. However, I have only one *master* `bib` file. That file currently contains 3543 references. Rather than send all of that to the journal to help typeset my manuscript, I only want to send a `bib` file with the references included in my article. This script does that.\n\n### Work flow\n\n- Read a `tex` file\n- Extract all of the citations\n- Read a master `bib` file\n- Match the citations from the `tex` file to the master library `bib` file\n- Output a minimal library for submission\n\n### To Do\n\n- I use `natbib` and so the citations I search for in the `tex` file are only `citet{}` and `citep{}`. Expand this to other citation functions.\n- I remove some crud from the library put there by my citation manager ([Mendeley](https://www.mendeley.com/)), including annotations and file locations, as well as abstracts. If there are other unneeded fields, they can be removed from the minimal library output.\n- Expand to work with multiple input libraries or multiple input `tex` files?\n",
    "url": "https://github.com/jamesfeigenbaum/extract_bib",
    "last_updated": "2021-11-29T02:21:01+00:00"
  },
  {
    "full_name": "ropensci/software-review",
    "name": "software-review",
    "description": "rOpenSci Software Peer Review. ",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-packages",
      "peer-reviews",
      "peer-review",
      "software-quality"
    ],
    "readme": "\n![](icon_lettering_color.png)\n\n# rOpenSci Software Peer Review\n\n*[README en español](README.es.md)*\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\nThank you for considering submitting your package to the rOpenSci suite.\nAll the packages contributed by community members go through a process\nof [open peer\nreview](https://ropensci.org/blog/2017/09/01/nf-softwarereview/) to\nensure a consistent level of quality for our users. This process also\nallows us to ensure that your package meets our guidelines and provides\nopportunity for discussion where exceptions are requested.\n\nThis README is a short intro to Software Peer Review for you as a\npotential author or reviewer. For more information, consult our [gitbook\n“rOpenSci Packages: Development, Maintenance, and Peer\nReview”](https://devguide.ropensci.org/).\n\n**Our [code of conduct](https://ropensci.org/code-of-conduct/) is\nmandatory for everyone involved in our review process.**\n\n- [Why and how submit your package to rOpenSci?](#why-submit)\n- [Why and how review for rOpenSci?](#why-review)\n- [Further resources](#further)\n- [Editors and reviewers](#editors)\n\n# <a href=\"#why-submit\" name=\"why-submit\"></a>Why and how submit your package to rOpenSci?\n\n- First, and foremost, we hope you submit your package for review\n  **because you value the feedback**. We aim to provide useful feedback\n  to package authors and for our review process to be open,\n  non-adversarial, and focused on improving software quality.\n- Once aboard, your package will continue to receive **support from\n  rOpenSci members**. You’ll retain ownership and control of your\n  package, but we can help with ongoing maintenance issues such as those\n  associated with updates to R and dependencies and CRAN policies.\n- rOpenSci will **promote your package** through our\n  [webpage](https://ropensci.org/packages/),\n  [blog](https://ropensci.org/blog/), and [social\n  media](https://twitter.com/ropensci). Packages in our suite also get a\n  [docume",
    "url": "https://github.com/ropensci/software-review",
    "last_updated": "2025-08-28T14:21:46+00:00"
  },
  {
    "full_name": "castorini/howl",
    "name": "howl",
    "description": "Wake word detection modeling toolkit for Firefox Voice, supporting open datasets like Speech Commands and Common Voice.",
    "language": "Python",
    "topics": [
      "wake-word-detection"
    ],
    "readme": "# Howl\n\n[![PyPI](https://img.shields.io/pypi/v/howl?color=brightgreen)](https://pypi.org/project/howl/)\n[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0)\n\nWake word detection modeling for Firefox Voice, supporting open datasets like Google Speech Commands and Mozilla Common Voice.\n\nCitation:\n\n```\n@inproceedings{tang-etal-2020-howl,\n    title = \"Howl: A Deployed, Open-Source Wake Word Detection System\",\n    author = \"Tang, Raphael and Lee, Jaejun and Razi, Afsaneh and Cambre, Julia and Bicking, Ian and Kaye, Jofish and Lin, Jimmy\",\n    booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\",\n    month = nov,\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.nlposs-1.9\",\n    doi = \"10.18653/v1/2020.nlposs-1.9\",\n    pages = \"61--65\"\n}\n```\n\n## Training Guide\n\n### Installation\n\n1. `git clone https://github.com/castorini/howl && cd howl`\n\n2. Install [PyTorch](https://pytorch.org) by following your platform-specific instructions.\n\n3. Install PyAudio and its dependencies through your distribution's package system.\n\n4. `pip install -r requirements.txt -r requirements_training.txt` (some apt packages might need to be installed)\n\n5. `./download_mfa.sh` to setup montreal forced aligner (MFA) for dataset generation\n\n### Preparing a Dataset\n\nGenerating a dataset for a custom wakeword requires three steps:\n1. Generating raw audio dataset that howl can load from open datasets\n2. Generate orthographic transcription alignments for each audio file.\n3. Attach the alignment to the raw audio dataset generated in step 1.\n\nHaving said that we recommend [Common Voice dataset](https://commonvoice.mozilla.org/) for the open audio datasets and [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/stable/installation.html) (MFA) for the transcription alignment.\nDownloading MFA can be achieved",
    "url": "https://github.com/castorini/howl",
    "last_updated": "2025-08-21T06:04:51+00:00"
  },
  {
    "full_name": "simonw/sqlite-utils",
    "name": "sqlite-utils",
    "description": "Python CLI utility and library for manipulating SQLite databases",
    "language": "Python",
    "topics": [
      "sqlite",
      "python",
      "datasette",
      "sqlite-database",
      "click",
      "cli",
      "datasette-io",
      "datasette-tool"
    ],
    "readme": "# sqlite-utils\n\n[![PyPI](https://img.shields.io/pypi/v/sqlite-utils.svg)](https://pypi.org/project/sqlite-utils/)\n[![Changelog](https://img.shields.io/github/v/release/simonw/sqlite-utils?include_prereleases&label=changelog)](https://sqlite-utils.datasette.io/en/stable/changelog.html)\n[![Python 3.x](https://img.shields.io/pypi/pyversions/sqlite-utils.svg?logo=python&logoColor=white)](https://pypi.org/project/sqlite-utils/)\n[![Tests](https://github.com/simonw/sqlite-utils/workflows/Test/badge.svg)](https://github.com/simonw/sqlite-utils/actions?query=workflow%3ATest)\n[![Documentation Status](https://readthedocs.org/projects/sqlite-utils/badge/?version=stable)](http://sqlite-utils.datasette.io/en/stable/?badge=stable)\n[![codecov](https://codecov.io/gh/simonw/sqlite-utils/branch/main/graph/badge.svg)](https://codecov.io/gh/simonw/sqlite-utils)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/sqlite-utils/blob/main/LICENSE)\n[![discord](https://img.shields.io/discord/823971286308356157?label=discord)](https://discord.gg/Ass7bCAMDw)\n\nPython CLI utility and library for manipulating SQLite databases.\n\n## Some feature highlights\n\n- [Pipe JSON](https://sqlite-utils.datasette.io/en/stable/cli.html#inserting-json-data) (or [CSV or TSV](https://sqlite-utils.datasette.io/en/stable/cli.html#inserting-csv-or-tsv-data)) directly into a new SQLite database file, automatically creating a table with the appropriate schema\n- [Run in-memory SQL queries](https://sqlite-utils.datasette.io/en/stable/cli.html#querying-data-directly-using-an-in-memory-database), including joins, directly against data in CSV, TSV or JSON files and view the results\n- [Configure SQLite full-text search](https://sqlite-utils.datasette.io/en/stable/cli.html#configuring-full-text-search) against your database tables and run search queries against them, ordered by relevance\n- Run [transformations against your tables](https://sqlite-utils.datasette.io/en/stable/cli.ht",
    "url": "https://github.com/simonw/sqlite-utils",
    "last_updated": "2025-08-30T07:34:40+00:00"
  },
  {
    "full_name": "hzhangxyz/gpt-bot",
    "name": "gpt-bot",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# gpt-bot\n\nA GPT Command line interface bot.\n\nThis is only for gpt-3.5-turbo currently.\nYou need to set `OPENAI_API_KEY` in environment variable.\nAnd maybe `OPENAI_PROXY` is also useful if you need proxy.\n\n## Usage\n\n`python -m gpt_bot` and enjoy.\n\n## Command\n\n- `/multiline`: toggle multiline input.\n- `/prompt`: change [system content](https://platform.openai.com/docs/guides/chat) during chatting.\n- `/rollback`: rollback the last conversation.\n- `/history`: show chat history.\n- `/edit`: edit what bot said just now.\n- `/record`: use openai's [whister](https://platform.openai.com/docs/guides/speech-to-text) to transcribe what you are saying.\n- `/quit`: quit.\n",
    "url": "https://github.com/hzhangxyz/gpt-bot",
    "last_updated": "2025-01-15T15:29:48+00:00"
  },
  {
    "full_name": "timvisee/ffsend",
    "name": "ffsend",
    "description": ":mailbox_with_mail: Easily and securely share files from the command line. A fully featured Firefox Send client.",
    "language": "Rust",
    "topics": [
      "firefox-send",
      "cli",
      "file-sharing",
      "file-upload",
      "encryption",
      "rust",
      "hacktoberfest"
    ],
    "readme": "[![Build status on GitLab CI][gitlab-ci-master-badge]][gitlab-ci-link]\n[![Newest release on crates.io][crate-version-badge]][crate-link]\n[![Project license][crate-license-badge]](LICENSE)\n\n[crate-license-badge]: https://img.shields.io/crates/l/ffsend.svg\n[crate-link]: https://crates.io/crates/ffsend\n[crate-version-badge]: https://img.shields.io/crates/v/ffsend.svg\n[gitlab-ci-link]: https://gitlab.com/timvisee/ffsend/pipelines\n[gitlab-ci-master-badge]: https://gitlab.com/timvisee/ffsend/badges/master/pipeline.svg\n\n*Notice: the default Send host is provided by [@timvisee][timvisee]\n([info](https://gitlab.com/timvisee/ffsend/-/issues/111)).\nPlease consider to [donate] and help keep it running.*\n\n# ffsend\n\n> Easily and securely share files from the command line.\n> A [Send][send] client.\n\nEasily and securely share files and directories from the command line through a\nsafe, private and encrypted link using a single simple command.\nFiles are shared using the [Send][send] service and may be up\nto 1GB. Others are able to download these files with this tool, or through\ntheir web browser.\n\n[![ffsend usage demo][usage-demo-svg]][usage-demo-asciinema]  \n_No demo visible here? View it on [asciinema][usage-demo-asciinema]._\n\nAll files are always encrypted on the client, and secrets are never shared with\nthe remote host. An optional password may be specified, and a default file\nlifetime of 1 (up to 20) download or 24 hours is enforced to ensure your stuff\ndoes not remain online forever.\nThis provides a secure platform to share your files.\nFind out more about security [here](#security).\n\n- [Features](#features)\n- [Usage](#usage)\n- [Requirements](#requirements)\n- [Install](#install) ([Linux](#linux-all-distributions), [macOS](#macos), [Windows](#windows), [FreeBSD](#freebsd), [Android](#android), [_Other OS/arch_](#other-os-or-architecture))\n- [Build](#build)\n- [Configuration and environment](#configuration-and-environment)\n- [Security](#security)\n- [Help](#help)\n- [Special thanks](#",
    "url": "https://github.com/timvisee/ffsend",
    "last_updated": "2025-09-01T11:20:54+00:00"
  },
  {
    "full_name": "hrbrmstr/decapitated",
    "name": "decapitated",
    "description": "Headless 'Chrome' Orchestration in R",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "headless-chrome",
      "web-scraping",
      "javascript",
      "r-cyber"
    ],
    "readme": "[![Project Status: Inactive – The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](https://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/#inactive)\n\nNOTE: I am putting my support behind the [{`crrri`} package](https://github.com/RLesur/crrri) and you should, too.\n\n------------\n\n# decapitated\n\nHeadless ‘Chrome’ Orchestration\n\n## Description\n\nThe ‘Chrome’ browser <https://www.google.com/chrome/> has a headless\nmode which can be instrumented programmatically. Tools are provided to\nperform headless ‘Chrome’ instrumentation on the command-line, including\nretrieving the javascript-executed web page, PDF output or screen shot\nof a URL.\n\n## IMPORTANT\n\nYou'll need to set an envrionment variable `HEADLESS_CHROME` to use this package.\n\nIf this value is not set, a location heuristic is used on package start which looks\nfor the following depending on the operating system:\n\n- Windows(32bit): `C:/Program Files/Google/Chrome/Application/chrome.exe`\n- Windows(64bit): `C:/Program Files (x86)/Google/Chrome/Application/chrome.exe`\n- macOS: `/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome`\n- Linux: `/usr/bin/google-chrome`\n\nIf a verification test fails, you will be notified. \n\n**It is HIGHLY recommended** that you use `decapitated::download_chromium()` to use\na standalone version of Chrome with this packge for your platform. \n\nIt's best to use `~/.Renviron` to store this value.\n\n## Working around headless Chrome & OS security restrictions:\n\nSecurity restrictions on various operating systems and OS configurations\ncan cause headless Chrome execution to fail. As a result, headless\nChrome operations should use a special directory for `decapitated`\npackage operations. You can pass this in as `work_dir`. If `work_dir` is\n`NULL` a `.rdecapdata` directory will be created in your home directory\nand used for the data, crash dumps and utility directories for Chrome",
    "url": "https://github.com/hrbrmstr/decapitated",
    "last_updated": "2025-03-22T11:16:59+00:00"
  },
  {
    "full_name": "tidwall/pogocache",
    "name": "pogocache",
    "description": "Fast caching software with a focus on low latency and cpu efficiency.",
    "language": "C",
    "topics": [],
    "readme": "<p align=\"center\">\n<img alt=\"Pogocache\" src=\".github/images/logo.png\" width=\"600\">\n</p>\n\nPogocache is fast caching software built from scratch with a focus on low latency and cpu efficency.\n\n**Faster**: Pogocache is faster than Memcache, Valkey, Redis, Dragonfly, and Garnet.\nIt has the lowest latency per request, providing the quickest response times.\nIt's optimized to scale from one to many cores, giving you the best single-threaded and multithreaded performance.\n\n**Cheaper**: Pogocache uses the fewest cpu cycles per request; minimizing server load, energy usage, and the overall cost to operate.\n\n**Easier**: Pogocache runs as a server-based program.\nIt supports Memcache, Valkey/Redis, HTTP, and Postgres wire protocols,\nallowing for the use of system tools such as curl and psql, and numerous client libraries that are available for most programming languages.\n\n**Embeddable**: Optionally instead of running Pogocache as a server-based program, \nthe self-contained pogocache.c file can be compiled into existing software,\nbypassing the network and directly accessing the cache programmatically.\nRunning embedded provides raw speed, with over 100M ops per second.\n\n---\n\n<!--\nCache       Throughput  Latency     CPU Cycles\n----------------------------------------------\nredis           909837    447μs          11799\nvalkey         1331683    263μs          15751\ndragonfly      1407584    255μs          14384\ngarnet         1548050    231μs          13396\nmemcache       2601306    191μs          13400\npogocache      3145960    111μs           6968\n-->\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"/.github/images/graphs-dark.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"/.github/images/graphs-light.png\">\n  <img alt=\"Graphs\" src=\"/.github/images/graphs-light.png\">\n</picture>\n\nThe above benchmarks measure the performance of Pogocache against other caching\nsoftware running 8 threads on an AWS c8g.8xlarge.  \nVisit https://github.com/tidwall/cache-be",
    "url": "https://github.com/tidwall/pogocache",
    "last_updated": "2025-09-02T10:10:55+00:00"
  },
  {
    "full_name": "magic-wormhole/magic-wormhole",
    "name": "magic-wormhole",
    "description": "get things from one computer to another, safely",
    "language": "Python",
    "topics": [],
    "readme": "# Magic Wormhole\n[![PyPI](http://img.shields.io/pypi/v/magic-wormhole.svg)](https://pypi.python.org/pypi/magic-wormhole)\n![Tests](https://github.com/magic-wormhole/magic-wormhole/workflows/Tests/badge.svg)\n[![Windows Build Status](https://ci.appveyor.com/api/projects/status/w1bdniovwm4egfyg/branch/master?svg=true)](https://ci.appveyor.com/project/warner/magic-wormhole)\n[![codecov.io](https://codecov.io/github/magic-wormhole/magic-wormhole/coverage.svg?branch=master)](https://codecov.io/github/magic-wormhole/magic-wormhole?branch=master)\n[![Docs](https://readthedocs.org/projects/magic-wormhole/badge/?version=latest)](https://magic-wormhole.readthedocs.io)\n[![Irc](https://img.shields.io/badge/irc.libera.chat-%23magic--wormhole-brightgreen)](https://web.libera.chat/)\n[![Matrix](https://img.shields.io/badge/matrix.org-%23magic--wormhole-brightgreen)](https://matrix.to/#/#magic-wormhole:matrix.org)\n\n\nGet things from one computer to another, safely.\n\nThis package provides a library and a command-line tool named `wormhole`,\nwhich makes it possible to get arbitrary-sized files and directories\n(or short pieces of text) from one computer to another. The two endpoints are\nidentified by using identical \"wormhole codes\": in general, the sending\nmachine generates and displays the code, which must then be typed into the\nreceiving machine.\n\nThe codes are short and human-pronounceable, using a phonetically-distinct\nwordlist. The receiving side offers tab-completion on the codewords, so\nusually only a few characters must be typed. Wormhole codes are single-use\nand do not need to be memorized.\n\n* PyCon 2016 presentation: [Slides](http://www.lothar.com/~warner/MagicWormhole-PyCon2016.pdf), [Video](https://youtu.be/oFrTqQw0_3c)\n\nFor complete documentation, please see https://magic-wormhole.readthedocs.io\nor the docs/ subdirectory.\n\nThis program uses two servers, whose source code is kept in separate\nrepositories: the\n[mailbox server](https://github.com/magic-wormhole/magic-wormhole-mail",
    "url": "https://github.com/magic-wormhole/magic-wormhole",
    "last_updated": "2025-09-02T09:37:05+00:00"
  },
  {
    "full_name": "gojiplus/image-to-text",
    "name": "image-to-text",
    "description": "Images of Text to Text: Call Tesseract from Python and OCR a directory of pdfs",
    "language": "Python",
    "topics": [],
    "readme": "### Image to Text\n\nThe script uses [Tesseract](https://github.com/tesseract-ocr) to get text from pdfs. It reads pdfs from a specified directory and outputs text files to another directory. Tesseract works well for documents with simple structure and fonts that are easily parsed but generally struggles with more complex layout. To fix errors in the recovered text, you may want to use [Edit Distance Based Search and Replace](https://github.com/soodoku/search-and-replace), exploiting the fact that errors in OCR tend of systematic. \n\nRather than use Tesseract, you can also use [Abbyy FineReader](https://github.com/soodoku/abbyyR) or [Captricity](https://github.com/soodoku/captr). And to estimate the error rate of OCR, you may want to use [recognize](https://github.com/soodoku/recognize).\n\nFor a general overview of how to convert paper to digitial and how to optimize that process, see [A Quick Scan: From Paper to Digital](http://gbytes.gsood.com/2014/05/28/a-quick-scan-from-paper-to-digital-data/)\n\n#### Usage\n\n`pdf2txt.py [options] pdf_directory`\n\n#### Command Line Options:\n```\n  -h, --help            show this help message and exit\n  -d DPI, --dpi=DPI     JPEG Resolution in DPI (default: 400)\n  -j JPGDIR, --jpgdir=JPGDIR\n                        JPEG output directory (default: jpg)\n  -t TXTDIR, --textdir=TXTDIR\n                        Text output directory (default: text)\n  -r, --resume          Resume OCR to Text (default: False)\n```            \n\n#### Example:\n`python pdf2txt.py pdf_dir`\n\nThe script will be post process all PDF files in `pdf_dir` directory and save the output text files to the `text` directory\n\n### License\nScripts are released under the [MIT License](https://opensource.org/licenses/MIT).\n",
    "url": "https://github.com/gojiplus/image-to-text",
    "last_updated": "2025-04-16T22:10:02+00:00"
  },
  {
    "full_name": "ypeleg/HungaBunga",
    "name": "HungaBunga",
    "description": "HungaBunga: Brute-Force all sklearn models with all parameters using .fit .predict!",
    "language": "Python",
    "topics": [
      "scikit-learn",
      "brute",
      "force",
      "machine",
      "learning",
      "kaggle",
      "automl",
      "sklearn",
      "fit",
      "predict"
    ],
    "readme": "\n\nHunga-Bunga\n============\n\nBrute Force all scikit-learn models and all scikit-learn parameters with **fit** **predict**.\n\n\n\n-----\n##### Lets brute force all sklearn models with all of sklearn parameters!  Ahhh Hunga Bunga!!\n\n```python\nfrom hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\n```\n\n##### And then simply: \n\n<p align=\"center\">\n  <img src=\"https://github.com/ypeleg/HungaBunga/blob/master/HungaBunga.png?raw=true\" width=\"400\">\n</p>\n\n-----\n\n\n\n#### What?\nYes.\n\n#### No! Really! What?\nMany believe that\n\n> most of the work of supervised (non-deep) Machine Learning lies in feature engineering, whereas the model-selection process is just running through all the models or just take xgboost.\n\nSo here is an automation for that.\n\n## HOW IT WORKS\nRuns through all `sklearn` models (both classification and regression), with **all possible hyperparameters**, and rank using cross-validation.\n\n## MODELS\nRuns **all the model** available on `sklearn` for supervised learning [here](http://scikit-learn.org/stable/supervised_learning.html). The categories are:\n\n* Generalized Linear Models\n* Kernel Ridge\n* Support Vector Machines\n* Nearest Neighbors\n* Gaussian Processes\n* Naive Bayes\n* Trees\n* Neural Networks\n* Ensemble methods\n\nNote: Some models were dropped out (nearly none of them..) and some crash or cause exceptions from time to time. It takes REALLY long to test this out so clearing exceptions took me a while.\n\n## Installation \n\n```python\npip install hunga-bunga\n```\n\nDependencies\n~~~~~~~~~~~~\n\n- Python (>= 2.7)\n- NumPy (>= 1.11.0)\n- SciPy (>= 0.17.0)\n- joblib (>= 0.11)\n- scikit-learn (>=0.20.0)\n- tabulate (>=0.8.2)\n- tqdm (>=4.28.1)\n\n~~~~~~~~~~~~\n\n\n\n## Option I (Recommended): brain = False\n\n\nAs any other sklearn model \n\n```python\nclf = HungaBungaClassifier()\nclf.fit(x, y)\nclf.predict(x)\n```\n    \nAnd import from here\n\n```python\nfrom hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\n```\n\n## Option II: brain = True\n\n\nAs any other sklearn model \n\n```\nclf = ",
    "url": "https://github.com/ypeleg/HungaBunga",
    "last_updated": "2025-08-23T07:50:47+00:00"
  },
  {
    "full_name": "justingrimmer/CausalInf",
    "name": "CausalInf",
    "description": "This the repository for the Spring Causal Inference Course",
    "language": "TeX",
    "topics": [],
    "readme": "# CausalInf\n",
    "url": "https://github.com/justingrimmer/CausalInf",
    "last_updated": "2025-08-19T13:56:37+00:00"
  },
  {
    "full_name": "babylonhealth/fastText_multilingual",
    "name": "fastText_multilingual",
    "description": "Multilingual word vectors in 78 languages",
    "language": "Jupyter Notebook",
    "topics": [
      "word-vectors",
      "machine-learning",
      "machine-translation",
      "natural-language-processing",
      "nlp",
      "distributed-representations"
    ],
    "readme": "> **Note**\r\n> This repository is no longer actively maintained by Babylon Health. For further assistance, reach out to the paper authors.\r\n\r\n# Aligning the fastText vectors of 78 languages\r\n\r\nFacebook recently open-sourced word vectors in [89 languages](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md). However these vectors are monolingual; meaning that while similar words within a language share similar vectors, translation words from different languages do not have similar vectors. In [a recent paper at ICLR 2017](https://arxiv.org/abs/1702.03859), we showed how the SVD can be used to learn a linear transformation (a matrix), which aligns monolingual vectors from two languages in a single vector space. In this repository we provide 78 matrices, which can be used to align the majority of the fastText languages in a single space.\r\n\r\nThis readme explains how the matrices should be used. We also present a simple evaluation task, where we show we are able to successfully predict the translations of words in multiple languages. Our procedure relies on collecting bilingual training dictionaries of word pairs in two languages, but remarkably we are able to successfully predict the translations of words between language pairs for which we had no training dictionary!\r\n\r\nWord embeddings define the similarity between two words by the normalised inner product of their vectors. The matrices in this repository place languages in a single space, **without changing any of these monolingual similarity relationships**. When you use the resulting multilingual vectors for monolingual tasks, they will perform exactly the same as the original vectors. To learn more about word embeddings, check out [Colah's blog](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) or [Sam's introduction to vector representations](https://web.archive.org/web/20201111222742/https://www.samtalksml.net/aligning-vector-representations/). \r\n\r\nNote that since we releas",
    "url": "https://github.com/babylonhealth/fastText_multilingual",
    "last_updated": "2025-08-27T08:24:10+00:00"
  },
  {
    "full_name": "grf-labs/grf",
    "name": "grf",
    "description": "Generalized Random Forests ",
    "language": "C++",
    "topics": [
      "causal-inference",
      "random-forest",
      "econometrics",
      "machine-learning",
      "statistics",
      "causal-forest"
    ],
    "readme": "# generalized random forests <a href='https://grf-labs.github.io/grf/'><img src='https://raw.githubusercontent.com/grf-labs/grf/master/images/logo/grf_logo_wbg_cropped.png' align=\"right\" height=\"120\" /></a>\n\n[![CRANstatus](https://www.r-pkg.org/badges/version/grf)](https://cran.r-project.org/package=grf)\n[![](https://cranlogs.r-pkg.org/badges/grand-total/grf)](https://cran.r-project.org/package=grf)\n[![Build Status](https://dev.azure.com/grf-labs/grf/_apis/build/status/grf-labs.grf?branchName=master)](https://dev.azure.com/grf-labs/grf/_build/latest?definitionId=2&branchName=master)\n\nA package for forest-based statistical estimation and inference. GRF provides non-parametric methods for heterogeneous treatment effects estimation (optionally using right-censored outcomes, multiple treatment arms or outcomes, or instrumental variables), as well as least-squares regression, quantile regression, and survival regression, all with support for missing covariates.\n\nIn addition, GRF supports 'honest' estimation (where one subset of the data is used for choosing splits, and another for populating the leaves of the tree), and confidence intervals for least-squares regression and treatment effect estimation.\n\nSome helpful links for getting started:\n\n- The [R package documentation](https://grf-labs.github.io/grf/) contains usage examples and method reference.\n- The [GRF reference](https://grf-labs.github.io/grf/REFERENCE.html) gives a detailed description of the GRF algorithm and includes troubleshooting suggestions.\n- For community questions and answers around usage, see [Github issues labelled 'question'](https://github.com/grf-labs/grf/issues?q=label%3Aquestion).\n\nThe repository first started as a fork of the [ranger](https://github.com/imbs-hl/ranger) repository -- we owe a great deal of thanks to the ranger authors for their useful and free package.\n\n### Installation\n\nThe latest release of the package can be installed through CRAN:\n\n```R\ninstall.packages(\"grf\")\n```\n\n`conda`",
    "url": "https://github.com/grf-labs/grf",
    "last_updated": "2025-09-01T05:29:09+00:00"
  },
  {
    "full_name": "sinhrks/stan-statespace",
    "name": "stan-statespace",
    "description": "Stan models for state space time series",
    "language": "R",
    "topics": [],
    "readme": "\nReproducing \"An Introduction to State Space Time Series Analysis\" using Stan\n============================================================================\n\nTrying to reproduce the examples introduced in \"An Introduction to State Space Time Series Analysis\" using Stan.\n\nExample data:\n,,,,,,,,,,,,,\n\nFrom http://www.ssfpack.com/CKbook.html:\n    - logUKpetrolprice.txt\n    - NorwayFinland.txt\n    - UKdriversKSI.txt\n    - UKinflation.txt\n    - UKfrontrearseatKSI.txt\n\nModels:\n,,,,,,,\n\n1. Introduction\n    - fig01_01.R: Linear regression\n2. The local level model\n    - fig02_01.R: Deterministic level\n    - fig02_03.R: Stochastic level\n    - fig02_05.R: The local level model and Norwegian fatalities\n3. The local linear trend model\n    - fig03_01.R: Stochastic level and slope\n    - fig03_04.R: Stochastic level and deterministic slope\n    - fig03_05.R: The local linear trend model and Finnish fatalities\n4. The local level model with seasonal\n    - fig04_02.R: Deterministic level and seasonal\n    - fig04_06.R: Stochastic level and seasonal\n    - fig04_10.R: The local level and seasonal model and UK inflation\n5. The local level model with explanatory variable\n    - fig05_01.R: Deterministic level and explanatory variable\n    - fig05_04.R: Stochastic level and explanatory variable\n6. The local level model with intervention variable\n    - fig06_01.R: Deterministic level and intervention variable\n    - fig06_04.R: Stochastic level and intervention variable\n7. The UK seat belt and inflation models\n    - fig07_01.R: Deterministic level and seasonal\n    - fig07_02.R: Stochastic level and seasonal\n    - fig07_04.R: The UK inflation model\n8. General treatment of univariate state space models\n9. Multivariate time series analysis\n10. State space and Box–Jenkins methods for time series analysis\n\n**IMPORTANT** Some models output different results from textbook and R's `{dlm}` package.\n\nJapanese\n--------\n\nStan で　\"状態空間時系列分析入門\" を再現する\n\nサンプルデータ:\n,,,,,,,,,,,,,,,\n\nhttp://www.ssfpack.com/CKbook.html ",
    "url": "https://github.com/sinhrks/stan-statespace",
    "last_updated": "2025-04-19T16:28:32+00:00"
  },
  {
    "full_name": "trinker/textreadr",
    "name": "textreadr",
    "description": "Tools to uniformly read in text data including semi-structured transcripts",
    "language": "R",
    "topics": [
      "r",
      "read-transcripts",
      "pdf-reading",
      "docx",
      "text-data",
      "text-mining",
      "doc"
    ],
    "readme": "textreadr\n============\n\n![](tools/textreadr_logo/r_textreadr.png)\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Build\nStatus](https://travis-ci.org/trinker/textreadr.svg?branch=master)](https://travis-ci.org/trinker/textreadr)\n[![Coverage\nStatus](https://coveralls.io/repos/trinker/textreadr/badge.svg?branch=master)](https://coveralls.io/github/trinker/textreadr)\n[![](http://cranlogs.r-pkg.org/badges/textreadr)](https://cran.r-project.org/package=textreadr)\n\n\n**textreadr** is a small collection of convenience tools for reading\ntext documents into R. This is not meant to be an exhaustive collection;\nfor more see the [**tm**](https://CRAN.R-project.org/package=tm)\npackage.\n\n\nTable of Contents\n============\n\n-   [Functions](#functions)\n-   [Installation](#installation)\n-   [Contact](#contact)\n-   [Demonstration](#demonstration)\n    -   [Load the Packages/Data](#load-the-packagesdata)\n    -   [Download & Browse](#download-browse)\n        -   [Download](#download)\n        -   [Browse](#browse)\n    -   [Generic Document Reading](#generic-document-reading)\n    -   [Read Directory Contents](#read-directory-contents)\n    -   [Basic Readers](#basic-readers)\n        -   [Read .doc](#read-.doc)\n        -   [Read .docx](#read-.docx)\n        -   [Read .html](#read-.html)\n        -   [Read .odt](#read-.odt)\n        -   [Read .pdf](#read-.pdf)\n        -   [Read .pptx](#read-.pptx)\n        -   [Read .rtf](#read-.rtf)\n    -   [Read Transcripts](#read-transcripts)\n        -   [doc](#doc)\n        -   [docx Simple](#docx-simple)\n        -   [docx With Skip](#docx-with-skip)\n        -   [docx With Dash Separator](#docx-with-dash-separator)\n        -   [odt](#odt)\n        -   [rtf](#rtf)\n        -   [xls and xlsx](#xls-and-xlsx)\n        -   [Reading Text](#reading-text)\n        -   [Authentic Interview](#authentic-interview)\n    -   [Pa",
    "url": "https://github.com/trinker/textreadr",
    "last_updated": "2025-07-31T18:12:40+00:00"
  },
  {
    "full_name": "nx10/fineslice",
    "name": "fineslice",
    "description": "Sample 3D-affine transformed images",
    "language": "Python",
    "topics": [],
    "readme": "# `fineslice`\n[![Build](https://github.com/nx10/fineslice/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/nx10/fineslice/actions/workflows/test.yaml?query=branch%3Amain)\n[![codecov](https://codecov.io/gh/nx10/fineslice/branch/main/graph/badge.svg?token=22HWWFWPW5)](https://codecov.io/gh/nx10/fineslice)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n![stability-stable](https://img.shields.io/badge/stability-stable-green.svg)\n[![BSD 3-Clause License](https://img.shields.io/badge/license-BSD_3--Clause-blue.svg)](https://github.com/nx10/fineslice/blob/main/LICENSE)\n[![pages](https://img.shields.io/badge/api-docs-blue)](https://childmindresearch.github.io/template-python-repository)\n\n`fineslice` is a lightweight sampler for 3D-affine transformed images (commonly used in neuroscience) implemented in \npure Python + NumPy.\n\nIt does not make any assumptions about the data. Pass _any_ image texture and affine matrix directly into it.\n\n### Features\n\n- Precision sampling (no need to 're-sample' and loose precision)\n- Automatically finds optimal dimensions\n- Only depends on NumPy\n\n### Usage with `nibabel`\n\nFor the best performance directly pass in the `nibabel` data object as a texture:\n\n```Python\nimport nibabel as nib\nimport fineslice as fine\n\nimg = nib.load('my_image.nii.gz')\n\nout = fine.sample_0d(\n    texture=img.dataobj,\n    affine=img.affine,\n    out_position=(0, 0, 0)\n)\n```",
    "url": "https://github.com/nx10/fineslice",
    "last_updated": "2025-08-25T17:09:01+00:00"
  },
  {
    "full_name": "horovod/horovod",
    "name": "horovod",
    "description": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.",
    "language": "Python",
    "topics": [
      "tensorflow",
      "uber",
      "machine-learning",
      "machinelearning",
      "mpi",
      "baidu",
      "deep-learning",
      "deeplearning",
      "keras",
      "pytorch",
      "mxnet",
      "spark",
      "ray"
    ],
    "readme": ".. raw:: html\n\n    <p align=\"center\"><img src=\"https://user-images.githubusercontent.com/16640218/34506318-84d0c06c-efe0-11e7-8831-0425772ed8f2.png\" alt=\"Logo\" width=\"200\"/></p>\n    <br/>\n\nHorovod\n=======\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. image:: https://badge.fury.io/py/horovod.svg\n   :target: https://badge.fury.io/py/horovod\n   :alt: PyPI Version\n\n.. image:: https://badge.buildkite.com/6f976bc161c69d9960fc00de01b69deb6199b25680a09e5e26.svg?branch=master\n   :target: https://buildkite.com/horovod/horovod\n   :alt: Build Status\n\n.. image:: https://readthedocs.org/projects/horovod/badge/?version=latest\n   :target: https://horovod.readthedocs.io/en/latest/\n   :alt: Documentation Status\n\n.. image:: https://img.shields.io/badge/slack-chat-green.svg?logo=slack\n   :target: https://forms.gle/cPGvty5hp31tGfg79\n   :alt: Slack\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License\n\n.. image:: https://app.fossa.com/api/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod.svg?type=shield\n   :target: https://app.fossa.com/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod?ref=badge_shield\n   :alt: FOSSA Status\n\n.. image:: https://bestpractices.coreinfrastructure.org/projects/2373/badge\n   :target: https://bestpractices.coreinfrastructure.org/projects/2373\n   :alt: CII Best Practices\n\n.. image:: https://pepy.tech/badge/horovod\n   :target: https://pepy.tech/project/horovod\n   :alt: Downloads\n\n.. raw:: html\n\n   </div>\n\n.. inclusion-marker-start-do-not-remove\n\n|\n\nHorovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\nThe goal of Horovod is to make distributed deep learning fast and easy to use.\n\n\n.. raw:: html\n\n   <p><img src=\"https://raw.githubusercontent.com/lfai/artwork/master/lfaidata-assets/lfaidata-project-badge/graduate/color/lfaidata-project-badge-graduate-col",
    "url": "https://github.com/horovod/horovod",
    "last_updated": "2025-09-01T06:35:40+00:00"
  },
  {
    "full_name": "mcdisc/UF-Election-Sciences",
    "name": "UF-Election-Sciences",
    "description": "A repository for activities associated with the US Elections Project http://www.electproject.org",
    "language": "R",
    "topics": [],
    "readme": "# UF-Election-Sciences\nA repository for activities associated with the US Elections Project http://www.electproject.org\n",
    "url": "https://github.com/mcdisc/UF-Election-Sciences",
    "last_updated": "2025-05-02T04:15:29+00:00"
  },
  {
    "full_name": "ing-bank/sparse_dot_topn",
    "name": "sparse_dot_topn",
    "description": "Python package to accelerate the sparse matrix multiplication and top-n similarity selection",
    "language": "C++",
    "topics": [
      "cosine-similarity",
      "sparse-matrix",
      "scipy",
      "cython"
    ],
    "readme": "# sparse\\_dot\\_topn\n\n[![MacOS](https://github.com/ing-bank/sparse_dot_topn/actions/workflows/macos.yml/badge.svg)](https://github.com/ing-bank/sparse_dot_topn/actions/workflows/macos.yml)\n[![Linux](https://github.com/ing-bank/sparse_dot_topn/actions/workflows/linux.yml/badge.svg)](https://github.com/ing-bank/sparse_dot_topn/actions/workflows/linux.yml)\n[![Windows](https://github.com/ing-bank/sparse_dot_topn/actions/workflows/windows.yml/badge.svg)](https://github.com/ing-bank/sparse_dot_topn/actions/workflows/windows.yml)\n[![License](https://img.shields.io/github/license/ing-bank/sparse_dot_topn)](https://github.com/ing-bank/sparse_dot_topn/blob/master/LICENSE)\n[![ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)\n\n\n[![Release_date](https://img.shields.io/github/release-date/ing-bank/sparse_dot_topn)](https://github.com/ing-bank/sparse_dot_topn/releases)\n[![PyPi](https://img.shields.io/pypi/v/sparse-dot-topn.svg)](https://pypi.org/project/sparse-dot-topn/)\n[![Downloads](https://pepy.tech/badge/sparse_dot_topn)](https://pepy.tech/project/sparse_dot_topn)\n\n**sparse\\_dot\\_topn** provides a fast way to performing a sparse matrix multiplication followed by top-n multiplication result selection.\n\nComparing very large feature vectors and picking the best matches, in practice often results in performing a sparse matrix multiplication followed by selecting the top-n multiplication results.\n\n**sparse\\_dot\\_topn** provides a (parallelised) sparse matrix multiplication implementation that integrates selecting the top-n values, resulting in a significantly lower memory footprint and improved performance.\nOn Apple M2 Pro over two 20k x 193k TF-IDF matrices **sparse\\_dot\\_topn** can be up to 6 times faster when retaining the top 10 values per row and utilising 8 cores.\nSee the benchmark directory for details.\n\n## Usage\n\n`sp_matmul_topn` supports `{CSR, CSC, COO}` matrices",
    "url": "https://github.com/ing-bank/sparse_dot_topn",
    "last_updated": "2025-08-20T08:13:43+00:00"
  },
  {
    "full_name": "pypa/sampleproject",
    "name": "sampleproject",
    "description": "A sample project that exists for PyPUG's \"Tutorial on Packaging and Distributing Projects\"",
    "language": "Python",
    "topics": [],
    "readme": "# A sample Python project\n\n![Python Logo](https://www.python.org/static/community_logos/python-logo.png \"Sample inline image\")\n\nA sample project that exists as an aid to the [Python Packaging User\nGuide][packaging guide]'s [Tutorial on Packaging and Distributing\nProjects][distribution tutorial].\n\nThis project does not aim to cover best practices for Python project\ndevelopment as a whole. For example, it does not provide guidance or tool\nrecommendations for version control, documentation, or testing.\n\n[The source for this project is available here][src].\n\nThe metadata for a Python project is defined in the `pyproject.toml` file,\nan example of which is included in this project. You should edit this file\naccordingly to adapt this sample project to your needs.\n\n----\n\nThis is the README file for the project.\n\nThe file should use UTF-8 encoding and can be written using\n[reStructuredText][rst] or [markdown][md use] with the appropriate [key set][md\nuse]. It will be used to generate the project webpage on PyPI and will be\ndisplayed as the project homepage on common code-hosting services, and should be\nwritten for that purpose.\n\nTypical contents for this file would include an overview of the project, basic\nusage examples, etc. Generally, including the project changelog in here is not a\ngood idea, although a simple “What's New” section for the most recent version\nmay be appropriate.\n\n[packaging guide]: https://packaging.python.org\n[distribution tutorial]: https://packaging.python.org/tutorials/packaging-projects/\n[src]: https://github.com/pypa/sampleproject\n[rst]: http://docutils.sourceforge.net/rst.html\n[md]: https://tools.ietf.org/html/rfc7764#section-3.5 \"CommonMark variant\"\n[md use]: https://packaging.python.org/specifications/core-metadata/#description-content-type-optional\n",
    "url": "https://github.com/pypa/sampleproject",
    "last_updated": "2025-09-01T02:36:07+00:00"
  },
  {
    "full_name": "laurencium/Causalinference",
    "name": "Causalinference",
    "description": "Causal Inference in Python",
    "language": "Python",
    "topics": [],
    "readme": "Causal Inference in Python\n==========================\n\n*Causal Inference in Python*, or *Causalinference* in short, is a software package that implements various statistical and econometric methods used in the field variously known as Causal Inference, Program Evaluation, or Treatment Effect Analysis.\n\nWork on *Causalinference* started in 2014 by Laurence Wong as a personal side project. It is distributed under the 3-Clause BSD license.\n\nImportant Links\n===============\n\nThe official website for *Causalinference* is\n\n  https://causalinferenceinpython.org\n\nThe most current development version is hosted on GitHub at\n\n  https://github.com/laurencium/causalinference\n\nPackage source and binary distribution files are available from PyPi at\n\n  https://pypi.python.org/pypi/causalinference\n\nFor an overview of the main features and uses of *Causalinference*, please refer to\n\n  https://github.com/laurencium/causalinference/blob/master/docs/tex/vignette.pdf\n\nA blog dedicated to providing a more detailed walkthrough of *Causalinference* and the econometric theory behind it can be found at\n\n  https://laurencium.github.io/causal-inference-blog/\n\nMain Features\n=============\n\n* Assessment of overlap in covariate distributions\n* Estimation of propensity score\n* Improvement of covariate balance through trimming\n* Subclassification on propensity score\n* Estimation of treatment effects via matching, blocking, weighting, and least squares\n\nDependencies\n============\n\n* NumPy: 1.8.2 or higher\n* SciPy: 0.13.3 or higher\n\nInstallation\n============\n\n*Causalinference* can be installed using ``pip``: ::\n\n  $ pip install causalinference\n\nFor help on setting up Pip, NumPy, and SciPy on Macs, check out this excellent `guide <http://www.sourabhbajaj.com/mac-setup>`_.\n\nMinimal Example\n===============\n\nThe following illustrates how to create an instance of CausalModel: ::\n\n  >>> from causalinference import CausalModel\n  >>> from causalinference.utils import random_data\n  >>> Y, D, X = random_data()\n  >",
    "url": "https://github.com/laurencium/Causalinference",
    "last_updated": "2025-08-31T15:48:02+00:00"
  },
  {
    "full_name": "jupyterlite/xeus-lite-demo",
    "name": "xeus-lite-demo",
    "description": "JupyterLite template repository with xeus kernels and pre-installed packages",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Xeus-Lite demo\n\n[![lite-badge](https://jupyterlite.rtfd.io/en/latest/_static/badge.svg)](https://jupyterlite.github.io/xeus-lite-demo/notebooks/?path=demo.ipynb)\n\nThis GitHub template allows you to create deployments of JupyterLite with a custom set of conda packages.\n\n## 💡 How to make your own deployment\n\nCreating a new deployment can be done in three easy steps:\n\n**Step 1: Apply the GitHub template**\n\n1. Click the **\"Use this template\"** button in the upper right corner of the GitHub repository.\n2. Choose a name for your project and select the GitHub organization where you want to create it.\n3. Then hit **\"Create repository from template\"** to finalize the setup.\n\n**Step 2: Enable building the GitHub pages from GitHub actions.**\n\n1. Once your repository is created, enable GitHub Pages by configuring GitHub Actions. This will build and deploy your site automatically.\n2. Your deployment will be accessible at the following URL: https://{USERNAME}.github.io/{DEMO_REPO_NAME}.\n\n**Step 3: Customize your conda environment**\n\n1. Update your ``environment.yml`` file to include the required packages.\n2. This ensures that your environment has all the necessary dependencies.\n\n## 🎬 Visual Guide\n\nFor a step-by-step visual guide, check out the screencast below:\n\n![Deploy your own](deploy.gif)\n\n## 📦 How to install kernels and packages\n\nYou can install specific kernels and extra packages by adding them to the ``environment.yml`` file.\n\nSee https://jupyterlite-xeus.readthedocs.io/en/latest/environment.html for more documentation.\n\n### Example 1: JupyterLite with NumPy and Matplotlib\n\nTo create a JupyterLite deployment with NumPy and Matplotlib pre-installed, edit the ``environment.yml`` file as follows:\n\n```yml\nname: xeus-kernel\nchannels:\n  - https://repo.prefix.dev/emscripten-forge-dev\n  - https://repo.prefix.dev/conda-forge\ndependencies:\n  - xeus-python\n  - numpy\n  - matplotlib\n```\n\n### Example 2: JupyterLite with R and coursekata\n\nTo use the R kernel and the coursekata package,",
    "url": "https://github.com/jupyterlite/xeus-lite-demo",
    "last_updated": "2025-07-19T19:57:24+00:00"
  },
  {
    "full_name": "google-deepmind/alphafold",
    "name": "alphafold",
    "description": "Open source code for AlphaFold 2.",
    "language": "Python",
    "topics": [],
    "readme": "![header](imgs/header.jpg)\n\n# AlphaFold\n\nThis package provides an implementation of the inference pipeline of AlphaFold\nv2. For simplicity, we refer to this model as AlphaFold throughout the rest of\nthis document.\n\nWe also provide:\n\n1.  An implementation of AlphaFold-Multimer. This represents a work in progress\n    and AlphaFold-Multimer isn't expected to be as stable as our monomer\n    AlphaFold system. [Read the guide](#updating-existing-installation) for how\n    to upgrade and update code.\n2.  The [technical note](docs/technical_note_v2.3.0.md) containing the models\n    and inference procedure for an updated AlphaFold v2.3.0.\n3.  A [CASP15 baseline](docs/casp15_predictions.zip) set of predictions along\n    with documentation of any manual interventions performed.\n\nAny publication that discloses findings arising from using this source code or\nthe model parameters should [cite](#citing-this-work) the\n[AlphaFold paper](https://doi.org/10.1038/s41586-021-03819-2) and, if\napplicable, the\n[AlphaFold-Multimer paper](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1).\n\nPlease also refer to the\n[Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf)\nfor a detailed description of the method.\n\n**You can use a slightly simplified version of AlphaFold with\ncommunity-supported versions (see below).\n\nIf you have any questions, please contact the AlphaFold team at\n[alphafold@deepmind.com](mailto:alphafold@deepmind.com).\n\n![CASP14 predictions](imgs/casp14_predictions.gif)\n\n## Installation and running your first prediction\n\nYou will need a machine running Linux, AlphaFold does not support other\noperating systems. Full installation requires up to 3 TB of disk space to keep\ngenetic databases (SSD storage is recommended) and a modern NVIDIA GPU (GPUs\nwith more memory can predict larger protein structures).\n\nPlease follow these steps:\n\n1.  Install [Docker](https://www.docker.com/).\n\n ",
    "url": "https://github.com/google-deepmind/alphafold",
    "last_updated": "2025-09-02T06:40:47+00:00"
  },
  {
    "full_name": "microsoft/table-transformer",
    "name": "table-transformer",
    "description": "Table Transformer (TATR) is a deep learning model for extracting tables from unstructured documents (PDFs and images). This is also the official repository for the PubTables-1M dataset and GriTS evaluation metric.",
    "language": "Python",
    "topics": [
      "table-detection",
      "table-extraction",
      "table-structure-recognition",
      "table-functional-analysis"
    ],
    "readme": "# Table Transformer (TATR)\n\nA deep learning model based on object detection for extracting tables from PDFs and images.\n\nFirst proposed in [\"PubTables-1M: Towards comprehensive table extraction from unstructured documents\"](https://openaccess.thecvf.com/content/CVPR2022/html/Smock_PubTables-1M_Towards_Comprehensive_Table_Extraction_From_Unstructured_Documents_CVPR_2022_paper.html).\n\n![table_extraction_v2](https://user-images.githubusercontent.com/10793386/139559159-cd23c972-8731-48ed-91df-f3f27e9f4d79.jpg)\n\nThis repository also contains the official code for these papers:\n- [\"GriTS: Grid table similarity metric for table structure recognition\"](https://arxiv.org/abs/2203.12555)\n- [\"Aligning benchmark datasets for table structure recognition\"](https://arxiv.org/abs/2303.00716)\n\nNote: If you are looking to use Table Transformer to extract your own tables, here are some helpful things to know:\n- TATR can be trained to work well across many document domains and everything needed to train your own model is included here. But at the moment pre-trained model weights are only available for TATR trained on the PubTables-1M dataset. (See the additional documentation for how to train your own multi-domain model.)\n- TATR is an object detection model that recognizes tables from image input. The inference code built on TATR needs text extraction (from OCR or directly from PDF) as a separate input in order to include text in its HTML or CSV output.\n\nAdditional information about this project for both users and researchers, including data, training, evaluation, and inference code is provided below.\n\n## News\n`08/22/2023`: We have released 3 new pre-trained models for TATR-v1.1 (trained on 1. PubTables-1M, 2. FinTabNet.c, and 3. both datasets combined) according to the details in [our paper](https://arxiv.org/abs/2303.00716).\\\n`04/19/2023`: Our latest papers ([link](https://arxiv.org/abs/2203.12555) and [link](https://arxiv.org/abs/2303.00716)) have been accepted at [ICDAR 2023](https",
    "url": "https://github.com/microsoft/table-transformer",
    "last_updated": "2025-09-02T06:47:14+00:00"
  },
  {
    "full_name": "networkx/networkx",
    "name": "networkx",
    "description": "Network Analysis in Python",
    "language": "Python",
    "topics": [
      "python",
      "complex-networks",
      "graph-theory",
      "graph-algorithms",
      "graph-analysis",
      "graph-generation",
      "graph-visualization"
    ],
    "readme": "NetworkX\n========\n\n\n.. image::\n    https://github.com/networkx/networkx/workflows/test/badge.svg?branch=main\n    :target: https://github.com/networkx/networkx/actions?query=workflow%3Atest\n\n.. image::\n    https://codecov.io/gh/networkx/networkx/branch/main/graph/badge.svg?\n    :target: https://app.codecov.io/gh/networkx/networkx/branch/main\n\n.. image::\n    https://img.shields.io/pypi/v/networkx.svg?\n    :target: https://pypi.python.org/pypi/networkx\n\n.. image::\n    https://img.shields.io/pypi/l/networkx.svg?\n    :target: https://github.com/networkx/networkx/blob/main/LICENSE.txt\n\n.. image::\n    https://img.shields.io/pypi/pyversions/networkx.svg?\n    :target: https://pypi.python.org/pypi/networkx\n\n.. image::\n    https://img.shields.io/github/labels/networkx/networkx/good%20first%20issue?color=green&label=contribute\n    :target: https://github.com/networkx/networkx/contribute\n\n\nNetworkX is a Python package for the creation, manipulation,\nand study of the structure, dynamics, and functions\nof complex networks.\n\n- **Website (including documentation):** https://networkx.org\n- **Mailing list:** https://groups.google.com/forum/#!forum/networkx-discuss\n- **Source:** https://github.com/networkx/networkx\n- **Bug reports:** https://github.com/networkx/networkx/issues\n- **Report a security vulnerability:** https://tidelift.com/security\n- **Tutorial:** https://networkx.org/documentation/latest/tutorial.html\n- **GitHub Discussions:** https://github.com/networkx/networkx/discussions\n- **Discord (Scientific Python) invite link:** https://discord.com/invite/vur45CbwMz\n- **NetworkX meetings calendar (open to all):** https://scientific-python.org/calendars/networkx.ics\n\nSimple example\n--------------\n\nFind the shortest path between two nodes in an undirected graph:\n\n.. code:: pycon\n\n    >>> import networkx as nx\n    >>> G = nx.Graph()\n    >>> G.add_edge(\"A\", \"B\", weight=4)\n    >>> G.add_edge(\"B\", \"D\", weight=2)\n    >>> G.add_edge(\"A\", \"C\", weight=3)\n    >>> G.add_edge(\"C\", \"D\", weight",
    "url": "https://github.com/networkx/networkx",
    "last_updated": "2025-09-02T06:59:10+00:00"
  },
  {
    "full_name": "msesia/deepknockoffs",
    "name": "deepknockoffs",
    "description": "Approximate knockoffs and model-free variable selection.",
    "language": "Python",
    "topics": [],
    "readme": "Deep Knockoffs\n==============\n\nThis repository provides a Python package for sampling approximate\nmodel-X knockoffs using deep generative models.\n\nAccompanying paper: https://arxiv.org/abs/1811.06687.\nPublished in the Journal of the American Statistical Association (https://doi.org/10.1080/01621459.2019.1660174)\n\nTo learn more about the algorithm implemented in this package, visit  https://web.stanford.edu/group/candes/deep-knockoffs/ and read the accompanying paper.\n\nTo learn more about the broader framework of knockoffs, visit https://web.stanford.edu/group/candes/knockoffs/.\n\n## Software dependencies\n\nThe code contained in this repository was tested on the following configuration of Python:\n\n- python=3.6.5\n- numpy=1.14.0\n- scipy=1.0.0\n- pytorch=0.4.1\n- cvxpy=1.0.10\n- cvxopt=1.2.0\n- pandas=0.23.4\n\n## Installation guide\n\n```bash\ncd DeepKnockoffs\npython setup.py install --user\n```\n\n## Examples\n\n - [examples/toy-example.ipynb](examples/toy-example.ipynb) A usage example on a toy problem with multivariate Gaussian variables is available in the form of a Jupyter Notebook.\n - [examples/experiments-1.ipynb](examples/experiments-1.ipynb) Code to train the machine used in the paper.\n - [examples/experiments-2.ipynb](examples/experiments-2.ipynb) Code to compute the goodness-of-fit diagnostics for the machine used in the paper.\n - [examples/experiments-3.ipynb](examples/experiments-3.ipynb) Code to perform the controlled variable selection experiments in the paper.\n - [examples/data-preprocessing.ipynb](examples/data-preprocessing.ipynb) Example of how to pre-process data containing extremely correlated variables.\n\n## License\n\nThis software is distributed under the [GPLv3 license](https://www.gnu.org/licenses/gpl-3.0.en.html) and it comes with ABSOLUTELY NO WARRANTY.\n",
    "url": "https://github.com/msesia/deepknockoffs",
    "last_updated": "2025-05-22T13:57:22+00:00"
  },
  {
    "full_name": "VertaAI/modeldb",
    "name": "modeldb",
    "description": "Open Source ML Model Versioning, Metadata, and Experiment Management",
    "language": "Java",
    "topics": [
      "machine-learning",
      "model-management",
      "modeldb",
      "mit",
      "verta",
      "model-versioning"
    ],
    "readme": "# ModelDB: An open-source system for Machine Learning model versioning, metadata, and experiment management.\n----\n\n<p align=\"center\">\n  <a href=\"https://hub.docker.com/u/vertaaiofficial\">\n    <img src=\"https://img.shields.io/docker/v/vertaaiofficial/modeldb-backend?color=534eb5&label=Docker%20image%20version&style=plastic\" alt=\"docker hub\" />\n  </a>\n  <a href=\"https://pypi.org/project/verta/\">\n    <img src=\"https://img.shields.io/pypi/v/verta?color=534eb5&style=plastic\" alt=\"PyPI\" />\n  </a>\n  <a href=\"https://anaconda.org/conda-forge/verta\">\n    <img src=\"https://img.shields.io/conda/v/conda-forge/verta?color=534eb5&style=plastic\" alt=\"Conda\" />\n  </a>\n  <a href=\"https://github.com/VertaAI/modeldb/blob/master/LICENSE\">\n    <img src=\"https://img.shields.io/pypi/l/verta?color=534eb5&style=plastic\" alt=\"License\" />\n  </a>\n  <br>\n  <a href=\"https://hub.docker.com/u/vertaaiofficial\">\n    <img src=\"https://img.shields.io/docker/pulls/vertaaiofficial/modeldb-backend?color=534eb5&style=plastic\" alt=\"docker hub\" />\n  </a>\n  <a href=\"https://pypi.org/project/verta/\">\n    <img src=\"https://img.shields.io/pypi/dm/verta?color=534eb5&label=PyPI%20Downloads&style=plastic\" alt=\"PyPI\" />\n  </a>\n  <a href=\"https://github.com/VertaAI/modeldb/graphs/commit-activity\">\n    <img src=\"https://img.shields.io/github/commit-activity/w/vertaai/modeldb?color=534eb5&style=plastic\" alt=\"Commits\" />\n  </a>\n  <a href=\"https://github.com/VertaAI/modeldb/graphs/commit-activity\">\n    <img src=\"https://img.shields.io/github/last-commit/vertaai/modeldb?color=534eb5&style=plastic\" alt=\"Last Commit\" />\n  </a>\n  <br>\n  <a href=\"https://github.com/VertaAI/modeldb/graphs/commit-activity\">\n    <img src=\"https://img.shields.io/github/stars/vertaai/modeldb?style=social\" alt=\"Forks\" />\n  </a>\n  <a href=\"https://twitter.com/intent/follow?screen_name=VertaAI\">\n    <img src=\"https://img.shields.io/twitter/follow/VertaAI?label=VertaAI&style=social\" alt=\"Twitter\" />\n  </a>\n  <a href=\"http://bit.ly/modeldb-mlops\">\n   ",
    "url": "https://github.com/VertaAI/modeldb",
    "last_updated": "2025-08-29T23:42:36+00:00"
  },
  {
    "full_name": "mariusbrataas/flowpoints_ml",
    "name": "flowpoints_ml",
    "description": "An intuitive approach to creating deep learning models",
    "language": "JavaScript",
    "topics": [
      "deep-learning",
      "pytorch",
      "machine-learning",
      "react",
      "js",
      "python",
      "flowchart",
      "tensorflow"
    ],
    "readme": "![](public/imagine_deep_learning.png)\n\n\n# Flowpoints\n\nCreate deep learning models without all the typing and dimension mismatches! Follow [this link](https://mariusbrataas.github.io/flowpoints_ml) to play around with this on your own :)\n\n[This npm package](https://www.npmjs.com/package/flowpoints) is a spin-off of this project, which this project is now utilizing.\n\nHere's a [colab](https://colab.research.google.com/drive/15zC7TMheMzwllEShgN45Y7JLXtS-syMn) using this PyTorch [model.](https://mariusbrataas.github.io/flowpoints_ml/?p=9fehu18ra4ty)\n\n![](public/cifar10net.png)\n\nAnd here's similar [colab](https://colab.research.google.com/drive/1rkJItfTb8fB0mWQRmL-shsiReFLJ57vR) using this TensorFlow [model](https://mariusbrataas.github.io/flowpoints_ml/?p=KlHpdLzP3SDx)\n\n![](public/tf_cifar10.png)\n\n## Overview\nThis project is used to host a website in which users can quickly create drafts for deep learning models and have the equivalent plug-and-play code output immediately.\n\nThe code output to the user is written in python and utilizes [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/alpha).\n\n- [Overview](#overview)\n- [User guide](#user-guide)\n\t- [How I use these diagrams](#how-i-use-these-diagrams)\n\t- [Building new models](#building-new-models)\n\t\t- [Adding flowpoints](#adding-flowpoints)\n\t\t- [Adding and removing connections](#adding-and-removing-connections)\n\t\t- [Changing the parameters of your model](#changing-the-parameters-of-your-model)\n\t- [Changing appearance](#changing-appearance)\n\t- [Extracting the model code](#extracting-the-model-code)\n\t- [Sharing your model](#sharing-your-model)\n- [Example output](#example-output)\n- [Contributing to this project](#contributing-to-this-project)\n- [Dependencies](#dependencies)\n- [License](#license)\n\n## User guide\n\nOr maybe just play around with it yourself?\n\n### How I use these diagrams\n1. Start by adding an input block and edit it's parameters to get the correct number of dimensions and features.\n2. A",
    "url": "https://github.com/mariusbrataas/flowpoints_ml",
    "last_updated": "2025-08-17T03:28:55+00:00"
  },
  {
    "full_name": "Dasonk/docstring",
    "name": "docstring",
    "description": "Provides docstring like functionality to R functions without requiring the need to create a package.",
    "language": "R",
    "topics": [
      "r",
      "documentation",
      "documentation-tool",
      "docstring",
      "roxygen-style",
      "devtools",
      "r-package"
    ],
    "readme": "# docstring\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version-last-release/docstring)](https://cran.r-project.org/package=docstring) \n[![Build Status](https://travis-ci.org/Dasonk/docstring.svg?branch=master)](https://travis-ci.org/Dasonk/docstring) \n[![Downloads](http://cranlogs.r-pkg.org/badges/docstring)](https://cran.r-project.org/package=docstring)\n\nThe docstring package is an R package that provides the ability to \ndisplay something analagous to\nPython's docstrings within R.  By allowing the user to document\ntheir functions as comments at the beginning of their function\nwithout requiring putting the function into a package we allow\nmore users to easily provide documentation viewable through the native help system within R.\nThe documentation can be viewed either using an accessor function\n(i.e. `docstring(your_function)`) or just like you would for\nany other function (i.e. `?your_function`) and it should\ndisplays just like any other R help files.\n\nThe user will need to be familiar with roxygen style comments (via the [roxygen2 package](https://cran.r-project.org/package=roxygen2))\nto fully utilize the package.  \n\nIdeally this will\nallow users not yet comfortable with package creation to still provide\ndocumentation for their functions. If they use the roxygen style comments when it\nis time to convert their work into a package all they will need to do is move\ntheir pre-existing documentation outside of the function and they will be set.\n\n## Examples\n\n\n```r\nlibrary(docstring)\n\nsquare <- function(x){\n\n    #' Square a number\n    #'\n    #' Calculates the square of the input\n    #'\n    #' @param x the input to be squared\n\n    return(x^2)\n}\n\ndocstring(square)\n# or\n?square\n```\n\n![Square](docs/images/square_web.png)\n\n### R Studio support\n\nIf you are running R through RStudio there is support for displaying the\ndocstring within the RStudio help pane directly.  This is enabled by default. It\nshould detect that you're running RStudio and unless you choose the `rstu",
    "url": "https://github.com/Dasonk/docstring",
    "last_updated": "2025-06-16T19:07:45+00:00"
  },
  {
    "full_name": "Bookworm-project/BookwormDB",
    "name": "BookwormDB",
    "description": "Tools for text tokenization and encoding",
    "language": "Python",
    "topics": [],
    "readme": "[![Travis Build Status](https://travis-ci.org/Bookworm-project/BookwormDB.svg?branch=master)](https://travis-ci.org/Bookworm-project/BookwormDB)\n\n[BookwormDB](https://github.com/bookworm-project/BookwormDB \"BookwormDB\") is the main code repository for the Bookworm project. Given simply formatted files and metadata, it creates an efficient and easily queryable MySQL database that can make full use of all the metadata and lexical data in the original source. It also includes a powerful API for asking a variety of unigrammatic queries about that data.\n\nA quick walkthrough is included below: other documentation is at [bookworm.culturomics.org]() and in a [Bookworm Manual](http://bookworm-project.github.io/Docs) on this repository (editable at the repo [here](https://github.com/Bookworm-project/Docs)).\n\n# Installation\n\nInstallation is tested on Ubuntu and OS X. It may work on other Unixes, but will probably not work on Windows.\n\n1. Install some dependencies; mysql or mariadb for databases.\n2. Download the latest release, either by cloning this git repo or downloading a zip.\n3. Navigate to the folder in the terminal, and type `pip install .`.\n4. Type `bookworm --help` to confirm the executable has worked. If this doesn't work, file\n   a bug report.\n5. (No longer?) Type `bookworm config mysql` for some interactive prompts to allow Bookworm to edit MySQL databases on your server. (Note that this makes some other changes to your mysql configuration files; you may want to copy them first if you're using it for other things.)\n\n## Releases\n\nThe `master` branch is regularly tested on Travis; you are generally best off installing the latest version.\n\n## Related projects\n\nThis builds a database and implements the Bookworm API on particular set of texts.\n\nSome basic, widely appealing visualizations of the data are possible with the Bookworm [web app](https://github.com/bookworm-project/BookwormGUI \"Bookworm web app\"), which runs on top of the API.\n\nA more wide-ranging set of visual",
    "url": "https://github.com/Bookworm-project/BookwormDB",
    "last_updated": "2024-05-14T23:08:45+00:00"
  },
  {
    "full_name": "mkearney/funique",
    "name": "funique",
    "description": "⌚️ A faster unique() function",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "unique",
      "duplicates",
      "r-package",
      "mkearney-r-package",
      "data-wrangling",
      "data-frame",
      "date-time",
      "posixct",
      "posix"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# funique <img src=\"man/figures/logo.png\" width=\"160px\" align=\"right\" />\n\n[![Travis build\nstatus](https://travis-ci.org/mkearney/funique.svg?branch=master)](https://travis-ci.org/mkearney/funique)\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n\n> ⌚️ A faster `unique()` function\n\n## Installation\n\nYou can install the released version of funique from Github with:\n\n``` r\n## install remotes pkg if not already\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\n\n## install funique from github\nremotes::install_github(\"mkearney/funique\")\n```\n\n## Usage\n\nThere’s one function `funique()`, which is the same as `base::unique()`\nonly optimized to be faster when data contain date-time variables.\n\n## Speed test: `funique()` vs. `base::unique()`\n\nThe code below creates a data frame with several duplicate rows and then\ncompares performance (in time) of `funique()` versus `base::unique()`.\n\n``` r\n## set seed\nset.seed(20180812)\n\n## generate data\nd <- data.frame(\n  x = rnorm(1000),\n  y = seq.POSIXt(as.POSIXct(\"2018-01-01\"),\n    as.POSIXct(\"2018-12-31\"), length.out = 10))\n\n## create data frame with duplicate rows\nd <- d[c(1:1000, sample(1:1000, 500, replace = TRUE)), ]\nrow.names(d) <- NULL\n\n## check the output against base::unique\nidentical(unique(d), funique(d))\n\n## bench mark\n(m <- microbenchmark::microbenchmark(unique(d), funique(d), \n  times = 200, unit = \"relative\"))\n\n## plot\nplot(drop_hl(m, n = 4)) + \n  ggplot2::ggsave(\"man/figures/r1.png\", width = 8, height = 4.5, units = \"in\")\n```\n\n<p align=\"center\">\n\n<img src=\"man/figures/r1.png\">\n\nHere’s another test this time using duplicate-infested Twitter data.\n\n``` r\n## search for data on 100 tweets\nrt <- rtweet::search_tweets(\"lang:en\", verbose = FALSE)\n\n## create duplicates\nrt2 <- rt[sample(1:nrow(rt), 1000, replace = TRUE), ]\n\n## benchmarks\n(mb <- micr",
    "url": "https://github.com/mkearney/funique",
    "last_updated": "2025-03-22T11:06:59+00:00"
  },
  {
    "full_name": "voteview/Rvoteview",
    "name": "Rvoteview",
    "description": "R package to query Voteview US roll call voting database",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\nRvoteview\n=========\n\n[![Travis-CI Build Status](https://travis-ci.org/voteview/Rvoteview.svg?branch=master)](https://travis-ci.org/voteview/Rvoteview) [![AppVeyor Build status](https://ci.appveyor.com/api/projects/status/n13u8s0tnfsau1o6?svg=true)](https://ci.appveyor.com/project/lukesonnet/rvoteview)\n\n**WARNING: This package is under construction. Please be patient and leave issues here on Github or contact [Luke Sonnet](mailto:luke.sonnet@gmail.com) with any questions.**\n\nThis is a package that enables you to query the Voteview database for roll calls and work with data frames or a `pscl` `rollcall` object.\n\nTo install this package, run the following (note you have to have devtools installed first):\n\n``` r\n# install.packages('devtools')\ndevtools::install_github(\"voteview/Rvoteview\")\n```\n\nFor more thorough documentation, see the help files for individual functions and the vignette [at this link here](https://github.com/voteview/Rvoteview/tree/master/vignettes).\n\nQuick Start: Using Rvoteview\n----------------------------\n\nTo use `Rvoteview`, you generally want to search the database to get a list of vote ids and then use those to return the individual votes. We query the database with a search term and some parameters to constrain the search. The default behavior is to search any words as key words, returning the roll calls that best match any key words you enter. Again, there are further examples in the [vignette](https://github.com/voteview/Rvoteview/tree/master/vignettes).\n\nSo let's start with a search for roll calls with the key word \"Iraq\".\n\n``` r\nlibrary(Rvoteview)\n  \nres <- voteview_search(\"Iraq\")\n#> Warning in strptime(x, fmt, tz = \"GMT\"): unknown timezone 'default/America/\n#> Los_Angeles'\n#> Query 'Iraq' returned 345 rollcalls...\nnames(res)\n#>  [1] \"id\"                \"congress\"          \"chamber\"          \n#>  [4] \"rollnumber\"        \"date\"              \"yea\"              \n#>  [7] \"nay\" ",
    "url": "https://github.com/voteview/Rvoteview",
    "last_updated": "2025-06-25T11:46:16+00:00"
  },
  {
    "full_name": "mattblackwell/gov2002-book",
    "name": "gov2002-book",
    "description": "An Introduction to Statistical Inference and Regression",
    "language": "TeX",
    "topics": [],
    "readme": "",
    "url": "https://github.com/mattblackwell/gov2002-book",
    "last_updated": "2025-06-17T23:00:34+00:00"
  },
  {
    "full_name": "coder/code-server",
    "name": "code-server",
    "description": "VS Code in the browser",
    "language": "TypeScript",
    "topics": [
      "vscode",
      "vscode-remote",
      "ide",
      "remote-work",
      "development-environment",
      "dev-tools",
      "browser-ide"
    ],
    "readme": "# code-server\n\n[![\"GitHub Discussions\"](https://img.shields.io/badge/%20GitHub-%20Discussions-gray.svg?longCache=true&logo=github&colorB=purple)](https://github.com/coder/code-server/discussions) [![\"Join us on Slack\"](https://img.shields.io/badge/join-us%20on%20slack-gray.svg?longCache=true&logo=slack&colorB=brightgreen)](https://coder.com/community) [![Twitter Follow](https://img.shields.io/twitter/follow/CoderHQ?label=%40CoderHQ&style=social)](https://twitter.com/coderhq) [![Discord](https://img.shields.io/discord/747933592273027093)](https://discord.com/invite/coder) [![codecov](https://codecov.io/gh/coder/code-server/branch/main/graph/badge.svg?token=5iM9farjnC)](https://codecov.io/gh/coder/code-server) [![See latest](https://img.shields.io/static/v1?label=Docs&message=see%20latest&color=blue)](https://coder.com/docs/code-server/latest)\n\nRun [VS Code](https://github.com/Microsoft/vscode) on any machine anywhere and\naccess it in the browser.\n\n![Screenshot](./assets/screenshot-1.png)\n![Screenshot](./assets/screenshot-2.png)\n\n## Highlights\n\n- Code on any device with a consistent development environment\n- Use cloud servers to speed up tests, compilations, downloads, and more\n- Preserve battery life when you're on the go; all intensive tasks run on your\n  server\n\n## Requirements\n\nSee [requirements](https://coder.com/docs/code-server/latest/requirements) for minimum specs, as well as instructions\non how to set up a Google VM on which you can install code-server.\n\n**TL;DR:** Linux machine with WebSockets enabled, 1 GB RAM, and 2 vCPUs\n\n## Getting started\n\nThere are five ways to get started:\n\n1. Using the [install\n   script](https://github.com/coder/code-server/blob/main/install.sh), which\n   automates most of the process. The script uses the system package manager if\n   possible.\n2. Manually [installing\n   code-server](https://coder.com/docs/code-server/latest/install)\n3. Deploy code-server to your team with [coder/coder](https://cdr.co/coder-github)\n4. Using our one-",
    "url": "https://github.com/coder/code-server",
    "last_updated": "2025-09-02T09:49:07+00:00"
  },
  {
    "full_name": "langchain-ai/langchain",
    "name": "langchain",
    "description": "🦜🔗 Build context-aware reasoning applications 🦜🔗",
    "language": "Jupyter Notebook",
    "topics": [
      "ai",
      "anthropic",
      "gemini",
      "langchain",
      "llm",
      "openai",
      "python"
    ],
    "readme": "<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/static/img/logo-dark.svg\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/static/img/logo-light.svg\">\n  <img alt=\"LangChain Logo\" src=\"docs/static/img/logo-dark.svg\" width=\"80%\">\n</picture>\n\n<div>\n<br>\n</div>\n\n[![Release Notes](https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square)](https://github.com/langchain-ai/langchain/releases)\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-core?style=flat-square)](https://opensource.org/licenses/MIT)\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain)](https://pypistats.org/packages/langchain-core)\n[![GitHub star chart](https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square)](https://star-history.com/#langchain-ai/langchain)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode&style=flat-square)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain)\n[<img src=\"https://github.com/codespaces/badge.svg\" alt=\"Open in Github Codespace\" title=\"Open in Github Codespace\" width=\"150\" height=\"20\">](https://codespaces.new/langchain-ai/langchain)\n[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/langchain-ai/langchain)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\n\n> [!NOTE]\n> Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\n\nLangChain is a framework for building LLM-powered applications. It helps you chain\ntogether interoperable components and third-party integrations to simplify AI\napplication development —  all while future-proofing decisions as the underlying\ntechnology evolves.\n\n```bash\npip install -U lang",
    "url": "https://github.com/langchain-ai/langchain",
    "last_updated": "2025-09-02T10:09:55+00:00"
  },
  {
    "full_name": "psincraian/pepy",
    "name": "pepy",
    "description": "pepy is a site to get statistics information about any Python package.",
    "language": "Python",
    "topics": [
      "python3",
      "mit",
      "flask",
      "pip",
      "website",
      "hacktoberfest"
    ],
    "readme": "<p align=\"center\">\n  <img width=\"100px\" alt=\"pepy-logo\" src=\"docs/logo.png\" />\n</p>\n\n<h2 align=\"center\"><code>PePy</code></h2>\n\n> **Important:**  \n> This repository contains the legacy public version of PePy. The latest version is now\n> maintained privately. We continue to use this repository solely to track issues and\n> preserve historical references.\n\n## 📜 About\n\n[PePy](https://pepy.tech) is a website that provides statistics about Python packages.\nThis repo houses the backend service for PePy.\n\n## 💖 Sponsors\n\nOur website stays alive thanks to your support and the continued help from our sponsors.\n\n[![DigitalOcean Referral Badge](https://web-platforms.sfo2.digitaloceanspaces.com/WWW/Badge%202.svg)](https://www.digitalocean.com/?refcode=7bf782110d6c&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge)\n\n<!-- sponsors --><a href=\"https://github.com/samuelcolvin\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;samuelcolvin.png\" width=\"60px\" alt=\"User avatar: Samuel Colvin\" /></a><a href=\"https://github.com/sethmlarson\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;sethmlarson.png\" width=\"60px\" alt=\"User avatar: Seth Michael Larson\" /></a><a href=\"https://github.com/guardrails-ai\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;guardrails-ai.png\" width=\"60px\" alt=\"User avatar: Guardrails AI\" /></a><!-- sponsors -->\n\n## ⚒️ Start contributing\nI wanted to make the setup of the environment as easy as possible. To start the environment you need the \nfollowing prerequisites:\n\n### Prerequisites\n  * bash (+4.3)\n  * docker (+17.05)\n  * docker-compose (+1.16.1)\n  * docker-py (+2.2.1)\n  * ansible (+2.3)\n  \n### Start environment\nYou only (_fingers crossed_) need to execute the following to start the environment:\n\n```commandline\nmake start-containers\n```\n\n## Architecture and patterns\nPrincipally I used some DDD concepts (like value objects, entities, and so on) and also CQS whose objective is to\nseparate commands from queries.\n\nThe structure of the code is the following",
    "url": "https://github.com/psincraian/pepy",
    "last_updated": "2025-08-25T04:08:21+00:00"
  },
  {
    "full_name": "approximateinference/approximateinference.github.io",
    "name": "approximateinference.github.io",
    "description": "NeurIPS workshop on Advances in Approximate Bayesian Inference",
    "language": "HTML",
    "topics": [],
    "readme": "# AABI Symposium Website\n\nThis website is written in [AstroJS](https://astro.build/).\nLearn more about Astro in its [documentation](https://docs.astro.build/en/getting-started/)!\nNote that Astro is Typescript-based, although a minimal knowledge is necessary:\nThe contents of this website are entirely written in Markdown!\n\nFor how-tos FAQs specific to the AABI website, please see below.\n\n## Table of Contents\n\n- [Setup](#setup)\n- [How-Tos](#how-tos)\n  - [Updateing CfP](#updating-call-of-papers-cfp)\n  - [Updating schedule](#updating-schedule)\n  - [Updating invited speakers/organizers list](#updating-invited-speakersorganizers-list)\n  - [Displaying accepted papers list](#displaying-list-of-accepted-papers)\n  - [Creating/updating contents in general](#creatingupdating-contents)\n\n## Setup\n\nInstall Bun (it's a faster, modern NodeJS alternative --- a Javascript runtime): <https://bun.sh/docs/installation>.\nThen, install all dependencies:\n\n```\nbun install\n```\n\nTo serve the website locally for development, run:\n\n```\nbun astro dev\n```\n\nAnd finally, go to `https://localhost:4321`.\n\nBefore pushing to Github, check and test-build the site:\n\n```\nbun astro check\nbun astro build\n```\n\nThe actual build & deploy to <https://approximateinference.org> are automatically handled by the Github action in `.github/workflows/deploy.yml`, as soon as the `master` branch is updated.\n\n## How-Tos\n\n### Updating call of papers (CfP)\n\nSimply update [`src/contents/call.md`](https://github.com/approximateinference/approximateinference/blob/2025/src/contents/call.md).\n\n### Updating schedule\n\n1. Go to [`src/contents/schedule.md`](https://github.com/approximateinference/approximateinference/blob/2025/src/contents/schedule.md).\n2. Simply write standard markdown tables to create the schedule. E.g. use this website: <https://www.tablesgenerator.com/markdown_tables>.\n\n### Updating invited-speakers/organizers list\n\n1. Put profile pictures/headshots into `public/images/people` directory.\n2. Update `src/assets/inv",
    "url": "https://github.com/approximateinference/approximateinference.github.io",
    "last_updated": "2025-04-30T03:56:48+00:00"
  },
  {
    "full_name": "Newmu/dcgan_code",
    "name": "dcgan_code",
    "description": "Deep Convolutional Generative Adversarial Networks",
    "language": "Python",
    "topics": [],
    "readme": "# Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n## [Alec Radford]((https://github.com/newmu)), [Luke Metz](https://github.com/lukemetz), [Soumith Chintala](https://github.com/soumith)\n\nAll images in this paper are generated by a neural network. They are NOT REAL.\n\nFull paper here: [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434)\n\n###Other implementations of DCGAN\n* [Torch](https://github.com/soumith/dcgan.torch)\n* [Chainer](https://github.com/mattya/chainer-DCGAN)\n* [TensorFlow](https://github.com/carpedm20/DCGAN-tensorflow)\n\n##Summary of DCGAN\nWe \n- stabilize Generative Adversarial networks with some architectural constraints\n  - Replace any pooling layers with strided convolutions (discriminator) and fractional-strided\nconvolutions (generator).\n  - Use batchnorm in both the generator and the discriminator\n  - Remove fully connected hidden layers for deeper architectures. Just use average pooling at the end.\n  - Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n  - Use LeakyReLU activation in the discriminator for all layers.\n- use the discriminator as a pre-trained net for CIFAR-10 classification and show pretty decent results.\n- generate really cool bedroom images that look super real\n- To convince you that the network is not cheating:\n  - show the interpolated latent space, where transitions are really smooth and every image in the latent space is a bedroom.\n  - show bedrooms after one epoch of training (with a 0.0002 learning rate), come on the network cant really memorize at this stage.\n- To explore what the representations that the network learnt,\n  - show deconvolution over the filters, to show that maximal activations occur at objects like windows and beds\n  - figure out a way to identify and remove filters that draw windows in generation. \n    - Now you can control the generator to not output certain objects.\n- Because we are tripping\n  - Smiling woman - ",
    "url": "https://github.com/Newmu/dcgan_code",
    "last_updated": "2025-08-25T15:03:20+00:00"
  },
  {
    "full_name": "eddelbuettel/samples-rmarkdown-metropolis",
    "name": "samples-rmarkdown-metropolis",
    "description": "RMarkdown with Metropolis/Mtheme for Beamer",
    "language": "Makefile",
    "topics": [
      "rmarkdown",
      "markdown",
      "beamer",
      "modern",
      "metropolis",
      "theme"
    ],
    "readme": "\n## samples-rmarkdown-metropolis\n\nExample for a [Metropolis](https://github.com/matze/mtheme)-themed\n[RMarkdown](http://rmarkdown.rstudio.com/)-driven LaTeX\n[Beamer](https://en.wikipedia.org/wiki/Beamer_(LaTeX)) presentation. \n\n### Note\n\nIf you are interested in this, you probably want to look at the [binb](https://github.com/eddelbuettel/binb) package which \nimplements Metropolis (and more) directly from RMarkdown.\n\n### About\n\n_NB: This dates mostly from 2016. For a more recent and more complete alternative, see [binb](https://github.com/eddelbuettel/binb)._\n\nFor the last two or so years, I have been using\n[mtheme](https://github.com/matze/mtheme) (and/or a local variant I called\n'm2') as well as the newer (renamed to metropolis) release\n[metropolis](https://github.com/matze/mtheme) for all my\n[RMarkdown](http://rmarkdown.rstudio.com/)-based presentations as you can see\n[from my presentations page](http://dirk.eddelbuettel.com/presentations.html).\n\nAnd earlier last year I cleaned this up and wrote myself _local_ Ubuntu packages\nwhich are [here on Launchpad](https://launchpad.net/~edd/+archive/ubuntu/misc/+packages).\nI also have two [GitHub](https://www.github.com) repos for the underlying .deb package code:\n- the [pkg-latex-metropolis](https://github.com/eddelbuettel/pkg-latex-metropolis) package for the LaTeX part\n(which is also in TeXlive in an older version)\n- the [pkg-fonts-fira](https://github.com/eddelbuettel/pkg-fonts-fira) for\n  the underlying (free) font (and this sadly cannot build on launchpad as it\n  needs a download step).\n\n### Example\n\nAn animated gif containing all pages of the very simple example included here follows:\n\n![](beamerPresentation.gif)\n\nAs an aside, creating the gif is trivial, I used \n\n```bash\nconvert -delay 250 -geometry 480x360 beamerPresentation.pdf beamerPresentation.gif\n```\n\nwhere `convert` is a component of [imagemagick](http://www.imagemagick.org/); you can probably rely on\n[graphicsmagick](http://www.graphicsmagick.org/) too.\n\n#",
    "url": "https://github.com/eddelbuettel/samples-rmarkdown-metropolis",
    "last_updated": "2025-04-17T00:55:10+00:00"
  },
  {
    "full_name": "abonica/WAR-Analysis-ideology",
    "name": "WAR-Analysis-ideology",
    "description": "An analysis of Split Ticket's WAR metric. ",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/abonica/WAR-Analysis-ideology",
    "last_updated": "2025-08-29T01:20:23+00:00"
  },
  {
    "full_name": "CeON/CERMINE",
    "name": "CERMINE",
    "description": "Content ExtRactor and MINEr",
    "language": "Java",
    "topics": [
      "metadata-extraction",
      "reference-parsing",
      "affiliation-parsing",
      "machine-learning",
      "java",
      "pdf"
    ],
    "readme": "Content ExtRactor and MINEr\n===========================\n\nCERMINE is a Java library and a web service ([cermine.ceon.pl](http://cermine.ceon.pl/)) for extracting metadata\nand content from PDF files containing academic publications.\nCERMINE is written in Java at [Centre for Open Science](http://ceon.pl/en/research/) at [Interdisciplinary Centre for Mathematical and Computational Modelling](http://www.icm.edu.pl/), [University of Warsaw](http://www.uw.edu.pl/).\n\nThe code is licensed under GNU Affero General Public License version 3.\n\nHow to cite CERMINE:\n\n\tDominika Tkaczyk, Pawel Szostek, Mateusz Fedoryszak, Piotr Jan Dendek and Lukasz Bolikowski. \n\tCERMINE: automatic extraction of structured metadata from scientific literature. \n\tIn International Journal on Document Analysis and Recognition (IJDAR), 2015, \n\tvol. 18, no. 4, pp. 317-335, doi: 10.1007/s10032-015-0249-8.\n\nDOI of CERMINE release 1.13:\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.569829.svg)](https://doi.org/10.5281/zenodo.569829)\n\nUsing CERMINE\n-------------\n\nCERMINE can be used for:\n\n  * extracting metadata, full text and parsed references from a PDF file,\n  * extracting metadata from reference strings,\n  * extracting metadata from affiliation strings.\n\nIn all tasks the default output format is [NLM JATS](http://jats.nlm.nih.gov/archiving/tag-library/1.1/).\n\nThere are three way of using CERMINE, depending on the user's needs:\n\n  * **standalone application** -- use this, if you need to process larger amounts of data locally on your laptop or server\n  * **Maven dependency** -- allows to use CERMINE's API in your own Java/Scala code\n  * **web application** -- for demonstration purposes and only small amounts (less than 50 files) of data\n\nRefer to one of the sections below for details.\n\n\n**Standalone application**\n\nThe easiest way to process files on a laptop/server is using CERMINE as a standalone application. All you will need is a single JAR file containing all the tools, external libraries and lea",
    "url": "https://github.com/CeON/CERMINE",
    "last_updated": "2025-08-29T03:43:43+00:00"
  },
  {
    "full_name": "Shreeshrii/imagesmar",
    "name": "imagesmar",
    "description": "Images and groundtruth text for Marathi for OCR evaluation",
    "language": "",
    "topics": [],
    "readme": "# imagesmar\n\nMarathi printed samples from [Full page images from the font sample booklet of a Devanagari printing press](http://kornai.com/Hindi/hindifont.tgz) are taken from [Hindi font samples](http://kornai.com/Hindi/) by Andras Kornai. \n",
    "url": "https://github.com/Shreeshrii/imagesmar",
    "last_updated": "2020-09-29T16:23:56+00:00"
  },
  {
    "full_name": "krlmlr/utf8",
    "name": "utf8",
    "description": "UTF-8 Text Processing (R Package)",
    "language": "C",
    "topics": [],
    "readme": "\n<!-- README.md and index.md are generated from README.Rmd. Please edit that file. -->\n\n# utf8\n\n<!-- badges: start -->\n\n[![rcc](https://github.com/patperry/r-utf8/workflows/rcc/badge.svg)](https://github.com/krlmlr/utf8/actions)\n[![Coverage\nStatus](https://codecov.io/github/patperry/r-utf8/coverage.svg?branch=main \"Code Coverage\")](https://app.codecov.io/github/patperry/r-utf8?branch=main \"Code Coverage\")\n[![CRAN\nStatus](https://www.r-pkg.org/badges/version/utf8 \"CRAN Page\")](https://cran.r-project.org/package=utf8 \"CRAN Page\")\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg \"Apache License, Version 2.0\")](https://www.apache.org/licenses/LICENSE-2.0.html \"Apache License, Version 2.0\")\n[![CRAN RStudio Mirror\nDownloads](https://cranlogs.r-pkg.org/badges/utf8 \"CRAN Downloads\")](https://cran.r-project.org/package=utf8 \"CRAN Page\")\n<!-- badges: end -->\n\n*utf8* is an R package for manipulating and printing UTF-8 text that\nfixes multiple bugs in R’s UTF-8 handling.\n\n## Installation\n\n### Stable version\n\n*utf8* is [available on\nCRAN](https://cran.r-project.org/package=utf8 \"CRAN Page\"). To install\nthe latest released version, run the following command in R:\n\n``` r\ninstall.packages(\"utf8\")\n```\n\n### Development version\n\nTo install the latest development version, run the following:\n\n``` r\ndevtools::install_github(\"patperry/r-utf8\")\n```\n\n## Usage\n\n``` r\nlibrary(utf8)\n```\n\n### Validate character data and convert to UTF-8\n\nUse `as_utf8()` to validate input text and convert to UTF-8 encoding.\nThe function alerts you if the input text has the wrong declared\nencoding:\n\n``` r\n# second entry is encoded in latin-1, but declared as UTF-8\nx <- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) <- c(\"UTF-8\", \"UTF-8\", \"bytes\")\nas_utf8(x) # fails\n#> Error in as_utf8(x): entry 2 has wrong Encoding; marked as \"UTF-8\" but leading byte 0xE7 followed by invalid continuation byte (0xdeadbeef) at position 4\n\n# mark the correct encoding\nEncoding(x[2]) <- \"latin1\"\nas_ut",
    "url": "https://github.com/krlmlr/utf8",
    "last_updated": "2025-08-05T00:54:16+00:00"
  },
  {
    "full_name": "tensorflow/adanet",
    "name": "adanet",
    "description": "Fast and flexible AutoML with learning guarantees.",
    "language": "Jupyter Notebook",
    "topics": [
      "automl",
      "tensorflow",
      "learning-theory",
      "deep-learning",
      "neural-architecture-search",
      "gpu",
      "machine-learning",
      "ensemble",
      "tpu",
      "python",
      "distributed-training"
    ],
    "readme": "# AdaNet\n\n<div align=\"center\">\n  <img src=\"https://tensorflow.github.io/adanet/images/adanet_tangram_logo.png\" alt=\"adanet_tangram_logo\"><br><br>\n</div>\n\n[![Documentation Status](https://readthedocs.org/projects/adanet/badge)](https://adanet.readthedocs.io)\n[![PyPI version](https://badge.fury.io/py/adanet.svg)](https://badge.fury.io/py/adanet)\n[![Travis](https://travis-ci.org/tensorflow/adanet.svg?branch=master)](https://travis-ci.org/tensorflow/adanet)\n[![codecov](https://codecov.io/gh/tensorflow/adanet/branch/master/graph/badge.svg)](https://codecov.io/gh/tensorflow/adanet)\n[![Gitter](https://badges.gitter.im/tensorflow/adanet.svg)](https://gitter.im/tensorflow/adanet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Downloads](https://pepy.tech/badge/adanet)](https://pepy.tech/project/adanet)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/tensorflow/adanet/blob/master/LICENSE)\n\n**AdaNet** is a lightweight TensorFlow-based framework for automatically learning high-quality models with minimal expert intervention. AdaNet builds on recent AutoML efforts to be fast and flexible while providing learning guarantees. Importantly, AdaNet provides a general framework for not only learning a neural network architecture, but also for learning to ensemble to obtain even better models.\n\nThis project is based on the _AdaNet algorithm_, presented in “[AdaNet: Adaptive Structural Learning of Artificial Neural Networks](http://proceedings.mlr.press/v70/cortes17a.html)” at [ICML 2017](https://icml.cc/Conferences/2017), for learning the structure of a neural network as an ensemble of subnetworks.\n\nAdaNet has the following goals:\n\n* _Ease of use_: Provide familiar APIs (e.g. Keras, Estimator) for training, evaluating, and serving models.\n* _Speed_: Scale with available compute and quickly produce high quality models.\n* _Flexibility_: Allow researchers and practitioners to extend AdaNet to novel subnetwork architectures, search sp",
    "url": "https://github.com/tensorflow/adanet",
    "last_updated": "2025-08-27T12:17:11+00:00"
  },
  {
    "full_name": "pgvector/pgvector",
    "name": "pgvector",
    "description": "Open-source vector similarity search for Postgres",
    "language": "C",
    "topics": [
      "nearest-neighbor-search",
      "approximate-nearest-neighbor-search"
    ],
    "readme": "# pgvector\n\nOpen-source vector similarity search for Postgres\n\nStore your vectors with the rest of your data. Supports:\n\n- exact and approximate nearest neighbor search\n- single-precision, half-precision, binary, and sparse vectors\n- L2 distance, inner product, cosine distance, L1 distance, Hamming distance, and Jaccard distance\n- any [language](#languages) with a Postgres client\n\nPlus [ACID](https://en.wikipedia.org/wiki/ACID) compliance, point-in-time recovery, JOINs, and all of the other [great features](https://www.postgresql.org/about/) of Postgres\n\n[![Build Status](https://github.com/pgvector/pgvector/actions/workflows/build.yml/badge.svg)](https://github.com/pgvector/pgvector/actions)\n\n## Installation\n\n### Linux and Mac\n\nCompile and install the extension (supports Postgres 13+)\n\n```sh\ncd /tmp\ngit clone --branch v0.8.0 https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nmake install # may need sudo\n```\n\nSee the [installation notes](#installation-notes---linux-and-mac) if you run into issues\n\nYou can also install it with [Docker](#docker), [Homebrew](#homebrew), [PGXN](#pgxn), [APT](#apt), [Yum](#yum), [pkg](#pkg), or [conda-forge](#conda-forge), and it comes preinstalled with [Postgres.app](#postgresapp) and many [hosted providers](#hosted-postgres). There are also instructions for [GitHub Actions](https://github.com/pgvector/setup-pgvector).\n\n### Windows\n\nEnsure [C++ support in Visual Studio](https://learn.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-170#download-and-install-the-tools) is installed and run `x64 Native Tools Command Prompt for VS [version]` as administrator. Then use `nmake` to build:\n\n```cmd\nset \"PGROOT=C:\\Program Files\\PostgreSQL\\17\"\ncd %TEMP%\ngit clone --branch v0.8.0 https://github.com/pgvector/pgvector.git\ncd pgvector\nnmake /F Makefile.win\nnmake /F Makefile.win install\n```\n\nSee the [installation notes](#installation-notes---windows) if you run into issues\n\nYou can also install it with [Docker](#docker) or [co",
    "url": "https://github.com/pgvector/pgvector",
    "last_updated": "2025-09-02T09:19:18+00:00"
  },
  {
    "full_name": "facebookarchive/eyescream",
    "name": "eyescream",
    "description": "natural image generation using ConvNets",
    "language": "Lua",
    "topics": [],
    "readme": "# The Eyescream Project\n\nGenerating Natural Images using Neural Networks.\n\nFor our research summary on this work, please read the Arxiv paper: [http://arxiv.org/abs/1506.05751](http://arxiv.org/abs/1506.05751)\n\nFor a high-level blog post with a live demo, please go to this website: [http://soumith.ch/eyescream](http://soumith.ch/eyescream)\n\nThis repository contains the code to train neural networks and reproduce our results from scratch.\n\n## Requirements\nEyescream requires or works with\n* Mac OS X or Linux\n* NVIDIA GPU with compute capability of 3.5 or above.\n\n## Installing Dependencies\n* Install [Torch](http://torch.ch)\n* Install the nngraph and tds packages:\n\n```\nluarocks install tds\nluarocks install nngraph\n```\n\n## Training your neural networks\n\n* If you want to train the CIFAR-10 image generators, read the README in the cifar/ folder\n* If you want to train the LSUN/Imagenet image generators, read the README in the lsun/ folder\n\n\n## Discuss the paper/code at\n* groups.google.com/forum/#!forum/torch7\n\nSee the CONTRIBUTING file for how to help out.\n\n## License\nEyescream is BSD-licensed. We also provide an additional patent grant.",
    "url": "https://github.com/facebookarchive/eyescream",
    "last_updated": "2025-01-20T11:27:18+00:00"
  },
  {
    "full_name": "causaltext/causal-text-papers",
    "name": "causal-text-papers",
    "description": "Curated research at the intersection of causal inference and natural language processing.",
    "language": "",
    "topics": [
      "causality",
      "natural-language-processing"
    ],
    "readme": "# Papers about Causal Inference and Language\n\nA collection of papers and codebases about influence, causality, and language. \n\n_Pull requests welcome!_\n\n# Table of Contents\n\n1. [Datasets and Simulations](#datasets-and-simulations)\n2. [Learning resources and blog posts](#learning-resources-and-blog-posts)\n3. [Causal Inference with Text Variables](#causal--inference-with-text-variables)\n    1. [Text as treatment](#text-as-treatment)\n    2. [Text as mediator](#text-as-mediator)\n    3. [Text as outcome](#text-as-outcome)\n    4. [Text as confounder](#text-as-confounder)\n3. [Causality to improve NLP](#causality-to-improve-nlp)\n    1. [Causal interpretations and explanations](#causal-interpretations-and-explanations)\n    2. [Sensitivity and rhobustness](#sensitivity-and-robustness)\n4. [Applications in the Social Sciences](#applications-in-the-social-sciences)\n    1. [Linguistics](#linguistics)\n    2. [Marketing](#marketing)\n    3. [Persuasion & Argumentation](#persuasion--argumentation)\n    4. [Mental health](#mental-health)\n    5. [Psychology](#psychology)\n    6. [Economics](#economics)\n    7. [Bias and Fairness](#bias-and-fairness)\n    8. [Social Media](#social-media)\n    9. [Law](#law)\n    10. [Online Hate Speech](#online-hate-speech)\n5. [Potential Connections to Language](#potential-connections-to-language)\n    1. [Vectorized treatments](#vectorized-treatments)\n\n\n\n# Datasets and Simulations\n\n| Type | Description |   Code |\n|-------|--|----|\n|  Semi-simulated    | Given text (amazon reviews), extracts treatments (0 or 5 stars) and confounds (product type), then samples outcomes (sales) conditioned on the extracted treatments and confounds. |    [git](https://github.com/rpryzant/causal-text/blob/main/src/simulation.py)  |\n|     Fully synthetic  |   Samples outcomes, treatments, and confounds from binomial distributions, then words from a uniform distribution conditioned on those sampled variables.      |  [git](https://github.com/zachwooddoughty/emnlp2018-causal/blob/mas",
    "url": "https://github.com/causaltext/causal-text-papers",
    "last_updated": "2025-08-07T04:03:12+00:00"
  },
  {
    "full_name": "douglasgscofield/fastrank",
    "name": "fastrank",
    "description": "R package providing fast ranking for integer and numeric vectors, an alternative to calling .Internal(rank(...))",
    "language": "C",
    "topics": [],
    "readme": "fastrank: faster than `rank`\n=================================\n\nWe are doing a re-write of the API.\n\n`fastrank` an R package providing fast ranking for integer, numeric, logical\nand complex vectors, as an alternative to calling `.Internal(rank(...))`, which\npackages cannot do.  Its API is a bit more restrictive, in the interests of\nspeed.  You cannot sort `character` vectors or vectors containing `NA` with\n`fastrank`. if you need these capabilities, use base `rank` or convert your\ndata to a form accepted by `fastrank`.\n\nThe package provides a general interface via the `fastrank` function, a\nreplacement for the base R `rank`.  It accepts any of the above accepted\ndatatypes and any `ties.method`:\n\n```R\nfastrank(x, ties.method = c(\"average\", \"first\", \"random\", \"max\", \"min\"))\n```\n\nThere are also direct interfaces for specific data types with specific\ntie-breaking methods, if you can guarantee the data type of your vectors.\nThese are slightly faster for shorter vectors because setup time is reduced:\n\n```R\nfastrank_num_avg(x)\n```\n\nFor all functions, the ranks of `x` are returned in a vector of the same length\nas `x`.  As with `rank`, for `ties.method = \"average\"` the vector returned is\n`numeric` because ranks can be fractional; for all other methods the vector is\n`integer`.\n\n`fastrank` is designed to handle all `ties.method` arguments identically to\nbase `rank`.  `fastrank` handles `\"first\"` and `\"random\"` in C code, so should\nbe much faster for these.  **Unfortunately, the `\"first\"` method is not\ncurrently comparable** because the sort routine used does not preserve the\norder of equivalent items.  In future `fastrank` may switch to using a stable\nsort if `\"first\"` is requested.\n\nNo `fastrank` entry handles `NA` in data, nor do they accept `character`\nvectors for ranking.  The `Scollate` internal R routines for comparing\ncharacter strings using locales is not part of the R API, and it would probably\nbe a bigger job to provide this than the rest of `fastrank`.\n\n\n\nPerforman",
    "url": "https://github.com/douglasgscofield/fastrank",
    "last_updated": "2023-02-14T11:07:56+00:00"
  },
  {
    "full_name": "facebookresearch/amortized-optimization-tutorial",
    "name": "amortized-optimization-tutorial",
    "description": "Tutorial on amortized optimization for learning to optimize over continuous domains",
    "language": "TeX",
    "topics": [],
    "readme": "# Tutorial on Amortized Optimization\nThis repository contains the source code for the paper\n[Tutorial on amortized optimization for\nlearning to optimize over continuous domains](https://arxiv.org/abs/2202.00665)\nby\n[Brandon Amos](http://bamos.github.io).\nThe main LaTeX source is in [paper](./paper)\nand the source code examples are in [code](./code).\nThe code that generates the following plots is also\nin [code/figures](./code/figures):\n\n## [main-example.py](./code/figures/main-example.py)\n![](./paper/fig/opt.png?raw=true)\n\n![](./paper/fig/learning-obj.png?raw=true)\n![](./paper/fig/learning-reg.png?raw=true)\n\n![](./paper/fig/learning-rl.png?raw=true)\n\n## [maxent-animation.py](./code/figures/maxent-animation.py)\n![](./paper/fig/maxent.gif?raw=true)\n\n## [maxent.py](./code/figures/maxent.py)\n![](./paper/fig/maxent.png?raw=true)\n\n## [ctrl.py](./code/figures/ctrl.py)\n![](./paper/fig/ctrl.png?raw=true)\n\n## [imaml.py](./code/figures/imaml.py)\n![](./paper/fig/imaml.png?raw=true)\n\n## [fixed-point.py](./code/figures/fixed-point.py)\n![](./paper/fig/fp.png?raw=true)\n\n## [loss-comp.py](./code/figures/loss-comp.py)\n![](./paper/fig/loss-comp.png?raw=true)\n\n## [smoothed-loss.py](./code/figures/smoothed-loss.py)\n![](./paper/fig/smoothed-loss.png?raw=true)\n\n# Licensing\nThe source code for this tutorial, plots, and\nsphere experiment is licensed under the\n[CC BY-NC 4.0 License](https://creativecommons.org/licenses/by-nc/4.0/).\n",
    "url": "https://github.com/facebookresearch/amortized-optimization-tutorial",
    "last_updated": "2025-06-16T12:05:41+00:00"
  },
  {
    "full_name": "veltman/endorsements",
    "name": "endorsements",
    "description": "Data on newspaper presidential endorsements",
    "language": "",
    "topics": [],
    "readme": "# Newspaper presidential endorsements, 1980-\n\nSee also: [http://noahveltman.com/endorsements/](http://noahveltman.com/endorsements/)\n\nData on who the top ~100 US newspapers by circulation endorsed for president in each election from 1980 through 2020. Additions/corrections welcome.\n\nThe data is provided as a CSV file with the following columns:\n\n* `publication`: the name of the publication, as of 2016 (see notes below about renamings/mergers/etc.)\n* `year`: the year of the endorsement\n* `endorsed`: who the paper endorsed that year\n* `source`, `source2`: sources for the data, either URLs or citations\n* `notes`: any other relevant notes about the endorsement or source\n\n### Notes on sources\n\nWhenever possible, I've linked to primary sources. In some cases, the source is something like the [American Presidency Project](http://www.presidency.ucsb.edu/data.php) because the original URL on the newspaper's site is gone. A number of pre-2000 endorsements were confirmed by a list provided by the Reagan Library and microfilm of old issues of *Editor & Publisher*. I tracked down the rest through a mix of various newspaper archives and emails and phone calls to local public libraries and the newsrooms themselves.\n\nIn cases where the source is an article from a paid database or old microfilm, I've tried to make the citation as detailed as possible, but posting all the originals would be a copyright issue. Contact me if you want to see the original article behind a citation.\n\nIn a few cases, I don't have the full citation but have included the details of who confirmed the endorsement.\n\nAt some point I'll try to add Internet Archive URLs to the CSV to give the existing URLs a longer shelf life.\n\n### Notes on mergers\n\nOne thing that makes this data collection particularly challenging is the intricate history of newspaper mergers, splits, and renamings. For example, many current papers are the result of a merger between a forming morning paper and a former afternoon/evening paper (e.",
    "url": "https://github.com/veltman/endorsements",
    "last_updated": "2024-10-28T22:24:39+00:00"
  },
  {
    "full_name": "lmullen/gender",
    "name": "gender",
    "description": "Predict Gender from Names Using Historical Data",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-package"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# gender\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/gender)](https://CRAN.R-project.org/package=gender)\n[![CRAN_Downloads](https://cranlogs.r-pkg.org/badges/grand-total/gender)](https://CRAN.R-project.org/package=gender)\n\n## Guidelines and warnings\n\nThis package attempts to infer gender (or more precisely, sex assigned\nat birth) based on first names using historical data, typically data\nthat was gathered by the state. This method has many limitations, and\nbefore you use this package be sure to take into account the following\nguidelines.\n\n1.  Your analysis and the way you report it should take into account the\n    limitations of this method, which include its reliance of data\n    created by the state and its inability to see beyond the\n    state-imposed gender binary. At a minimum, be sure to read our\n    article explaining the limitations of this method, as well as the\n    review article that is critical of this sort of methodology, both\n    cited below.\n\n2.  Do not use this package to study individuals: it is at most useful\n    for studying populations in the aggregate.\n\n3.  Resort to this method only when the alternative is not a more\n    nuanced and justifiable approach to studying gender, but where the\n    alternative is not studying gender at all. For instance, for many\n    historical sources this approach might be the only way to get a\n    sense of the sex ratios in a population. But ask whether you really\n    need to use this method, whether you are using it responsibly, or\n    whether you could use a better approach instead.\n\nBlevins, Cameron, and Lincoln A. Mullen, “Jane, John … Leslie? A\nHistorical Method for Algorithmic Gender Prediction,” *Digital\nHumanities Quarterly* 9, no. 3 (2015).\n<http://www.digitalhumanities.org/dhq/vol/9/3/000223/000223.html>\n\nMihaljević, Helena, Marco Tullney, Lucía Santamaría, and Christian\nSteinfeldt. “Reflections on Gender Analyses of Bi",
    "url": "https://github.com/lmullen/gender",
    "last_updated": "2025-07-16T08:42:11+00:00"
  },
  {
    "full_name": "r-hub/cranlogs",
    "name": "cranlogs",
    "description": "Download Logs from the RStudio CRAN Mirror",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-package"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# cranlogs <a href='https://r-hub.github.io/cranlogs'><img src='man/figures/logo.png' align=\"right\" height=\"138.5\" /></a>\n\n> Download logs from the RStudio CRAN mirror\n\n<!-- badges: start -->\n\n[![Linux Build\nStatus](https://travis-ci.org/r-hub/cranlogs.svg?branch=master)](https://travis-ci.org/r-hub/cranlogs)\n[![Windows Build\nstatus](https://ci.appveyor.com/api/projects/status/github/metacran/cranlogs?svg=true)](https://ci.appveyor.com/project/gaborcsardi/cranlogs)\n[![CRAN\nversion](http://www.r-pkg.org/badges/version/cranlogs)](http://www.r-pkg.org/pkg/cranlogs)\n[![CRAN RStudio mirror\ndownloads](http://cranlogs.r-pkg.org/badges/cranlogs)](http://www.r-pkg.org/pkg/cranlogs)\n[![cran\nchecks](https://cranchecks.info/badges/worst/cranlogs)](https://cranchecks.info/pkgs/cranlogs)\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Gitter\nchat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/r-hub/community)\n<!-- badges: end -->\n\n[RStudio](http://www.rstudio.com) publishes the download logs from their\nCRAN package mirror daily at <http://cran-logs.rstudio.com>.\n\nThis R package queries a [web API maintained by\nR-hub](https://github.com/r-hub/cranlogs.app#the-api-of-the-cran-downloads-database)\nthat contains the daily download numbers for each package.\n\nThe RStudio CRAN mirror is not the only CRAN mirror, but it’s a popular\none: it’s the default choice for RStudio users. The actual number of\ndownloads over all CRAN mirrors is unknown.\n\n## Installation\n\nYou can install `cranlogs` from CRAN:\n\n``` r\ninstall.packages(\"cranlogs\")\n```\n\nOr get the development version from Github:\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"r-hub/cranlogs\")\n```\n\n## Usage\n\nIt has a very simple API. By default it shows the total number of\npackage d",
    "url": "https://github.com/r-hub/cranlogs",
    "last_updated": "2025-08-25T16:29:24+00:00"
  },
  {
    "full_name": "facebookarchive/SCRNNs",
    "name": "SCRNNs",
    "description": "This is a self contained software accompanying the paper titled: Learning Longer Memory in Recurrent Neural Networks: http://arxiv.org/abs/1412.7753.",
    "language": "Lua",
    "topics": [],
    "readme": "# Structurally Constrained Recurrent Neural Network\n\nThis is a self contained software accompanying the paper titled: Learning\nLonger Memory in Recurrent Neural Networks: http://arxiv.org/abs/1412.7753.\nThe code allows you to reproduce our results on two language modeling datasets:\n* PenntreeBank\n* Text8\n\nThe code implements three recurrent models:\n* Standard Recurrent Neural Networks\n* Long Short Term Memory Recurrent Neural Networks\n* Structurally Constrained Recurrent Neural Networks\n\nIt also allows you to play around with various hyper-parameters.\n\n## Examples\nHere are some of the examples of how to use the code.\n\n* To run a standard RNN model on PenntreeBank with following\nhyper-parameters:\n  * hidden units: 100\n  * minibatch size: 32\n  * learning rate: 0.05\n\nyou type\n```\nth -i main.lua -dset ptb -name srnn_sm -nhid 100 -batchsz 32 -eta 0.05\n```\n\n* To run a LSTM RNN model on Text8 with following\nhyper-parameters:\n  * hidden units: 100\n  * minibatch size: 32\n  * learning rate: 0.05\n  * unfolding depth: 20\n  * backprop frequency: 5\n\nyou type\n```\nth -i main.lua -dset text8 -name lstm_sm -nhid 100 -batchsz 32 -eta 0.05 -blen 20 -bfreq 5\n```\n\n* To run a Structurally Constrained RNN model on PenntreeBank with following\nhyper-parameters:\n  * hidden units: 100\n  * number of constrained units: 20\n  * minibatch size: 32\n  * learning rate: 0.05\n  * unfolding depth: 30\n  * backprop frequency: 5\n\nyou type\n```\nth -i main.lua -dset text8 -name scrnn_sm -nhid 100 -nslow 20 -batchsz 32 -eta 0.05 -blen 30 -bfreq 5\n```\n\nTo list all the options available, you need to type\n```\nth main.lua --help\n```\n\n\n## Requirements\nThe software requires you to have the following two packages already\ninstalled on your systems:\n* Torch 7\n* fbcunn\n* Installation instructions for both on Ubuntu 14.04 are here: https://github.com/facebook/fbcunn/blob/master/INSTALL.md\n\n\n## Installing\nDownload the files in an appropriate directory and run\nthe code from there. See below.\n\n\n## How Structurally Constraine",
    "url": "https://github.com/facebookarchive/SCRNNs",
    "last_updated": "2025-01-26T04:13:50+00:00"
  },
  {
    "full_name": "ropensci/tesseract",
    "name": "tesseract",
    "description": "Bindings to Tesseract OCR engine for R",
    "language": "R",
    "topics": [
      "tesseract",
      "ocr",
      "r",
      "rstats",
      "r-package",
      "tesseract-ocr"
    ],
    "readme": "# tesseract\n\n> Bindings to [Tesseract-OCR](https://opensource.google/projects/tesseract): \n  a powerful optical character recognition (OCR) engine that supports over 100 languages.\n  The engine is highly configurable in order to tune the detection algorithms and\n  obtain the best possible results.\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/tesseract)](https://cran.r-project.org/package=tesseract)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/tesseract)](https://cran.r-project.org/package=tesseract)\n\n - Upstream Tesseract-OCR documentation: https://tesseract-ocr.github.io/tessdoc/\n - Introduction: https://docs.ropensci.org/tesseract/articles/intro.html\n - Reference: https://docs.ropensci.org/tesseract/reference/ocr.html\n\n## Hello World\n\nSimple example\n\n```r\n# Simple example\ntext <- ocr(\"https://jeroen.github.io/images/testocr.png\")\ncat(text)\n\n# Get XML HOCR output\nxml <- ocr(\"https://jeroen.github.io/images/testocr.png\", HOCR = TRUE)\ncat(xml)\n```\n\nRoundtrip test: render PDF to image and OCR it back to text\n\n```r\n# Full roundtrip test: render PDF to image and OCR it back to text\ncurl::curl_download(\"https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\", \"R-intro.pdf\")\norig <- pdftools::pdf_text(\"R-intro.pdf\")[1]\n\n# Render pdf to png image\nimg_file <- pdftools::pdf_convert(\"R-intro.pdf\", format = 'tiff', pages = 1, dpi = 400)\n\n# Extract text from png image\ntext <- ocr(img_file)\nunlink(img_file)\ncat(text)\n```\n\n## Installation\n\nOn Windows and MacOS the package binary package can be installed from CRAN:\n\n```r\ninstall.packages(\"tesseract\")\n```\n\nInstallation from source on Linux or OSX requires the `Tesseract` library (see below).\n\n### Install from source\n\n On __Debian__ or __Ubuntu__ install [libtesseract-dev](https://packages.debian.",
    "url": "https://github.com/ropensci/tesseract",
    "last_updated": "2025-08-12T04:28:54+00:00"
  },
  {
    "full_name": "ionelmc/cookiecutter-pylibrary",
    "name": "cookiecutter-pylibrary",
    "description": "Enhanced cookiecutter template for Python libraries.",
    "language": "Python",
    "topics": [
      "python",
      "cookiecutter-template",
      "template",
      "cookiecutter"
    ],
    "readme": "======================\ncookiecutter-pylibrary\n======================\n\nCookiecutter_ template for a Python library.\n\n*Notes*:\n\n* This is largely designed to address this `blog post about packaging python\n  libraries <https://blog.ionelmc.ro/2014/05/25/python-packaging/>`_.\n\n  * ... and it will save you from `packaging pitfalls\n    <https://blog.ionelmc.ro/2014/06/25/python-packaging-pitfalls/>`_.\n* There's a bare library using this template (if you're curious about the final\n  result): https://github.com/ionelmc/python-nameless.\n* If you have a web application (not a library) you might want to take a look at\n  `django-docker <https://github.com/evozon/django-docker>`_.\n\n.. contents:: Table of Contents\n\nFeatures\n--------\n\nThis is an \"all inclusive\" sort of template.\n\n* Choice of various licenses.\n* Tox_ for managing test environments for Python 2.7, 3.7+, PyPy etc.\n* Pytest_ or Nose_ for testing Python 2.7, 3.7+, PyPy etc.\n* *Optional* support for creating a tests matrix out of dependencies and python versions.\n* Travis-CI_ and AppVeyor_ for continuous testing.\n* Coveralls_ or Codecov_ for coverage tracking (using Tox_).\n* Documentation with Sphinx_, ready for ReadTheDocs_.\n* Configurations for:\n\n  * isort_\n  * bumpversion_ (bump2version_ required)\n\n* Support for C extensions (including coverage measurement for the C code). See c_extension_support_.\n* Packaging and code quality checks. This template comes with a tox environment (``check``) that will:\n\n  * Check if your ``README.rst`` is valid.\n  * Check if the ``MANIFEST.in`` has any issues.\n\nRequirements\n------------\n\nProjects using this template have these minimal dependencies:\n\n* Cookiecutter_ - just for creating the project\n* Tox_ - for running the tests\n* Setuptools_ - for building the package, wheels etc. Now-days Setuptools is widely available, it shouldn't pose a\n  problem :)\n\nTo get quickly started on a new system, just `install setuptools\n<https://pypi.org/project/setuptools#installation-instructions>`_ and ",
    "url": "https://github.com/ionelmc/cookiecutter-pylibrary",
    "last_updated": "2025-08-23T08:27:07+00:00"
  },
  {
    "full_name": "jbryer/Rgitbook",
    "name": "Rgitbook",
    "description": "Gitbook for R Markdown",
    "language": "CSS",
    "topics": [],
    "readme": "# Gitbook Projects with R Markdown\n\n#### Author: Jason Bryer ([jason@bryer.org](mailto:jason@bryer.org))    \n#### Website: [jason.bryer.org/Rgitbook](http://jason.bryer.org/Rgitbook)\n\nThis page is for an R package used to interface with the [Gitbook.io](http://gitbook.io) framework for developing books using [Markdown](https://daringfireball.net/projects/markdown/). It provides complete access to the Gitbook commands while adding support for [R Markdown](https://www.rstudio.com/ide/docs/r_markdown) and [MathJax](http://www.mathjax.org/). See the [R Gitbook](http://jason.bryer.org/Rgitbook) for a tutorial on how to use this R package.\n\nThis R package can be downloaded directly from Github using the `devtools` package:\n\n```\ndevtools::install_github('jbryer/Rgitbook')\n```\n\n#### Updates\n\n* 2014-04-23 - Added support for citations using the `knitcitations` package. The `newGitbook` will create a `references.Rmd` and `references.bib` file.\n* 2014-04-23 - Update to support new plugin architecture for using themes in Gitbook.\n\n",
    "url": "https://github.com/jbryer/Rgitbook",
    "last_updated": "2025-07-20T19:28:41+00:00"
  },
  {
    "full_name": "dill/emoGG",
    "name": "emoGG",
    "description": ":chart_with_upwards_trend::heart_eyes_cat: Emoji in ggplot2 :heart_eyes_cat::chart_with_upwards_trend:",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\nemoGG(plot)\n===========\n\nUse emoji in your `ggplot2` plots.\n\nThis is silly.\n\nInstallation\n------------\n\n    devtools::install_github(\"dill/emoGG\")\n\n`ggplot2` versions\n------------------\n\nNote that this branch works with `ggplot2` version 2 or higher, now\navailable on CRAN. If you have an older version of `ggplot2` please look\nat the `ggplot2-pre2` branch.\n\nUsage\n-----\n\n    library(ggplot2)\n    library(emoGG)\n\nFirst need to find an emoji, using the `emoji_search` function. First\nlook for a tulip:\n\n    emoji_search(\"tulip\")\n    #>          emoji  code keyword\n    #> 626      tulip 1f337 flowers\n    #> 627      tulip 1f337   plant\n    #> 628      tulip 1f337  nature\n    #> 629      tulip 1f337  summer\n    #> 630      tulip 1f337  spring\n    #> 3051 copyright    a9      ip\n\nThe `iris` example with **real** irises (well, tulips...)\n\n    ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +\n      geom_emoji(emoji=\"1f337\")\n\n![](figure/iris_ex-1.png)\n\nWhat about plotting `mtcars` with **real** cars?\n\n    ggplot(mtcars, aes(wt, mpg))+ geom_emoji(emoji=\"1f697\")\n\n![](figure/mtcars-1.png)\n\nSome random cats?\n\n    posx <- runif(50, 0, 10)\n    posy <- runif(50, 0, 10)\n    ggplot(data.frame(x = posx, y = posy), aes(x, y)) + geom_emoji(emoji=\"1f63b\")\n\n![](figure/catplotlib-1.png)\n\nWe can also just put a big emoji in the background:\n\n    qplot(x=Sepal.Length, y=Sepal.Width, data=iris, geom=\"point\") + add_emoji(emoji=\"1f337\")\n\n![](figure/big-emoji-1.png)\n\nAcknowledgements\n----------------\n\nEmoji lookup is from @muan's\n[`emojilib`](https://github.com/muan/emojilib).\n\nEmoji are loaded from a CDN using Twitter's\n[`twemoji`](https://github.com/twitter/twemoji), which is [CC-BY\n4.0](https://creativecommons.org/licenses/by/4.0/) licensed. You can get\nattribution details [on the project\npage](https://github.com/twitter/twemoji#attribution-requirements).\n\nWith apologies, DLM.\n",
    "url": "https://github.com/dill/emoGG",
    "last_updated": "2025-06-13T02:22:17+00:00"
  },
  {
    "full_name": "aeksco/aws-pdf-textract-pipeline",
    "name": "aws-pdf-textract-pipeline",
    "description": ":mag: Data pipeline for crawling PDFs from the Web and transforming their contents into structured data using AWS textract. Built with AWS CDK + TypeScript ",
    "language": "TypeScript",
    "topics": [
      "aws",
      "textract",
      "lambda",
      "dynamodb",
      "sns",
      "typescript",
      "pdf",
      "serverless",
      "webscraping",
      "s3",
      "aws-textract",
      "aws-cdk",
      "data-pipeline",
      "puppeteer",
      "cdk",
      "jest",
      "cloudformation"
    ],
    "readme": "# aws-pdf-textract-pipeline [![Mentioned in Awesome CDK](https://awesome.re/mentioned-badge.svg)](https://github.com/kolomied/awesome-cdk)\n\n:mag: Data pipeline for crawling PDFs from the Web and transforming their contents into structured data using [AWS Textract](https://aws.amazon.com/textract/). Built with AWS CDK + TypeScript.\n\nThis is an example data pipeline that illustrates one possible approach for large-scale serverless PDF processing - it should serve as a good foundation to modify for your own purposes.\n\n![Example Extension Popup](https://i.imgur.com/3F89JQK.png \"Example Extension Popup\")\n\n<!-- https://cloudcraft.co/view/e135397e-a673-411e-9ee7-05a5618052b2?key=R-OLiwplnkA9dtQxtkVqOw&interactive=true&embed=true -->\n\n**Getting Started**\n\nRun the following commands to install dependencies, build the CDK stack, and deploy the CDK Stack to AWS.\n\n```\nyarn install\nyarn build\ncdk bootstrap\ncdk deploy\n```\n\n### Overview\n\nThe following is an overview of each process performed by this CDK stack.\n\n1. **Scrape PDF download URLs from a website**\n\n   Scraping data from the [COGCC](https://cogcc.state.co.us/) website.\n\n2. **Store PDF download URL in DynamoDB**\n\n   ![Example Extension Popup](https://i.imgur.com/bmFJGDW.png \"Example Extension Popup\")\n\n3. **Download the PDF to S3**\n\n   A lambda fires off when a new PDF download URL has been created in DynamoDB.\n\n4. **Process the PDF with AWS Textract**\n\n   Another lambda fires off when a PDF has been downloaded to the S3 bucket.\n\n5. **Process the AWS Textract results**\n\n   When an SNS event is detected from AWS Textract, a lambda is fired off to process the result.\n\n6. **Save the processed Textract result to DynamoDB.**\n\n   After the full result is pruned down the the desired datastructure, we save the data in DynamoDB.\n   ![Example Extension Popup](https://i.imgur.com/HkTtLmi.png \"Example Extension Popup\")\n\n### Scripts\n\n- `yarn install` - installs dependencies\n- `yarn build` - builds the production-ready CDK Stack\n- `yarn ",
    "url": "https://github.com/aeksco/aws-pdf-textract-pipeline",
    "last_updated": "2025-07-17T12:18:18+00:00"
  },
  {
    "full_name": "gshotwell/easyMake",
    "name": "easyMake",
    "description": "Automatically generate Makefiles for R projects",
    "language": "HTML",
    "topics": [],
    "readme": "**Note** This package is not maintained, I recommend using the Drake package for project automation:  https://github.com/ropensci/drake\n\nEasy Make\n================\neasyMake is a proof of concept for a simple way to generate Makefiles based on an R dataframe listing file dependencies. It is not on CRAN, but you can install it with:\n\neasyMake is a proof of concept for a simple way to generate Makefiles based on an R dataframe listing file dependencies. It is not on CRAN, but you can install it with:\n\n    devtools::install_github(\"GShotwell/easyMake\")\n\nMake provides an incredibly powerful way to manage analysis projects. By using a Makefile to specify the way in which the files in your project depend on one another you can ensure that your analysis is always up to date, and that files are not being needlessly regenerated if nothing has changed. Using a Makefile is one of the best things that you can do to ensure that your analysis project is robust and reproducible. Writing a Makefile, however, requires learning a new programming paradigm, which is something many R users are uncomfortable with. Since it is often easier to edit an existing Makefile than to generate a new one from scratch, easyMake provides a set of tools to quickly and easily set up your own Makefile.\n\neasyMake is based on the principle that most R projects are built around R scripts which execute various actions, and artifacts which are the inputs and outputs to those scripts. For instance, you might write a script which reads in a dataset, alters it in some way, and then saves it as a new file for another script to read. The Input -&gt; Script -&gt; Output structure of many R projects lets us detect dependencies between files by detecting which artifacts are read into each script. If a script imports a file `data.csv` with, `read.csv()` then `data.csv` is a pre-requisite for that script. If it then saves it as `data2.RData`, then we know that the script is itself a pre-requisite for `data2.RData`.\n\nTh",
    "url": "https://github.com/gshotwell/easyMake",
    "last_updated": "2025-03-22T08:14:46+00:00"
  },
  {
    "full_name": "srkobakian/taipan",
    "name": "taipan",
    "description": "Image Annotations in R",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# Taipan <img src=\"man/figures/taipan.png\" align=\"right\" />\n\n[![Travis build\nstatus](https://travis-ci.org/srkobakian/taipan.svg?branch=master)](https://travis-ci.org/srkobakian/taipan)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/taipan)](https://cran.r-project.org/package=taipan)\n[![Downloads](http://cranlogs.r-pkg.org/badges/taipan?color=brightgreen)](https://cran.r-project.org/package=taipan)\n\nTaipan is a Tool for Annotating Images in Preparation for ANalysis.\n\nThe Taipan package creates simple shiny surveys. The web applications\ncan be used locally or deployed and allow people to save survey results\nas tidy data that is ready to use for analysis.\n\nThe app was initially created to streamline the process of manually\ntagging images to create a training set. It allows users to provide\ninformation regarding entire images, and smaller regions within. It is a\nsurvey style tool, with questions being posed for each image and area\nidentified.\n\n## Installation\n\nYou can install the **stable** version from\n[CRAN](https://cran.r-project.org/package=taipan).\n\n``` r\ninstall.packages('taipan')\n```\n\nYou can install the development version from Github using:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"srkobakian/taipan\")\n```\n\n## Usage\n\nTaipan builds a shiny app suitable for annotating images. Questions can\nbe provided for two scenarios in the app: ‘scene’ and ‘selection’. Scene\nquestions are suitable for questions that apply to the whole image, and\nare shown when no selection is made. Selection questions are appropriate\nfor selected areas of the image, and are shown when a selection is made.\n\nThese lists of questions can be flexibly produced using the\n`taipanQuestions` function, where any shiny inputs and web elements can\nbe used to build your own survey UI. For example, to train a model for\nidentifying hotdogs and condiments, you could use this interface:\n\n``` r\nlibrary(ta",
    "url": "https://github.com/srkobakian/taipan",
    "last_updated": "2025-03-22T08:14:19+00:00"
  },
  {
    "full_name": "johnmyleswhite/room_temperatures",
    "name": "room_temperatures",
    "description": "Dataset of temperatures in 6 rooms of my home recorded once per minute",
    "language": "R",
    "topics": [],
    "readme": "# The Room Temperatures Dataset\n\nTo demonstrate how complicated even a simple system with clear causal factors can be, I'm releasing data about the temperature in the rooms of my house. The temperature has been recorded once per minute using a Govee thermometer. The data is found in temperatures.csv, which contains three columns:\n\n1. room: The room in which the temperature was recorded.\n2. time: The time in YMD-HMS format at which the temperature was recorded.\n3. temperature: The temperature in Fahrenheit.\n\nThe dataset contains readings from September 17th, 2018 through October 17th, 2018.\n\nThe time series for this dataset is shown below:\n\n![Plot of Temperature over Time in 6 Rooms](time_series.png)\n\nThere some minor anomalies in the data: about 10 readings are missing for unknown reasons. Because of the high temporal resolution of the data, the missing readings are easily imputed or ignored.\n",
    "url": "https://github.com/johnmyleswhite/room_temperatures",
    "last_updated": "2023-11-08T23:51:03+00:00"
  },
  {
    "full_name": "facebookresearch/nougat",
    "name": "nougat",
    "description": "Implementation of Nougat Neural Optical Understanding for Academic Documents",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n<h1>Nougat: Neural Optical Understanding for Academic Documents</h1>\n\n[![Paper](https://img.shields.io/badge/Paper-arxiv.2308.13418-white)](https://arxiv.org/abs/2308.13418)\n[![GitHub](https://img.shields.io/github/license/facebookresearch/nougat)](https://github.com/facebookresearch/nougat)\n[![PyPI](https://img.shields.io/pypi/v/nougat-ocr?logo=pypi)](https://pypi.org/project/nougat-ocr)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Hugging Face Spaces](https://img.shields.io/badge/🤗%20Hugging%20Face-Community%20Space-blue)](https://huggingface.co/spaces/ysharma/nougat)\n\n</div>\n\nThis is the official repository for Nougat, the academic document PDF parser that understands LaTeX math and tables.\n\nProject page: https://facebookresearch.github.io/nougat/\n\n## Install\n\nFrom pip:\n```\npip install nougat-ocr\n```\n\nFrom repository:\n```\npip install git+https://github.com/facebookresearch/nougat\n```\n\n> Note, on Windows: If you want to utilize a GPU, make sure you first install the correct PyTorch version. Follow instructions [here](https://pytorch.org/get-started/locally/)\n\nThere are extra dependencies if you want to call the model from an API or generate a dataset.\nInstall via\n\n`pip install \"nougat-ocr[api]\"` or `pip install \"nougat-ocr[dataset]\"`\n\n### Get prediction for a PDF\n#### CLI\n\nTo get predictions for a PDF run\n\n```\n$ nougat path/to/file.pdf -o output_directory\n```\n\nA path to a directory or to a file where each line is a path to a PDF can also be passed as a positional argument\n\n```\n$ nougat path/to/directory -o output_directory\n```\n\n```\nusage: nougat [-h] [--batchsize BATCHSIZE] [--checkpoint CHECKPOINT] [--model MODEL] [--out OUT]\n              [--recompute] [--markdown] [--no-skipping] pdf [pdf ...]\n\npositional arguments:\n  pdf                   PDF(s) to p",
    "url": "https://github.com/facebookresearch/nougat",
    "last_updated": "2025-09-02T07:25:56+00:00"
  },
  {
    "full_name": "karpathy/ulogme",
    "name": "ulogme",
    "description": "Automatically collect and visualize usage statistics in Ubuntu/OSX environments.",
    "language": "Python",
    "topics": [],
    "readme": "\n# ulogme\n\n\n> ### How productive were you today? How much code have you written? Where did your time go?\n\nKeep track of your computer activity throughout the day: visualize your active window titles and the number of keystrokes in beautiful HTML timelines. Current features:\n\n- Records your **active window** title throughout the day\n- Records the **frequency of key presses** throughout the day\n- Record custom **note annotations** for particular times of day, or for day in general\n- Everything runs **completely locally**: none of your data is uploaded anywhere\n- **Beautiful, customizable UI** in HTML/CSS/JS (d3js).\n\nThe project currently **only works on Ubuntu and OSX**, and uses new fancy **Promises** feature of ECMAScript 6. This might not be implemented in all browsers. My Chrome has it, but for example my Firefox doesn't.\n\n## Demo\n\nSee a blog post (along with multiple screenshots) describing the project [here.](http://karpathy.github.io/2014/08/03/quantifying-productivity/)\n\n## Getting Started\n\n**To start recording**\n\n1. Clone the repository to some folder: `$ git clone https://github.com/karpathy/ulogme.git`\n2. If you're on Ubuntu, make sure you have the dependencies: `$ sudo apt-get install xdotool wmctrl`. On Fedora, you may also need `sudo yum install gnome-screensaver`.\n3. `cd` inside and run `$ ./ulogme.sh` (note: this will ask you for sudo authentication which is required for `showkey` command). This will launch two scripts. One records the frequency of keystrokes and the other records active window titles. Both write their logs into log files in the `logs/` directory. Every log file is very simply just the unix time stamp followed by data, one per line.\n4. For **OSX** only: there might be an additional step where you have to go to System Preferences > Security & Privacy > Accessibility, and make sure that Terminal (or iTerm2, or whatever you use to launch ulogme) is checked. If it wasn't checked previously and you just checked it, you may need to restart u",
    "url": "https://github.com/karpathy/ulogme",
    "last_updated": "2025-08-31T03:33:44+00:00"
  },
  {
    "full_name": "openml/openml-r",
    "name": "openml-r",
    "description": "R package to interface with OpenML",
    "language": "HTML",
    "topics": [
      "openml",
      "cran",
      "r",
      "dataset",
      "database",
      "machine-learning",
      "reproducible-research",
      "open-data",
      "data-science",
      "datasets",
      "opendata",
      "open-science",
      "openscience",
      "machine-learning-algorithms",
      "statistics",
      "classification",
      "regression",
      "benchmarking",
      "benchmarking-suite",
      "arff"
    ],
    "readme": "# R interface to [OpenML.org](http://www.openml.org/) \n\n[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![Rdoc](http://www.rdocumentation.org/badges/version/OpenML)](http://www.rdocumentation.org/packages/OpenML)\n[![CRAN Status Badge](http://www.r-pkg.org/badges/version/OpenML)](http://cran.r-project.org/web/packages/OpenML)\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/openml/openml-r/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/openml/openml-r/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/OpenML)](http://cran.rstudio.com/web/packages/OpenML/index.html)\n[![codecov](https://codecov.io/gh/openml/openml-r/branch/master/graph/badge.svg)](https://codecov.io/gh/openml/openml-r)\n\n[OpenML.org](http://www.openml.org/frontend/page/home) is an online machine learning platform where researchers can access open data, download and upload data sets, share their machine learning tasks and experiments and organize them online to work and collaborate with other researchers. \nThe R interface allows to query for data sets with specific properties, and allows the downloading and uploading of data sets, tasks, flows and runs.\n\nFor more information, have a look at our\n- <a href=\"https://openml.github.io/openml-r/articles/OpenML.html\" target=\"_blank\">R Tutorial</a>\n- <a href=\"https://github.com/openml/openml-r/blob/master/vignettes/openml-cheatsheet.pdf\" target=\"_blank\">R Cheatsheet</a>\n- <a href=\"https://openml.github.io/openml-r/reference\" target=\"_blank\">Function Reference</a>\n- <a href=\"http://dx.doi.org/10.1007/s00180-017-0742-2\" target=\"_blank\">OpenML R Package Publication</a>\n- <a href=\"https://www.openml.org/api_docs\" target=\"_blank\">OpenML API Guide</a>\n\n# Deprecated\n\nThis package relies on the [mlr](https://github.com/mlr-org/mlr) framework, which is now retired in favor of the newer [mlr3](https://mlr3.mlr-org",
    "url": "https://github.com/openml/openml-r",
    "last_updated": "2025-06-18T08:19:22+00:00"
  },
  {
    "full_name": "andrewheiss/diff-means-half-dozen-ways",
    "name": "diff-means-half-dozen-ways",
    "description": "Run standard t-tests, simulations, and Bayesian difference in means tests with R and Stan",
    "language": "Stan",
    "topics": [
      "r",
      "stan",
      "brms",
      "inference"
    ],
    "readme": "Half a dozen frequentist and Bayesian ways to measure the difference in\nmeans in two groups\n================\n\n> See the [actual blog\n> post](https://www.andrewheiss.com/blog/2019/01/29/diff-means-half-dozen-ways/).\n\n-----\n\nTaking a sample from two groups from a population and seeing if there’s\na significant or substantial difference between them is a standard task\nin statistics. Measuring performance on a test before and after some\nsort of intervention, measuring average GDP in two different continents,\nmeasuring average height in two groups of flowers, etc.—we like to know\nif any group differences we see are attributable to chance / measurement\nerror, or if they’re real.\n\nClassical frequentist statistics typically measures the difference\nbetween groups with a\n[t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), but t-tests\nare 100+ years old and statistical methods have advanced a lot since\n1908. Nowadays, we can use simulation and/or Bayesian methods to get\nricher information about the differences between two groups without\nworrying so much about the assumptions and preconditions for classical\nt-tests.\n\nMostly as a resource to future me, here are a bunch of different ways to\nmeasure the difference in means in two groups. I’ve done them all in\nreal life projects, but I’m tired of constantly searching my computer\nfor the code to do them:)\n\nThese ways can all be adapted to different situations (i.e. difference\nin proportions, one-sample difference in means, etc.). The process for\nsimulation and Bayesian approaches will be roughly the same, while for\nfrequentist approaches, you’ll need to walk through a [statistical test\nworkflow](https://www.google.com/search?q=statistical+test+workflow&tbm=isch)\nto find the appropriate test.\n\nAlso, this is long and really kind of meant as a general reference.\nHere’s a tl;dr table of contents:\n\n  - [Data](#data)\n  - [Classical frequentist t-tests](#classical-frequentist-t-tests)\n      - [t-test, assuming equal\n        variance",
    "url": "https://github.com/andrewheiss/diff-means-half-dozen-ways",
    "last_updated": "2024-11-02T17:41:21+00:00"
  },
  {
    "full_name": "rstudio/tufte",
    "name": "tufte",
    "description": "Tufte Styles for R Markdown Documents",
    "language": "R",
    "topics": [
      "tufte",
      "tufte-style",
      "r-package",
      "r-markdown"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tufte\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/tufte)](https://CRAN.R-project.org/package=tufte)\n[![Downloads from the RStudio CRAN\nmirror](https://cranlogs.r-pkg.org/badges/tufte)](https://cran.r-project.org/package=tufte)\n[![R-CMD-check](https://github.com/rstudio/tufte/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/tufte/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/rstudio/tufte/graph/badge.svg)](https://app.codecov.io/gh/rstudio/tufte)\n<!-- badges: end -->\n\nThis R package provides a few R Markdown output formats that use the\nTufte style. See <https://rstudio.github.io/tufte/> for a comprehensive\nexample.\n\n## Books\n\n<a href=\"https://bookdown.org/yihui/rmarkdown/tufte-handouts.html\"><img src=\"https://bookdown.org/yihui/rmarkdown/images/cover.png\" alt=\"R Markdown: The Definitive Guide\" class=\"book\" height=\"400\"/></a>\n\nSee about the Tufte Handouts format in R Markdown Definitive Guide\n\n## Installation\n\nYou can install the last available released version from\n[CRAN](https://cran.r-project.org/package=tufte)\n\n``` r\ninstall.packages('tufte')\n```\n\nYou can also install the development version of **tufte** from\n[GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"rstudio/tufte\")\n```\n\n## Usage\n\nThe easiest way to make a new R Markdown document using Tufte style is\nfrom within RStudio. Go to *File \\> New File \\> R Markdown \\> From\ntemplate \\> Tufte Handout*.\n\nThis can also be created from the command line using\n\n``` r\nrmarkdown::draft(\"tufte.Rmd\", \"tufte_html\", \"tufte\")\n```\n\n## Getting help\n\nThere are two main places to get help:\n\n1.  The [RStudio\n    community](https://forum.posit.co/c/quarto-r-markdown/10) is a\n    friendly place to ask any questions about rmarkdown and the R\n    Markdown family of packages. Use tag **tufte** in your post.\n\n2.  [Stack\n    Overflow](https:",
    "url": "https://github.com/rstudio/tufte",
    "last_updated": "2025-08-29T00:56:26+00:00"
  },
  {
    "full_name": "mlampros/ClusterR",
    "name": "ClusterR",
    "description": "Gaussian mixture models, k-means, mini-batch-kmeans and k-medoids clustering",
    "language": "R",
    "topics": [
      "gmm",
      "kmeans",
      "mini-batch-kmeans",
      "kmedoids-clustering",
      "rcpparmadillo",
      "cpp11",
      "r",
      "affinity-propagation"
    ],
    "readme": "\n[![tic](https://github.com/mlampros/ClusterR/workflows/tic/badge.svg?branch=master)](https://github.com/mlampros/ClusterR/actions)\n[![codecov.io](https://codecov.io/github/mlampros/ClusterR/coverage.svg?branch=master)](https://codecov.io/github/mlampros/ClusterR?branch=master)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/ClusterR)](http://cran.r-project.org/package=ClusterR)\n[![Downloads](http://cranlogs.r-pkg.org/badges/grand-total/ClusterR?color=blue)](http://www.r-pkg.org/pkg/ClusterR)\n<a href=\"https://www.buymeacoffee.com/VY0x8snyh\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Coffee\" height=\"21px\" ></a>\n[![](https://img.shields.io/docker/automated/mlampros/clusterr.svg)](https://hub.docker.com/r/mlampros/clusterr)\n[![Dependencies](https://tinyverse.netlify.com/badge/ClusterR)](https://cran.r-project.org/package=ClusterR)\n\n\n## ClusterR\n<br>\n\nThe ClusterR package consists of Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering algorithms with the option to plot, validate, predict (new data) and find the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. More details on the functionality of ClusterR can be found in the blog-posts ([first](http://mlampros.github.io/2016/09/12/clusterR_package/) and [second](http://mlampros.github.io/2022/12/04/comparison_partition_around_medoid/)), Vignette and in the package Documentation ( *scroll down for information on how to use the* **docker image** )\n<br><br>\n\n**UPDATE 16-08-2018**\n\n\nAs of version 1.1.4 the *ClusterR* package allows R package maintainers to perform **linking between packages at a C++ code (Rcpp) level**. This means that the Rcpp functions of the *ClusterR* package can be called in the C++ files of another package. In the next lines I'll give detailed explanations on how this can be done:\n\n<br>\n\nAs",
    "url": "https://github.com/mlampros/ClusterR",
    "last_updated": "2025-04-26T22:10:42+00:00"
  },
  {
    "full_name": "jupyterlite/pyodide-kernel",
    "name": "pyodide-kernel",
    "description": "Python kernel for JupyterLite powered by Pyodide",
    "language": "Python",
    "topics": [],
    "readme": "# jupyterlite-pyodide-kernel\n\n> A Python kernel for [JupyterLite](https://jupyterlite.rtfd.io) powered by\n> [Pyodide](https://pyodide.org),\n\n[![ci-badge]][ci] [![lite-badge]][lite] [![docs-badge]][docs]\n\n[ci-badge]: https://github.com/jupyterlite/pyodide-kernel/workflows/Build/badge.svg\n[lite-badge]: https://jupyterlite.rtfd.io/en/latest/_static/badge.svg\n[lite]: https://jupyterlite-pyodide-kernel.rtfd.io/en/latest/_static\n[ci]: https://github.com/jupyterlite/pyodide-kernel/actions?query=branch%3Amain\n[docs-badge]:\n  https://readthedocs.org/projects/jupyterlite-pyodide-kernel/badge/?version=latest\n[docs]: https://jupyterlite-pyodide-kernel.readthedocs.io/en/latest/?badge=latest\n\n## Requirements\n\n- `python >=3.9`\n\n### Compatibility\n\n#### With Jupyter\n\n| status | `jupyterlite-pyodide-kernel` | `jupyterlite-core` |  `jupyterlab`  |   `notebook`   |  `retrolab`  |\n| :----: | :--------------------------: | :----------------: | :------------: | :------------: | :----------: |\n| alpha  |           `0.7.*`            |    `>=0.6,<0.7`    | `>=4.4.5,<4.5` | `>=7.4.5,<7.5` |      -       |\n| stable |           `0.6.*`            |    `>=0.6,<0.7`    | `>=4.4.3,<4.5` | `>=7.4.3,<7.5` |      -       |\n| stable |           `0.5.*`            |    `>=0.5,<0.6`    | `>=4.3.0,<4.4` | `>=7.3.0,<7.4` |      -       |\n| stable |           `0.4.*`            |    `>=0.4,<0.5`    | `>=4.2.0,<4.3` | `>=7.2.0,<7.3` |      -       |\n| stable |           `0.3.*`            |    `>=0.3,<0.4`    | `>=4.1.1,<4.2` | `>=7.1.0,<7.2` |      -       |\n| stable |           `0.2.*`            |    `>=0.2,<0.3`    | `>=4.0.7,<4.1` |  `>=7.0.5,<8`  |      -       |\n| stable |           `0.1.*`            |    `>=0.1,<0.2`    |  `>=3.5,<3.6`  |       -        | `>=0.3,<0.4` |\n\nInstalling the matching version of JupyterLab with your package manager can help ensure\nmatching labextension assets and kernel dependencies, even though this kernel does not\nyet work in a full, `jupyter_server`-hosted client such",
    "url": "https://github.com/jupyterlite/pyodide-kernel",
    "last_updated": "2025-08-22T15:11:24+00:00"
  },
  {
    "full_name": "AnswerDotAI/fastcore",
    "name": "fastcore",
    "description": "Python supercharged for the fastai library",
    "language": "Jupyter Notebook",
    "topics": [
      "python",
      "fastai",
      "languages",
      "developer-tools",
      "data-structures",
      "functional-programming",
      "parallel-processing",
      "dispatch",
      "documentation-generator"
    ],
    "readme": "# Welcome to fastcore\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\nPython is a powerful, dynamic language. Rather than bake everything into\nthe language, it lets the programmer customize it to make it work for\nthem. `fastcore` uses this flexibility to add to Python features\ninspired by other languages we’ve loved, mixins from Ruby, and currying,\nbinding, and more from Haskell. It also adds some “missing features” and\nclean up some rough edges in the Python standard library, such as\nsimplifying parallel processing, and bringing ideas from NumPy over to\nPython’s `list` type.\n\n## Getting started\n\nTo install fastcore run: `conda install fastcore -c fastai` (if you use\nAnaconda, which we recommend) or `pip install fastcore`. For an\n[editable\ninstall](https://stackoverflow.com/questions/35064426/when-would-the-e-editable-option-be-useful-with-pip-install),\nclone this repo and run: `pip install -e \".[dev]\"`. fastcore is tested\nto work on Ubuntu, macOS and Windows (versions tested are those shown\nwith the `-latest` suffix\n[here](https://docs.github.com/en/actions/reference/specifications-for-github-hosted-runners#supported-runners-and-hardware-resources)).\n\n`fastcore` contains many features, including:\n\n- `fastcore.test`: Simple testing functions\n- `fastcore.foundation`: Mixins, delegation, composition, and more\n- `fastcore.xtras`: Utility functions to help with functional-style\n  programming, parallel processing, and more\n\nTo get started, we recommend you read through [the fastcore\ntour](https://fastcore.fast.ai/tour.html).\n\n## Contributing\n\nAfter you clone this repository, please run `nbdev_install_hooks` in\nyour terminal. This sets up git hooks, which clean up the notebooks to\nremove the extraneous stuff stored in the notebooks (e.g. which cells\nyou ran) which causes unnecessary merge conflicts.\n\nTo run the tests in parallel, launch `nbdev_test`.\n\nBefore submitting a PR, check that the local library and notebooks\nmatch.\n\n- If you made a change to the no",
    "url": "https://github.com/AnswerDotAI/fastcore",
    "last_updated": "2025-08-30T17:00:12+00:00"
  },
  {
    "full_name": "mkearney/tweetbotornot",
    "name": "tweetbotornot",
    "description": "🤖 R package for detecting Twitter bots via machine learning ",
    "language": "R",
    "topics": [
      "twitter",
      "twitter-bots",
      "machine-learning",
      "rstats",
      "rtweet",
      "r",
      "r-package",
      "twitter-api",
      "mkearney-r-package"
    ],
    "readme": "\n# tweetbotornot <img width=\"160px\" src=\"man/figures/logo.png\" align=\"right\" />\n\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n[![Travis build\nstatus](https://travis-ci.org/mkearney/tweetbotornot.svg?branch=master)](https://travis-ci.org/mkearney/tweetbotornot)\n[![Coverage\nstatus](https://codecov.io/gh/mkearney/tweetbotornot/branch/master/graph/badge.svg)](https://codecov.io/github/mkearney/tweetbotornot?branch=master)\n\nAn R package for classifying Twitter accounts as `bot or not`.\n\n## Features\n\nUses machine learning to classify Twitter accounts as bots or not bots.\nThe **default model** is 93.53% accurate when classifying bots and\n95.32% accurate when classifying non-bots. The **fast model** is 91.78%\naccurate when classifying bots and 92.61% accurate when classifying\nnon-bots.\n\nOverall, the **default model** is correct 93.8% of the time.\n\nOverall, the **fast model** is correct 91.9% of the time.\n\n## Install\n\nInstall from CRAN:\n\n``` r\n## install from CRAN\ninstall.packages(\"tweetbotornot\")\n```\n\nInstall the development version from Github:\n\n``` r\n## install remotes if not already\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\n\n## install tweetbotornot from github\ndevtools::install_github(\"mkearney/tweetbotornot\")\n```\n\n## API authorization\n\nUsers must be authorized in order to interact with Twitter’s API. To\nsetup your machine to make authorized requests, you’ll either need to be\nsigned into Twitter and working in an interactive session of R–the\nbrowser will open asking you to authorize the rtweet client\n(rstats2twitter)–or you’ll need to create an app (and have a developer\naccount) and your own API token. The latter has the benefit of (a)\nhaving sufficient permissions for write-acess and DM (direct messages)\nread-access levels and (b) more stability if Twitter decides to shut\ndown \\[@kearneymw\\](<https://twitter.com/kearneymw>)’s access to Twitter\n",
    "url": "https://github.com/mkearney/tweetbotornot",
    "last_updated": "2025-08-27T12:15:47+00:00"
  },
  {
    "full_name": "protocolbuffers/protobuf",
    "name": "protobuf",
    "description": "Protocol Buffers - Google's data interchange format",
    "language": "C++",
    "topics": [
      "protobuf",
      "protocol-buffers",
      "protocol-compiler",
      "protobuf-runtime",
      "protoc",
      "serialization",
      "marshalling",
      "rpc"
    ],
    "readme": "Protocol Buffers - Google's data interchange format\n===================================================\n\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/protocolbuffers/protobuf/badge)](https://securityscorecards.dev/viewer/?uri=github.com/protocolbuffers/protobuf)\n\nCopyright 2023 Google LLC\n\nOverview\n--------\n\nProtocol Buffers (a.k.a., protobuf) are Google's language-neutral,\nplatform-neutral, extensible mechanism for serializing structured data. You\ncan learn more about it in [protobuf's documentation](https://protobuf.dev).\n\nThis README file contains protobuf installation instructions. To install\nprotobuf, you need to install the protocol compiler (used to compile .proto\nfiles) and the protobuf runtime for your chosen programming language.\n\nWorking With Protobuf Source Code\n---------------------------------\n\nMost users will find working from\n[supported releases](https://github.com/protocolbuffers/protobuf/releases) to be\nthe easiest path.\n\nIf you choose to work from the head revision of the main branch your build will\noccasionally be broken by source-incompatible changes and insufficiently-tested\n(and therefore broken) behavior.\n\nIf you are using C++ or otherwise need to build protobuf from source as a part\nof your project, you should pin to a release commit on a release branch.\n\nThis is because even release branches can experience some instability in between\nrelease commits.\n\n### Bazel with Bzlmod\n\nProtobuf supports\n[Bzlmod](https://bazel.build/external/module) with Bazel 7 +.\nUsers should specify a dependency on protobuf in their MODULE.bazel file as\nfollows.\n\n```\nbazel_dep(name = \"protobuf\", version = <VERSION>)\n```\n\nUsers can optionally override the repo name, such as for compatibility with\nWORKSPACE.\n\n```\nbazel_dep(name = \"protobuf\", version = <VERSION>, repo_name = \"com_google_protobuf\")\n```\n\n### Bazel with WORKSPACE\n\nUsers can also add the following to their legacy\n[WORKSPACE](https://bazel.build/external/overview#workspace-sys",
    "url": "https://github.com/protocolbuffers/protobuf",
    "last_updated": "2025-09-02T09:07:03+00:00"
  },
  {
    "full_name": "facebookresearch/pytext",
    "name": "pytext",
    "description": "A natural language modeling framework based on PyTorch",
    "language": "Python",
    "topics": [],
    "readme": "<table align=\"center\">\n<tr>\n<td>\n<h1 align=\"center\">\n⚠️ Please migrate to <a href=\"https://github.com/pytorch/text\"><b>torchtext</b></a> ⚠️\n</h1>\n<p align=\"center\">\nPyText is deprecated and will no longer be actively maintained. Please check out <a href=\"https://github.com/pytorch/text\">torchtext</a> and contribute there!\n</p>\n</td>\n</tr>\n</table>\n\n\n# Overview\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n[![CircleCI](https://circleci.com/gh/facebookresearch/pytext.svg?style=svg&circle-token=2e0e0cb6dc686b646df887c2e0f07a8429712243)](https://circleci.com/gh/facebookresearch/pytext)\n\nPyText is a deep-learning based NLP modeling framework built on PyTorch. PyText addresses the often-conflicting requirements of enabling rapid experimentation and of serving models at scale. It achieves this by providing simple and extensible interfaces and abstractions for model components, and by using PyTorch’s capabilities of exporting models for inference via the optimized Caffe2 execution engine. We are using PyText in Facebook to iterate quickly on new modeling ideas and then seamlessly ship them at scale.\n\n**Core PyText features:**\n- Production ready models for various NLP/NLU tasks:\n  - Text classifiers\n    - [Yoon Kim (2014): Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n    - [Lin et al. (2017): A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)\n  - Sequence taggers\n    - [Lample et al. (2016): Neural Architectures for Named Entity Recognition](https://www.aclweb.org/anthology/N16-1030)\n  - Joint intent-slot model\n    - [Zhang et al. (2016): A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding](https://www.ijcai.org/Proceedings/16/Papers/425.pdf)\n  - Contextual intent-slot models\n- Distributed-training support built on the new C10d backend in PyTorch 1.0\n- Mixed prec",
    "url": "https://github.com/facebookresearch/pytext",
    "last_updated": "2025-08-29T04:41:13+00:00"
  },
  {
    "full_name": "qpdf/qpdf",
    "name": "qpdf",
    "description": "qpdf: A content-preserving PDF document transformer",
    "language": "C++",
    "topics": [
      "pdf",
      "pdf-document-processor"
    ],
    "readme": "[![qpdf](logo/qpdf.svg)](https://qpdf.sourceforge.io)\n\n[![qpdf Build](https://github.com/qpdf/qpdf/workflows/QPDF%20Build/badge.svg)](https://github.com/qpdf/qpdf/actions)\n[![Documentation Status](https://readthedocs.org/projects/qpdf/badge/?version=latest)](https://qpdf.readthedocs.io/en/latest/?badge=latest)\n\nqpdf is a command-line tool and C++ library that performs content-preserving transformations on PDF files. It supports\nlinearization, encryption, and numerous other features. It can also be used for splitting and merging files, creating\nPDF files (but you have to supply all the content yourself), and inspecting files for study or analysis. qpdf does not\nrender PDFs or perform text extraction, and it does not contain higher-level interfaces for working with page contents.\nIt is a low-level tool for working with the structure of PDF files and can be a valuable tool for anyone who wants to do\nprogrammatic or command-line-based manipulation of PDF files.\n\nThe [qpdf manual](https://qpdf.readthedocs.io) is hosted online at https://qpdf.readthedocs.io. The project website\nis https://qpdf.sourceforge.io. The source code repository is hosted at GitHub: https://github.com/qpdf/qpdf.\n\n# Verifying Distributions\n\nThe public key used to sign qpdf source distributions has\nfingerprint `C2C9 6B10 011F E009 E6D1  DF82 8A75 D109 9801 2C7E` and can be found at https://q.ql.org/pubkey.asc or\ndownloaded from a public key server.\n\n# Copyright, License\n\nqpdf is copyright (c) 2005-2021 Jay Berkenbilt, 2022-2025 Jay Berkenbilt and Manfred Holger\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\nLicense. You may obtain a copy of the License at\n\nhttps://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"\nAS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the",
    "url": "https://github.com/qpdf/qpdf",
    "last_updated": "2025-09-02T09:23:35+00:00"
  },
  {
    "full_name": "CenterForAssessment/SGP",
    "name": "SGP",
    "description": "Functions to calculate student growth percentiles and percentile growth projections/trajectories for students using large scale,          longitudinal assessment data.  Functions use quantile regression to estimate the conditional density associated          with each student's achievement history.  Percentile growth projections/trajectories are calculated using the coefficient matrices derived from  \tthe quantile regression analyses and specify what percentile growth is required for students to reach future achievement targets.",
    "language": "R",
    "topics": [
      "student-growth-percentiles",
      "r",
      "student-growth-projections",
      "sgp",
      "sgp-analyses",
      "percentile-growth-projections",
      "quantile-regression",
      "cran"
    ],
    "readme": "SGP\n===\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6471237.svg)](https://doi.org/10.5281/zenodo.6471237)\n[![R-CMD-check](https://github.com/CenterForAssessment/SGP/workflows/R-CMD-check/badge.svg)](https://github.com/CenterForAssessment/SGP/actions)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/SGP)](https://cran.r-project.org/package=SGP)\n[![Development Version](https://img.shields.io/badge/devel-2.2--2.2-brightgreen.svg)](https://github.com/CenterForAssessment/SGP)\n[![Rstudio mirror downloads](https://cranlogs.r-pkg.org/badges/grand-total/SGP)](https://github.com/metacran/cranlogs.app)\n[![License](https://img.shields.io/badge/license-GPL%203-brightgreen.svg?style=flat)](https://github.com/CenterForAssessment/SGP/blob/master/LICENSE.md)\n\n\n# Overview\n\nThe SGP Package is open source software built for the [**R** software environment](https://www.r-project.org/). The classes, functions and data within the SGP package are used to calculate student growth percentiles and percentile growth projections/trajectories using large scale, longitudinal assessment data. Quantile regression is used to estimate the conditional density associated with each student's achievement history. Percentile growth projections/trajectories are calculated using the derived coefficient matrices and show the percentile growth needed to reach future achievement targets.\n\n\n# Installation\n\n## From [CRAN](https://CRAN.R-project.org/package=SGP)\n\nTo install the latest stable release of SGP from [CRAN](https://CRAN.R-project.org/package=SGP)\n\n```R\n> install.packages(\"SGP\")\n```\n\n## From [Github](https://github.com/CenterForAssessment/SGP/)\n\nTo install the development release of SGP from [GitHub](https://github.com/CenterForAssessment/SGP/):\n\n```R\n> devtools::install_github(\"CenterForAssessment/SGP\")\n```\n\n\n# Resources\n\n* [SGP GitHub Pages](https://sgp.io)\n* [CRAN Repo](https://CRAN.R-project.org/package=SGP)\n* [Norm- and Criterion-Referenced Student Growth](https://github.com/Cent",
    "url": "https://github.com/CenterForAssessment/SGP",
    "last_updated": "2025-07-29T21:45:41+00:00"
  },
  {
    "full_name": "jamesbond90/aacommpyDownloader",
    "name": "aacommpyDownloader",
    "description": "aacommpyDownloader",
    "language": "Python",
    "topics": [],
    "readme": "",
    "url": "https://github.com/jamesbond90/aacommpyDownloader",
    "last_updated": "2025-01-15T15:29:17+00:00"
  },
  {
    "full_name": "soodoku/unclear_gap",
    "name": "unclear_gap",
    "description": "Survey Research. How Vague Response Options Produce Partisan Knowledge Gaps",
    "language": "TeX",
    "topics": [],
    "readme": "## An Unclear Gap: How Vague Response Options Produce Partisan Knowledge Gaps\n\nRoush and Sood (2023) use a dataset of 162,083 responses to 187 items on 47 surveys to find that partisan gaps are smaller and less frequent than commonly understood. The average is a mere six and a half points and gaps' ''signs'' run counter to expectations roughly 30\\% of the time. However, one exception is the size of gaps in retrospection items on the ANES, which are considerably bigger. These retrospection items use vague response options, e.g., 'Got better', 'Remained about the same,' etc. Vague response options can inflate partisan gaps by offering partisans the opportunity to interpret the same data differently. For instance, a .2\\% improvement in unemployment under Democrats can be reasonably interpreted as 'unemployment remained about the same' or as 'unemployment got better.' (The 'reasonableness' comes from the implied variance in the time series, etc. though the point stands independent of the reasonableness standard.) In an experiment, we test whether these vague options inflate partisan gaps. We present partisans data indicating a small improvement in economic indicators and manipulate the partisan tint of the change by manipulating who is responsible for the change.\n\n* During 2016, (when Barack Obama was president | when Republicans were in control of both Houses of Congress), unemployment decreased from 5.0\\% to 4.8\\%, a change of 0.2 percentage points. How would you interpret this change? Would you say that unemployment got better, stayed about the same, or got worse?\n\n* In 2016, inflation also decreased from 2.1\\% to 1.9\\%, a change of 0.2 percentage points. How would you interpret this change? Would you say that inflation got better, stayed about the same, or got worse?\n\nWe find that significantly fewer partisans pick the option that '[things] got better' when presented with an out-partisan cue than a co-partisan cue. Our findings suggest that vague options can induce ",
    "url": "https://github.com/soodoku/unclear_gap",
    "last_updated": "2023-01-09T22:35:20+00:00"
  },
  {
    "full_name": "slowkow/ggrepel",
    "name": "ggrepel",
    "description": ":round_pushpin: Repel overlapping text labels away from each other in your ggplot2 figures.",
    "language": "R",
    "topics": [
      "ggplot2",
      "cran",
      "visualization",
      "text",
      "rstats"
    ],
    "readme": "ggrepel <img src=\"man/figures/logo.svg\" width=\"181px\" align=\"right\" />\n============================================\n\n[![Build Status][bb]][githubactions] [![CRAN_Status_Badge][cb]][cran] [![CRAN_Downloads_Badge][db]][r-pkg]\n\n[bb]: https://github.com/slowkow/ggrepel/workflows/R-CMD-check/badge.svg\n[githubactions]: https://github.com/slowkow/ggrepel/actions?query=workflow%3AR-CMD-check\n\n[cb]: https://www.r-pkg.org/badges/version/ggrepel?color=blue\n[cran]: https://CRAN.R-project.org/package=ggrepel\n\n[db]: https://cranlogs.r-pkg.org/badges/ggrepel\n[r-pkg]: https://cranlogs.r-pkg.org\n\nOverview\n--------\n\nggrepel provides geoms for [ggplot2] to repel overlapping text labels:\n\n[ggplot2]: https://ggplot2.tidyverse.org\n\n- `geom_text_repel()`\n- `geom_label_repel()`\n\nText labels repel away from each other, away from data points, and away\nfrom edges of the plotting area.\n\n```r\nlibrary(ggrepel)\nggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) +\n  geom_text_repel() +\n  geom_point(color = 'red') +\n  theme_classic(base_size = 16)\n```\n<p align=\"center\">\n<img src=\"https://imgur.com/ii9ova8.gif\" />\n</p>\n\nInstallation\n------------\n\n```r\n# The easiest way to get ggrepel is to install it from CRAN:\ninstall.packages(\"ggrepel\")\n\n# Or get the the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"slowkow/ggrepel\")\n```\n\nUsage\n-----\n\nSee the [examples] page to learn more about how to use ggrepel in your project.\n\n[examples]: https://ggrepel.slowkow.com/articles/examples.html\n\nExamples\n--------\n\nClick one of the images below to go to see the code example:\n\n<a href=\"https://ggrepel.slowkow.com/articles/examples.html#hide-some-of-the-labels\"><img width=\"200\" src=\"https://raw.githubusercontent.com/slowkow/ggrepel/master/docs/articles/examples_files/figure-html/empty_string-1.png\" alt=\"Hide some of the labels\"></img></a>\n<a href=\"https://ggrepel.slowkow.com/articles/examples.html#always-show-all-labels-even-when-they-have-too-many-overlaps\"><img width=\"2",
    "url": "https://github.com/slowkow/ggrepel",
    "last_updated": "2025-08-25T14:45:11+00:00"
  },
  {
    "full_name": "IllDepence/unarXive",
    "name": "unarXive",
    "description": "A data set based on all arXiv publications, pre-processed for NLP, including structured full-text and citation network",
    "language": "Python",
    "topics": [],
    "readme": "# unarXive\n\n**Access**\n\n* Data Set on Zenodo: [full](https://doi.org/10.5281/zenodo.7752754) / [permissively licensed subset](https://doi.org/10.5281/zenodo.7752615)\n* [Data Sample](doc/unarXive_data_sample.tar.gz)\n* ML Data on Hugging Face: [citation recommendation](https://huggingface.co/datasets/saier/unarXive_citrec) / [IMRaD classification](https://huggingface.co/datasets/saier/unarXive_imrad_clf)\n\n**Documentation**\n\n* Publications\n    * [*Scientometrics*](http://link.springer.com/article/10.1007/s11192-020-03382-z) ([author copy](https://doi.org/10.5445/IR/1000118786/pre)) (2020)\n    * [*JCDL 2023*](https://doi.org/10.1109/JCDL57899.2023.00020) ([author copy](https://doi.org/10.48550/arXiv.2303.14957)) (2023)\n* [Data Format](#data)\n* [Usage](#usage)\n* [Development](#development)\n* [Cite](#cite-as)\n\n# Data\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/IllDepence/unarXive/master/doc/schema.svg\" alt=\"unarXive schema\" width=\"100%\">\n</p>\n\n**unarXive contains**\n\n* 1.9 M structured paper full-texts, containing\n    * 63 M references (28 M linked to OpenAlex)\n    * 134 M in-text citation markers (65 M linked)\n    * 9 M figure captions\n    * 2 M table captions\n    * 742 M pieces of mathematical notation preserved as LaTeX\n\nA comprehensive documentation of the **data format** can be found [here](doc/data_format.md).\n\nYou can find a **data sample** [here](doc/unarXive_data_sample.tar.gz).\n\n# Usage\n\n### Hugging Face Datasets\n\nIf you want to use unarXive for *citation recommendation* or *IMRaD classification*, you can simply use our Hugging Face datasets:\n\n* [Citation Recommendation](https://huggingface.co/datasets/saier/unarxive_citrec)\n* [IMRaD Classification](https://huggingface.co/datasets/saier/unarXive_imrad_clf)\n\nFor example, in the case of citation recommendation:\n\n```\nfrom datasets import load_dataset\n\ncitrec_data = load_dataset('saier/unarxive_citrec')\ncitrec_data = citrec_data.class_encode_column('label')  # assign target label column\ncitrec_d",
    "url": "https://github.com/IllDepence/unarXive",
    "last_updated": "2025-08-31T10:21:48+00:00"
  },
  {
    "full_name": "kytta/bumerge",
    "name": "bumerge",
    "description": "Merge Butane configurations",
    "language": "Python",
    "topics": [],
    "readme": "<!--\nSPDX-FileCopyrightText: © 2023 Nikita Karamov <me@kytta.dev>\nSPDX-License-Identifier: CC-BY-4.0 OR BSD-3-Clause\n-->\n\n# bumerge is now on Codeberg\n\nI've had enough with GitHub. This repository is now on Codeberg: https://codeberg.org/kytta/bumerge.git\n\nThe repository on GitHub will stay archived (read-only) for a few months before I delete it for good.\n\n<details><summary>Previous README</summary>\n\n# bumerge\n\n> Merge Butane configurations\n\nThis is a simple Python script that will merge [Butane] configurations from\nmultiple files into one. Makes your job easier when you manage servers.\n\n- **merges** multiple `.bu` files into one\n- **inlines** external files into the configs\n- **checks** source configs for errors\n\nbumerge currently supports [Fedora CoreOS Specification v1.5.0][fcos-1.5].\nSupport for other distributions is planned, but not prioritized.\n\n## Install\n\n```sh\npipx install bumerge\n```\n\nAlternatively, with Homebrew:\n\n```sh\nbrew install kytta/python/bumerge\n```\n\n## Use\n\nJust pass the list of the files to the app\n\n```sh\nbumerge root.bu modules/time.bu modules/user.bu\n```\n\n**Important:** bumerge will perform a deep merge. If there are key conflicts,\nthe latter file takes precedence.\n\n### Command-line arguments\n\n```sh\nusage: bumerge [-h] [--version] [--output FILE] [--variant {fcos}]\n               [--spec-version {1.5.0}]\n               FILE [FILE ...]\n\npositional arguments:\n  FILE                  config files to merge\n\noptions:\n  -h, --help               show this help message and exit\n  --version, -V            show program's version number and exit\n  --output FILE, -o FILE   output file. Outputs to stdout by default\n  --variant {fcos}         Butane specification variant\n  --spec-version {1.5.0}   Butane specification version\n```\n\n## Licence\n\n© 2023 [Nikita Karamov]\\\nLicensed under the [BSD 3-Clause \"New\" or \"Revised\" License][bsd-3-clause].\n\nThis README can also be licensed under the\n[Creative Commons Attribution 4.0 International][cc-by-4.0]\n\n__________",
    "url": "https://github.com/kytta/bumerge",
    "last_updated": "2025-08-12T05:37:39+00:00"
  },
  {
    "full_name": "peiyunh/tiny",
    "name": "tiny",
    "description": "Tiny Face Detector, CVPR 2017",
    "language": "MATLAB",
    "topics": [
      "face-detector",
      "face-detection",
      "scale-invariance",
      "detection",
      "detector"
    ],
    "readme": "![Demo result](https://raw.githubusercontent.com/peiyunh/tiny/master/selfie.png)\n\n# Finding Tiny Faces\nBy Peiyun Hu and Deva Ramanan at Carnegie Mellon University. \n\n## Introduction\nWe develop a face detector (Tiny Face Detector) that can find ~800 faces out of ~1000 reportedly present, by making use of novel characterization of scale, resolution, and context to find small objects. Can you confidently identify errors? \n\nTiny Face Detector was initially described in an [arXiv tech report](https://arxiv.org/abs/1612.04402). \n\nIn this repo, we provide a MATLAB implementation of Tiny face detector, including both training and testing code. A demo script is also provided. \n\n### Citing us\nIf you find our work useful in your research, please consider citing: \n```latex\n@InProceedings{Hu_2017_CVPR,\n  author = {Hu, Peiyun and Ramanan, Deva},\n  title = {Finding Tiny Faces},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  year = {2017}\n}\n```\n\n## Installation \nClone the repo recursively so you have [my fork of MatConvNet](https://github.com/peiyunh/matconvnet/tree/9822ec97f35cf5a56ae22707cc1c04e0d738e7db). \n```zsh\ngit clone --recursive git@github.com:peiyunh/tiny.git\n```\n\nCompile MatConvNet by running following commands in MATLAB (see [Installing - MatConvNet](http://www.vlfeat.org/matconvnet/install/) for more details): \n```Matlab\n>> cd matconvnet/;\n>> addpath matlab/; \n>> vl_compilenn('enableImreadJpeg', true, 'enableGpu', true, 'cudaRoot', [cuda_dir],...\n                'cudaMethod', 'nvcc', 'enableCudnn', true, 'cudnnRoot', [cudnn_dir]);\n>> vl_testnn('gpu', true);  % vl_testnn('gpu', false) for cpu-only \n```\n\nCompile our MEX function in MATLAB and test if it works as expected: \n```Matlab\n>> cd utils/;\n>> compile_mex;\n>> test_compute_dense_overlap;\n```\n\n\nDownload [WIDER FACE](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/) and unzip data and annotation files to `data/widerface` such that: \n```zsh\n$ ls data/widerface\n",
    "url": "https://github.com/peiyunh/tiny",
    "last_updated": "2025-07-01T12:11:20+00:00"
  },
  {
    "full_name": "dgtlmoon/changedetection.io",
    "name": "changedetection.io",
    "description": "Best and simplest tool for website change detection, web page monitoring, and website change alerts. Perfect for tracking content changes, price drops, restock alerts, and website defacement monitoring—all for free or enjoy our SaaS plan!",
    "language": "Python",
    "topics": [
      "website-monitor",
      "website-monitoring",
      "change-detection",
      "monitoring",
      "self-hosted",
      "change-alert",
      "change-monitoring",
      "website-change-monitor",
      "url-monitor",
      "website-change-detector",
      "website-change-detection",
      "website-change-tracker",
      "website-change-notification",
      "notifications",
      "web-scraping",
      "restock-monitor",
      "website-defacement-monitoring",
      "back-in-stock",
      "website-watcher",
      "rss"
    ],
    "readme": "# Detect Website Changes Automatically — Monitor Web Page Changes in Real Time\n\nMonitor websites for updates — get notified via Discord, Email, Slack, Telegram, Webhook and many more.\n\n**Detect web page content changes and get instant alerts.**  \n\nIdeal for monitoring price changes, content edits, conditional changes and more.\n\n\n[<img src=\"https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot.png\" style=\"max-width:100%;\" alt=\"Web site page change monitoring\"  title=\"Web site page change monitoring\"  />](https://changedetection.io?src=github)\n\n[![Release Version][release-shield]][release-link] [![Docker Pulls][docker-pulls]][docker-link] [![License][license-shield]](LICENSE.md)\n\n![changedetection.io](https://github.com/dgtlmoon/changedetection.io/actions/workflows/test-only.yml/badge.svg?branch=master)\n\n[**Get started with website page change monitoring straight away. Don't have time? Try our $8.99/month subscription, use our proxies and support!**](https://changedetection.io) , _half the price of other website change monitoring services!_\n\n\n- Chrome browser included.\n- Nothing to install, access via browser login after signup.\n- Super fast, no registration needed setup.\n- Get started watching and receiving website change notifications straight away.\n- See our [tutorials and how-to page for more inspiration](https://changedetection.io/tutorials) \n\n### Target specific parts of the webpage using the Visual Selector tool.\n\nAvailable when connected to a <a href=\"https://github.com/dgtlmoon/changedetection.io/wiki/Playwright-content-fetcher\">playwright content fetcher</a> (included as part of our subscription service)\n\n[<img src=\"https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/visualselector-anim.gif\" style=\"max-width:100%;\" alt=\"Select parts and elements of a web page to monitor for changes\"  title=\"Select parts and elements of a web page to monitor for changes\" />](https://changedetection.io?src=github)\n\n### Easil",
    "url": "https://github.com/dgtlmoon/changedetection.io",
    "last_updated": "2025-09-02T09:49:25+00:00"
  },
  {
    "full_name": "whomwah/qlstephen",
    "name": "qlstephen",
    "description": "A QuickLook plugin that lets you view plain text files without a file extension",
    "language": "Objective-C",
    "topics": [],
    "readme": "# QuicklookStephen\n\nQLStephen is a QuickLook plugin that lets you view text files without their own dedicated QuickLook plugin. Files like:\n\n    README\n    INSTALL\n    Capfile\n    CHANGELOG\n    package.json\n    etc...\n\n## Installation\n\n### Homebrew\n\n    brew install --cask qlstephen\n\n### Pre-compiled\n\n* [Download the latest version of QuickLookStephen](https://github.com/whomwah/qlstephen/releases)\n* Unzip\n* Copy the file into `~/Library/QuickLook` (You can create the `QuickLook` folder if it doesn’t exist)\n\n### Manually Compiled\n\nCompiling the project yourself? Run:\n\n    make\n    make install\n    \n## Permissions (Quarantine)\n\nIf you run into issues with macOS not letting you run the plugin because it's not signed by a verified developer you can follow these steps:\n\n1. Install the plugin using one of the methods above\n1. run `xattr -cr ~/Library/QuickLook/QLStephen.qlgenerator` (sudo if needed)\n1. run `qlmanage -r`\n1. run `qlmanage -r cache`\n1. Restart Finder by...\n    -  Restarting your computer\n    -  or holding down the option key and right click on Finder’s dock icon, then select “Relaunch” from the menu\n\nFor more disucssion on this you can read up on [#81](https://github.com/whomwah/qlstephen/issues/81) [starting here](https://github.com/whomwah/qlstephen/issues/81#issuecomment-582207278)\n\n## Settings\n\n### Maximum file size\n\nTo keep quickview fast the preview is limited in its number of shown bytes.\nThe default value is 100kB. You can change this using the shell to set your own max size.\n\n    defaults write com.whomwah.quicklookstephen maxFileSize 102400\n\n## Trouble?\n\nIf you’ve installed the plugin, but don’t see any changes:\n\n- Run `qlmanage -m` and look for the `public.data` line to make sure a different application's `qlgenerator` file hasn't taken precedence. (Not all `qlgenerator` files are in `~/Library/QuickLook/` or `/Library/QuickLook/`!)\n- Make sure you are editing (a) the correct plist of (b) the correct bundle.\n  (For example, you might have two `QL",
    "url": "https://github.com/whomwah/qlstephen",
    "last_updated": "2025-08-18T14:51:52+00:00"
  },
  {
    "full_name": "lyst/rpforest",
    "name": "rpforest",
    "description": "It is a forest of random projection trees",
    "language": "Python",
    "topics": [],
    "readme": "# rpforest\n\n![rpforest](https://raw.githubusercontent.com/lyst/rpforest/master/rpforest.jpg)\n\n[![CircleCI](https://circleci.com/gh/lyst/rpforest/tree/master.svg?style=svg&circle-token=6ab982f5b17307152e1f3b42b00b8ecc074a764d)](https://circleci.com/gh/lyst/rpforest/tree/master)\n\nrpforest is a Python library for approximate nearest neighbours search: finding points in a high-dimensional space that are close to a given query point in a fast but approximate manner.\n\nrpforest differs from alternative ANN packages such as [annoy](https://github.com/spotify/annoy) by not requiring the storage of all the vectors indexed in the model. Used in this way, rpforest serves to produce a list of candidate ANNs for use by a further service where point vectors are stored (for example, a relational database).\n\n## How it works\n\nIt works by building a forest of N binary random projection trees.\n\nIn each tree, the set of training points is recursively partitioned into smaller and smaller subsets until a leaf node of at most M points is reached. Each parition is based on the cosine of the angle the points make with a randomly drawn hyperplane: points whose angle is smaller than the median angle fall in the left partition, and the remaining points fall in the right partition.\n\nThe resulting tree has predictable leaf size (no larger than M) and is approximately balanced because of median splits, leading to consistent tree traversal times.\n\nQuerying the model is accomplished by traversing each tree to the query point's leaf node to retrieve ANN candidates from that tree, then merging them and sorting by distance to the query point.\n\n## Installation\n\n1. Install numpy first.\n2. Install rpforest using pip: `pip install rpforest`\n\n## Usage\n\n### Fitting\n\nModel fitting is straightforward:\n\n```python\nfrom rpforest import RPForest\n\nmodel = RPForest(leaf_size=50, no_trees=10)\nmodel.fit(X)\n```\n\nThe speed-precision tradeoff is governed by the `leaf_size` and `no_trees` parameters. Increasing `leaf_size",
    "url": "https://github.com/lyst/rpforest",
    "last_updated": "2025-08-13T03:13:55+00:00"
  },
  {
    "full_name": "ramnathv/htmlwidgets",
    "name": "htmlwidgets",
    "description": "HTML Widgets for R",
    "language": "R",
    "topics": [],
    "readme": "### HTML Widgets for R\n\n<!-- badges: start -->\n[![R build status](https://github.com/ramnathv/htmlwidgets/workflows/R-CMD-check/badge.svg)](https://github.com/ramnathv/htmlwidgets/actions)\n[![CRAN status](https://www.r-pkg.org/badges/version/htmlwidgets)](https://CRAN.R-project.org/package=htmlwidgets)\n[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n<!-- badges: end -->\n\nThe **htmlwidgets** package provides a framework for easily creating R bindings to JavaScript libraries. Widgets created using the framework can be:\n\n* Used at the R console for data analysis just like conventional R plots (via RStudio Viewer).\n* Seamlessly embedded within [R Markdown](https://rmarkdown.rstudio.com/) documents and [Shiny](https://shiny.posit.co/) web applications.\n* Saved as standalone web pages for ad-hoc sharing via email, Dropbox, etc.\n\nThere are already several R packages based on **htmlwidgets**, including:\n\n* [leaflet](https://github.com/rstudio/leaflet) -- Interactive maps with OpenStreetMap\n* [dygraphs](https://rstudio.github.io/dygraphs/) --- Interactive time series visualization\n* [networkD3](https://christophergandrud.github.io/networkD3/) --- Network visualization with D3\n* [sparkline](https://github.com/htmlwidgets/sparkline) --- Small inline charts\n* [DT](https://rstudio.github.io/DT/) --- Tabular data via DataTables\n* [rthreejs](https://github.com/bwlewis/rthreejs) -- Interactive 3D graphics\n\nThe package was created in collaboration by Ramnath Vaidyanathan, Joe Cheng, JJ Allaire, Yihui Xie, and Kenton Russell. We've all spent countless hours building bindings between R and the web, and were motivated to create a framework that made this as easy as possible for all R developers. \n\n### Getting Started\n\nIf you know R and a bit of JavaScript it's very straightforward to create your own widgets. You can install the **htmlwidgets** package from CRAN:\n\n```r\ninstall.packages(\"htmlwidgets\")",
    "url": "https://github.com/ramnathv/htmlwidgets",
    "last_updated": "2025-05-03T02:08:19+00:00"
  },
  {
    "full_name": "calcom/cal.com",
    "name": "cal.com",
    "description": "Scheduling infrastructure for absolutely everyone.",
    "language": "TypeScript",
    "topics": [
      "open-source",
      "typescript",
      "nextjs",
      "next-auth",
      "postgresql",
      "prisma",
      "t3-stack",
      "tailwindcss",
      "trpc",
      "turborepo",
      "zod"
    ],
    "readme": "<!-- PROJECT LOGO -->\n<p align=\"center\">\n  <a href=\"https://github.com/calcom/cal.com\">\n   <img src=\"https://user-images.githubusercontent.com/8019099/210054112-5955e812-a76e-4160-9ddd-58f2c72f1cce.png\" alt=\"Logo\">\n  </a>\n\n  <h3 align=\"center\">Cal.com (formerly Calendso)</h3>\n\n  <p align=\"center\">\n    The open-source Calendly successor.\n    <br />\n    <a href=\"https://cal.com\"><strong>Learn more »</strong></a>\n    <br />\n    <br />\n    <a href=\"https://github.com/calcom/cal.com/discussions\">Discussions</a>\n    ·\n    <a href=\"https://cal.com\">Website</a>\n    ·\n    <a href=\"https://github.com/calcom/cal.com/issues\">Issues</a>\n    ·\n    <a href=\"https://cal.com/roadmap\">Roadmap</a>\n  </p>\n</p>\n\n<p align=\"center\">\n   <a href=\"https://www.producthunt.com/products/cal-com\"><img src=\"https://img.shields.io/badge/Product%20Hunt-%231%20Product%20of%20the%20Month-%23DA552E\" alt=\"Product Hunt\"></a>\n   <img src=\"https://api.checklyhq.com/v1/badges/groups/1120718?style=flat&theme=default\" alt=\"Checkly QA\">\n   <a href=\"https://status.cal.com\"><img height=\"20px\" src=\"https://betteruptime.com/status-badges/v1/monitor/a9kf.svg\" alt=\"Uptime\"></a>\n   <a href=\"https://github.com/calcom/cal.com/stargazers\"><img src=\"https://img.shields.io/github/stars/calcom/cal.com\" alt=\"Github Stars\"></a>\n   <a href=\"https://news.ycombinator.com/item?id=34507672\"><img src=\"https://img.shields.io/badge/Hacker%20News-%231-%23FF6600\" alt=\"Hacker News\"></a>\n   <a href=\"https://github.com/calcom/cal.com/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-AGPLv3-purple\" alt=\"License\"></a>\n   <a href=\"https://github.com/calcom/cal.com/pulse\"><img src=\"https://img.shields.io/github/commit-activity/m/calcom/cal.com\" alt=\"Commits-per-month\"></a>\n   <a href=\"https://cal.com/pricing\"><img src=\"https://img.shields.io/badge/Pricing-Free-brightgreen\" alt=\"Pricing\"></a>\n   <a href=\"https://jitsu.com?utm_source=github/calcom/cal.com\"><img src=\"https://img.shields.io/badge/Metrics_tracked_by-JITSU-AA00FF?",
    "url": "https://github.com/calcom/cal.com",
    "last_updated": "2025-09-02T08:54:58+00:00"
  },
  {
    "full_name": "hellohaptik/chatbot_ner",
    "name": "chatbot_ner",
    "description": "chatbot_ner: Named Entity Recognition for chatbots.",
    "language": "Python",
    "topics": [
      "named-entity-recognition",
      "chatbots",
      "natural-language-processing",
      "nlp-library",
      "ner",
      "nlp",
      "chatbot-ner",
      "entity",
      "chatbot",
      "elasticsearch",
      "gazetteer"
    ],
    "readme": "# Named Entity Recognition for chatbots\n\n![chatbotner logo](docs/images/chatbotner_logo.png)\n\nChatbot NER is an open source framework custom built to supports entity recognition in text messages. After doing\nthorough research on existing [NER](https://en.wikipedia.org/wiki/Named-entity_recognition) systems, team at Haptik felt\nthe strong need of building a framework which is tailored for Conversational AI and also supports Indian languages.\nCurrently Chatbot-ner supports **English, Hindi, Gujarati, Marathi, Bengali and Tamil** and their code mixed form.\nCurrently this framework uses common patterns along with few NLP techniques to extract necessary entities from languages\nwith sparse data. API structure of Chatbot ner is designed keeping in mind usability for Conversational AI\napplications. Team at Haptik is continuously working towards porting this framework for **all Indian languages and their\nrespective local dialects**.\n\n### **Installation**\nDetailed documentation on how to setup Chatbot NER on your system using docker is available [here](docs/install.md).\n\n### **Supported Entities**\n\n| Entity type   | Code reference       | Description                              | example                           | Supported languages - **ISO 639-1** code |\n| :------------ | -------------------- | :--------------------------------------- | --------------------------------- | ---------------------------------------- |\n| Time          | [TimeDetector](https://github.com/hellohaptik/chatbot_ner/tree/develop/ner_v2/detectors/temporal/time) | Detect time from given text. | tomorrow morning at 5, कल सुबह ५ बजे, kal subah 5 baje | 'en', 'hi', 'gu', 'bn', 'mr', 'ta' |\n| Date          | [DateAdvancedDetector](https://github.com/hellohaptik/chatbot_ner/tree/develop/ner_v2/detectors/temporal/date) | Detect date from given text | next monday, agle somvar, अगले सोमवार | 'en', 'hi', 'gu', 'bn', 'mr', 'ta' |\n| Number        | [NumberDetector](https://github.com/hellohaptik/chatbot_ner/tree",
    "url": "https://github.com/hellohaptik/chatbot_ner",
    "last_updated": "2025-07-28T14:03:28+00:00"
  },
  {
    "full_name": "pvbhanuteja/repaper",
    "name": "repaper",
    "description": "Convert any form image into an online editable forms or to an editable PDFs.",
    "language": "Python",
    "topics": [],
    "readme": "=======\nrepaper\n=======\n\n\n.. image:: https://img.shields.io/pypi/v/repaper.svg\n        :target: https://pypi.python.org/pypi/repaper\n\n.. image:: https://img.shields.io/travis/pvbhanuteja/repaper.svg\n        :target: https://travis-ci.com/pvbhanuteja/repaper\n\n.. image:: https://readthedocs.org/projects/re-paper/badge/?version=latest\n        :target: https://re-paper.readthedocs.io/en/latest/?version=latest\n        :alt: Documentation Status\n\n.. image:: https://pyup.io/repos/github/pvbhanuteja/repaper/shield.svg\n        :target: https://pyup.io/repos/github/pvbhanuteja/repaper/\n        :alt: Updates\n\n.. image:: https://pepy.tech/badge/repaper\n        :target: https://pepy.tech/badge/repaper\n        :alt: Downloads\n\n\nConvert photo of a form to web based froms or editable pdf forms. \n\n\n* Free software: MIT license\n* Documentation: https://re-paper.readthedocs.io.\n\n============\nInstallation\n============\n\n\nStable release\n--------------\n\nTo install repaper, run this command in your terminal:\n\n.. code-block:: console\n\n    $ pip install repaper\n\nThis is the preferred method to install repaper, as it will always install the most recent stable release.\n\n=====\nUsage\n=====\n\n\nTo use repaper in a project::\n\n    from repaper import repaper\n\n\nTo generate a google form from the a form image::\n\n    from repaper import repaper\n\n    re_paper = repaper('../samples/test.jpg')\n\n    form_id = re_paper.make_google_from('../secrets/credentials.json')\n\n    print(f'''Form created with form id: {form_id[\"formId\"]} and is accessible at: \\n https://docs.google.com/forms/d/{form_id['formId']}/viewform \\n\n    edit and publish the form to make it accessible to others''')\n\nCommand line usage to generate google form from image::\n\n    repaper google-form --img_path ./samples/test.jpg --oauth_json ./secrets/credentials.json\n\n\nDevelopment Lead\n----------------\n\n* Bhanu Pallakonda <pvbhanuteja@gmail.com>\n* Gaurav Sood <gsood07@gmail.com>\n\nContributors\n------------\n\nNone yet. Why not be the first?\n\n\nFeature",
    "url": "https://github.com/pvbhanuteja/repaper",
    "last_updated": "2023-04-28T23:20:42+00:00"
  },
  {
    "full_name": "ivankokan/Excel2LaTeX",
    "name": "Excel2LaTeX",
    "description": "The Excel add-in for creating LaTeX tables",
    "language": "VBA",
    "topics": [
      "latex",
      "excel",
      "converter"
    ],
    "readme": "# Excel2LaTeX\nMaking tables in LaTeX can be tedious, especially if some columns are calculated.\nThis converter allows you to write a table in Excel instead, and export the current selection as LaTeX markup\nwhich can be pasted into an existing LaTeX document, or exported to a file and included via the `\\input` command.\n\nKnown to be compatible with Windows Excel 2000&ndash;2016 (32-bit and 64-bit) and Mac Excel 2004, 2011, and 2016.\nMay also be compatible with other versions that support `.xla` add-ins. Does not work on LibreOffice Calc.\n\n![Excel and Excel2LaTeX comparison](https://i.imgur.com/UNKCihT.png)\n\n## Features\nMost Excel formatting is supported.\n * Bold and italic (if applied to the whole cell)\n * Left, right, center, and general alignment (per-cell or per-column)\n * Vertical and horizontal borders (per-cell or per-column, single or double)\n * Font color (using the `xcolor` package)\n * Fill color (using the `colortbl` package)\n * Rotation (using the `rotating` package)\n * Merged cells (using the `multirow` package, if needed)\n * Can convert `\\`, `$`, `_`, `^`, `%`, `&`, and `#` to appropriate macros, or leave them in-place\n * Supports `booktabs` package\n * Uses `bigstrut` package when `booktabs` is not available\n * Makes standard LaTeX `tabular` environment\n * Can surround `tabular` environment with `table` environment template\n * Copy output to clipboard or export to a `.tex` file for inclusion using `\\include`\n * Save table specifications to your Excel worksheet, then export all tables at once\n\n## Using\nJust open the file Excel2LaTeX.xla in Excel. Then you will have two additional \nmenu items in your **Tools** menu and a new toolbar with two buttons on it. For \nExcel 2007 and later, you will have two new buttons in the **Add-Ins** ribbon. If \nyou plan to use the program frequently, you can save it in your addin directory \nand add it with **Tools**&rarr;**Add-Ins**. This way it will be loaded whenever Excel is \nopened.\n\nSelect the table to convert and hit th",
    "url": "https://github.com/ivankokan/Excel2LaTeX",
    "last_updated": "2025-08-25T08:15:35+00:00"
  },
  {
    "full_name": "gojiplus/clarifai",
    "name": "clarifai",
    "description": "R Client for the Clarifai API. More at: http://soodoku.github.io/clarifai/",
    "language": "R",
    "topics": [
      "clarifai",
      "clarifai-api",
      "cran"
    ],
    "readme": "## clarifai: R Client for the Clarifai API\n\n[![Build Status](https://travis-ci.org/soodoku/clarifai.svg?branch=master)](https://travis-ci.org/soodoku/clarifai)\n[![Build status](https://ci.appveyor.com/api/projects/status/4aa0x74ggm51075o?svg=true)](https://ci.appveyor.com/project/soodoku/clarifai)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/clarifai)](https://cran.r-project.org/package=clarifai)\n![](http://cranlogs.r-pkg.org/badges/grand-total/clarifai)\n[![codecov](https://codecov.io/gh/soodoku/clarifai/branch/master/graph/badge.svg)](https://codecov.io/gh/soodoku/clarifai)\n[![Research software impact](http://depsy.org/api/package/cran/clarifai/badge.svg)](http://depsy.org/package/r/clarifai)\n[![Github Stars](https://img.shields.io/github/stars/soodoku/clarifai.svg?style=social&label=Github)](https://github.com/soodoku/clarifai)\n\nFind out what is in a (moving) image with perhaps the best off-the-shelf solution: [clarifai.com](http://clarifai.com). Clarifai provides descriptors for images along with how confident it is about each of the descriptors. It is a bit magical. (See also the 2013 [ImageNet LSVR Classification Results](https://github.com/soodoku/clarifai/blob/master/inst/extdata/lsvrc_results.pdf).)\n\n### Installation\n\nTo get the current released version from CRAN:\n```r\ninstall.packages(\"clarifai\")\n```\n\nTo get the current development version from GitHub:\n\n```r\ninstall.packages(\"devtools\")\ndevtools::install_github(\"soodoku/clarifai\", build_vignettes = TRUE)\n```\n\nTo learn about how to use clarifai, see [using_clarifai](vignettes/using_clarifai.Rmd) ([html version](http://htmlpreview.github.io/?https://github.com/soodoku/clarifai/blob/master/vignettes/using_clarifai.html)) or [analyzing instagram accounts of politicians](vignettes/poligrams.Rmd) ([html version](http://htmlpreview.github.io/?https://github.com/soodoku/clarifai/blob/master/vignettes/poligrams.html)). There are lots of other interesting ways you can use clarifai. For instance, you can ",
    "url": "https://github.com/gojiplus/clarifai",
    "last_updated": "2023-06-27T03:00:45+00:00"
  },
  {
    "full_name": "chris-alexiuk-1/GitGood-Cloning",
    "name": "GitGood-Cloning",
    "description": "This repository simply explains how to clone it locally!",
    "language": "Python",
    "topics": [],
    "readme": "# GitGood at cloning!\n\nFirst, let's define what \"cloning\" a repository means!\n\nEssentially, we're copying the repository (as well as it's history, though how much can depend on what flags we use) to our local file-system!\n\n### How to clone:\n\n1. Navigate to the public repository you wish to clone on GitHub.com\n2. Since we've already set-up SSH, we'll want to collect the SSH address for the GitHub.com repository!\n\n    a. You can either use the provided screenshot to navigate to the SSH address and copy it:\n  ![image](https://user-images.githubusercontent.com/114439245/220232065-27c3eec5-5a30-483e-938d-2b3ed50f002b.png)\n    b. Or you can use the pattern: `git@github.com:<USERNAME>/<REPOSITORY NAME>.git`. \n    \n    c. As an example, this repository's SSH address is: `git@github.com:chris-alexiuk/GitGood-Cloning.git`\n\n3. Now that you have the SSH address for the repository, you can use the `git clone <SSH ADDRESS> command in your terminal to clone the repository the SSH address points to, to your present working directory (PWD). \n\n    a. Your PWD is whatever directory your terminal says you're in! As an example, my PWD in this screenshot is `~/Projects/GitExamples`\n    \n    ![image](https://user-images.githubusercontent.com/114439245/220232744-630fb73c-239a-4028-a26d-7c05b2d61db4.png)\n    \n    b. There are a number of additional flags you can use, and many other ways you can use the command. [This documentation page](https://git-scm.com/docs/git-clone) has everything you need, and don't forget you can always `git help clone`!\n    \n    c. An example of a command might be: `git clone git@github.com:chris-alexiuk/GitGood-Cloning`\n    \n    \n4. Now you should have a local copy! This comes with some additional bonuses that I will discuss in a later video! \n\n",
    "url": "https://github.com/chris-alexiuk-1/GitGood-Cloning",
    "last_updated": "2025-06-18T05:13:44+00:00"
  },
  {
    "full_name": "susam/texme",
    "name": "texme",
    "description": "Self-rendering Markdown + LaTeX documents",
    "language": "JavaScript",
    "topics": [
      "commonmark",
      "markdown",
      "latex",
      "markdown-latex",
      "mathjax",
      "html5",
      "javascript",
      "npm-package",
      "mathematics",
      "math"
    ],
    "readme": "TeXMe\n=====\n\nTeXMe is a lightweight JavaScript utility to create self-rendering\nMarkdown + LaTeX documents.\n\n[![View Demo][Demo SVG]][Demo URL]\n[![NPM Version][Version SVG]][NPM URL]\n[![JSDelivr Hits][JSDelivr SVG]][JSDelivr URL]\n[![MIT License][License SVG]][L]\n[![Twitter][Twitter SVG]][Twitter URL]\n[![Mastodon][Mastodon SVG]][Mastodon URL]\n\n[Demo SVG]: https://img.shields.io/badge/view-demo-brightgreen\n[Demo URL]: https://susam.github.io/texme/\n[Version SVG]: https://img.shields.io/npm/v/texme\n[NPM URL]: https://www.npmjs.com/package/texme\n[License SVG]: https://img.shields.io/badge/license-MIT-%233ea639\n[JSDelivr SVG]: https://data.jsdelivr.com/v1/package/npm/texme/badge?style=rounded\n[JSDelivr URL]: https://www.jsdelivr.com/package/npm/texme\n[Twitter SVG]: https://img.shields.io/badge/twitter-%40susam-%231d9bf0\n[Twitter URL]: https://twitter.com/susam\n[Mastodon SVG]: https://img.shields.io/badge/mastodon-%40susam-%236364ff\n[Mastodon URL]: https://mastodon.social/@susam\n\n\nContents\n--------\n\n* [Get Started](#get-started)\n* [CDN URLs](#cdn-urls)\n* [Valid HTML5](#valid-html5)\n* [Use TeXMe in Web Pages](#use-texme-in-web-pages)\n  * [Style](#style)\n  * [Render Markdown Without MathJax](#render-markdown-without-mathjax)\n  * [Skip Automatic Rendering on Load](#skip-automatic-rendering-on-load)\n  * [Set Options After Loading](#set-options-after-loading)\n  * [Content in Body](#content-in-body)\n* [Use TeXMe as a Library](#use-texme-as-a-library)\n  * [Install TeXMe](#install-texme)\n  * [Render Markdown and LaTeX](#render-markdown-and-latex)\n* [TeXMe API Documentation](#texme-api-documentation)\n* [Configuration Options](#configuration-options)\n* [Self-Hosting TeXMe](#self-hosting-texme)\n* [Markdown Priority Environment](#markdown-priority-environment)\n  * [Protect Dollar Sign in Code](#protect-dollar-sign-in-code)\n  * [Protect Dollar Sign in Image Description](#protect-dollar-sign-in-image-description)\n  * [Parsing Precedence](#parsing-precedence)\n  * [Unlimited Variants](#u",
    "url": "https://github.com/susam/texme",
    "last_updated": "2025-08-31T07:31:12+00:00"
  },
  {
    "full_name": "amplab-extras/SparkR-pkg",
    "name": "SparkR-pkg",
    "description": "R frontend for Spark",
    "language": "R",
    "topics": [],
    "readme": "# R on Spark\n\n[![Build Status](https://travis-ci.org/amplab-extras/SparkR-pkg.png?branch=master)](https://travis-ci.org/amplab-extras/SparkR-pkg)\n\nSparkR is an R package that provides a light-weight frontend to use Spark from\nR.\n\n*NOTE: As of April 2015, SparkR has been [merged](https://github.com/apache/spark/pull/5096) into Apache Spark and is shipping in an upcoming release (1.4) due early summer 2015. This repo currently targets users using released versions of Spark. __This repo no longer accepts new pull requests, and they should now be submitted to [apache/spark](https://github.com/apache/spark); see [here](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark) for some instructions.__*\n\n*NOTE: The API from the upcoming Spark release (1.4) will <b>not</b> have the same API as described by this github repo. Initial support for Spark in R be focussed on high level operations instead of low level ETL. This may change in the (1.5) version. You can contribute and follow SparkR developments on the Apache Spark mailing lists and [issue tracker](https://issues.apache.org/jira/browse/SPARK/component/12325400).*\n\n## Installing SparkR\n\n### Requirements\nSparkR requires\n\n* Scala 2.10, and\n* Spark version >= 0.9.0 and <= 1.2.\n\nCurrent build by default uses Apache Spark 1.1.0. You can also build SparkR against a\ndifferent Spark version (>= 0.9.0) by modifying `pkg/src/build.sbt`.\n\n*DataFrame*: [DataFrame](https://spark.apache.org/docs/1.3.0/sql-programming-guide.html) was introduced in Spark 1.3; the 1.3-compatible SparkR version can be found in the [`sparkr-sql` branch](https://github.com/amplab-extras/SparkR-pkg/tree/sparkr-sql), which includes a preliminary R API to work with DataFrames.\n\n### Package installation\nTo develop SparkR, you can build the scala package and the R package using\n\n    ./install-dev.sh\n\nIf you wish to try out the package directly from github, you can use [`install_github`](http://www.inside-r.org/packages/cran/devtools/docs/instal",
    "url": "https://github.com/amplab-extras/SparkR-pkg",
    "last_updated": "2025-07-12T16:09:05+00:00"
  },
  {
    "full_name": "antiwork/gumroad",
    "name": "gumroad",
    "description": "Sell stuff and see what sticks",
    "language": "Ruby",
    "topics": [],
    "readme": "<p align=\"center\">\n  <picture>\n    <source srcset=\"https://public-files.gumroad.com/logo/gumroad-dark.svg\" media=\"(prefers-color-scheme: dark)\">\n    <source srcset=\"https://public-files.gumroad.com/logo/gumroad.svg\" media=\"(prefers-color-scheme: light)\">\n    <img src=\"https://public-files.gumroad.com/logo/gumroad.svg\" height=\"100\" alt=\"Gumroad logo\">\n  </picture>\n</p>\n\n<p align=\"center\">\n  <strong>Sell your stuff. See what sticks.</strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://gumroad.com\">Gumroad</a> is an e-commerce platform that enables creators to sell products directly to consumers. This repository contains the source code for the Gumroad web application.\n</p>\n\n## Table of Contents\n\n- [Getting Started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n  - [Configuration](#configuration)\n  - [Running Locally](#running-locally)\n- [Development](#development)\n  - [Logging in](#logging-in)\n  - [Resetting Elasticsearch indices](#resetting-elasticsearch-indices)\n  - [Push Notifications](#push-notifications)\n  - [Common Development Tasks](#common-development-tasks)\n  - [Linting](#linting)\n\n## Getting Started\n\n### Prerequisites\n\n> 💡 If you're on Windows, follow our [Windows setup guide](docs/development/windows.md) instead.\n\nBefore you begin, ensure you have the following installed:\n\n#### Ruby\n\n- https://www.ruby-lang.org/en/documentation/installation/\n- Install the version listed in [the .ruby-version file](./.ruby-version)\n\n#### Node.js\n\n- https://nodejs.org/en/download\n- Install the version listed in [the .node-version file](./.node-version)\n\n#### Docker\n\nWe use Docker to setup the services for development environment.\n\n- For MacOS: Download the Docker app from the [Docker website](https://www.docker.com/products/docker-desktop)\n- For Linux:\n\n```bash\nsudo wget -qO- https://get.docker.com/ | sh\nsudo usermod -aG docker $(whoami)\n```\n\n#### MySQL & Percona Toolkit\n\nInstall a local version of MySQL 8.0.x to match the version run",
    "url": "https://github.com/antiwork/gumroad",
    "last_updated": "2025-09-02T03:41:52+00:00"
  },
  {
    "full_name": "thomasp85/gganimate",
    "name": "gganimate",
    "description": "A Grammar of Animated Graphics",
    "language": "R",
    "topics": [
      "rstats",
      "ggplot2",
      "animation",
      "transition",
      "data-visualization",
      "ggplot-extension"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# gganimate <img src=\"man/figures/logo.png\" align=\"right\" style=\"padding-left:10px;background-color:white;\" />\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/thomasp85/gganimate/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/thomasp85/gganimate/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version-ago/gganimate)](https://cran.r-project.org/package=gganimate)\n[![CRAN_Download_Badge](http://cranlogs.r-pkg.org/badges/gganimate)](https://cran.r-project.org/package=gganimate)\n[![Codecov test\ncoverage](https://codecov.io/gh/thomasp85/gganimate/graph/badge.svg)](https://app.codecov.io/gh/thomasp85/gganimate)\n<!-- badges: end -->\n\n`gganimate` extends the grammar of graphics as implemented by\n[`ggplot2`](https://github.com/tidyverse/ggplot2) to include the\ndescription of animation. It does this by providing a range of new\ngrammar classes that can be added to the plot object in order to\ncustomise how it should change with time.\n\n- `transition_*()` defines how the data should be spread out and how it\n  relates to itself across time.\n- `view_*()` defines how the positional scales should change along the\n  animation.\n- `shadow_*()` defines how data from other points in time should be\n  presented in the given point in time.\n- `enter_*()`/`exit_*()` defines how new data should appear and how old\n  data should disappear during the course of the animation.\n- `ease_aes()` defines how different aesthetics should be eased during\n  transitions.\n\n## An Example\n\nAll of the above might seem a bit abstract. Let’s try with a contrived\nexample:\n\n``` r\nlibrary(ggplot2)\nlibrary(gganimate)\n\nggplot(mtcars, aes(factor(cyl), mpg)) +\n  geom_boxplot() +\n  # Here comes the gganimate code\n  transition_states(\n    gear,\n    transition_length = 2,\n    state_length = 1\n  ) +\n  enter_fade() +\n  exit_shrink() +\n  ease_aes('sine-in-out')\n```\n\n![](man/figures/README-",
    "url": "https://github.com/thomasp85/gganimate",
    "last_updated": "2025-09-01T22:21:45+00:00"
  },
  {
    "full_name": "spro/practical-pytorch",
    "name": "practical-pytorch",
    "description": "Go to https://github.com/pytorch/tutorials - this repo is deprecated and no longer maintained",
    "language": "Jupyter Notebook",
    "topics": [
      "natural-language-processing",
      "natural-language-generation",
      "nlp",
      "nlg",
      "seq2seq"
    ],
    "readme": "**These tutorials have been merged into [the official PyTorch tutorials](https://github.com/pytorch/tutorials). Please go there for better maintained versions of these tutorials compatible with newer versions of PyTorch.**\n\n---\n\n![Practical Pytorch](https://i.imgur.com/eBRPvWB.png)\n\nLearn PyTorch with project-based tutorials. These tutorials demonstrate modern techniques with readable code and use regular data from the internet.\n\n## Tutorials\n\n#### Series 1: RNNs for NLP\n\nApplying recurrent neural networks to natural language tasks, from classification to generation.\n\n* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb)\n* [Generating Shakespeare with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)\n* [Translation with a Sequence to Sequence Network and Attention](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n* [Exploring Word Vectors with GloVe](https://github.com/spro/practical-pytorch/blob/master/glove-word-vectors/glove-word-vectors.ipynb)\n* *WIP* Sentiment Analysis with a Word-Level RNN and GloVe Embeddings\n\n#### Series 2: RNNs for timeseries data\n\n* *WIP* Predicting discrete events with an RNN\n\n## Get Started\n\nThe quickest way to run these on a fresh Linux or Mac machine is to install [Anaconda](https://www.continuum.io/anaconda-overview):\n```\ncurl -LO https://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh\nbash Anaconda3-4.3.0-Linux-x86_64.sh\n```\n\nThen install PyTorch:\n\n```\nconda install pytorch -c soumith\n```\n\nThen clone this repo and start Jupyter Notebook:\n\n```\ngit clone http://github.com/spro/practical-pytorch\ncd practical-pytorch\njupyter notebook\n```\n\n## ",
    "url": "https://github.com/spro/practical-pytorch",
    "last_updated": "2025-08-03T21:00:01+00:00"
  },
  {
    "full_name": "hrbrmstr/markdowntemplates",
    "name": "markdowntemplates",
    "description": ":white_check_mark::small_red_triangle_down: A collection of alternate R markdown templates",
    "language": "CSS",
    "topics": [
      "r",
      "rstats",
      "markdown",
      "rmarkdown"
    ],
    "readme": "---\noutput:\n  html_document:\n    keep_md: true\n  md_document:\n    variant: markdown_github\n---\n[![Build Status](https://travis-ci.org/hrbrmstr/markdowntemplates.svg)](https://travis-ci.org/hrbrmstr/markdowntemplates)\n\nA package of R markdown templates and knitr knit engine replacments\n\nThese are a set of alternate R markdown templates that do not use Bootstrap and do not include or rely on jQuery. These are primarily here for demonstration purposes but are also useful in and of themselves (it's always good to have choices). If you have a particular framework you like and it's not here, file an issue and I'll add it.\n\n- **bulma** (`output: markdowntemplates::bulma`) uses the [Bulma CSS framework](http://bulma.io) and has optional `navlink` YAML header elements and also has support for arbitrary footer content & a subset of OpenGraph tags (see the example Rmd) which will be used in header of the generated file.\n- **skeleton** (`output: markdowntemplates::skeleton`) or **default** (`output: markdowntemplates::default`) uses the [Skeleton CSS framework](http://getskeleton.com) and has optional `navlink` YAML header elements and also has support for arbitrary footer content & a subset of OpenGraph tags (see the example Rmd) which will be used in header of the generated file.\n- **kube** (`output: markdowntemplates::kube`) uses the [Kube CSS framework](https://imperavi.com/kube/) (contributed by Kieran Healy)\n- **minimal** (`output: markdowntemplates::minimal`) uses no CSS framework and only uses the `title` YAML header element.\n- **hugo** (`output: markdowntemplates::hugo`) produces markdown suitable for [Hugo](https://gohugo.io/) websites. It makes a broad assumption that you will save/use the `hugo` document in the `content` directory of your Hugo websites. \n- **prismpress** (`output: markdowntemplates::prismpress`) produces markdown suitable for use in WordPress with Jetpack (markdown-enabled) and Prism\n- **prismskel** (`output: markdowntemplates::hrbrskel`) is a revam",
    "url": "https://github.com/hrbrmstr/markdowntemplates",
    "last_updated": "2025-06-19T03:54:09+00:00"
  },
  {
    "full_name": "rochelleterman/text-analysis-dhbsi",
    "name": "text-analysis-dhbsi",
    "description": "Computational Text Analysis Workshop Materials",
    "language": "HTML",
    "topics": [],
    "readme": "# Computational Text Analysis Workshop\n\n### About\n\nThis workshop was originally prepared for the [2015 Digital Humanities @ Berkeley Summer Institute](http://digitalhumanities.berkeley.edu/summer-institute-2015/). It has since been taught elsewhere.\n\nThis course introduces students to modern quantitative text analysis techniques, with the ultimate goal of providing the skills necessary to apply the methods in their own research. We will use the open source programming language `R`. Demonstration corpora are provided. \n\n### Topics Covered\n\n* Acquiring and Preprocessing texts\n* Discriminating Words\n* Dictionary Methods and Sentiment Analysis\n* The Vector Space Model and the Geometry of Text (Multi-dimensional Scaling, Most Similar Texts, Clustering)\n* Topic Models\n* Quantifying Style: Grammar, Alliteration, and other Poetic Concerns\n\nSee the entire syllabus [here](A-syllabus.md).\n\n### Requirements\n\nThis workship will be using the R programming language. See the software requirements [here](B-Tech-Requirements.md).\n\nStudents are strongly encouraged to complete [this brief tutorial](https://www.codeschool.com/courses/try-r) to learn the basic syntax of the R programming language.\n\n### Contact\n[Rochelle Terman](https://github.com/rochelleterman/): rterman@gmail.com\n",
    "url": "https://github.com/rochelleterman/text-analysis-dhbsi",
    "last_updated": "2023-10-20T22:18:14+00:00"
  },
  {
    "full_name": "parsegon/math-css",
    "name": "math-css",
    "description": "Easy way to represent math by a few lines of HTML via CSS. ",
    "language": "CSS",
    "topics": [],
    "readme": "# MathCSS\n\nA verbose, responsive, and easy way to represent basic math and calculus by a few lines of HTML without the need for a heavier JavaScript library. Built exclusively in CSS using a block-chain technique.  Sister library to: https://github.com/mathexl/chemistry-css\n\n\n\n![Render Example](/example/render7.png)\n\n**New in 2.5.0**\n* Added support for Matrixes\n* Added support for display in white\n\n**What's MathCSS good for?**\n* Quick depictions of integrals, summations, products, and alike.\n* Fast loading time.  MathCSS uses no JS.  None!\n* Scalable, responsive design.  MathCSS is built like a **c**hoo-**c**hoo-**s** train.\n* Special math symbols without looking up the unicode.\n\n\n**Support soon to be added for**:\n* Multi-bounds\n\n\n## Usage\n\n### Getting Started\n\nFirst, add the CSS file to your page:\n\n```HTML\n<link href=\"path/to/math.css\" rel=\"stylesheet\">\n```\n\nAnd you're ready to go! Documentation is easy as provided below. Simply, add an `equation` attribute to begin as follows (you can\nuse the `mathbox` alias as well):\n\n```HTML\n<div equation>\n   <!-- Your equation will go here -->\n</div>\n```\n\nIf you want the display to be entirely in white, add the class `white` to the\n`<div>` tag.  So:\n\n```HTML\n<div equation class=\"white\">\n<!-- Your equation will go here -->\n</div>\n```\n\n### Integrals, Products, Summations\n\n#### The goal of MathCSS is so that your HTML reads like math. You can easily add `integral`, `doubleintegral`, `tripleintegral`, `product`, `summation` like such:\n\n```HTML\n<div equation>\n    <div integral>\n\n    </div>\n</div>\n```\n\nTo specify bounds and input, simply use `upperbound`, `lowerbound`, and `of` attributes:\n\n```HTML\n<div integral>\n    <div upperbound>\n        5x\n    </div>\n    <div lowerbound>\n        39x\n    </div>\n    <div of>\n        35x + 45\n    </div>\n</div>\n```\n\n`upperbound`, `lowerbound`, and `of` will only display correctly when inside `integral`, `doubleintegral`, `tripleintegral`, `product`, or `summation`.  \n\nSince `upperbound` and `lowerb",
    "url": "https://github.com/parsegon/math-css",
    "last_updated": "2025-04-09T22:47:46+00:00"
  },
  {
    "full_name": "r-rust/svgbob",
    "name": "svgbob",
    "description": "R wrapper for the 'svgbob' Rust Cargo crate.",
    "language": "C",
    "topics": [],
    "readme": "# SvgBob in R\n\n[![Build Status](https://travis-ci.org/r-rust/svgbob.svg)](https://travis-ci.org/r-rust/svgbob)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/r-rust/svgbob)](https://ci.appveyor.com/project/jeroen/svgbob)\n\n> Svgbob is a diagramming model which uses a set of typing characters\n  to approximate the intended shape. It allows for converting ascii art into\n  beautiful svg images.\n  \nThis R package wraps the [svgbob](https://crates.io/crates/svgbob) cargo crate.\n  \n## Examples\n\nInput text:\n\n```\n    E +-------------------------*--+     E |                         o    \n    D |-------------------*--*--|--*     D |                   o  o  |  o \n    C |-------------*--*  |  |  |  |     C |             o  o  |  |  |  | \n    B |-------*--*  |  |  |  |  |  |     B |       o  o  |  |  |  |  |  | \n    A +-*--*--+--+--+--+--+--+--+--+     A +-o--o--|--|--|--|--|--|--|--| \n        5 10 15 20 25 30 35 40 45 50         5 10 15 20 25 30 35 40 45 50 \n```\n\n```r\ntext <- readLines(system.file('examples/demo.bob', package = 'svgbob'))\nsvgbob(text[66:72], 'chart.svg')\n```\n\n![chart_svg](chart.svg)\n\nSuppose input text looks like this:\n\n```\n\n     +10-15V           ___0,047R\n    +--------o------o-|___|-o--o---------o----o-------o\n  + |        |      |       |  |         |    |       |\n  -===-     _|_     |       | .+.        |    |       |\n  -===-     .-.     |       | | | 2k2    |    |       |\n  -===-   470| +    |       | | |        |    |       |\n  - |      uF|      '--.    | '+'       .+.   |       |\n    +--------o         |6   |7 |8    1k | |   |       |\n          ___|___    .-+----+--+--.     | |   |       |\n           -═══-     |            |     '+'   |       |\n             -       |            |1     |  |/  BC    |\n            GND      |            |------o--+   547   |\n                     |            |      |  |`>       |\n                     |            |     .+.   |       |\n                     |            | 220R| |   o----||-+  IRF9Z3",
    "url": "https://github.com/r-rust/svgbob",
    "last_updated": "2024-09-30T06:37:36+00:00"
  },
  {
    "full_name": "cbboyan/solverpy",
    "name": "solverpy",
    "description": "Python Interface for Automated Solvers",
    "language": "Python",
    "topics": [],
    "readme": "# `solverpy`: Python Interface for Automated Solvers\n\n`solverpy` is a Python package providing a uniform interface to launch automated problem solvers from Python and process their outputs.  Currently supported solvers are:\n\n* E Prover (solver.atp.eprover.E)\n* Vampire (solver.atp.vampire.Vampire)\n* Prover9 (solver.atp.prover9.Prover9)\n* Lash (solver.atp.lash.Lash)\n* cvc5 (solver.smt.cvc5.Cvc5)\n* Z3 (solver.smt.z3.Z3)\n* Bitwuzla (solver.smt.bitwuzla.Bitwuzla)\n\nNote that the solver binaries are not part of this Python package and must be installed separately by the user.  The respective binaries must be (by default) in `PATH` if you wish to use them from `solverpy`.\n\n## Installation\n\n```bash\n$ pip install solverpy\n```\n\n## Single problem solving\n\nTo call the solver on one problem instance, start by creating the solver object.\n\n```python\nfrom solverpy.solver.smt.cvc5 import Cvc5\n\ncvc5 = Cvc5(\"T5\")  # time limit of 5 seconds\n```\n\nThe constructor argument is a resource limit string, in this case, a time limit `T` in seconds.  All solvers support `T` and additional resource limits might be available depending on the solver.  Multiple resource limits can be used (separated by `-`, like `T10-R50000`).  The limit string must, however, always start with `T`.\n\nThen call the `solve` method:\n\n```python\nresult = cvc5.solve(\"myproblem.smt2\", \"--enum-inst\")\n```\n\nThe first argument is the problem filename, the second is the solver-dependent strategy description (typically command line options as a string).\n\nThe result is a Python `dict` with results and statistics.  The keys and values are solver-specific.  Nevertheless, the result always contains keys `status` (with the value of type `str`) and `runtime` (type `float`).\n\nHint: Call `cvc5.run(p,s)` instead of `cvc5.solve(p,s)` to get the raw solver output without any processing.  Call `cvc5.command(p,s)` to output the shell command that is going to be executed to launch the solver.\n\n## Parallel benchmark evaluation\n\nTo evaluate a set",
    "url": "https://github.com/cbboyan/solverpy",
    "last_updated": "2025-08-30T18:18:01+00:00"
  },
  {
    "full_name": "yaringal/DropoutUncertaintyDemos",
    "name": "DropoutUncertaintyDemos",
    "description": "What My Deep Model Doesn't Know...",
    "language": "JavaScript",
    "topics": [],
    "readme": "# What My Deep Model Doesn't Know...\n\nThis repository contains the code for the demos in the blog post [What My Deep Model Doesn't Know...](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html). [ConvnetJS](http://cs.stanford.edu/people/karpathy/convnetjs/) is used here as a framework to interactively demonstrate the properties underlying dropout uncertainty. ConvnetJS was originally developed by Karpathy under the MIT license which is used here as well. \n",
    "url": "https://github.com/yaringal/DropoutUncertaintyDemos",
    "last_updated": "2025-07-05T02:07:12+00:00"
  },
  {
    "full_name": "hrbrmstr/newsflash",
    "name": "newsflash",
    "description": "Tools to Work with the Internet Archive and GDELT Television Explorer in R",
    "language": "R",
    "topics": [
      "internet-archive",
      "gdelt-television-explorer",
      "r",
      "rstats",
      "r-cyber"
    ],
    "readme": "\n\\*\\*\\* BREAKING CHANGES \\*\\*\\*\n\n# newsflash\n\nTools to Work with the Internet Archive and GDELT Television\n    Explorer\n\n## Description\n\nRef:\n\n  - <http://television.gdeltproject.org/cgi-bin/iatv_ftxtsearch/iatv_ftxtsearch>\n  - <https://archive.org/details/third-eye>\n\nTV Explorer: \n\n>_“In collaboration with the Internet Archive’s\nTelevision News Archive, GDELT’s Television Explorer allows you to\nkeyword search the closed captioning streams of the Archive’s 6 years of\nAmerican television news and explore macro-level trends in how America’s\ntelevision news is shaping the conversation around key societal issues.\nUnlike the Archive’s primary Television News interface, which returns\nresults at the level of an hour or half-hour”show,\" the interface here\nreaches inside of those six years of programming and breaks the more\nthan one million shows into individual sentences and counts how many of\nthose sentences contain your keyword of interest. Instead of reporting\nthat CNN had 24 hour-long shows yesterday that mentioned Donald Trump,\nthe interface here will count how many sentences uttered on CNN\nyesterday mentioned his name - a vastly more accurate metric for\nassessing media attention.“_\n\nThird Eye: \n\n>_The TV News Archive’s Third Eye project captures the\nchyrons–or narrative text–that appear on the lower third of TV news\nscreens and turns them into downloadable data and a Twitter feed for\nresearch, journalism, online tools, and other projects. At project\nlaunch (September 2017) we are collecting chyrons from BBC News, CNN,\nFox News, and MSNBC–more than four million collected over just two\nweeks.“_\n\nAn advantage of using this over the TV Explorer interactive selector &\ndownloader or Third Eye API is that you get tidy tibbles with this\npackage, ready to use in R.\n\nNOTE: While I don’t claim that this alpha-package is anywhere near\nperfect, the IA/GDELT TV API hiccups every so often so when there are\ncritical errors run the same query in their web interface before\nsubmitting a",
    "url": "https://github.com/hrbrmstr/newsflash",
    "last_updated": "2025-05-11T05:39:31+00:00"
  },
  {
    "full_name": "google/redgrep",
    "name": "redgrep",
    "description": "♥ Janusz Brzozowski",
    "language": "C++",
    "topics": [],
    "readme": "# redgrep\n\n## About\n\nredgrep is a grep based on regular expression derivatives. That is, it uses\nregular expression derivatives to construct the DFA. It then uses LLVM to JIT\nthe DFA.\n\nSince regular expression derivatives permit the three basic Boolean operations\nof disjunction (`|`), conjunction (`&`) and complement (`!`), redgrep enables\nyou to write very powerful regular expressions very easily and guarantees to\nmatch them in linear time.\n\n## Building\n\nYou must have Bazel, GNU bison and either GCC or Clang.\n\nredgrep attempts to keep up with LLVM development, so you should\n[get the source code and build LLVM](https://llvm.org/docs/GettingStarted.html#getting-the-source-code-and-building-llvm).\n(Debian and Ubuntu users might prefer to install the\n[nightly packages](https://apt.llvm.org/) instead.)\n\n`llvm-config-17` must be in your path.\n\n## Contact\n\n[redgrep@googlegroups.com](mailto:redgrep@googlegroups.com)\n\n## Disclaimer\n\nThis is not an official Google product.\n",
    "url": "https://github.com/google/redgrep",
    "last_updated": "2025-08-13T09:04:18+00:00"
  },
  {
    "full_name": "alexbyrnes/FCC-Political-Ads",
    "name": "FCC-Political-Ads",
    "description": "Archive of political ad data from the Federal Communications Commission",
    "language": "",
    "topics": [],
    "readme": "### The FCC Political Ad Data Archive\n\n\nThis archive contains data extracted from documents disclosed during the 2014 US midterm elections on political advertisements.  The original files are available at [stations.fcc.gov](https://stations.fcc.gov/).\n\nIn July 2014, the Federal Communications Commission began requiring all broadcast TV stations to disclose their [public inspection files](https://stations.fcc.gov/about-station-profiles/) online, including documents on political ads.  These files are available as they are produced during the election.  However, most of the files are in image-based PDF format, which makes comprehensive study of them very difficult.  \n\nThe purpose of this project is to make the documents machine-readable with optical character recognition (OCR) software and other open source tools so the data for this and future elections is available to a wider audience including individuals, nonprofits, and news organizations.  Over 80,000 documents have been extracted, 130,000 identified by type, and all 260,000 by date, url, and other metadata.  Some documents were scanned at a low resolution or have legibility problems, or are in a format specific to a few stations and have not been extracted.\n\n\n#### Common Document Types: Invoices, Orders, and Contracts\n\nThe three most prevalent and usable documents are invoices, orders, and contracts.  The data extraction was focused on these forms because they tell the most about the groups taking out the advertisements, and the transactions that occurred.  Around half of each form type are in a text-based format, which makes extraction much less difficult.  See [Schema and Field Documentation](#schema-and-field-documentation) below or the [schema](schema.tsv), which includes all columns.\n\n#### Examples of Forms that have been identified but not extracted\n\n* Credit Advisory\n* Political Issue Ad form\n* Political Inquiry\n* Record of Request for Broadcast\n* Candidate Advertising Change Checklist\n* Issue Advertising",
    "url": "https://github.com/alexbyrnes/FCC-Political-Ads",
    "last_updated": "2021-10-16T11:48:51+00:00"
  },
  {
    "full_name": "zalandoresearch/fashion-mnist",
    "name": "fashion-mnist",
    "description": "A MNIST-like fashion product database. Benchmark :point_down: ",
    "language": "Python",
    "topics": [
      "mnist",
      "deep-learning",
      "benchmark",
      "machine-learning",
      "dataset",
      "computer-vision",
      "fashion",
      "fashion-mnist",
      "gan",
      "zalando",
      "convolutional-neural-networks"
    ],
    "readme": "# Fashion-MNIST\n\n[![GitHub stars](https://img.shields.io/github/stars/zalandoresearch/fashion-mnist.svg?style=flat&label=Star)](https://github.com/zalandoresearch/fashion-mnist/)\n[![Gitter](https://badges.gitter.im/zalandoresearch/fashion-mnist.svg)](https://gitter.im/fashion-mnist/Lobby?utm_source=share-link&utm_medium=link&utm_campaign=share-link)\n[![Readme-CN](https://img.shields.io/badge/README-中文-green.svg)](README.zh-CN.md)\n[![Readme-JA](https://img.shields.io/badge/README-日本語-green.svg)](README.ja.md)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Year-In-Review](https://img.shields.io/badge/%F0%9F%8E%82-Year%20in%20Review-orange.svg)](https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/)\n\n<details><summary>Table of Contents</summary><p>\n\n* [Why we made Fashion-MNIST](#why-we-made-fashion-mnist)\n* [Get the Data](#get-the-data)\n* [Usage](#usage)\n* [Benchmark](#benchmark)\n* [Visualization](#visualization)\n* [Contributing](#contributing)\n* [Contact](#contact)\n* [Citing Fashion-MNIST](#citing-fashion-mnist)\n* [License](#license)\n</p></details><p></p>\n\n\n`Fashion-MNIST` is a dataset of [Zalando](https://jobs.zalando.com/tech/)'s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend `Fashion-MNIST` to serve as a direct **drop-in replacement** for the original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nHere's an example of how the data looks (*each class takes three-rows*):\n\n![](doc/img/fashion-mnist-sprite.png)\n\n<img src=\"doc/img/embedding.gif\" width=\"100%\">\n\n## Why we made Fashion-MNIST\n\nThe original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains a lot of handwritten digits. Members of the AI/ML/Data Science commu",
    "url": "https://github.com/zalandoresearch/fashion-mnist",
    "last_updated": "2025-08-31T08:26:09+00:00"
  },
  {
    "full_name": "mkearney/nyt",
    "name": "nyt",
    "description": "📰🗞 New York Times data ",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# nyt <img src=\"man/figures/logo.png\" width=\"160px\" align=\"right\" />\n\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n\nCollecting nytimes data.\n\n## Installation\n\nYou can install **{nyt}** from [Github](https://github.com) with:\n\n``` r\ndevtools::install_github(\"mkearney/nyt\")\n```\n\n## `nyt_articles()`\n\nThis is a basic example showing how to search for articles from the `US`\nsection:\n\n``` r\n## basic example code\nus1 <- nyt_articles(section = \"us\", page = 1)\n\n## view data\nus1\n#> # A tibble: 10 x 17\n#>    first_published… byline tone  add_sort_date       data_type data_id url   created             uri   summary\n#>  * <lgl>            <chr>  <chr> <dttm>              <chr>     <chr>   <chr> <dttm>              <chr> <chr>  \n#>  1 NA               By MI… NEWS  2018-08-03 18:22:34 article   100000… http… 2018-08-03 18:22:34 nyt:… The fa…\n#>  2 NA               By MA… NEWS  2018-08-03 18:10:40 article   100000… http… 2018-08-03 18:10:40 nyt:… Prosec…\n#>  3 NA               By AN… NEWS  2018-08-03 17:55:06 article   100000… http… 2018-08-03 17:55:06 nyt:… No fai…\n#>  4 NA               By JO… NEWS  2018-08-03 17:18:09 article   100000… http… 2018-08-03 17:18:09 nyt:… Here’s…\n#>  5 NA               By CH… NEWS  2018-08-03 17:11:47 article   100000… http… 2018-08-03 17:11:47 nyt:… The in…\n#>  6 NA               By AN… NEWS  2018-08-03 16:49:01 article   100000… http… 2018-08-03 16:49:01 nyt:… The Un…\n#>  7 NA               By JU… NEWS  2018-08-03 16:36:10 article   100000… http… 2018-08-03 16:36:10 nyt:… A 187-…\n#>  8 NA               By BR… NEWS  2018-08-03 16:14:27 article   100000… http… 2018-08-03 16:14:27 nyt:… At the…\n#>  9 NA               By KA… NEWS  2018-08-03 15:26:01 article   100000… http… 2018-08-03 15:26:01 nyt:… Their …\n#> 10 NA               By MA… NEWS  2018-08-03 14:12:29 article   100000… http… 2018-08-03 ",
    "url": "https://github.com/mkearney/nyt",
    "last_updated": "2025-03-04T14:16:07+00:00"
  },
  {
    "full_name": "cmpolis/us-town-names-vis",
    "name": "us-town-names-vis",
    "description": "d3.js visualization prototype code used to build: http://www.bytemuse.com/post/us-town-names-canvas-d3/",
    "language": "JavaScript",
    "topics": [],
    "readme": "",
    "url": "https://github.com/cmpolis/us-town-names-vis",
    "last_updated": "2024-10-13T20:59:30+00:00"
  },
  {
    "full_name": "jessegrabowski/pymc_statespace",
    "name": "pymc_statespace",
    "description": "A system for Bayesian estimation of state space models using PyMC",
    "language": "Python",
    "topics": [],
    "readme": "# UPDATE\n\nThis repo is no longer maintained! Statespace models are now a part of [pymc-extras](https://github.com/pymc-devs/pymc-extras/), and are maintained by the PyMC development team. Please look over there for the most up-to-date Bayesian State Space models!\n\nFor specific examples of how to use statespace models in pymc, you can check the following resources:\n\n- Time Series Analysis with Bayesian State Space Models in PyMC ([YouTube](https://www.youtube.com/watch?v=G9VWXZdbtKQ), [Github](https://github.com/jessegrabowski/statespace-presentation))\n- Forecasting Hurricane Trajectories with State Space Models ([PyMC Examples](https://www.pymc.io/projects/examples/en/latest/case_studies/ssm_hurricane_tracking.html))\n- Making a Custom State Space Model ([Github](https://github.com/pymc-devs/pymc-extras/blob/main/notebooks/Making%20a%20Custom%20Statespace%20Model.ipynb))\n- Structural Time Series Modeling in PyMC ([Github](https://github.com/pymc-devs/pymc-extras/blob/main/notebooks/Structural%20Timeseries%20Modeling.ipynb))\n- PyMC Statespace: VARMAX Example ([Github](https://github.com/pymc-devs/pymc-extras/blob/main/notebooks/VARMAX%20Example.ipynb))\n- PyMC Statespace: SARIMA Example ([Github](https://github.com/pymc-devs/pymc-extras/blob/main/notebooks/SARMA%20Example.ipynb))\n- PyMC Statespace: Exponential Trend Smoothing Example ([Github](https://github.com/pymc-devs/pymc-extras/blob/main/notebooks/Exponential%20Trend%20Smoothing.ipynb))\n",
    "url": "https://github.com/jessegrabowski/pymc_statespace",
    "last_updated": "2025-07-14T11:37:19+00:00"
  },
  {
    "full_name": "ropensci/rtika",
    "name": "rtika",
    "description": "R Interface to Apache Tika",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-package",
      "peer-reviewed",
      "tika",
      "extract-metadata",
      "extract-text",
      "pdf-files",
      "parse",
      "java",
      "tesseract"
    ],
    "readme": "\n# rtika\n\n***Extract text or metadata from over a thousand file types.***\n\n[![R-CMD-check](https://github.com/ropensci/rtika/workflows/R-CMD-check/badge.svg)](https://github.com/ropensci/rtika/actions/)\n[![ROpenSci](https://badges.ropensci.org/191_status.svg)](https://github.com/ropensci/software-review/issues/191/)\n[![Coverage\nstatus](https://codecov.io/gh/ropensci/rtika/branch/master/graph/badge.svg)](https://app.codecov.io/github/ropensci/rtika?branch=master)\n[![Cranlogs\nDownloads](https://cranlogs.r-pkg.org/badges/rtika)](https://CRAN.R-project.org/package=rtika)\n\n> Apache Tika is a content detection and analysis framework, written in\n> Java, stewarded at the Apache Software Foundation. It detects and\n> extracts metadata and text from over a thousand different file types,\n> and as well as providing a Java library, has server and command-line\n> editions suitable for use from other programming languages …\n\n> For most of the more common and popular formats, Tika then provides\n> content extraction, metadata extraction and language identification\n> capabilities. (From <https://en.wikipedia.org/wiki/Apache_Tika>,\n> accessed Jan 18, 2018)\n\nThis is an R interface to the Tika software.\n\n## Installation\n\nTo start, you need R and `Java 8` or `OpenJDK 1.8`. Higher versions\nwork. To check your version, run the command `java -version` from a\nterminal. Get Java installation tips at\n<https://www.java.com/en/download/> or <https://openjdk.org/install/>.\nBecause the `rJava` package is ***not*** required, installation is\nsimple. You can cut and paste the following snippet:\n\n``` r\ninstall.packages('rtika', repos = 'https://cloud.r-project.org')\n\nlibrary('rtika')\n\n# You need to install the Apache Tika .jar once.\ninstall_tika()\n```\n\nRead an [introductory\narticle](https://docs.ropensci.org/rtika/articles/rtika_introduction.html)\nat <https://docs.ropensci.org/rtika/articles/rtika_introduction.html>.\n\n## Key Features\n\n- `tika_text()` to extract plain text.\n- `tika_xml()` and `tika_html(",
    "url": "https://github.com/ropensci/rtika",
    "last_updated": "2025-03-22T11:09:45+00:00"
  },
  {
    "full_name": "neilfws/PubMed",
    "name": "PubMed",
    "description": "PubMed analysis: code and data",
    "language": "R",
    "topics": [],
    "readme": "# PubMed\n\nCode and data for analysis of data from [NCBI PubMed](http://www.pubmed.gov).\n\n## Current contents\n\n1. [adverbs](https://github.com/neilfws/PubMed/tree/master/adverbs): code and data for the blog post [Interestingly: the sentence adverbs of PubMed Central](http://nsaunders.wordpress.com/2013/07/16/interestingly-the-sentence-adverbs-of-pubmed-central/)\n1. [commons](https://github.com/neilfws/PubMed/tree/master/commons): code and data for the blog post [An analysis of contributions to PubMed Commons](https://nsaunders.wordpress.com/2016/12/02/an-analysis-of-contributions-to-pubmed-commons/)\n1. [novelty](https://github.com/neilfws/PubMed/tree/master/novelty): code and data for the blog post [Novelty: an update](https://nsaunders.wordpress.com/2015/10/21/novelty-an-update/)\n1. [omics](https://github.com/neilfws/PubMed/tree/master/omics): code and data for the blog post [-omics in 2013](http://nsaunders.wordpress.com/2013/06/25/omics-in-2013/)\n1. [retractions](https://github.com/neilfws/PubMed/tree/master/retractions): code and data for my [retracted article report](http://neilfws.github.io/PubMed/pmretract/pmretract.html) \n1. [software_names](https://github.com/neilfws/PubMed/tree/master/software_names): code and data for the blog post [Searching for duplicate resource names in PMC article titles](https://nsaunders.wordpress.com/2015/09/16/searching-for-duplicate-resource-names-in-pmc-article-titles/)\n1. [turnaround](https://github.com/neilfws/PubMed/tree/master/turnaround): code and data for the blog post [Bioinformatics journals: time from submission to acceptance, revisited](https://nsaunders.wordpress.com/2014/10/14/bioinformatics-journals-time-from-submission-to-acceptance-revisited/)\n1. [oa_growth](https://github.com/neilfws/PubMed/tree/master/oa_growth): code and data for the blog post on open access growth [Counting things is hard for a given value of “things”](https://nsaunders.wordpress.com/2014/12/02/counting-things-is-hard-for-a-given-value-of-thin",
    "url": "https://github.com/neilfws/PubMed",
    "last_updated": "2025-03-29T00:33:41+00:00"
  },
  {
    "full_name": "rich-iannone/DiagrammeR",
    "name": "DiagrammeR",
    "description": "Graph and network visualization using tabular data in R",
    "language": "R",
    "topics": [
      "r",
      "network-graph",
      "graph",
      "visualization",
      "property-graph",
      "graph-functions"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<div align=\"center\">\n\n<hr style=\"color:transparent\" />\n<a href='https://rich-iannone.github.io/DiagrammeR/'><img src=\"man/figures/logo.svg\" width=\"350px\"/></a>\n<hr style=\"color:transparent\"/>\n\n<!-- badges: start -->\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/DiagrammeR)](https://CRAN.R-project.org/package=DiagrammeR)\n[![R-CMD-check](https://github.com/rich-iannone/DiagrammeR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rich-iannone/DiagrammeR/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/rich-iannone/DiagrammeR/branch/main/graph/badge.svg)](https://app.codecov.io/gh/rich-iannone/DiagrammeR?branch=main)\n[![Monthly\nDownloads](https://cranlogs.r-pkg.org/badges/DiagrammeR)](https://CRAN.R-project.org/package=DiagrammeR)\n[![Total\nDownloads](https://cranlogs.r-pkg.org/badges/grand-total/DiagrammeR)](https://CRAN.R-project.org/package=DiagrammeR)\n[![Contributor\nCovenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://www.contributor-covenant.org/version/2/1/code_of_conduct.html)\n<!-- badges: end -->\n\n<hr style=\"color:transparent\" />\n\n<br />\n\n</div>\n\nWith the **DiagrammeR** package you can create, modify, analyze, and\nvisualize network graph diagrams. The output can be incorporated into\n**R Markdown** documents, integrated with **Shiny** web apps, converted\nto other graph formats, or exported as image files.\n\n<img src=\"man/figures/simple_graph.png\">\n\nThe graph above can be created with this combination of **DiagrammeR**\nfunctions:\n\n``` r\nexample_graph <-\n  create_graph() %>%\n  add_pa_graph(\n    n = 50, m = 1,\n    set_seed = 23\n  ) %>%\n  add_gnp_graph(\n    n = 50, p = 1/100,\n    set_seed = 23\n  ) %>%\n  join_node_attrs(df = get_betweenness(.)) %>%\n  join_node_attrs(df = get_degree_total(.)) %>%\n  colorize_node_attrs(\n    node_attr_from = total_degree,\n    node_attr_to = fillcolor,\n    palette = \"",
    "url": "https://github.com/rich-iannone/DiagrammeR",
    "last_updated": "2025-08-28T06:51:43+00:00"
  },
  {
    "full_name": "datameet/railways",
    "name": "railways",
    "description": "Indian Railways Data",
    "language": "",
    "topics": [],
    "readme": "# Indian Railways Data\n\nThis repository has Indian Railways data that [Sanjay](https://twitter.com/sanjaybhangar) and [Sajjad](https://twitter.com/geohacker) have been gathering for a few months. Read [more here]( http://sajjad.in/2016/08/gathering-indian-railways-data/). \n\nThere are three JSON files:\n\n### Stations\n[GeoJSON](http://geojson.org/) FeatureCollection, each Feature is a Station and looks like:\n\n```json\n{\n    \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [75.4516454, 27.2520587]\n    },\n    \"type\": \"Feature\",\n    \"properties\": {\n        \"state\": \"Rajasthan\",\n        \"code\": \"BDHL\",\n        \"name\": \"Badhal\",\n        \"zone\": \"NWR\",\n        \"address\": \"Kishangarh Renwal, Rajasthan\"\n    }\n}\n```\n\n### Trains\nGeoJSON FeatureCollection, each Feature is a Train and looks like:\n\n```json\n{\n    \"geometry\": {\n        \"type\": \"LineString\",\n        \"coordinates\": [\n            [72.89173899999999, 19.070320000000002],\n            [78.2266994458, 26.0352337224],\n            [78.18700399999999, 26.145594],\n            [78.18229199999999, 26.216483]\n        ]\n    },\n    \"type\": \"Feature\",\n    \"properties\": {\n        \"third_ac\": true,\n        \"arrival\": \"15:35:00\",\n        \"from_station_code\": \"LTT\",\n        \"name\": \"Mumbai LTT - Gwalior (Weekly) Special\",\n        \"zone\": \"CR\",\n        \"chair_car\": true,\n        \"first_class\": true,\n        \"duration_m\": 45,\n        \"sleeper\": true,\n        \"from_station_name\": \"LOKMANYA TILAK TERM\",\n        \"number\": \"01101\",\n        \"departure\": \"15:50:00\",\n        \"return_train\": \"01102\",\n        \"to_station_code\": \"GWL\",\n        \"second_ac\": true,\n        \"classes\": \"\",\n        \"to_station_name\": \"GWALIOR JN\",\n        \"duration_h\": 23,\n        \"type\": \"Exp\",\n        \"first_ac\": true,\n        \"distance\": 1216\n    }\n}\n```\n\n### Schedules\nAn array of objects. Each object is a schedule which defines a Train stop at a Station. \n\n```json\n{\n    \"arrival\": \"None\",\n    \"day\": 1,\n    \"train_name\": \"Falaknuma Lingampalli MMTS\",\n    \"s",
    "url": "https://github.com/datameet/railways",
    "last_updated": "2025-09-02T10:14:18+00:00"
  },
  {
    "full_name": "pablobarbera/eui-text-workshop",
    "name": "eui-text-workshop",
    "description": "Methods workshop: Automated Text Analysis with R",
    "language": "HTML",
    "topics": [],
    "readme": "\n# Workshop: Automated Text Analysis with R\n\n## Sponsored by \n* [Quantitative Methods Working Group, European University Institute](https://sites.google.com/site/qmwgroup/)\n\n## May 19, 2016\n \n## Instructor\n\n* [Pablo Barber&aacute;](http://pablobarbera.com/)\n\n(with some content based on materials prepared by [Dan Cervone](http://dcervone.com/), [Alex Hanna](http://alex-hanna.com), [Ken Benoit](http://www.kenbenoit.net/), [Paul Nulty](https://github.com/pnulty), [Kevin Munger](https://github.com/kmunger), and [Justin Grimmer](http://www.justingrimmer.org/).)\n\n## Description\n\nThe popularity of text as data is increasing rapidly within the social sciences. “Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research” (Grimmer and Stewart 2013) and elsewhere in the social sciences. This situation has changed with increasing computing power and more capable computing tools. In the coming years, the relevance of text data will further increase as more and more human communication is recorded online.\n\nThis workshop provides an introduction to text analysis using R. We will cover methods to conduct quantitative analysis of textual and web data, with an emphasis on social media data, applied to the study of social science questions. The workshop is made up of three \"modules\", each consisting of an introduction to a topic followed by examples and applications using R. The first module will cover how to format and input source texts, how to prepare the data for analysis, and how to extract descriptive statistics. The second module will discuss automated classification of text sources into categories using dictionary methods and supervised learning. Finally, the third module will discuss unsupervised classification of text into categories using topic modeling.\n\n\n## Setup and Preparation\n\nYou will need to bring a laptop to all sessions of the workshop. You will need [R](htt",
    "url": "https://github.com/pablobarbera/eui-text-workshop",
    "last_updated": "2023-07-27T11:19:42+00:00"
  },
  {
    "full_name": "dylanaraps/pure-bash-bible",
    "name": "pure-bash-bible",
    "description": "📖 A collection of pure bash alternatives to external processes.",
    "language": "Shell",
    "topics": [
      "bash",
      "shell",
      "script",
      "guide",
      "list",
      "bible",
      "book",
      "reference",
      "handbook",
      "learning",
      "how-to",
      "shell-scripts"
    ],
    "readme": "<p align=\"center\"><b>NEW: <a href=\"https://github.com/dylanaraps/pure-sh-bible\">pure sh bible (📖 A collection of pure POSIX sh alternatives to external processes).</a></b></p>\n\n<br>\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/odb/official-bash-logo/master/assets/Logos/Icons/PNG/512x512.png\" width=\"200px\"></p>\n<h1 align=\"center\">pure bash bible</h1> <p\nalign=\"center\">A collection of pure bash alternatives to external\nprocesses.</p>\n\n<p align=\"center\"> <a\nhref=\"https://travis-ci.com/dylanaraps/pure-bash-bible\"><img\nsrc=\"https://travis-ci.com/dylanaraps/pure-bash-bible.svg?branch=master\"></a>\n<a href=\"./LICENSE.md\"><img\nsrc=\"https://img.shields.io/badge/license-MIT-blue.svg\"></a>\n</p>\n\n<br>\n\n<a href=\"https://leanpub.com/bash/\">\n<img src=\"https://s3.amazonaws.com/titlepages.leanpub.com/bash/hero\" width=\"40%\" align=\"right\">\n</a>\n\nThe goal of this book is to document commonly-known and lesser-known methods of doing various tasks using only built-in `bash` features. Using the snippets from this bible can help remove unneeded dependencies from scripts and in most cases make them faster. I came across these tips and discovered a few while developing [neofetch](https://github.com/dylanaraps/neofetch), [pxltrm](https://github.com/dylanaraps/pxltrm) and other smaller projects.\n\nThe snippets below are linted using `shellcheck` and tests have been written where applicable. Want to contribute? Read the [CONTRIBUTING.md](https://github.com/dylanaraps/pure-bash-bible/blob/master/CONTRIBUTING.md). It outlines how the unit tests work and what is required when adding snippets to the bible.\n\nSee something incorrectly described, buggy or outright wrong? Open an issue or send a pull request. If the bible is missing something, open an issue and a solution will be found.\n\n<br>\n<p align=\"center\"><b>This book is also available to purchase on leanpub. https://leanpub.com/bash</b></p>\n<p align=\"center\"><b>Or you can buy me a coffee.</b>\n<a href=\"https://www.paypal.com/cgi-bin",
    "url": "https://github.com/dylanaraps/pure-bash-bible",
    "last_updated": "2025-09-02T09:49:01+00:00"
  },
  {
    "full_name": "jeffreyhorner/rapache",
    "name": "rapache",
    "description": "R embedded inside Apache",
    "language": "Shell",
    "topics": [],
    "readme": "The Rapache Project\n===================\n\n  What is it?\n  -----------\n  Rapache is a project dedicated to embedding the R interpreter inside\n  the Apache 2.0 (and beyond) web server. It's composed of two parts:\n\n  mod_R: the Apache 2.0 module that implements the glue to load the\n  R interpreter.\n\n  libapreq 2.0.4: an Apache sponsored project for parsing request input.\n  If you don't want to compile and install this version, then you can\n  specify which libapreq2 library to use during configuration.\n\n  The Latest Version\n  ------------------\n\n  Details of the latest version can be found at the R/Apache\n  project page:\n\n\thttps://jeffreyhorner.github.io/rapache\n\n  Prerequisites\n  -------------\n  This release has been tested under Debian Linux with Apache 2.2.4,\n  R 2.5.1, and libapreq 2.0.4.  Apache2 _MUST_ be compiled with the\n  prefork MPM to compile mod_R. Also, R must have been compiled with\n  the --enable-R-shlib configure flag.\n\n  Installation\n  ------------\n  The following is the preferred way to compile and install rapache:\n\n    $ ./configure\n    $ make\n    $ make install\n\n  configure will try to find the needed programs to compile rapache,\n  but if it fails to locate them or if you have installed the\n  prerequisites in non-standard places, then you can specify their\n  locations with the following flags:\n\n    --with-apache2-apxs=/path/to/apxs\n    --with-R=/path/to/R\n    --with-apreq2-config=/path/to/apreq2-config\n\n  Configuration\n  -------------\n  Add something similar to this to the apache config file:\n\n    LoadModule R_module /path/to/mod_R.so\n\n    # Output R errors and warnings to the browser\n    ROutputErrors\n\n    # Displays information about rapache and R\n    <Location /RApacheInfo>\n        SetHandler r-info\n    </Location>\n\n    # Process all files under /path/to/brew/scripts with\n    # package brew and function brew\n    <Directory /path/to/brew/scripts>\n        SetHandler r-script\n        RHandler brew::brew\n    </Directory>\n\n    # This url will run the fi",
    "url": "https://github.com/jeffreyhorner/rapache",
    "last_updated": "2025-06-18T15:19:08+00:00"
  },
  {
    "full_name": "dbroockman/repeated-online-panel-experiments",
    "name": "repeated-online-panel-experiments",
    "description": "Repository of detailed instructions for running online panel field experiments.",
    "language": "HTML",
    "topics": [],
    "readme": "# How to use this repository.\n\nThis guide keeps a running list of the vendors, notes about them, and other practices for running repeated online panel experiments. [The Design of Field Experiments With Survey Outcomes: A Framework for Selecting More Efficient, Robust, and Ethical Designs](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2742869) describes the design and an application study. This repository assumes knowledge of that paper.\n\nOur companion paper [Increasing Response Rates and Representativeness of Online Panels Recruited by Mail: Evidence from Experiments in 12 Original Surveys](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3136245) also contains recommendations on optimal incentives and anticipated response rates.\n\nThe power calculator available [here](http://experiments.berkeley.edu) allows one to plan out a study using the repeated online panel design and appreciate the benefits of this experimental design. The code for that power calcualtor is available in this repository [here](rop_power_calc).\n\n# Have a question about using the design?\n\nPlease use the [GitHub issues](https://github.com/dbroockman/repeated-online-panel-experiments/issues) feature to ask a question or make a request. Then you'll get a ping when we answer the question or update the repository to answer it.\n\n# Vendors\n\n## All-In-One Vendors\n\n[Civiqs](https://www.civiqs.com/research) offers all-in-one experiment administration services for the repeated survey design, using their custom-built online polling platform. They provide time-saving expert support and guidance for your project from start to finish, including mail, online surveys, data management, and incentive fulfillment. For more information, you can email <inquiries@civiqs.com> or visit [www.civiqs.com/research](https://www.civiqs.com/research). (Disclosure: the authors of this repository David Broockman and Joshua Kalla are volunteer members of Civiqs' scientific advisory board.)\n\n[Change-Point Analytics](https://",
    "url": "https://github.com/dbroockman/repeated-online-panel-experiments",
    "last_updated": "2024-04-16T20:54:26+00:00"
  },
  {
    "full_name": "komal-preet/sustainable-rep-quotas",
    "name": "sustainable-rep-quotas",
    "description": "Replication files for Sustainable representation through electoral quotas: evidence from India",
    "language": "R",
    "topics": [],
    "readme": "# sustainable-rep-quotas\nReplication files for Sustainable representation through electoral quotas: evidence from India\n",
    "url": "https://github.com/komal-preet/sustainable-rep-quotas",
    "last_updated": "2025-05-31T13:38:56+00:00"
  },
  {
    "full_name": "markjrieke/2026-war",
    "name": "2026-war",
    "description": "Bayesian estimation of house and senate candidate WAR",
    "language": "Python",
    "topics": [],
    "readme": "# 2025-war\n\nThis repository contains the source code for estimating candidate quality via a modified Wins Above Replacement (WAR) metric. The model is written in [Stan](https://mc-stan.org/) and the core pipeline is written in [Python](https://www.python.org/).\n\nThis work is released under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en). You are free to redistribute and adapt this work provided that the authors are correctly credited and you indicate if any changes were made.",
    "url": "https://github.com/markjrieke/2026-war",
    "last_updated": "2025-08-23T15:12:02+00:00"
  },
  {
    "full_name": "daroczig/botor",
    "name": "botor",
    "description": "Reticulate wrapper on 'boto3' with convenient helper functions -- aka \"boto fo(u)r R\"",
    "language": "R",
    "topics": [
      "r",
      "python",
      "aws",
      "boto3",
      "amazon-web-services",
      "rstats"
    ],
    "readme": "# botor: Reticulate wrapper on 'boto3' <a href=\"https://daroczig.github.io/botor/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"botor website\" /></a>\n\n<!-- badges: start -->\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active) ![CRAN](https://www.r-pkg.org/badges/version/botor) [![R-CMD-check](https://github.com/daroczig/botor/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/daroczig/botor/actions/workflows/R-CMD-check.yaml) [![Code Coverage](https://codecov.io/gh/daroczig/botor/branch/master/graph/badge.svg)](https://app.codecov.io/gh/daroczig/botor)\n<!-- badges: end -->\n\nThis R package provides raw access to the 'Amazon Web Services' ('AWS') 'SDK' via the 'boto3' Python module and some convenient helper functions (currently for S3 and KMS) and workarounds, eg taking care of spawning new resources in forked R processes.\n\n## Installation\n\nThis package requires Python to be installed along with the `boto3` Python module, which can be installed from R via:\n\n```r\nreticulate::py_install('boto3')\n```\n\nIf that might result in technical problems that you cannot solve, then it's probably easier to install a standalone Python along with the system dependencies etc via [`rminiconda`](https://github.com/hafen/rminiconda).\n\nOnce the Python dependencies are resolved, you can either install from CRAN or the most recent (development version) of `botor` can be installed from  GitHub:\n\n```r\nremotes::install_github('daroczig/botor')\n```\n\n## Loading the package\n\nLoading the `botor` package might take a while as it will also `import` the `boto3` Python module in the background:\n\n```r\nsystem.time(library(botor))\n#>    user  system elapsed\n#>   1.131   0.250   1.191\n```\n\n## Getting started\n\nQuick examples:\n\n1. Check the currently used AWS user's name:\n\n    ```r\n    iam_whoami()\n    #> [1] \"gergely-dev\"\n    ``",
    "url": "https://github.com/daroczig/botor",
    "last_updated": "2025-03-22T11:03:23+00:00"
  },
  {
    "full_name": "ray-project/ray",
    "name": "ray",
    "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
    "language": "Python",
    "topics": [
      "ray",
      "distributed",
      "parallel",
      "machine-learning",
      "reinforcement-learning",
      "deep-learning",
      "python",
      "rllib",
      "hyperparameter-search",
      "optimization",
      "data-science",
      "hyperparameter-optimization",
      "serving",
      "deployment",
      "pytorch",
      "tensorflow",
      "llm-serving",
      "large-language-models",
      "llm",
      "llm-inference"
    ],
    "readme": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://x.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable an",
    "url": "https://github.com/ray-project/ray",
    "last_updated": "2025-09-02T10:15:23+00:00"
  },
  {
    "full_name": "bearloga/maltese",
    "name": "maltese",
    "description": "Little R utility package for making time series data more machine learning-friendly",
    "language": "R",
    "topics": [
      "machine-learning",
      "time-series",
      "forecasting",
      "rstats",
      "r-package",
      "r"
    ],
    "readme": "# maltese: machine learning for time series\n\n[![Build Status](https://travis-ci.org/bearloga/maltese.svg?branch=master)](https://travis-ci.org/bearloga/maltese)\n\n## Installing\n\n```R\n# install.packages(\"remotes\")\nremotes::install_github(\"bearloga/maltese\")\n```\n\n## Example\n\n### Data\n\nThe included dataset is a tidy time series of pageviews for R's article on English Wikipedia from 2015-10-01 to 2017-01-30.\n\n```R\nlibrary(maltese)\nhead(r_enwiki)\n```\n\n|date       | pageviews|\n|:----------|---------:|\n|2015-10-01 |      3072|\n|2015-10-02 |      2575|\n|2015-10-03 |      1431|\n|2015-10-04 |      1540|\n|2015-10-05 |      3041|\n|2015-10-06 |      3695|\n\nWe can use `mlts_transform` to convert the data into a machine learning-friendly format with a 7-day lag:\n\n```R\nmlts <- mlts_transform(\n  r_enwiki, date, pageviews,\n  p = 7, # how many previous points of data to use as features\n  granularity = \"day\", # optional, can be automatically detected,\n  extras = TRUE, extrasAsFactors = TRUE # FALSE by default :D\n)\nhead(mlts)\n```\n\n|dt         |    y|mlts_extras_monthday |mlts_extras_weekday |mlts_extras_week |mlts_extras_month |mlts_extras_year | mlts_lag_1| mlts_lag_2| mlts_lag_3| mlts_lag_4| mlts_lag_5| mlts_lag_6| mlts_lag_7|\n|:----------|----:|:--------------------|:-------------------|:----------------|:-----------------|:----------------|----------:|----------:|----------:|----------:|----------:|----------:|----------:|\n|2015-10-08 | 3278|8                    |Thursday            |41               |October           |2015             |       3385|       3695|       3041|       1540|       1431|       2575|       3072|\n|2015-10-09 | 2886|9                    |Friday              |41               |October           |2015             |       3278|       3385|       3695|       3041|       1540|       1431|       2575|\n|2015-10-10 | 1692|10                   |Saturday            |41               |October           |2015             |       2886|       3278|       3385|       3695|  ",
    "url": "https://github.com/bearloga/maltese",
    "last_updated": "2025-01-14T15:50:29+00:00"
  },
  {
    "full_name": "r-hub/rhub",
    "name": "rhub",
    "description": "R-hub API client",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-package"
    ],
    "readme": "\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# rhub\n\n> R-hub v2\n\n<!-- badges: start -->\n[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)\n[![R-CMD-check](https://github.com/r-hub/rhub/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-hub/rhub/actions/workflows/R-CMD-check.yaml)\n[![](https://www.r-pkg.org/badges/version/rhub)](https://www.r-pkg.org/pkg/rhub)\n[![Codecov test coverage](https://codecov.io/gh/r-hub/rhub/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-hub/rhub?branch=main)\n<!-- badges: end -->\n\nR-hub v2 uses GitHub Actions to run `R CMD check` and similar package checks.\nThe rhub package helps you set up R-hub v2 for your R package, and start\nrunning checks.\n\n---\n\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Requirements](#requirements)\n  - [Private repositories](#private-repositories)\n  - [Setup](#setup)\n  - [Run checks](#run-checks)\n- [The R Consortium runners](#the-r-consortium-runners)\n  - [Limitations of the R Consortium\n    runners](#limitations-of-the-r-consortium-runners)\n- [Code of Conduct](#code-of-conduct)\n- [License](#license)\n\n## Installation\n\nInstall rhub from CRAN:\n\n``` r\npak::pkg_install(\"rhub\")\n```\n\n## Usage\n\n### Requirements\n\n- A Github account.\n- Your R package must be in a GitHub repository.\n- You need a GitHub [Personal Access\n  Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).\n  You can use the [gitcreds package](https://gitcreds.r-lib.org/) to add\n  the token to the git credential store.\n\nSee the [R Consortium runners](#the-r-consortium-runners) section for\nusing rhub if your package is not on GitHub.\n\n### Private repositories\n\nrhub uses GitHub Actions, which is free for public repositories. For\nprivate repositories you also get some minutes for free, depending on\nthe GitHub subscription you h",
    "url": "https://github.com/r-hub/rhub",
    "last_updated": "2025-08-25T16:35:17+00:00"
  },
  {
    "full_name": "RunLLM/aqueduct",
    "name": "aqueduct",
    "description": "Aqueduct is no longer being maintained. Aqueduct allows you to run LLM and ML workloads on any cloud infrastructure.",
    "language": "Go",
    "topics": [
      "data-science",
      "machine-learning",
      "ml-infrastructure",
      "ai",
      "data",
      "ml",
      "ml-monitoring",
      "python",
      "python3",
      "mlops",
      "orchestration",
      "llm",
      "llms",
      "kubernetes"
    ],
    "readme": "\n\n<div align=\"center\">\n  <a href=\"https://aqueducthq.com\">\n    <img src=\"https://aqueduct-public-assets-bucket.s3.us-east-2.amazonaws.com/webapp/logos/aqueduct-logo-two-tone/1x/aqueduct-logo-two-tone-1x.png\" width=\"40%\" />\n  </a>\n  \n  <h2 style=\"border: 0px white;\">Run LLMs and ML on any cloud infrastructure</h2>\n\n### 📢 [Slack](https://slack.aqueducthq.com)&nbsp;&nbsp;|&nbsp;&nbsp;🗺️ [Roadmap](https://roadmap.aqueducthq.com)&nbsp;&nbsp;|&nbsp;&nbsp;🐞 [Report a bug](https://github.com/aqueducthq/aqueduct/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBUG%5D)&nbsp;&nbsp;|&nbsp;&nbsp;✍️ [Blog](https://blog.aqueducthq.com)\n\n  \n[![Start Sandbox](https://img.shields.io/static/v1?label=%20&logo=github&message=Start%20Sandbox&color=black)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=496844646)\n[![Downloads](https://pepy.tech/badge/aqueduct-ml/month)](https://pypi.org/project/aqueduct-ml/)\n[![Slack](https://img.shields.io/static/v1.svg?label=chat&message=on%20slack&color=27b1ff&style=flat)](https://join.slack.com/t/aqueductusers/shared_invite/zt-11hby91cx-cpmgfK0qfXqEYXv25hqD6A)\n[![GitHub license](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://github.com/aqueducthq/aqueduct/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/aqueduct-ml.svg)](https://pypi.org/project/aqueduct-ml/)\n[![Tests](https://github.com/aqueducthq/aqueduct/actions/workflows/integration-tests.yml/badge.svg)](https://github.com/aqueducthq/aqueduct/actions/workflows/integration-tests.yml)\n</div>\n\n**Aqueduct is an MLOps framework that allows you to define and deploy machine learning and LLM workloads on any cloud infrastructure. [Check out our quickstart guide! →](https://docs.aqueducthq.com/quickstart-guide)**\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/867892/230214641-b0aec53b-4988-4581-84ed-134f97ed9276.png\" width=\"80%\" />\n</p>\n\nAqueduct is an open-source MLOps framework that allows you to write cod",
    "url": "https://github.com/RunLLM/aqueduct",
    "last_updated": "2025-08-20T02:54:59+00:00"
  },
  {
    "full_name": "mailgun/flanker",
    "name": "flanker",
    "description": "Python email address and Mime parsing library",
    "language": "Python",
    "topics": [],
    "readme": "Flanker - email address and MIME parsing for Python\n===================================================\n\n.. image:: https://travis-ci.org/mailgun/flanker.svg?branch=master\n    :target: https://travis-ci.org/mailgun/flanker\n\n.. image:: https://coveralls.io/repos/github/mailgun/flanker/badge.svg?branch=master\n    :target: https://coveralls.io/github/mailgun/flanker?branch=master\n\nFlanker is an open source parsing library written in Python by the Mailgun Team.\nFlanker currently consists of an address parsing library (`flanker.addresslib`) as\nwell as a MIME parsing library (`flanker.mime`).\n\nDetailed documentation is provided in the `User Manual <https://github.com/mailgun/flanker/blob/master/docs/User%20Manual.md>`_ as well as the\n`API Reference <https://github.com/mailgun/flanker/blob/master/docs/API%20Reference.md>`_. A Quickstart Guide is provided below.\n\nPython Versions\n---------------\n\nFlanker is heavily used by `Mailgun <www.mailgun.com>`_ in production with\nPython 2.7. The current production version is v0.8.5.\n\nSupport for Python 3 was added in v0.9.0 by popular demand from the community.\nWe are not using Flanker with Python 3 in the house. All we know is that tests\npass with Python 3.6, so use at your own risk. Feel free to report Python 3\nspecific issues if you see any.\n\nInstalling\n----------\n\nYou can install flanker via `pip` or clone the repo from GitHub.\n\nYou'll need Python headers files before you start working with flanker, so install them first:\n\n.. code-block:: bash\n\n   # ubuntu \n   sudo apt-get install python-dev\n   # fedora \n   sudo yum install python-devel\n\nIf you are using `pip`, simply type:\n\n.. code-block:: bash\n\n   pip install flanker\n\nIf you are cloning from GitHub, you can type:\n\n.. code-block:: bash\n\n   git clone git@github.com:mailgun/flanker.git\n   cd flanker\n   pip install -e .\n\nAddress Parsing\n---------------\n\nTo parse a single mailbox (display name as well as email address):\n\n.. code-block:: py\n\n   >>> from flanker.addresslib import addre",
    "url": "https://github.com/mailgun/flanker",
    "last_updated": "2025-08-27T12:01:02+00:00"
  },
  {
    "full_name": "CorrelAid/newsanchor",
    "name": "newsanchor",
    "description": "The newsanchor package provides an API wrapper for https://newsapi.org/",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\nnewsanchor <img src=\"newsanchor.png\" width=\"160px\" align=\"right\" />\n===================================================================\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active) [![Build Status](https://github.com/CorrelAid/newsanchor/workflows/R-CMD-check/badge.svg)](https://travis-ci.org/CorrelAid/newsanchor) [![codecov](https://codecov.io/gh/CorrelAid/newsanchor/branch/master/graph/badge.svg)](https://codecov.io/gh/CorrelAid/newsanchor) [![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/newsanchor)](https://cran.r-project.org/package=newsanchor) [![CRAN\\_Download\\_Badge](https://cranlogs.r-pkg.org/badges/newsanchor)](https://cran.r-project.org/package=newsanchor)\n\nPurpose/Description\n-------------------\n\n**newsanchor** provides a wrapper for <https://newsapi.org/>. **News API** is a simple HTTP REST API for searching and retrieving live articles from all over the web. You can get breaking news headlines, and search for articles from over 30,000 news sources and blogs.\n\nThe API can generally be used for free for non-commercial use cases. However, please be aware, that this results in some restrictions. For instance, you cannot download more than 1.000 results. Please see also <https://newsapi.org/pricing> for commerical options.\n\nThe package helps you to answer questions like the following:\n\n-   What are the **top stories** running right now in the world?\n-   What **articles** were published about your favorite politician, movie star, singer, or celebrity **today**?\n-   What are the breaking news in **business**, **entertainment**, or **sports**?\n\nStatus\n------\n\nThe package is under continuous development and will be extended with additional features in the future.\n\nInstallation\n------------\n\n### Stable Version\n\n``` r\n# install pac",
    "url": "https://github.com/CorrelAid/newsanchor",
    "last_updated": "2025-03-22T10:59:09+00:00"
  },
  {
    "full_name": "r-lib/lobstr",
    "name": "lobstr",
    "description": "Understanding complex R objects with tools similar to str()",
    "language": "R",
    "topics": [
      "r"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# lobstr <a href=\"https://lobstr.r-lib.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"lobstr website\" /></a>\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/lobstr)](https://cran.r-project.org/package=lobstr)\n[![R-CMD-check](https://github.com/r-lib/lobstr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/lobstr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/lobstr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-lib/lobstr?branch=main)\n<!-- badges: end -->\n\nlobstr provides tools in the same vein as `str()`, which allow you to\ndig into the detail of an object.\n\n## Installation\n\nInstall the released version of lobstr from CRAN:\n\n``` r\ninstall.packages(\"lobstr\")\n```\n\nYou can install the development version with:\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"r-lib/lobstr\")\n```\n\n## Example\n\n### Abstract syntax trees\n\n`ast()` draws the abstract syntax tree of R expressions:\n\n``` r\nast(a + b + c)\n#> █─`+` \n#> ├─█─`+` \n#> │ ├─a \n#> │ └─b \n#> └─c\n\nast(function(x = 1) {\n  if (x > 0) print(\"Hi!\")\n})\n#> █─`function` \n#> ├─█─x = 1 \n#> ├─█─`{` \n#> │ └─█─`if` \n#> │   ├─█─`>` \n#> │   │ ├─x \n#> │   │ └─0 \n#> │   └─█─print \n#> │     └─\"Hi!\" \n#> └─<inline srcref>\n```\n\n### References\n\n`ref()` shows hows objects can be shared across data structures by\ndigging into the underlying \\_\\_ref\\_\\_erences:\n\n``` r\nx <- 1:1e6\ny <- list(x, x, x)\nref(y)\n#> █ [1:0x1063ac468] <list> \n#> ├─[2:0x11629ba38] <int> \n#> ├─[2:0x11629ba38] \n#> └─[2:0x11629ba38]\n\ne <- rlang::env()\ne$self <- e\nref(e)\n#> █ [1:0x1068a2218] <env> \n#> └─self = [1:0x1068a2218]\n```\n\nA related tool is `obj_size()`, which computes the size of an object\ntaking these shared references into account:\n\n``` r\nobj_size(x)\n#> 680 B\nobj_size(y)\n#> 760 B\n```\n\n### Call stack trees\n\n`cst()` shows how frames on the call stack are connected:\n\n``` r\nf <- f",
    "url": "https://github.com/r-lib/lobstr",
    "last_updated": "2025-08-08T19:57:27+00:00"
  },
  {
    "full_name": "k88hudson/git-flight-rules",
    "name": "git-flight-rules",
    "description": "Flight rules for git",
    "language": "",
    "topics": [],
    "readme": "# Flight rules for Git\n\n🌍\n*[English](README.md) ∙ [Español](README_es.md)  ∙  [Русский](README_ru.md) ∙ [繁體中文](README_zh-TW.md) ∙ [简体中文](README_zh-CN.md) ∙ [한국어](README_kr.md)  ∙  [Tiếng Việt](README_vi.md) ∙ [Français](README_fr.md) ∙ [日本語](README_ja.md)*\n\n#### What are \"flight rules\"?\n\nA guide for astronauts (now, programmers using Git) about what to do when things go wrong.\n\n>  *Flight Rules* are the hard-earned body of knowledge recorded in manuals that list, step-by-step, what to do if X occurs, and why. Essentially, they are extremely detailed, scenario-specific standard operating procedures. [...]\n\n> NASA has been capturing our missteps, disasters and solutions since the early 1960s, when Mercury-era ground teams first started gathering \"lessons learned\" into a compendium that now lists thousands of problematic situations, from engine failure to busted hatch handles to computer glitches, and their solutions.\n\n&mdash; Chris Hadfield, *An Astronaut's Guide to Life on Earth*.\n\n#### Conventions for this document\n\nFor clarity's sake all examples in this document use a customized bash prompt in order to indicate the current branch and whether or not there are staged changes. The branch is enclosed in parentheses, and a `*` next to the branch name indicates staged changes.\n\nAll commands should work for at least git version 2.13.0. See the [git website](https://www.git-scm.com/) to update your local git version.\n\n[![Join the chat at https://gitter.im/k88hudson/git-flight-rules](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/k88hudson/git-flight-rules?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*\n\n  - [Repositories](#repositories)\n    - [I want to start a local repository](#i-want-to-start-a-local",
    "url": "https://github.com/k88hudson/git-flight-rules",
    "last_updated": "2025-09-01T10:44:50+00:00"
  },
  {
    "full_name": "mannau/h5",
    "name": "h5",
    "description": "Interface to the HDF5 Library",
    "language": "R",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/mannau/h5.svg?branch=master)](https://travis-ci.org/mannau/h5) \n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/mannau/h5?branch=master&svg=true)](https://ci.appveyor.com/project/mannau/h5)\n[![codecov.io](http://codecov.io/github/mannau/h5/coverage.svg?branch=master)](http://codecov.io/github/mannau/h5?branch=master) \n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/h5)](http://cran.r-project.org/package=h5)\n\n# DEPRECATION NOTICE\n\n> Dear **h5** Users,\n>\n>as of October 25th the **h5** package is officially deprecated in favor of the **hdf5r** package. **hdf5r** is more feature complete than **h5** while having less design flaws (e.g. no C++ API). **hdf5r** also includes most of **h5**'s API which should make the transition for existing **h5** users easier. We would therefore strongly suggest to switch to **hdf5r** ASAP.\n\nThe **hdf5r** package can be obtained as follows:\n\n| Source | Version | Link\n|--------|---------|--------------|\n| CRAN   | stable  | https://cran.r-project.org/package=hdf5r\n| Github | development | https://github.com/hhoeflin/hdf5r\n\n# Introduction\n\n**[h5](http://cran.r-project.org/web/packages/h5/index.html)** is an R \ninterface to the [HDF5](https://www.hdfgroup.org/HDF5) library under active development. It is available on [Github](https://github.com/mannau/h5) and already released on [CRAN](https://cran.r-project.org/web/packages/h5/index.html) for all major platforms (Windows, OS X, Linux). \n\n[HDF5](https://www.hdfgroup.org/HDF5/) is an excellent library and data model to \nstore huge amounts of data in a binary file format. Supporting most major \nplatforms and programming languages it can be used to exchange data files in a \nlanguage independent format. Compared to R's integrated *save()* and *load()* \nfunctions it also supports access to only parts of the binary data files and can\ntherefore be used to process data not fitting into memory.\n\n**[h5](http://cran.r-project",
    "url": "https://github.com/mannau/h5",
    "last_updated": "2024-12-28T14:56:56+00:00"
  },
  {
    "full_name": "dwillis/politico-api",
    "name": "politico-api",
    "description": "No reason, really.",
    "language": "Python",
    "topics": [],
    "readme": "# An Unofficial Politico API\n",
    "url": "https://github.com/dwillis/politico-api",
    "last_updated": "2025-04-12T14:42:15+00:00"
  },
  {
    "full_name": "notnews/digital-tv-coverage-in-uk",
    "name": "digital-tv-coverage-in-uk",
    "description": "Digital TV Coverage By Postcode in the U.K.",
    "language": "Python",
    "topics": [
      "digital-tv-coverage"
    ],
    "readme": "### UK Digital TV Coverage\n\n[UK Digital TV Coverage](http://www.digitaluk.co.uk/coveragechecker/) data, and scripts for getting that data. \n\n#### Getting the Data\n\n1. Download the Data: \n\t\n\t* Start with a file containing [all the postcodes](http://dx.doi.org/10.7910/DVN/NRY5OP) (Harvard DVN Link). The data are from UK's [Office of National Statistics](http://www.ons.gov.uk/ons/guide-method/geography/products/postcode-directories/-nspp-/index.html).\n\n    * Write a script that pings the [coverage checker website](http://www.digitaluk.co.uk/coveragechecker/) for each 7 character postcode (column name: `pcd`). Download the html if there is data (Sample data pages: [AB101AA](data/sample_src_data/AB101AA.html), [AB101AB](data/sample_src_data/AB101AB.html)). Write postcode to `error` file if there is no data. ([Sample error page](data/sample_src_data/Error.html).)\n\n    ```{r sample_scraper_code}\n    # sample code\n\n    library(httr)\n    library(rvest)\n\n    for (i in 1:nrow(postcodes_ons)) {\n\n    request <- httr::GET(paste0(\"http://www.digitaluk.co.uk/coveragechecker/main/display/detailed/\",paste(strsplit(postcodes_ons$pcd[i], \" \")[[1]], collapse=\"+\"),\"/NA/0\"))\n\n    webpage <- html(request)\n    if(length(webpage %>% html_nodes(\"#error-frame\"))!=0)\n    ...\n\n    }\n\n    ```\n    \n    * Given there are 2.5 million postcodes, run multiple instances. For instance, if a page takes 1 second to return, we need approximately 700 hours or nearly 29 days to download the data using a single instance. ([See here](https://gist.github.com/soodoku/3e3eb8d842a73400d9a8) for basic installs for initializing a R based scraper on Ubuntu.)\n\n    ```{r sample_run_code}\n\n    nohup RScript downloader.R & \n\n    ```\n\n2. Concatenate all the `error` files and put all the html files in a single folder.\n    ```\n    cat *error > errors\n    \n    ```\n\n3. Parse the Data: \n    * Run [converter.py](converter.py) with the folder containing html files as the source folder ([sample html files folder](data/sample_src_",
    "url": "https://github.com/notnews/digital-tv-coverage-in-uk",
    "last_updated": "2025-04-16T22:16:22+00:00"
  },
  {
    "full_name": "rafnixg/rafnixg-lib",
    "name": "rafnixg-lib",
    "description": "Rafnix Guzman Personal Card",
    "language": "Python",
    "topics": [
      "bussiness-card",
      "personal-card",
      "python"
    ],
    "readme": "# Rafnix Guzmán - Personal Card\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)\n[![Python Version](https://img.shields.io/badge/Python-%3E=3.9-blue)](https://www.python.org/downloads/)\n[![PyPI Version](https://img.shields.io/pypi/v/rafnixg)](https://pypi.org/project/rafnixg/)\n[![Build Status](https://github.com/rafnixg/rafnixg-lib/actions/workflows/python-publish.yml/badge.svg)](https://github.com/rafnixg/rafnixg-lib/actions)\n[![Downloads](https://img.shields.io/pypi/dm/rafnixg)](https://pypi.org/project/rafnixg/)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/rafnixg/rafnixg-lib)\n\n## 👋 Introduction\n\nWelcome to **Rafnix Guzmán - Personal Card**, a Python library that provides a personal card, blog post manager, and resume tools for developers. This library is designed to showcase your personal information, blog posts, and resume in a console-friendly format.\n\nVisit my personal website: [rafnixg.dev](https://rafnixg.dev)\n\n![image](https://github.com/user-attachments/assets/4c1c368b-83ca-4ff7-89d0-c131efe60c9f)\n\n---\n\n## 🚀 Features\n\n- **Personal Card**: Display your personal information in a styled console table.\n- **Blog Posts**: Fetch and display your latest blog posts from Hashnode.\n- **Resume Tools**: Retrieve and display your resume details, including work experience, education, skills, and more.\n- **Customizable Links**: Manage and display your personal links.\n\n---\n\n## 🛠 Installation\n\nInstall the library using pip:\n\n```bash\npip install rafnixg\n```\n\n---\n\n## 📖 Usage\n\n### Display Personal Card\n\nRun the following command to display your personal card:\n\n```bash\nrafnixg\n```\n\n### Blog Posts\n\nFetch and display your latest blog posts:\n\n```python\nfrom rafnixg import BlogPosts\n\nblog_posts = BlogPosts()\nposts = blog_posts.get_posts()\nfor post in posts:\n    print(post.title, post.link)\n```\n\n### Resume Tools\n\nRetrieve and display your resume details:\n\n```python\nfrom rafnixg import Resume\n\nresume = Resume()\nresume_data = ",
    "url": "https://github.com/rafnixg/rafnixg-lib",
    "last_updated": "2025-08-27T01:11:26+00:00"
  },
  {
    "full_name": "doc-analysis/DocBank",
    "name": "DocBank",
    "description": "DocBank: A Benchmark Dataset for Document Layout Analysis",
    "language": "Python",
    "topics": [],
    "readme": "# DocBank\n\nDocBank is a new large-scale dataset that is constructed using a weak supervision approach. It enables models to integrate both the textual and layout information for downstream tasks. The current DocBank dataset totally includes 500K document pages, where 400K for training, 50K for validation and 50K for testing.\n\n## News\n- **We have uploaded the datasets on [HuggingFace](https://huggingface.co/datasets/liminghao1630/DocBank).**\n- **We update the license to Apache-2.0.**\n- **The MSCOCO Format Annotation can be download from the [DocBank dataset homepage](https://doc-analysis.github.io/docbank-page/index.html).**\n- **The ResNeXt-101 model has been added to the [Model Zoo](MODEL_ZOO.md).**\n- **Our paper has been accepted in [COLING2020](https://coling2020.org/pages/accepted_papers_main_conference) and [the Camera-ready version paper](https://arxiv.org/abs/2006.01038) has been updated on arXiv.com**\n- **We provide a dataset loader named [DocBankLoader](https://github.com/doc-analysis/DocBankLoader) and it can also convert DocBank to the Object Detection models' format**\n- **DocBank is a natural extension of the TableBank ([repo](https://github.com/doc-analysis/TableBank), [paper](https://arxiv.org/abs/1903.01949)) dataset**\n- **LayoutLM ([repo](https://github.com/microsoft/unilm/tree/master/layoutlm), [paper](https://arxiv.org/abs/1912.13318)) is an effective pre-training method of text and layout and archives the SOTA result on DocBank**\n\n## Introduction\n\nFor document layout analysis tasks, there have been some image-based document layout datasets, while most of them are built for computer vision approaches and they are difficult to apply to NLP methods. In addition, image-based datasets mainly include the page images and the bounding boxes of large semantic structures, which are not fine-grained token-level annotations. Moreover, it is also time-consuming and labor-intensive to produce human-labeled and fine-grained token-level text block arrangement. The",
    "url": "https://github.com/doc-analysis/DocBank",
    "last_updated": "2025-08-15T00:09:44+00:00"
  },
  {
    "full_name": "saurfang/rwebhdfs",
    "name": "rwebhdfs",
    "description": "R Package for WebHDFS REST API",
    "language": "R",
    "topics": [],
    "readme": "rwebhdfs\n========\n[![Build Status](https://travis-ci.org/saurfang/rwebhdfs.svg?branch=master)](https://travis-ci.org/saurfang/rwebhdfs)\nR Package for WebHDFS REST API\n\n## Overview\nThis R package provides access to HDFS via WebHDFS REST API. For more information, please see:\nhttp://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/WebHDFS.html\n\n## Hadoop Configuration\nEnsure that WebHDFS is enabled in the `hdfs-site.xml`\n```\n<property>\n    <name>dfs.webhdfs.enabled</name>\n    <value>true</value>\n </property>\n```\n\n## How to Use\nMore examples will arrive in the function help pages but for now, here's a brief guide on how to use `rwebhdfs`\n\n#### Environment\nI'm recommend HDP 2.0 for quick demo and testing: http://hortonworks.com/hdp/downloads/\n\n#### Create your webhdfs object\nWebHDFS is a S3 object and can be created using \n```R\nhdfs <- webhdfs(\"localhost\", 50070, \"hue\")\n```\n\n#### List the files under you home directory\n```R\ndir_stat(hdfs, \"\")\n```\n\n#### Creates an empty file named \"test\" and get its information\n```R\nwrite_file(hdfs, \"test\")\nfile_stat(hdfs, \"test\")\n```\n\n#### Write local file onto HDFS and see what we just wrote\n```R\nfoo <- tempfile()\nwriteLines(\"foobar\", foo)\nwrite_file(hdfs, \"foo\", foo)\nread_file(hdfs, \"foo\")\n```\n\n#### Creates a directory and move our file in it\n```R\nmkdir(hdfs, \"bar\")\nrename_file(hdfs, \"foo\", \"bar/foo\")\n```\n\n#### Finally delete the test file and folder\n```R\ndelete_file(hdfs, \"test\")\ndelete_file(hdfs, \"bar\", recursive=TRUE)\n```\n\n## How to Install\nrwebhdfs is not on CRAN yet. I plan to play with it in a couple Hadoop projects before submission to CRAN. So that I can decide if all functions are intuitive and well designed.\n\nTo get latest version on Github:\n```R\ndevtools::install_github(c(\"saurfang/rwebhdfs\"))\n```\n\n## Implementation\n`webhdfs` has been implemented as a S3 object and all common FileSystem related functions are coded as S3 methods. Since R provides some basic FileSystem functions like `list.files`, `file.info`, `r",
    "url": "https://github.com/saurfang/rwebhdfs",
    "last_updated": "2021-03-24T16:18:04+00:00"
  },
  {
    "full_name": "lichess-org/compression",
    "name": "compression",
    "description": "Chess clock and move compression algorithms for lichess.org",
    "language": "Scala",
    "topics": [],
    "readme": "# Chess clock and move compression algorithms for lichess.org\n\n[![](https://jitpack.io/v/lichess-org/compression.svg)](https://jitpack.io/#lichess-org/compression)\n\n## Disclaimer\n\nThis library was migrated from the Java language to the Scala language.\nOnly the language syntax changed to Scala; the design and paradigms of the Java program were kept.\nThis is not how Scala code should be written, it is not idiomatic.\n\n## Blog posts\n\n- [A better game clock history](https://lichess.org/blog/WOEVrjAAALNI-fWS/a-better-game-clock-history)\n- [Developer update: 275% improved game compression](https://lichess.org/blog/Wqa7GiAAAOIpBLoY/developer-update-275-improved-game-compression)\n\n## Benchmarks\n\n```bash\nsbt 'benchmarks/jmh:run -i 5 -wi 3 -f1 -t1 org.lichess.compression.benchmark.*'\n```\n\n## License\n\nThis library is licensed under the GNU Affero General Public License 3 or\nany later version at your option. See LICENSE for the full license text.\n",
    "url": "https://github.com/lichess-org/compression",
    "last_updated": "2025-08-13T11:18:09+00:00"
  },
  {
    "full_name": "jsfenfen/parsing-prickly-pdfs",
    "name": "parsing-prickly-pdfs",
    "description": "NICAR 2016 talk about PDFs!",
    "language": "",
    "topics": [],
    "readme": "# [bit.ly/parsing-prickly-pdfs](https://bit.ly/parsing-prickly-pdfs)\n\n\nResources and worksheet for the NICAR 2016 workshop of the same name. Instructors: [Jacob Fenton](https://github.com/jsfenfen) ([jsfenfen@gmail.com](mailto:jsfenfen@gmail.com)) and [Jeremy Singer-Vine](https://github.com/jsvine) ([jsvine@gmail.com](mailto:jsvine@gmail.com)).\n\n## What We'll Cover\n\n- [The One Weird Thing You Need To Know About PDFs](#the-one-weird-thing-you-need-to-know-about-pdfs)\n- [Optical Character Recognition](#optical-character-recognition)\n- [Extracting Structured Data From PDFs](#extracting-structured-data-from-pdfs) \n- [Splitting, Merging, and Rotating PDFs](#splitting-merging-and-rotating-pdfs)\n- [Additional Resources](#additional-resources)\n\n\n## The One Weird Thing You Need To Know About PDFs\n\n- It's a print format--the point is to tell a printer what things to print where. There's not much more order than that. But for our purposes we'll talk about three types of PDFs:\n\t\n\t- \"Text-based PDFs\": The document has characters, fonts and font sizes, and information about where to place them stored in the document in a format that (a program) can read. Created by programs that can save to pdf directly. Dragging the mouse will highlight text.\n\t\n\t- \"Image-based PDFs\": The document has images of pages, but not words themselves. Typically the result of scanning. You can't highlight text.\n\t\n\t- \"Embedded-text PDFs\": The document has images of pages, but there's invisible text 'attached' to the document, so you can select text. Typically created by scanners that also run OCR. Sticky wicket: should you assume the attached text is better than what you'd get by running tesseract? Not necessarily (but it probably is...)\n\n## Optical Character Recognition\n\nOptical Character Recognition (OCR) tools try to extract text from image-based PDFs. Depending on the software, the tool may either (a) convert your image-based PDF into an embedded-text PDF, or (b) simply output the text it has found. So",
    "url": "https://github.com/jsfenfen/parsing-prickly-pdfs",
    "last_updated": "2025-07-28T14:32:07+00:00"
  },
  {
    "full_name": "rich-iannone/stationaRy",
    "name": "stationaRy",
    "description": "Get hourly meteorological data from one of thousands of global stations",
    "language": "R",
    "topics": [
      "r",
      "met-data",
      "dataset",
      "global"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# stationaRy <a href='http://rich-iannone.github.io/stationaRy/'><img src=\"man/figures/logo.svg\" align=\"right\" height=\"250px\" /></a>\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/stationaRy)](https://CRAN.R-project.org/package=stationaRy)\n[![R build\nstatus](https://github.com/rich-iannone/stationaRy/workflows/R-CMD-check/badge.svg)](https://github.com/rich-iannone/stationaRy/actions)\n[![Codecov test\ncoverage](https://codecov.io/gh/rich-iannone/stationaRy/branch/master/graph/badge.svg)](https://codecov.io/gh/rich-iannone/stationaRy?branch=master)\n\n## Overview\n\nGet meteorological data from met stations located all over the world.\nThat’s what you can do with this **R** package. There are *LOTS* of\nstations too (29,729 available in this dataset) and many have data that\ngo pretty far back in time. The data comes from the Integrated Surface\nDataset (ISD), which is maintained by the National Oceanic and\nAtmospheric Administration (NOAA).\n\n### Retrieving Met Data with a `station_id`\n\nLet’s get some met data from La Guardia Airport in New York City (the\nstation ID value is `\"725030-14732\"`). This station has a pretty long\nhistory (starting operations in 1973) but we’ll just obtain data from\nthe years of 2017 and 2018.\n\n``` r\nlga_met_data <- \n  get_met_data(\n    station_id = \"725030-14732\",\n    years = 2017:2018\n  )\n```\n\n``` r\nlga_met_data\n#> # A tibble: 17,520 x 10\n#>    id    time                 temp    wd    ws atmos_pres dew_point    rh\n#>    <chr> <dttm>              <dbl> <dbl> <dbl>      <dbl>     <dbl> <dbl>\n#>  1 7250… 2017-01-01 00:00:00   7.2   230   5.7      1012.      -4.4  43.5\n#>  2 7250… 2017-01-01 01:00:00   7.8   230   4.6      1012.      -3.9  43.4\n#>  3 7250… 2017-01-01 02:00:00   7.2   230   3.6      1012.      -2.2  51.3\n#>  4 7250… 2017-01-01 03:00:00   7.8   240   5.7      1013.      -3.3  45.4\n#>  5 7250… 2017-01-01 04:00:00   7.8   240   4.6      1013.      -3.9  43.4\n#",
    "url": "https://github.com/rich-iannone/stationaRy",
    "last_updated": "2025-05-05T14:45:44+00:00"
  },
  {
    "full_name": "duckduckgo/tracker-radar",
    "name": "tracker-radar",
    "description": "Data set of top third party web domains with rich metadata about them",
    "language": "JavaScript",
    "topics": [],
    "readme": "# DuckDuckGo Tracker Radar\n\nThis is not a block list, but a data set of the most common third party domains on the web with information about their behavior, classification and ownership. It allows for easy custom solutions with the significant metadata it has for each domain: parent entity, prevalence, use of fingerprinting, cookies, privacy policy, and performance. The data on individual domains can be found in the [domains](/domains) directory.\n\n## Related Resources\n\n- Read [our blog post](https://spreadprivacy.com/duckduckgo-tracker-radar/) for more information on the Tracker Radar\n\n- To learn more about the Tracker Radar data format, see [Data Model](docs/DATA_MODEL.md)\n\n- The code used to build the Tracker Radar: [Tracker Radar Collector](http://github.com/duckduckgo/tracker-radar-collector) and [Tracker Radar Detector](https://github.com/duckduckgo/tracker-radar-detector)\n\n## Licensing\n\nCopyright (c) 2020 Duck Duck Go, Inc.\n\nThe Tracker Radar data is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/). If you'd like to license the list for commercial use, [please reach out](https://help.duckduckgo.com/duckduckgo-help-pages/company/contact-us/).\n\n## Questions\n\n- **Questions about the Tracker Radar?** See our [frequently asked questions](docs/FAQ.md)\n\n- **Need help with other DuckDuckGo things?** See [DuckDuckGo Help Pages](https://help.duckduckgo.com/).\n",
    "url": "https://github.com/duckduckgo/tracker-radar",
    "last_updated": "2025-08-27T16:07:41+00:00"
  },
  {
    "full_name": "meetfranz/franz",
    "name": "franz",
    "description": "Franz is a free messaging app for services like WhatsApp, Slack, Messenger and many more.",
    "language": "JavaScript",
    "topics": [
      "messaging",
      "electron",
      "franz",
      "messenger",
      "whatsapp",
      "slack",
      "telegram",
      "productivity",
      "office"
    ],
    "readme": "<img src=\"./build-helpers/images/icon.png\" alt=\"\" width=\"150\"/>\n\n# Franz 5\n[![Build status Windows](https://ci.appveyor.com/api/projects/status/9yman4ye19x4274o/branch/master?svg=true)](https://ci.appveyor.com/project/adlk/franz/branch/master)\n [![Build Status Mac & Linux](https://travis-ci.com/meetfranz/franz.svg?branch=master)](https://travis-ci.com/meetfranz/franz) [![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://meetfranz.com/payment.html)\n\nMessaging app for WhatsApp, Slack, Telegram, HipChat, Hangouts and many many more.\n\n## [Download Franz](https://www.meetfranz.com)\n👉 www.meetfranz.com\n\n### Or use homebrew (macOS only)\n\n`$ brew cask install franz`\n\n(Don't know homebrew? [brew.sh](https://brew.sh/))\n\n## Development\n\n### Preparations\n\n#### Install Linux OS dependencies\n[Guide: Linux distribution specific dependencies](docs/linux.md)\n\n#### Fix native modules to match current electron node version\n```bash\n$ npm run rebuild\n```\n\n### Install dependencies\nRun the following command to install all dependencies, and link sibling modules with Franz.\n```bash\n$ npx lerna bootstrap\n```\n\nIf you previously ran `npm install` it sometimes is necessary to delete your `node_modules` folder before running `npx lerna bootstrap`. \n\n### Run Franz Development App\nRun these two commands __simultaneously__ in different console tabs.\n\n```bash\n$ npm run dev\n$ npm run start\n```\nBe aware that the development database will be reset regularly.\n\n## Packaging\n```bash\n$ npm run build\n```\n\n## How can I support the project?\nIf you have found a bug that hasn't been reported yet or want to request a new feature, please open a new issue.\n\n## I need help?\nJoin the Franz community on [Slack](http://slack.franz.im) and get in touch with us.\n\n## Create your own plugins/recipes\nYou can find all the Information at the [Plugins repository](https://github.com/meetfranz/plugins).\nFor questions feel free to ask in the [community Slack](http://slack.franz.im)\n\n## License\nFranz 5 is ope",
    "url": "https://github.com/meetfranz/franz",
    "last_updated": "2025-09-02T04:28:58+00:00"
  },
  {
    "full_name": "prasmussen/glot-snippets",
    "name": "glot-snippets",
    "description": "API for storing code snippets",
    "language": "Erlang",
    "topics": [],
    "readme": "glot-snippets\n=============\n\n## THIS REPO IS DEPRECATED\n## THE LOGIC HAS BEEN REIMPLEMENTED IN [glot-www](https://github.com/glotcode/glot-www) AND IS NO LONGER REQUIRED\n\n\n[![Build Status](https://travis-ci.org/prasmussen/glot-snippets.svg?branch=master)](https://travis-ci.org/prasmussen/glot-snippets)\n\n## Overview\nglot-snippets provides a http api for storing and managing snippets.\nSnippets can be stored anonymously or as a user by including an api token\nwith the request. CouchDB is used as the datastore.\nThe api is described [here](https://github.com/prasmussen/glot-snippets/tree/master/api_docs).\n\n## Run\nThe download above is a standard erlang release that includes a start script.\nTo start glot-snippets in the foreground type: `glot/bin/glot foreground`.\n\n## Environment variables\nglot-snippets takes it's configuration from environment variables.\nAll vars needs to be set, no default values are provided.\n\n| Variable name        | Allowed values                | Example                  | Description                                                  |\n|:---------------------|:------------------------------|:-------------------------|:-------------------------------------------------------------|\n| API_ENVIRONMENT      | development &#124; production | production               | Development mode will enable auto compiling of changed files |\n| API_HTTP_LISTEN_IP   | &lt;ipv4 address&gt;          | 0.0.0.0                  | Listen ip                                                    |\n| API_HTTP_LISTEN_PORT | 1-65535                       | 8090                     | Listen port                                                  |\n| LOG_PATH             | &lt;filepath&gt;              | /home/app/log/           | Path to save logs                                            |\n| BASE_URL             | &lt;url&gt;                   | https://snippets.glot.io | Base url to where the api is hosted                          |\n| ADMIN_TOKEN          | &lt;string&gt;            ",
    "url": "https://github.com/prasmussen/glot-snippets",
    "last_updated": "2024-02-22T03:12:39+00:00"
  },
  {
    "full_name": "ops-class/www",
    "name": "www",
    "description": "www.ops-class.org static website sources, including assignments, guides, slides, and course-specific materials.",
    "language": "HTML",
    "topics": [],
    "readme": "= `ops-class.org` Website Sources\n\n== Installing and Building\n\n. Install https://nodejs.org/en/[`node.js`] and `npm`, the node package\nmanager.\n//\n** I suggest you use https://github.com/creationix/nvm[`nvm`] (the node\nversion manager) or install `node.js` from source.\n//\nThe `apt-get` packages are fairly old.\n//\n. Join the https://github.com/ops-class[`ops-class`] group\non GitHub and add an SSH key.\n//\n. `git clone git@github.com:ops-class/www.git`.\n//\nIf this doesn’t work, you probably don’t have a key installed. Return to Step\n2.\n//\n. `cd www; make install`\n//\n** The first time this will take some time since `npm install` is running and\nperforming some local compilation.\n//\nNote that you also need ImageMagick installed locally to build the node\npackages.\n//\n`sudo apt-get install libmagick++-dev` does the trick on Ubuntu 14.04.\n//\n. `make run`, then open `localhost:8080` in your browser.\n//\n. When you edit, rerun `make` and the site will be rebuilt. `make check` will\nbuild and check the site.\n//\n. I suggest a three terminal environment using `tmux`:\n//\n.. An editing window.\n//\n.. A window to run `make` or `make check`.\n//\n.. A window to run `make run` to serve the sources.\n\n== Deploying\n\n. `make check` runs several sets of build-time checks:\n//\n** Broken links by\n//\nhttp://npmjs.com/package/metalsmith-linkcheck[`metalsmith-linkcheck`].\n//\nBroken external links will not fail the build but will generate a warning\n//\nBroken internal links will.\n//\n** Formatting checks by\n//\nhttps://www.npmjs.com/package/metalsmith-formatcheck[`metalsmith-formatcheck`].\n//\n*Format failures will fail the build*, but I doubt you'll be changing the\nformat much.\n//\n** Spelling checks by\n//\nhttps://www.npmjs.com/package/metalsmith-spellcheck[`metalsmith-spellcheck`].\n//\n*Spelling failures will halt the build.*\n//\n** I maintain all three of these http://www.metalsmith.io/[Metalsmith]\nplugins, so if you find bugs let me know.\n\n. If the build doesn’t complete, fix the problems and rerun make.",
    "url": "https://github.com/ops-class/www",
    "last_updated": "2025-05-13T11:49:06+00:00"
  },
  {
    "full_name": "aws-samples/amazon-textract-response-parser",
    "name": "amazon-textract-response-parser",
    "description": "Parse JSON response of Amazon Textract",
    "language": "TypeScript",
    "topics": [
      "amazon-textract"
    ],
    "readme": "# Textract Response Parser\n\nYou can use Textract response parser library to easily parse JSON returned by Amazon Textract. The library parses JSON and provides programming language specific constructs to work with different parts of the document. [textractor](https://github.com/aws-samples/amazon-textract-textractor) is an example of a PoC batch processing tool that takes advantage of the Textract response parser library and generates output in multiple formats.\n\n## Python Usage\n\nFor documentation on usage see: [src-python/README.md](src-python/README.md)\n\n## JavaScript/TypeScript Usage\n\nFor documentation on usage see: [src-js/README.md](src-js/README.md)\n\n## C# Usage\n\n### Forms\n\n```csharp\ndocument.Pages.ForEach(page => {\n    Console.WriteLine(\"Print Lines and Words:\");\n    page.Lines.ForEach(line => {\n        Console.WriteLine(\"{0}--{1}\", line.Text, line.Confidence);\n        line.Words.ForEach(word => {\n            Console.WriteLine(\"{0}--{1}\", word.Text, word.Confidence);\n        });\n    });\n    Console.WriteLine(\"Print Fields:\");\n    page.Form.Fields.ForEach(f => {\n        Console.WriteLine(\"Field: Key: {0}, Value {1}\", f.Key, f.Value);\n    });\n    Console.WriteLine(\"Get Field by Key:\");\n    var key = \"Phone Number:\";\n    var field = page.Form.GetFieldByKey(key);\n    if(field != null) {\n        Console.WriteLine(\"Field: Key: {0}, Value: {1}\", field.Key, field.Value);\n    }\n});\n```\n\n### Tables\n\n```csharp\ndocument.Pages.ForEach(page => {\n    page.Tables.ForEach(table => {\n        var r = 0;\n        table.Rows.ForEach(row => {\n            r++;\n            var c = 0;\n            row.Cells.ForEach(cell => {\n                c++;\n                Console.WriteLine(\"Table [{0}][{1}] = {2}--{3}\", r, c, cell.Text, cell.Confidence);\n            });\n        });\n    });\n});\n```\n\nCheck out the `src-csharp` folder for instructions on how to run [.NET Core C#](src-csharp/readme.md) samples\n\n## Other Resources\n\n- [Large scale document processing with Amazon Textract - Reference Ar",
    "url": "https://github.com/aws-samples/amazon-textract-response-parser",
    "last_updated": "2025-08-19T10:06:51+00:00"
  },
  {
    "full_name": "eddelbuettel/linl",
    "name": "linl",
    "description": "Linl Is Not Letter -- Markdown-based LaTeX Letter Template",
    "language": "TeX",
    "topics": [
      "r",
      "package",
      "markdown",
      "letter",
      "pandoc",
      "cran"
    ],
    "readme": "## linl: Linl is not Letter\n\n[![CI](https://github.com/eddelbuettel/linl/workflows/ci/badge.svg)](https://github.com/eddelbuettel/linl/actions?query=workflow%3Aci)\n[![Package-License](https://img.shields.io/badge/license-GPL--3-brightgreen.svg?style=flat)](http://www.gnu.org/licenses/gpl-3.0.html) \n[![CRAN](https://www.r-pkg.org/badges/version/linl)](https://cran.r-project.org/package=linl) \n[![Dependencies](https://tinyverse.netlify.app/badge/linl)](https://cran.r-project.org/package=linl) \n[![Downloads](https://cranlogs.r-pkg.org/badges/linl?color=brightgreen)](https://www.r-pkg.org:443/pkg/linl)\n[![Last Commit](https://img.shields.io/github/last-commit/eddelbuettel/linl)](https://github.com/eddelbuettel/linl)\n\n### Motivation\n\nThe LaTeX Letter class is very useful for simple letters.  As such\nletters are also somewhat standardized, they make for excellent\ncandidates for use by Markdown.\n\nThis package leans on earlier work by [Aaron Wolen](https://www.aaronwolen.com/) in his\n[pandoc-letter](https://github.com/aaronwolen/pandoc-letter) repository, and extends it for use from\nR via the [rmarkdown](https://cran.r-project.org/package=rmarkdown) package.\n\n### Example\n\n#### Skeleton\n\nThe skeleton creates a very simple letter.  This shows the (complete) source on the left and the\nrendered pdf on the right:\n\n![](https://eddelbuettel.github.io/linl/skeleton.png)\n\nSeveral formatting defaults for font, fontsize, indentation are in use. See `help(linl)` for a \ncomplete list and default values.\n\n#### Vignette\n\nThe vignette example is a little more featureful and shows how to include a letterhead on-demand, a\nsignature, and a few formatting settings.  All of these are driven by simple YAML headers as seen on\nthe left:\n\n![](https://eddelbuettel.github.io/linl/vignette.png)\n\nThe vignette also contains the few lines of vignette metadata one would exclude from a normal\nletter.\n        \n### Status\n\nThe package is fully working, and on [CRAN](https://cran.r-project.org/).\n\n### Usage \n",
    "url": "https://github.com/eddelbuettel/linl",
    "last_updated": "2025-06-18T13:08:15+00:00"
  },
  {
    "full_name": "StevenBlack/hosts",
    "name": "hosts",
    "description": "🔒 Consolidating and extending hosts files from several well-curated sources. Optionally pick extensions for porn, social media, and other categories.",
    "language": "Python",
    "topics": [
      "python",
      "unified-hosts",
      "malware",
      "ad-blocker",
      "porn-filter",
      "social-media-filter",
      "hosts",
      "privacy",
      "protection",
      "pornblocker",
      "gambling-filter",
      "ransomware",
      "trojans",
      "security",
      "curated-sources",
      "anti-virus"
    ],
    "readme": "**Take Note!**\n\nWith the exception of issues and PRs regarding changes to\n`hosts/data/StevenBlack/hosts`, all other issues regarding the content of the\nproduced hosts files should be made with the appropriate data source that\ncontributed the content in question. The contact information for all of the data\nsources can be found in the `hosts/data/` directory.\n\n---\n\n![Logo](https://raw.githubusercontent.com/StevenBlack/hosts/master/.github/logo.png)\n\n[![latest release](https://img.shields.io/github/release/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/releases)\n[![license](https://img.shields.io/github/license/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/blob/master/license.txt)\n[![repo size](https://img.shields.io/github/repo-size/StevenBlack/hosts)](https://github.com/StevenBlack/hosts)\n[![contributors](https://img.shields.io/github/contributors/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/graphs/contributors)\n[![Build Status](https://img.shields.io/github/actions/workflow/status/StevenBlack/hosts/ci.yml?branch=master)](https://github.com/StevenBlack/hosts/actions/workflows/ci.yml?query=branch%3Amaster)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000)](https://github.com/python/black)\n[![commits since last release](https://img.shields.io/github/commits-since/StevenBlack/hosts/latest)](https://github.com/StevenBlack/hosts/commits/master)\n[![last commit](https://img.shields.io/github/last-commit/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/commits/master)\n[![commit activity](https://img.shields.io/github/commit-activity/y/StevenBlack/hosts)](https://github.com/StevenBlack/hosts/commits/master)\n\n# Unified hosts file with base extensions\n\nThis repository consolidates several reputable `hosts` files, and merges them\ninto a unified hosts file with duplicates removed. A variety of tailored hosts\nfiles are provided.\n\n**Therefore this repository is a hosts file aggregator.**\n\n![Aggregator](https://raw",
    "url": "https://github.com/StevenBlack/hosts",
    "last_updated": "2025-09-02T08:05:25+00:00"
  },
  {
    "full_name": "fureigh/unbias-me",
    "name": "unbias-me",
    "description": "A Chrome extension to let you review GitHub, LinkedIn and Twitter profiles with minimal attribution information.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Unbias Me\n\nReducing unconscious bias in reviewing job candidates' online materials.\n\n## What it is\n\nThis is a [Chrome extension](https://chrome.google.com/webstore/detail/bghhadboobnoikppffdojcebigmcgmam/) that hides a candidate's profile picture and name when you're looking at their GitHub or LinkedIn page.\n\n## Why?\n\nMore than two decades of research have indicated that despite our best efforts, unconscious bias is inescapable. But removing room for biases can help. (Auditioning musicians behind a privacy screen, for instance, is [famously credited with increasing the percentage of women musicians](http://gender.stanford.edu/news/2013/leveling-playing-field) in orchestras from 5 percent to 25 percent since the 1970s.)\n\n## How do I install it?\n\nIt's easy: **[install directly from the Chrome web store](https://chrome.google.com/webstore/detail/bghhadboobnoikppffdojcebigmcgmam/)**! (It's free.)\n\n## Copyright and license\n\nCopyright [Fureigh](https://www.github.com/fureigh), 2016. MIT License.\n",
    "url": "https://github.com/fureigh/unbias-me",
    "last_updated": "2023-03-17T17:24:40+00:00"
  },
  {
    "full_name": "ferronweb/ferron",
    "name": "ferron",
    "description": "A fast, memory-safe web server written in Rust.",
    "language": "Rust",
    "topics": [
      "rust",
      "server",
      "web",
      "webserver"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://www.ferronweb.org\" target=\"_blank\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"wwwroot/img/logo-dark.png\">\n      <img alt=\"Ferron logo\" src=\"wwwroot/img/logo.png\" width=\"256\">\n    </picture>\n  </a>\n</p>\n<p align=\"center\">\n  <b>Ferron</b> - a fast, memory-safe web server written in Rust\n</p>\n<p align=\"center\">\n  <a href=\"https://www.ferronweb.org/docs\" target=\"_blank\"><img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Documentation-orange\"></a>\n  <a href=\"https://www.ferronweb.org\" target=\"_blank\"><img alt=\"Website\" src=\"https://img.shields.io/website?url=https%3A%2F%2Fwww.ferronweb.org\"></a>\n  <a href=\"https://matrix.to/#/#ferronweb:matrix.org\" target=\"_blank\"><img alt=\"Chat\" src=\"https://img.shields.io/matrix/ferronweb%3Amatrix.org\"></a>\n  <a href=\"https://x.com/ferron_web\" target=\"_blank\"><img alt=\"X (formerly Twitter) Follow\" src=\"https://img.shields.io/twitter/follow/ferron_web\"></a>\n  <a href=\"https://hub.docker.com/r/ferronserver/ferron\" target=\"_blank\"><img alt=\"Docker Pulls\" src=\"https://img.shields.io/docker/pulls/ferronserver/ferron\"></a>\n  <a href=\"https://github.com/ferronweb/ferron\" target=\"_blank\"><img alt=\"GitHub Repo stars\" src=\"https://img.shields.io/github/stars/ferronweb/ferron\"></a>\n</p>\n\n* * *\n\n## Features\n\n- **High performance** - built with Rust’s async capabilities for optimal speed.\n- **Memory-safe** - built with Rust, which is a programming language offering memory safety.\n- **Extensibility** - modular architecture for easy customization.\n- **Secure** - focus on robust security practices and safe concurrency.\n\n## Components\n\nFerron consists of multiple components:\n\n- **`ferron`**: The main web server.\n- **`ferron-passwd`**: A tool for generating user entries with hashed passwords, which can be copied into the web server's configuration file.\n\n## Building Ferron from source\n\nYou can clone the repository and explore the existing code:\n\n```sh\ngit clone https://github.co",
    "url": "https://github.com/ferronweb/ferron",
    "last_updated": "2025-09-01T06:05:15+00:00"
  },
  {
    "full_name": "paultopia/scrape-pdf",
    "name": "scrape-pdf",
    "description": "pdf scraping chrome extension (experiment)",
    "language": "JavaScript",
    "topics": [],
    "readme": "This is a demo of a chrome extension that can intercept user-clicked links, grab binary data from them, convert that binary data to \nbase64 encoded strings, and then do whatever one wants to do with it (in this example, just log it to the console). \n\nObviously, in actual production one wouldn't want to grab every single link on the page (some of them might not even be binary data!), \nalso one wouldn't want to just log that sucker to the console. To actually send the data somewhere, you'll have to make use of \nchrome's [message passing API](https://developer.chrome.com/extensions/messaging) to get it from a content script (that has access \nto the page) to a background script (that has access to the outside world).\n\nMy own use for this is as an automated backup to research assistants \nwho will be viewing a bunch of pdfs online and collecting data from them---I will be using this technique to also capture the PDFs they \nexamine in the database and associate it with the entered data without having to trust RAs to remember to save and upload every file. \n\nIf you have similar needs, it may be helpful.\n\nThe testcont directory has example content that this code is verified to work with (may not work locally, but worked off a server). \n(Also, I don't *think* this will be dependent on server implementation, but it might be, if some servers return weird response bodies \nor something in response to requests for binary files.  I've only tested on a generic LAMP stack static site server.)\n\nThe testprocess directory has example Python code for converting a pdf to and from base64 locally. I've verified that the decoding \npart of the Python code can successfully decode PDFs that have been encoded by the chrome extension, and also that the python code makes \na successful round trip on its own.\n\nPython version tested: 2.7.11. Python 3 handles strings in a weird new way, and there are absolutely no guarantees that the Python side \nof the code will work there.\n\nChrome version tested: 51",
    "url": "https://github.com/paultopia/scrape-pdf",
    "last_updated": "2023-09-09T14:32:28+00:00"
  },
  {
    "full_name": "rlabbe/Kalman-and-Bayesian-Filters-in-Python",
    "name": "Kalman-and-Bayesian-Filters-in-Python",
    "description": "Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs.  Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# [Kalman and Bayesian Filters in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)\n\n\nIntroductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Jupyter Notebook so that you can run and modify the code in your browser. What better way to learn?\n\n\n**\"Kalman and Bayesian Filters in Python\" looks amazing! ... your book is just what I needed** - Allen Downey, Professor and O'Reilly author.\n\n**Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.** - Sam Rodkey, SpaceX\n\nStart reading online now by clicking the binder or Azure badge below:\n\n\n[![Binder](http://mybinder.org/badge.svg)](https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master)\n\n\n![alt tag](https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif)\n\nWhat are Kalman and Bayesian Filters?\n-----\n\nSensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.\n\nIn simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, ",
    "url": "https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python",
    "last_updated": "2025-09-02T01:54:59+00:00"
  },
  {
    "full_name": "lmullen/ocrquality",
    "name": "ocrquality",
    "description": "An R package to measure OCR quality",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\nocrquality\n----------\n\nAn R package for measuring OCR quality\n\n**Author:** [Lincoln Mullen](http://lincolnmullen.com)<br> **License:** [MIT](http://opensource.org/licenses/MIT)<br> **Status:** In development\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/ocrquality)](http://cran.r-project.org/package=ocrquality) [![CRAN\\_Downloads](http://cranlogs.r-pkg.org/badges/grand-total/ocrquality)](http://cran.r-project.org/package=ocrquality) [![Travis-CI Build Status](https://travis-ci.org/lmullen/ocrquality.svg?branch=master)](https://travis-ci.org/lmullen/ocrquality>) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/lmullen/ocrquality?branch=master)](https://ci.appveyor.com/project/lmullen/ocrquality)\n\n### Description\n\nMeasuring OCR rigorously is probably more effort than it is worth, if it can even be done properly. But sometimes you have a corpus, perhaps one for which you have done the OCR yourself, and need to check the reliability of the OCR to make sure that the texts are about the same quality. That's what this package is for. It provides a few quick-and-dirty methods of estimating the quality of OCR. These estimates do not rely on any ground truth, so they are not an absolute measure of the quality of the texts. But they do provide a relative measure within the corpus, so that you can detect texts which are significantly worse than others.\n",
    "url": "https://github.com/lmullen/ocrquality",
    "last_updated": "2023-01-27T21:13:14+00:00"
  },
  {
    "full_name": "open-guides/og-aws",
    "name": "og-aws",
    "description": "📙 Amazon Web Services — a practical guide",
    "language": "Shell",
    "topics": [],
    "readme": "![An Open Guide](figures/signpost-horiz1-1600.jpg)\n\nThe Open Guide to Amazon Web Services\n=====================================\n\n[![Slack Chat](https://img.shields.io/badge/Chat-Slack-ff69b4.svg \"Join us. Anyone is welcome!\")](http://slackhatesthe.cloud) ⇦ Join us!\n\n[Credits](AUTHORS.md) ∙ [Contributing guidelines](CONTRIBUTING.md)\n\nTable of Contents\n-----------------\n\n**Purpose**\n\n-\t[Why an Open Guide?](#why-an-open-guide)\n-\t[Scope](#scope)\n-\t[Legend](#legend)\n\n**AWS in General**\n\n-\t[General Information](#general-information)\n-\t[Learning and Career Development](#learning-and-career-development)\n-\t[Managing AWS](#managing-aws)\n-\t[Managing Servers and Applications](#managing-servers-and-applications)\n\n| Specific AWS Services                 | Basics                         | Tips                          | Gotchas                                        |\n|---------------------------------------|--------------------------------|-------------------------------|------------------------------------------------|\n| [ALB](#alb) | [📗](#alb-basics) | [📘](#alb-tips) | [📙](#alb-gotchas-and-limitations) |\n| [AMIs](#amis) | [📗](#ami-basics) | [📘](#ami-tips) | [📙](#ami-gotchas-and-limitations) |\n| [API Gateway](#api-gateway) | [📗](#api-gateway-basics) | [📘](#api-gateway-tips) | [📙](#api-gateway-gotchas-and-limitations) |\n| [Auto Scaling](#auto-scaling) | [📗](#auto-scaling-basics) | [📘](#auto-scaling-tips) | [📙](#auto-scaling-gotchas-and-limitations) |\n| [Batch](#batch) | [📗](#batch-basics) | [📘](#batch-tips) |\n| [Certificate Manager](#certificate-manager) | [📗](#certificate-manager-basics) | [📘](#certificate-manager-tips) | [📙](#certificate-manager-gotchas-and-limitations) |\n| [CLB (ELB)](#clb) | [📗](#clb-basics) | [📘](#clb-tips) | [📙](#clb-gotchas-and-limitations) |\n| [CloudFront](#cloudfront) | [📗](#cloudfront-basics) | [📘](#cloudfront-tips) | [📙](#cloudfront-gotchas-and-limitations) |\n| [CloudFormation](#cloudformation) | [📗](#cloudformation-basics) | [📘](#cloudformation-tips) ",
    "url": "https://github.com/open-guides/og-aws",
    "last_updated": "2025-09-01T14:11:06+00:00"
  },
  {
    "full_name": "mAAdhaTTah/wordpress-github-sync",
    "name": "wordpress-github-sync",
    "description": "A WordPress plugin to sync content with a GitHub repository (or Jekyll site)",
    "language": "PHP",
    "topics": [
      "wordpress-plugin"
    ],
    "readme": "# WordPress <--> GitHub Sync #\n**Contributors:** JamesDiGioia, benbalter  \n**Tags:** github, git, version control, content, collaboration, publishing  \n**Requires at least:** 3.9  \n**Tested up to:** 4.8  \n**Stable tag:** 2.0.1  \n**License:** GPLv2  \n**License URI:** http://www.gnu.org/licenses/gpl-2.0.html  \n\n## Description ##\n\n*A WordPress plugin to sync content with a GitHub repository (or Jekyll site)*\n\n[![Build Status](https://scrutinizer-ci.com/g/mAAdhaTTah/wordpress-github-sync/badges/build.png?b=master)](https://scrutinizer-ci.com/g/mAAdhaTTah/wordpress-github-sync/build-status/master) [![Code Coverage](https://scrutinizer-ci.com/g/mAAdhaTTah/wordpress-github-sync/badges/coverage.png?b=master)](https://scrutinizer-ci.com/g/mAAdhaTTah/wordpress-github-sync/?branch=master) [![Scrutinizer Code Quality](https://scrutinizer-ci.com/g/mAAdhaTTah/wordpress-github-sync/badges/quality-score.png?b=master)](https://scrutinizer-ci.com/g/mAAdhaTTah/wordpress-github-sync/?branch=master)\n\nEver wish you could collaboratively author content for your WordPress site (or expose change history publicly and accept pull requests from your readers)?\n\nLooking to tinker with Jekyll, but wish you could use WordPress's best-of-breed web editing interface instead of Atom? (gasp!)\n\nWell, now you can! Introducing [WordPress <--> GitHub Sync](https://github.com/mAAdhaTTah/wordpress-github-sync)!\n\n### WordPress <--> GitHub Sync does three things: ###\n\n1.  Allows content publishers to version their content in GitHub, exposing \"who made what change when\" to readers\n2.  Allows readers to submit proposed improvements to WordPress-served content via GitHub's Pull Request model\n3.  Allows non-technical writers to draft and edit a Jekyll site in WordPress's best-of-breed editing interface\n\n### WordPress <--> GitHub sync might be able to do some other cool things: ###\n\n* Allow teams to collaboratively write and edit posts using GitHub (e.g., pull requests, issues, comments)\n* Allow you to sync the co",
    "url": "https://github.com/mAAdhaTTah/wordpress-github-sync",
    "last_updated": "2025-08-24T05:53:06+00:00"
  },
  {
    "full_name": "rwaldron/johnny-five",
    "name": "johnny-five",
    "description": "JavaScript Robotics and IoT programming framework, developed at Bocoup.",
    "language": "JavaScript",
    "topics": [
      "javascript",
      "arduino",
      "tessel",
      "raspberry-pi",
      "chip",
      "photon",
      "pcduino",
      "gpio",
      "i2c",
      "adc",
      "dac",
      "pwm",
      "spi",
      "serial",
      "1-wire",
      "bluetooth",
      "iot",
      "robotics",
      "beaglebone-black",
      "intel"
    ],
    "readme": "![](https://github.com/rwaldron/johnny-five/raw/main/assets/sgier-johnny-five.png)\n\n# Johnny-Five\n### The JavaScript Robotics Programming Framework\n\n<!-- \n\n    Hello!\n\n    Please don't edit this file!\n\n    If you'd like to make changes to the readme contents, please make them in the tpl/.readme.md file. If you'd like to add an example: \n\n    1. Add the file in `eg/`\n    2. Add a breadboard image in `docs/breadboards`\n    3. Add an entry to `tpl/programs.json`. \n    4. Generated the markdown with: `grunt examples`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-->\n\n\n_Artwork by [Mike Sgier](http://msgierillustration.com)_\n\n[![Build, Lint, Test and Measure Coverage](https://github.com/rwaldron/johnny-five/actions/workflows/npm-grunt.yml/badge.svg)](https://github.com/rwaldron/johnny-five/actions)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/hmke71k7uemtnami/branch/main?svg=true)](https://ci.appveyor.com/project/rwaldron/johnny-five)\n[![Coverage Status](https://coveralls.io/repos/github/rwaldron/johnny-five/badge.svg?branch=main)](https://coveralls.io/github/rwaldron/johnny-five?branch=main)\n[![Install Size](https://packagephobia.now.sh/badge?p=johnny-five)](https://packagephobia.now.sh/result?p=johnny-five)\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/rwaldron/johnny-five)\n\n\n\n**Johnny-Five is an Open Source, Firmata Protocol based, IoT and Robotics programming framework, developed by the [Nodebots](https://twitter.com/nodebots) Community. Johnny-Five programs can be written for Arduino (all models), Electric Imp, Beagle Bone, Intel Galileo & Edison, Linino One, Pinoccio, pcDuino3, Raspberry Pi, Particle/Spark Core & Photon, Tessel 2, TI Launchpad and more!**\n\nJohnny-Five has grown from a passion project into a tool for inspiring learning and creativity for people of all ages, backgrounds, and from all across the world.\n\nJust interested in learning and building awesome things? You might want to start with the officia",
    "url": "https://github.com/rwaldron/johnny-five",
    "last_updated": "2025-08-31T16:03:01+00:00"
  },
  {
    "full_name": "soodoku/tldr",
    "name": "tldr",
    "description": "Distilling key points, reorganizing, and modestly augmenting the points from books and lectures.",
    "language": "Shell",
    "topics": [
      "summaries",
      "books",
      "lectures"
    ],
    "readme": "## TL;DR\n\nThe density of original points in an average work---book, paper, article, lecture---tends to be pretty low. Most work also tends to be badly organized. Distilling work is thus a time intensive exercise. So I thought it would be useful to share the fruits of that labor.\n\n1. [Eyal, Nir. Hooked.](summaries/01_eyal_hooked.md)  \n   [Link to Amazon](https://www.amazon.com/dp/B00LMGLXTS/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)\n\n2. [Schmidt, Eric, and Jonathan Rosenberg. How Google Works.](summaries/02_schmidt_rosenberg_how_goog_works.md)  \n   [Link to Amazon](https://www.amazon.com/How-Google-Works-Eric-Schmidt-ebook/dp/B00HUU13Y0/)\n\n3. [Wickham, Hadley. ggplot2.](summaries/03_wickham_ggplot2.md)  \n   [Link to Amazon](https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis-ebook/dp/B01GVCRF6M/)\n\n4. [Greenstein, Fred. Children and Politics.](summaries/04_greenstein_children_and_politics.md)  \n   [Link to Amazon](https://smile.amazon.com/Children-Politics-Political-Science-Study/dp/0300013191/)\n\n5. [Newport, Cal. Deep Work.](summaries/05_newport_deep_work.md)  \n   [Link to Amazon](https://smile.amazon.com/Deep-Work-Focused-Success-Distracted-ebook/dp/B00X47ZVXM/)\n\n6. [Coffey, Diane, and Dean Spears. Where India Goes.](summaries/06_coffey_spears_where_india_goes.md)  \n   [Link to Amazon](https://smile.amazon.com/Where-India-Goes-Abandoned-Development-ebook/dp/B072WKXMML/)\n\n7. [Stephens-Davidowitz, Seth. Everybody Lies.](summaries/07_stephens_davidowitz_everybody_lies.md)  \n   [Link to Amazon](https://smile.amazon.com/Everybody-Lies-Internet-About-Really-ebook/dp/B01AFXZ2F4/)\n\n8. [Cialdini, Robert. Influence.](summaries/08_cialdini_influence.md)  \n    [Link to Amazon](https://smile.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/006124189X/)\n\n9. [Hamming, Richard. You and Your Research.](summaries/09_hamming_how_to_do_good_work.md)  \n    http://www.cs.virginia.edu/~robins/YouAndYourResearch.pdf\n\n10. [Sunstein, Cass and Reid Hastie. Wiser: Get",
    "url": "https://github.com/soodoku/tldr",
    "last_updated": "2025-06-11T22:59:08+00:00"
  },
  {
    "full_name": "trifle/twitter-diversity",
    "name": "twitter-diversity",
    "description": "Data files for the paper \"Forecasting the pulse: How deviations from regular patterns in online data can identify offline phenomena\"",
    "language": "",
    "topics": [],
    "readme": "twitter-diversity\n=================\n\nData files for the paper \"Forecasting the pulse: How deviations from regular patterns in online data can identify offline phenomena\"\n\nThere are two comma-separated files:\n\n- tag-volume.csv contains 1000 data series consisting of the tweet volume of the 1000 most popular hashtags\n- total-volume.csv contains a single data series of the total volume of tweets per hour\n\nBoth files include a date field that follows the ISO standard, ie: YYYYMMDDHH (year, month, day, hour).\n\nThese files can be easily imported into a statistics package such as R:\n\n\ttags <- read.csv(\"tag-volume.csv\", header=TRUE)\n\ttags$date <- as.POSIXct(strptime(tags$date, format='%Y%m%d%H'))\n\t\nQuestions and suggestions can be submitted via pull request or directed at the authors:  \n\npascal.juergens@googlemail.com and  \nandreas.jungherr@googlemail.com",
    "url": "https://github.com/trifle/twitter-diversity",
    "last_updated": "2018-03-06T20:48:12+00:00"
  },
  {
    "full_name": "eddelbuettel/ctv-hpc",
    "name": "ctv-hpc",
    "description": "CRAN Task View: High-Performance Computing with R",
    "language": "R",
    "topics": [
      "cran",
      "task-view",
      "high-performance-computing",
      "hpc",
      "hpc-applications",
      "r"
    ],
    "readme": "\n## CRAN Task View High Performance and Parallel Computing\n\nMaintainer: [Dirk Eddelbuettel](https://dirk.eddelbuettel.com)  \nDate: 2025-07-22   \n\n\nThis CRAN Task View contains a list of packages, grouped by topic, that are useful for\nhigh-performance computing (HPC) with R. In this context, we are defining 'high-performance\ncomputing' rather loosely as just about anything related to pushing R a little further: using\ncompiled code, parallel computing (in both explicit and implicit modes), working with large objects\nas well as profiling.\n\nUnless otherwise mentioned, all packages presented with hyperlinks are available from the\n[Comprehensive R Archive Network (CRAN)](https://cran.r-project.org).\n\nSeveral of the areas discussed in this Task View are undergoing rapid change. Please send\nsuggestions for additions and extensions for this task view via e-mail to the maintainer or submit\nan issue or pull request in the GitHub repository linked above.  See the [Contributing\npage](https://github.com/cran-task-views/ctv/blob/main/Contributing.md) in the [CRAN Task\nViews](https://github.com/cran-task-views) repo for details.\n\nSuggestions and corrections by Achim Zeileis, Markus Schmidberger, Martin Morgan, Max Kuhn, Tomas\nRadivoyevitch, Jochen Knaus, Tobias Verbeke, Hao Yu, David Rosenberg, Marco Enea, Ivo Welch, Jay\nEmerson, Wei-Chen Chen, Bill Cleveland, Ross Boylan, Ramon Diaz-Uriarte, Mark Zeligman, Kevin Ushey,\nGraham Jeffries, Will Landau, Tim Flutre, Reza Mohammadi, Ralf Stubner, Bob Jansen, Matt Fidler,\nBrent Brewington and Ben Bolder (as well as others I may have forgotten to add here) are gratefully\nacknowledged.\n\nThe `ctv` package supports these Task Views. Its functions `install.views` and `update.views` allow,\nrespectively, installation or update of packages from a given Task View; the option `coreOnly` can\nrestrict operations to packages labeled as *core* below.\n\nDirect support in R started with release 2.14.0 which includes a new package **parallel**\nincorporatin",
    "url": "https://github.com/eddelbuettel/ctv-hpc",
    "last_updated": "2025-07-22T12:03:15+00:00"
  },
  {
    "full_name": "khakieconomics/heterogeneous_treatments",
    "name": "heterogeneous_treatments",
    "description": "Various models of heterogeneous treatment effects",
    "language": "Stan",
    "topics": [],
    "readme": "",
    "url": "https://github.com/khakieconomics/heterogeneous_treatments",
    "last_updated": "2021-08-12T21:39:43+00:00"
  },
  {
    "full_name": "hrbrmstr/knitrengines",
    "name": "knitrengines",
    "description": "An R package to collect and seamlessly add new language engines to knitr",
    "language": "HTML",
    "topics": [],
    "readme": "\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-0%25-lightgrey.svg)\n\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-4.0.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# knitrengines\n\nCollection of Language Engines for Use with the ‘knitr’ Package\n\n## Description\n\nThe knitr package lets you use other languages in knitr ‘chunks’. This\npackage is a collection of languages processors not built-in with knitr.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n- `knitr_cawk_engine`: knitr engine for the CSV-enhanced Awk language\n- `knitr_elixir_engine`: knitr engine for the Elixir language\n- `knitr_gnuplot_engine`: knitr engine for the Go language\n- `knitr_pygments_engine`: knitr engine for the pygments syntax\n  highlighting (w/o execution)\n\n## Come Again?\n\n`knitrengines` is package to collect and seamlessly add new language\nengines to knitr\n\nYou can thank [Wendy\nSmoak](http://wsmoak.net/2015/09/01/executable-elixir-tufte-handout.html)\nfor this package as a [comment on a DDS\npost](http://datadrivensecurity.info/blog/posts/2015/Jun/running-other-languages-in-r-markdown-files/)\nsparked it.\n\nThe knitr package already has support for a\n[plethora](https://bookdown.org/yihui/rmarkdown/language-engines.html)\nof languages besides R code chunks (26 as of the last update to this\npackage). That is probably sufficient for the vast majority of users.\n\nHowever, if you need to perform some processing in another languages and\nwant to include it in your reproducible workflow, this package will\nallow you to incorporate those language code chunks provided there is a\nmatching knitr language processor available.\n\nTo use ",
    "url": "https://github.com/hrbrmstr/knitrengines",
    "last_updated": "2025-03-22T11:18:58+00:00"
  },
  {
    "full_name": "dinghanshen/SWEM",
    "name": "SWEM",
    "description": "The Tensorflow code for this ACL 2018 paper: \"Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms\"",
    "language": "Python",
    "topics": [
      "natural-language-processing",
      "deep-learning",
      "representation-learning",
      "tensorflow"
    ],
    "readme": "# SWEM (Simple Word-Embedding-based Models)\n\nThis repository contains source code necessary to reproduce the results presented in the following paper:\n* [*Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms*](https://arxiv.org/abs/1805.09843) (ACL 2018)\n\nThis project is maintained by [Dinghan Shen](https://sites.google.com/view/dinghanshen). Feel free to contact dinghan.shen@duke.edu for any relevant issues.\n\n## Prerequisite: \n* CUDA, cudnn\n* Python 2.7\n* Tensorflow (version >1.0). We used tensorflow 1.5.\n* Run: `pip install -r requirements.txt` to install requirements\n\n## Data: \n* For convenience, we provide pre-processed versions for the following datasets: DBpedia, SNLI, Yahoo. Data are prepared in pickle format, and each `.p` file has the same fields in the same order: \n\t* `train_text`, `val_text`, `test_text`, `train_label`, `val_label`, `test_label`, `dictionary(wordtoix)`, `reverse dictionary(ixtoword)`\n\n* These `.p` files can be downloaded from the links below. After downloading, you can put them into a `data` folder:\n\n \t* Ontology classification: [DBpedia (591MB)](https://drive.google.com/open?id=1EBmMise0LQu0QpO7T4a32WMFuTxAb6T0)\n\n \t* Natural language inference: [SNLI (101MB)](https://drive.google.com/open?id=1M13UswHThZYt-ARrHg6sN7Dlel-d6BB3),  [SNLI-word-embeddings (129MB)](https://drive.google.com/open?id=1qzClw-ZJzWZ4ce0eXAG4dDXYUNnmeX4f)\n\n \t* Topic categorization: [Yahoo (1.7GB)](https://drive.google.com/open?id=1Dorz_CWZkHHpojVS4K4YUEhhczVLQgRc)\n\n## Run \n* Run: `python eval_dbpedia_emb.py` for ontology classification on the DBpedia dataset\n* Run: `python eval_snli_emb.py` for natural language inference on the SNLI dataset\n* Run: `python eval_yahoo_emb.py` for topic categorization on the Yahoo! Answer dataset\n\n* Options: options can be made by changing `option` class in any of the above three files: \n- `opt.emb_size`: number of word embedding dimensions.\n- `opt.drop_rate`: the keep rate of dropout layer.\n-",
    "url": "https://github.com/dinghanshen/SWEM",
    "last_updated": "2025-08-07T12:22:49+00:00"
  },
  {
    "full_name": "dahtah/imager",
    "name": "imager",
    "description": "R package for image processing",
    "language": "C++",
    "topics": [
      "image-processing",
      "r",
      "cimg"
    ],
    "readme": "# IMPORTANT REDIRECT\n\nFor future (as of 14/02/2023) **imager** updates and general maintenance enhancements please use:\n\nhttps://github.com/asgr/imager\n\nThis (dahtah/imager) repo will no longer be receiving *pull* requests or fixing *issues*. All *pull* requests and *issues* must go to asgr/imager.\n",
    "url": "https://github.com/dahtah/imager",
    "last_updated": "2025-07-29T01:52:43+00:00"
  },
  {
    "full_name": "Vijayanand-debug/pollstats",
    "name": "pollstats",
    "description": "",
    "language": "CSS",
    "topics": [],
    "readme": "=====\npollstats\n=====\n\nPollstats is a Django app which display the entire number of Voters, calculates the percentage of voters who took part in the \nonline poll. This library is exclusively designed for a college project and may not have all the dynamic features.\n\nQuick start\n-----------\n\n1. Add \"pollstats.apps.PollstatsConfig\" to your INSTALLED_APPS setting like this::\n\n    INSTALLED_APPS = [\n        ...\n        'pollstats.apps.PollstatsConfig',\n    ]\n\n2. Include the polls URLconf in your project urls.py like this::\n    path('pollstats/',include(\"pollstats.urls\")),\n\n3. This library uses Django's default User Object and the data in it is accessed by using from django.contrib.auth.models import User\n\n4. This library strictly uses the following models to calculate the data:\n    \n    class Events(models.Model):\n     event_name = models.CharField(max_length=200)\n     event_code = models.CharField(max_length=20)\n     referal_code = models.CharField(max_length=20)\n     hosted_by = models.ForeignKey(User, default=1, on_delete=models.CASCADE)\n     event_description = models.TextField()\n     event_status = models.IntegerField(default=0)\n     starting_date = models.DateField(default=None)\n     ending_date = models.DateField(default=None)\n\n    class Options(models.Model):\n     option_name = models.CharField(max_length=100)\n     count = models.IntegerField(default=0)\n     event_code = models.CharField(max_length=200)\n    \n    class Transactions(models.Model):\n     voter = models.ForeignKey(User, default=1, on_delete=models.CASCADE)\n     voting_time = models.DateTimeField(auto_now_add=True) \n     option_name = models.CharField(max_length=150)\n     event_name = models.CharField(max_length=150)  \n     event_code = models.CharField(max_length=70)\n     referal_code = models.CharField(max_length=70)\n\n5. Django admin can be used to insert the data in to Events table and also to insert initial data in to Options table with \n   count as 0\n\n6. Then the count should be updated when a use",
    "url": "https://github.com/Vijayanand-debug/pollstats",
    "last_updated": "2025-07-29T21:49:47+00:00"
  },
  {
    "full_name": "AntonOsika/gpt-engineer",
    "name": "gpt-engineer",
    "description": "CLI platform to experiment with codegen. Precursor to: https://lovable.dev",
    "language": "Python",
    "topics": [
      "ai",
      "coding-assistant",
      "gpt-4",
      "openai",
      "python",
      "autonomous-agent",
      "codebase-generation",
      "code-generation",
      "codegen",
      "gpt-engineer"
    ],
    "readme": "# gpt-engineer\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/gpt-engineer-org/gpt-engineer?style=social)](https://github.com/gpt-engineer-org/gpt-engineer)\n[![Discord Follow](https://dcbadge.vercel.app/api/server/8tcDQ89Ej2?style=flat)](https://discord.gg/8tcDQ89Ej2)\n[![License](https://img.shields.io/github/license/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/LICENSE)\n[![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/issues)\n![GitHub Release](https://img.shields.io/github/v/release/gpt-engineer-org/gpt-engineer)\n[![Twitter Follow](https://img.shields.io/twitter/follow/antonosika?style=social)](https://twitter.com/antonosika)\n\nThe OG code genereation experimentation platform!\n\nIf you are looking for the evolution that is an opinionated, managed service – check out gptengineer.app.\n\nIf you are looking for a well maintained hackable CLI for – check out aider.\n\n\ngpt-engineer lets you:\n- Specify software in natural language\n- Sit back and watch as an AI writes and executes the code\n- Ask the AI to implement improvements\n\n## Getting Started\n\n### Install gpt-engineer\n\nFor **stable** release:\n\n- `python -m pip install gpt-engineer`\n\nFor **development**:\n- `git clone https://github.com/gpt-engineer-org/gpt-engineer.git`\n- `cd gpt-engineer`\n- `poetry install`\n- `poetry shell` to activate the virtual environment\n\nWe actively support Python 3.10 - 3.12. The last version to support Python 3.8 - 3.9 was [0.2.6](https://pypi.org/project/gpt-engineer/0.2.6/).\n\n### Setup API key\n\nChoose **one** of:\n- Export env variable (you can add this to .bashrc so that you don't have to do it each time you start the terminal)\n    - `export OPENAI_API_KEY=[your api key]`\n- .env file:\n    - Create a copy of `.env.template` named `.env`\n    - Add your OPENAI_API_KEY in .env\n- Custom model:\n    - See [docs](https://gpt-engineer.readthedocs",
    "url": "https://github.com/AntonOsika/gpt-engineer",
    "last_updated": "2025-09-02T10:09:58+00:00"
  },
  {
    "full_name": "dsrahul30/Christofides",
    "name": "Christofides",
    "description": "Implementation of Christofides Algorithm in Python for TSP",
    "language": "Python",
    "topics": [],
    "readme": "========================\n Christofides Algorithm\n========================\n\nThis package(Christofides) provides a way to implement Christofides algorithm\nfor solving Travelling Saleman Problem(TSP) to obtain an approximate solution\non an undirected graph(Distance Matrix) provided as an upper Triangular matrix.\nThe Distance from a node on to itself is assumed 0.\n\nUsage\n======\n\nUse the compute() function which takes as input a distance_matrix and returns a Christofides solution as follows:\n\n\tfrom Christofides import christofides\n\tTSP = christofides.compute(distance_matrix)\n\t\nThe Distance Matrix is an upper Triangular matrix with distance from a node on to itself 0, since Christofides algorithm \ncould only be applied for undirected graphs. Also the distance between a node on to itself is practically 0.\nExample for distance_matrix is as follows,\ndistance_matrix = :\n\n\t[[0,45,65,15],\n\t [0,0,56,12],\n\t [0,0,0,89],\n\t [0,0,0,0]] \n\t \nThe above distance_matrix should be provided as an input to christofides.compute(), when we want to calculate TSP for\ndistance = :\n\t\n\t[[0,45,65,15],\n\t[45,0,56,12],\n\t[65,56,0,89],\n\t[15,12,89,0]]\n\t\nchristofides.compute(distance_matrix) returns a Dictionary with following Keys:\n\tChristofides_Solution,  \n\tTravel_Cost,\n\tMST, \n\tOdd_Vertices\n\tIndexes, \n\tMultigraph, \n\tEuler_Tour\n\t\t\n* Christofides_Solution: A list consisting of approximate tour for TSP.\n\tUse: TSP['Chistofides_Solution']\n* Travel_Cost: The cost of TSP tour generated.\n\tUse: TSP['Travel_Cost']\n* MST: The minimum spanning tree generated during the Christofides algorithm.\n\tUse: TSP['MST']\n* Odd_Vertices: A list of odd vertices of minimum spanning tree.\n\tUse: TSP['Odd_Vertices']\n* Indexes: List of edges from minimum cost perfect matching of odd vertices.\n\tUse: TSP['Indexes']\n* Multigraph: Edges of multigraph formed after Indexing.\n\tUse: TSP['Multigraph']\n* Euler_Tour: The Euler Tour of the Multigraph.\n\tUse: TSP['Euler_Tour']\n\t\t\nSupport Functions in christofides\n=================================\n\n",
    "url": "https://github.com/dsrahul30/Christofides",
    "last_updated": "2025-04-28T04:12:08+00:00"
  },
  {
    "full_name": "wireservice/agate",
    "name": "agate",
    "description": "A Python data analysis library that is optimized for humans instead of machines.",
    "language": "Python",
    "topics": [],
    "readme": ".. image:: https://github.com/wireservice/agate/workflows/CI/badge.svg\n    :target: https://github.com/wireservice/agate/actions\n    :alt: Build status\n\n.. image:: https://coveralls.io/repos/wireservice/agate/badge.svg?branch=master\n    :target: https://coveralls.io/r/wireservice/agate\n    :alt: Coverage status\n\n.. image:: https://img.shields.io/pypi/dm/agate.svg\n    :target: https://pypi.python.org/pypi/agate\n    :alt: PyPI downloads\n\n.. image:: https://img.shields.io/pypi/v/agate.svg\n    :target: https://pypi.python.org/pypi/agate\n    :alt: Version\n\n.. image:: https://img.shields.io/pypi/l/agate.svg\n    :target: https://pypi.python.org/pypi/agate\n    :alt: License\n\n.. image:: https://img.shields.io/pypi/pyversions/agate.svg\n    :target: https://pypi.python.org/pypi/agate\n    :alt: Support Python versions\n\nagate is a Python data analysis library that is optimized for humans instead of machines. It is an alternative to numpy and pandas that solves real-world problems with readable code.\n\nagate was previously known as journalism.\n\nImportant links:\n\n* Documentation:    https://agate.rtfd.org\n* Repository:       https://github.com/wireservice/agate\n* Issues:           https://github.com/wireservice/agate/issues\n",
    "url": "https://github.com/wireservice/agate",
    "last_updated": "2025-09-02T01:52:40+00:00"
  },
  {
    "full_name": "gojiplus/reporoulette",
    "name": "reporoulette",
    "description": "Sample Random GitHub Repositories",
    "language": "Python",
    "topics": [
      "github",
      "github-tools",
      "sampling"
    ],
    "readme": "## RepoRoulette 🎲: Randomly Sample Repositories from GitHub\n\n> Spin the wheel and see which GitHub repositories you get!\n\n[![PyPI version](https://img.shields.io/pypi/v/reporoulette.svg)](https://pypi.org/project/reporoulette/)\n[![License](https://img.shields.io/pypi/l/reporoulette.svg)](https://pypi.org/project/reporoulette/)\n[![Downloads](https://static.pepy.tech/badge/reporoulette)](https://pepy.tech/project/reporoulette)\n[![Python application](https://github.com/gojiplus/reporoulette/actions/workflows/python-app.yml/badge.svg)](https://github.com/gojiplus/reporoulette/actions/workflows/python-app.yml)\n\n## 🚀 Installation\n\n```bash\n# Using pip\npip install reporoulette\n\n# From source\ngit clone https://github.com/gojiplus/reporoulette.git\ncd reporoulette\npip install -e .\n```\n\n## 📖 Sampling Methods\n\nRepoRoulette provides three distinct methods for random GitHub repository sampling:\n\n### 1. 🎯 ID-Based Sampling\n\nUses GitHub's sequential repository ID system to generate truly random samples by probing random IDs from the valid ID range. The downside of using the method is that the hit rate can be low (as many IDs are invalid, partly because the repo. is private or abandoned, etc.) And any filtering on repo. characteristics must wait till you have the names.\n\nThe function will continue to sample till either `max_attempts` or till `n_samples`. You can pass the `seed` for reproducibility.\n\n```python\nfrom reporoulette import IDSampler\n\n# Initialize the sampler\nsampler = IDSampler(token=\"your_github_token\")\n\n# Get 50 random repositories\nrepos = sampler.sample(n_samples=50)\n\n# Print basic stats\nprint(f\"Success rate: {sampler.success_rate:.2f}%\")\nprint(f\"Samples collected: {len(repos)}\")\n```\n\n### 2. ⏱️ Temporal Sampling\n\nRandomly selects time points (date/hour combinations) within a specified range and then retrieves repositories updated during those periods. \n\n```python\nfrom reporoulette import TemporalSampler\nfrom datetime import datetime, timedelta\n\n# Define a date range (la",
    "url": "https://github.com/gojiplus/reporoulette",
    "last_updated": "2025-06-28T23:35:50+00:00"
  },
  {
    "full_name": "jdan/tota11y",
    "name": "tota11y",
    "description": "an accessibility (a11y) visualization toolkit",
    "language": "JavaScript",
    "topics": [],
    "readme": "# tota11y [![Build Status](https://travis-ci.org/Khan/tota11y.svg?branch=master)](https://travis-ci.org/Khan/tota11y)\n\nAn accessibility visualization toolkit\n\n<img src=\"http://khan.github.io/tota11y/img/tota11y-logo.png\" alt=\"tota11y logo\" width=\"200\">\n\n[Try tota11y in your browser](http://khan.github.io/tota11y/#Try-it), or\n[read why we built tota11y](http://engineering.khanacademy.org/posts/tota11y.htm).\n\n## Deprecation Notice\n\ntota11y was created at a time when tooling to assist in developing accessible solutions was sparse. Since then, not only have some great browser extensions and automated tooling come into existence, like [axe][1] and [axe-core][2] from [deque][3], but browsers have added specific development tooling to support accessibility. As such, we are no longer actively developing or maintaining tota11y.\n\nAfter careful consideration of the options available, we decided to archive the repository and deprecate the project. As such, we will no longer be accepting pull requests or issues.\n\nThank you for all the support this project has received over the years and for all those who work hard to ensure the web is accessible to all.\n\n## Special thanks\n\nMany of tota11y's features come straight from [Google Chrome's Accessibility Developer Tools](https://github.com/GoogleChrome/accessibility-developer-tools). We use this library heavily at [Khan Academy](http://khanacademy.org).\n\nThe awesome glasses in our logo were created by [Kyle Scott](https://thenounproject.com/Kyle/) and are licensed under [CC BY 3.0](http://creativecommons.org/licenses/by/3.0/us/legalcode).\n\n## License\n\n[MIT License](LICENSE.txt)\n\n[1]: https://www.deque.com/axe\n[2]: https://github.com/dequelabs/axe-core\n[3]: https://www.deque.com\n",
    "url": "https://github.com/jdan/tota11y",
    "last_updated": "2025-08-17T18:59:39+00:00"
  },
  {
    "full_name": "SerCeMan/fontogen",
    "name": "fontogen",
    "description": "Hey, Computer, Make Me a Font",
    "language": "Python",
    "topics": [
      "ai",
      "fonts",
      "ml"
    ],
    "readme": "# FontoGen\n\nGenerate your very own font with FontoGen. Read more about the project in\nmy [blog article](https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font).\n\n![screenshot](./img/fontogen.png)\n\n## Installation\n\n```bash\npipenv install\npipenv shell\n```\n\n### Training\n\nPlease only train the model on open source fonts.\n`./train_example.sh` contains an example of the training process.\n\n```bash\n# the input fonts\nls ./example/dataset/\n# prepare the dataset and start training\n./train_example.sh\n```\n\n### Inference\n\nThe model needs to be re-trained on a large dataset of OFL fonts. If anyone would like to contribute and re-train the model, please reach out and I'll be happy to help you set up the environment.\n",
    "url": "https://github.com/SerCeMan/fontogen",
    "last_updated": "2025-08-13T16:36:55+00:00"
  },
  {
    "full_name": "thejcannon/botocore-a-la-carte",
    "name": "botocore-a-la-carte",
    "description": "Re-package botocore with a-la-carte data",
    "language": "Python",
    "topics": [],
    "readme": "``botocore-a-la-carte``\n-----------------------\n\n``botocore-a-la-carte`` is a re-packaging of ``botocore`` such that each service's data JSON exists in a\nseparate package, and are opted-into through package extras.\n\nThe package ``botocore-a-la-carte`` contains the base code/resources for ``botocore`` to operate, and has\na package extra per service (referencing a package which contains just the JSON data for that service).\n\nInstallation and Usage\n----------------------\n\n⚠️ The package extras must be installed in the same directory as this package (such as in a virtual environment).\nThis package does not support being installed in separate locations pointed to by ``sys.path``.\nThis is a limitation of the core ``botocore`` package and the Python packaging ecosystem. ⚠️\n\nExample:\n\n``botocore-a-la-carte`` with no extras allows you to use core ``botocore`` functionality\n(e.g. load credentials, sign requests, etc...).\n\n.. code-block:: console\n\n    $ pip install botocore-a-la-carte\n    ...\n    $ python\n    >>> import botocore\n    ...\n\nIf you require specific service support, specify the service names as an extras:\n\n.. code-block:: console\n\n    $ pip install botocore-a-la-carte[s3, ec2]\n    ...\n    $ python\n    >>> import botocore.session\n    >>> session = botocore.session.get_session()\n    >>> client = session.create_client('ec2')\n    >>> print(client.describe_instances())\n\n\n",
    "url": "https://github.com/thejcannon/botocore-a-la-carte",
    "last_updated": "2025-01-15T15:30:20+00:00"
  },
  {
    "full_name": "democrats/data",
    "name": "data",
    "description": "Datasets that list various boolean, categorical, and date data for elections and elected officials.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# DNC Data\n\nThis repo contains various datasets that the DNC Tech team has compiled.\n\nIf you'd like to contribute to any of these data set, please create a\npull request. Specific data sets may have more specific instructions\nin their README's.\n\nBy downloading the data, you certify that you understand and agree that\nthe data provided by the DNC is provided as-is, without warranty of any\nkind as to the quality, accuracy or other characteristics of the data.\nYou further understand and agree that in providing the data the DNC\ndisclaims any implied warranty of merchantability or fitness for a\nparticular purpose as well as any warranty arising from course of\nperformance, course of dealing or usage of trade.\n\nThe data provided is subject to continual change and you are encouraged\nto independently verify that the data is up to date at the time of your\nusage.\n",
    "url": "https://github.com/democrats/data",
    "last_updated": "2024-12-02T21:53:33+00:00"
  },
  {
    "full_name": "briatte/ggnet",
    "name": "ggnet",
    "description": "Network visualization with ggplot2",
    "language": "R",
    "topics": [
      "r",
      "network-visualization",
      "ggplot2"
    ],
    "readme": "# ggnet: network visualization with ggplot2\n\n![](inst/demo.png)\n\nThis repository contains the latest versions of the `ggnet` and `ggnet2` functions, which allow to visualize networks as [`ggplot2`](http://ggplot2.org/) objects.\n\n## INSTALL\n\n`ggnet` and `ggnet2` are part of the [`GGally`](https://cran.r-project.org/web/packages/GGally/) package. Install it from CRAN:\n\n```{r}\ninstall.packages(\"GGally\")\n```\n\nYou can also install `ggnet` and `ggnet2` as a small standalone package:\n\n```{r}\ndevtools::install_github(\"briatte/ggnet\")\n```\n\nNote that you will need the latest version of [`ggplot2`](http://docs.ggplot2.org/current/) (2.0.0) for any of the functions to work properly.\n\n## VIGNETTE\n\nThe `ggnet2` function is fully documented in [this vignette](https://briatte.github.io/ggnet/).\n\nThe data for one of the examples, a Twitter network of French Members of Parliament, is [included in this repository](inst/extdata), as is the [vignette source](vignettes).\n\n## THANKS\n\n- [Moritz Marbach](https://github.com/sumtxt) coded the [very first version of `ggnet`](http://sumtxt.wordpress.com/2011/07/02/visualizing-networks-with-ggplot2-in-r/)\n- [Heike Hoffmann](https://github.com/heike) and [Ming-Yu Liu](https://github.com/ethen8181) provided code to improve the display of edge arrows\n- [Pedro Jordano](https://github.com/pedroj) suggested adding support for [bipartite networks](https://github.com/pedroj/bipartite_plots)\n- [Baptiste Coulmont](http://coulmont.com/index.php?s=d%C3%A9put%C3%A9s) and [Ewen Gallic](http://freakonometrics.blog.free.fr/index.php?post/Twitter-deputes) provided further inspiration\n- [Barret Schloerke](https://github.com/schloerke) helps by maintaining the `GGally` package\n",
    "url": "https://github.com/briatte/ggnet",
    "last_updated": "2025-08-25T16:26:45+00:00"
  },
  {
    "full_name": "r-lib/pkgdown",
    "name": "pkgdown",
    "description": "Generate static html documentation for an R package",
    "language": "R",
    "topics": [
      "r",
      "package",
      "documentation-tool"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# pkgdown <img src=\"man/figures/logo.png\" align=\"right\" alt=\"\" width=\"120\" />\n\n<!-- badges: start -->\n\n<a href=\"https://cran.r-project.org/package=pkgdown\"\nclass=\"pkgdown-release\"><img\nsrc=\"https://www.r-pkg.org/badges/version/pkgdown\"\nalt=\"CRAN Status\" /></a> <a\nhref=\"https://github.com/r-lib/pkgdown/actions/workflows/R-CMD-check.yaml\"\nclass=\"pkgdown-devel\"><img\nsrc=\"https://github.com/r-lib/pkgdown/actions/workflows/R-CMD-check.yaml/badge.svg\"\nalt=\"R-CMD-check\" /></a> [![Codecov test\ncoverage](https://codecov.io/gh/r-lib/pkgdown/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-lib/pkgdown?branch=main)\n<!-- badges: end -->\n\npkgdown is designed to make it quick and easy to build a website for\nyour package. You can see pkgdown in action at\n<https://pkgdown.r-lib.org>: this is the output of pkgdown applied to\nthe latest version of pkgdown. Learn more in `vignette(\"pkgdown\")` or\n`?build_site`.\n\n## Installation\n\n<div class=\".pkgdown-release\">\n\n``` r\n# Install released version from CRAN\ninstall.packages(\"pkgdown\")\n```\n\n</div>\n\n<div class=\".pkgdown-devel\">\n\n``` r\n# Install development version from GitHub\n# install.packages(\"pak\")\npak::pak(\"r-lib/pkgdown\")\n```\n\n</div>\n\n## Usage\n\nGet started with [usethis](https://usethis.r-lib.org/):\n\n``` r\n# Run once to configure your package to use and deploy pkgdown\nusethis::use_pkgdown_github_pages()\n```\n\n``` r\n# Preview your site locally before publishing\npkgdown::build_site()\n```\n\nThis adds the necessary components and sets up GitHub Actions[^1] for\nautomatic site building when deploying. Your `README.md` becomes the\nhomepage, documentation in `man/` generates a function reference, and\nvignettes will be rendered into `articles/`.\n\n### pkgdown 2.0.0 and Bootstrap 5\n\npkgdown 2.0.0 includes an upgrade from Bootstrap 3 to Bootstrap 5, which\nis accompanied by a whole bunch of minor UI improvements. If you’ve\nheavily customised your site, there’s a small chan",
    "url": "https://github.com/r-lib/pkgdown",
    "last_updated": "2025-08-27T23:18:18+00:00"
  },
  {
    "full_name": "mpadge/stellar",
    "name": "stellar",
    "description": "Search your github stars in R",
    "language": "R",
    "topics": [
      "github",
      "github-api",
      "github-stars",
      "r",
      "r-package",
      "rstats"
    ],
    "readme": "[![Build Status](https://travis-ci.org/ropenscilabs/stellar.svg)](https://travis-ci.org/ropenscilabs/stellar) [![Project Status: Concept.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active) [![Coverage status](https://codecov.io/gh/ropenscilabs/stellar/branch/master/graph/badge.svg)](https://codecov.io/github/ropenscilabs/stellar?branch=master)\n\n# stellar\n\nSearch your github stars in R\n\n![](https://user-images.githubusercontent.com/6697851/39176684-b219d7b4-47ad-11e8-9aec-b30e284631e5.png)\n\n## Installation\n\n`stellar` can be installed directly from GitHub via any of the following methods\n\n```r\n# install.packages(\"devtools\")\ndevtools::install_packages(\"ropenscilabs/stellar\")\n```\n\nor\n\n```r\n# install.packages(\"remotes\")\nremotes::install_packages(\"ropenscilabs/stellar\")\n```\n\n## How?\n\nThe only thing you need is a Personal Access Token from github. If you don't\nknow how:\n1. Go to your personal settings (under your profile pic, top right)\n2. On the left, under the main \"Personal Settings\" box, click \"Developer\n   Settings\" -> \"Personal Access Tokens\" and generate a new one. You'll need to\n   check the box for accessing repository data via the github API v4.\n3. Save this as an **R** environmental variable called `GITHUB_GRAPHQL_TOKEN`\n   with `Sys.setenv(\"GITHUB_GRAPHQL_TOKEN\" = <your token>)`. This can either be\n   done within a single session, or automatically for all sessions by pasting\n   this command within your `~/.Rprofile` file. (Simply create this file if it\n   doesn't already exist.)\n    \n\n### Code of Conduct\n\nPlease note that this project is released with a [Contributor Code of\nConduct](CONDUCT.md).  By participating in this project you agree to abide by\nits terms.\n\n",
    "url": "https://github.com/mpadge/stellar",
    "last_updated": "2025-03-22T11:08:03+00:00"
  },
  {
    "full_name": "hrbrmstr/mhn",
    "name": "mhn",
    "description": ":honey_pot: Analyze and Visualize Data from Modern Honey Network Servers with R",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "mhn",
      "r-cyber",
      "honeypot"
    ],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n![](honeypot.png)\n\nmhn is an R package to interface with the MHN API and (eventually) provide tools to analyze and visualize MHN data.\n\nThe API wrapper functions in this package all rely on a MHN server API key residing in the environment variable `MHN_API_KEY` or being passed in as a parameter. The former is useful in simple deplpoyments where there is only one MHN server. In such cases, the easiest way to accomplish this is to set it in the `.Renviron` file in your home directory.\n\nThis package pairs nicely with:\n\n-   [iptools](https://github.com/hrbrmstr/iptools)\n-   [urltools](https://github.com/Ironholds/urltools)\n-   [cymruservices](https://github.com/hrbrmstr/cymruservices)\n-   [shodan](https://github.com/hrbrmstr/shodan)\n-   [rgeolocate](https://github.com/Ironholds/rgeolocate)\n-   [whoisxmlapi](https://github.com/hrbrmstr/whoisxmlapi)\n-   [domaintools](https://github.com/hrbrmstr/domaintools)\n-   [whoapi](https://github.com/Ironholds/whoapi)\n\nThe following functions are implemented:\n\n-   `all_metadata`: Get all collected metadata\n-   `attacker_stats`: Get attacker (IP) statistics\n-   `feed_info`: Get data for a specific hpfeed\n-   `hp_feeds`: Getthe raw hpfeeds data collected over a specific channel.\n-   `intel_feed`: Get intel feed\n-   `ip_metadata`: Get IP metadata (mainly from 'p0f' honeypot data)\n-   `metadata_for`: Get metadata for a specific id\n-   `mhn_api_key`: Get or set `MHN_API_KEY` value\n-   `mhn_dork`: MHN dork *(I have no idea what this is tho)*\n-   `mhn_file`: MHN file *(I have no idea what this is tho)*\n-   `mhn_url`: MHN url *(I have no idea what this is tho)*\n-   `sensors`: Get sensors.\n-   `sessions`: Get normalized sessions/connection data\n-   `session_info`: Get data for a specific attack session\n-   `top_attackers`: Get top attackers\n\n### News\n\n-   Version 0.0.1.9000 : paarmeterized the MHN URL \n-   Version 0.0.0.9000 : initial release\n\n### Installation\n\n``` r\ndevto",
    "url": "https://github.com/hrbrmstr/mhn",
    "last_updated": "2025-03-22T10:58:59+00:00"
  },
  {
    "full_name": "py-econometrics/pyfixest",
    "name": "pyfixest",
    "description": "Fast High-Dimensional Fixed Effects Regression in Python following fixest-syntax",
    "language": "Python",
    "topics": [],
    "readme": "![](figures/pyfixest-logo.png)\n\n# PyFixest: Fast High-Dimensional Fixed Effects Regression in Python\n\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/license/mit)\n![Python Versions](https://img.shields.io/badge/Python-3.9–3.12-blue)\n[![PyPI -Version](https://img.shields.io/pypi/v/pyfixest.svg)](https://pypi.org/project/pyfixest/)\n[![Project Chat][chat-badge]][chat-url]\n[![image](https://codecov.io/gh/py-econometrics/pyfixest/branch/master/graph/badge.svg)](https://codecov.io/gh/py-econometrics/pyfixest)\n[![Known Bugs](https://img.shields.io/github/issues/py-econometrics/pyfixest/bug?color=red&label=Bugs)](https://github.com/py-econometrics/pyfixest/issues?q=is%3Aissue+is%3Aopen+label%3Abug)\n[![File an Issue](https://img.shields.io/github/issues/py-econometrics/pyfixest)](https://github.com/py-econometrics/pyfixest/issues)\n[![Downloads](https://static.pepy.tech/badge/pyfixest)](https://pepy.tech/project/pyfixest)\n[![Downloads](https://static.pepy.tech/badge/pyfixest/month)](https://pepy.tech/project/pyfixest)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Pixi Badge][pixi-badge]][pixi-url]\n[![Donate | GiveDirectly](https://img.shields.io/static/v1?label=GiveDirectly&message=Donate&color=blue&style=flat-square)](https://github.com/py-econometrics/pyfixest?tab=readme-ov-file#support-pyfixest)\n[![PyPI](https://img.shields.io/pypi/v/pyfixest)](https://pypi.org/project/pyfixest)\n[![Citation](https://img.shields.io/badge/Cite%20as-PyFixest-blue)](https://github.com/py-econometrics/pyfixest?tab=readme-ov-file#how-to-cite)\n[![Documentation](https://img.shields.io/badge/Open-Documentation-orange)](https://py-econometrics.github.io/pyfixest/pyfixest.html)\n[![Function Reference](https://img.shields.io/badge/Open-Function%20Reference-yellow)](https://py-econometrics.github.io/pyfixest/reference/)\n\n[pixi-badge]:https://img.shield",
    "url": "https://github.com/py-econometrics/pyfixest",
    "last_updated": "2025-09-01T21:59:38+00:00"
  },
  {
    "full_name": "jekyll/jemoji",
    "name": "jemoji",
    "description": "GitHub-flavored emoji plugin for Jekyll",
    "language": "Ruby",
    "topics": [],
    "readme": "# Jemoji\n\nGitHub-flavored Emoji plugin for Jekyll\n\n[![Gem Version](https://badge.fury.io/rb/jemoji.svg)](http://badge.fury.io/rb/jemoji)\n[![Build Status](https://travis-ci.org/jekyll/jemoji.svg?branch=master)](https://travis-ci.org/jekyll/jemoji)\n\n## Usage\n\nAdd the following to your site's `Gemfile`\n\n```\ngem 'jemoji'\n```\n\nAnd add the following to your site's `_config.yml`\n\n```yml\nplugins:\n  - jemoji\n```\n\n💡 If you are using a Jekyll version less than 3.5.0, use the `gems` key instead of `plugins`.\n\nIn any page or post, use emoji as you would normally, e.g.\n\n```markdown\nI give this plugin two :+1:!\n```\n\n## Emoji images\n\nFor GitHub Pages sites built on GitHub.com, emoji images are served from the GitHub.com CDN, with a base URL of `https://github.githubassets.com`, which results in emoji image URLs like `https://github.githubassets.com/images/icons/emoji/unicode/1f604.png`.\n\nOn GitHub Enterprise installs, page builds receive the `ASSET_HOST_URL` environment variable, which contain a value like `https://assets.ghe.my-company.com`. This results in emoji images for GitHub Pages sites built on a GitHub Enterprise install being served at URLs like `https://assets.ghe.my-company.com/images/icons/emoji/unicode/1f604.png`.\n\n## Customizing\n\nIf you'd like to serve emoji images locally, or use a custom emoji source, you can specify so in your `_config.yml` file:\n\n```yml\nemoji:\n  src: \"/assets/images/emoji\"\n```\n\nSee the [Gemoji](https://github.com/github/gemoji) documentation for generating image files.\n",
    "url": "https://github.com/jekyll/jemoji",
    "last_updated": "2025-08-03T12:22:26+00:00"
  },
  {
    "full_name": "stoltzmaniac/TV-Scripts-Seinfeld",
    "name": "TV-Scripts-Seinfeld",
    "description": "Quick look at Seinfeld Scripts",
    "language": "HTML",
    "topics": [],
    "readme": "# TV-Scripts-Seinfeld\nQuick look at Seinfeld Scripts\n",
    "url": "https://github.com/stoltzmaniac/TV-Scripts-Seinfeld",
    "last_updated": "2020-06-06T04:39:57+00:00"
  },
  {
    "full_name": "robertzk/s3mpi",
    "name": "s3mpi",
    "description": "R message passing interface using S3 storage",
    "language": "R",
    "topics": [],
    "readme": "R and AWS S3 [![Build Status](https://travis-ci.org/robertzk/s3mpi.svg?branch=master)](https://travis-ci.org/robertzk/s3mpi) [![Coverage Status](https://coveralls.io/repos/robertzk/s3mpi/badge.png)](https://coveralls.io/r/robertzk/s3mpi) [![Documentation](https://img.shields.io/badge/rocco--docs-%E2%9C%93-blue.svg)](http://robertzk.github.io/s3mpi/)\n=========\n\nA common problem for data scientists is passing data or models to each\nother without interrupting their workflow. There are typically two approaches:\n\n  1. Writing CSV and RDS files and passing them around using tools like\n     email, Dropbox, or SFTP. Typically, these files are too large for\n     inclusion in version control.\n\n  2. Building an API infrastructure around some data backends, such as\n     databases, data warehouses, and streaming providers like Kafka.\n\nThe former works well for small teams consisting of 1-3 people but soon\nbecomes prohibitive. Additionally, tracking the array of files and outputs\nsoon becomes cumbersome and interrupts the data scientist's workflow.\n\nThe second option is an inevitable progression for any sufficiently large data\nteam, but requires major coordination with software or data engineers\nand may not be practical for small teams or experimental projects. It is\nalso usually limited by well-defined specification of the formats that\nare being passed into consoles and outputted to data storage systems.\n\nOn the other hand, S3mpi (S3 [*message passing interface*](https://en.wikipedia.org/wiki/Message_Passing_Interface),\naffectionately named after the distributed message passing library) \nallows for **storage and serialization of arbitrary R objects** and does\nnot have the limits of the second approach, while providing **on-demand\naccess to stored data and objects**, avoiding the need for large amounts of\ndisk space locally.\n\nHere, S3 stands for [Amazon's cloud storage](https://aws.amazon.com/s3/) which\nyou can think of as an infinite hard drive. You write an object to a path,\nan",
    "url": "https://github.com/robertzk/s3mpi",
    "last_updated": "2024-11-09T04:11:13+00:00"
  },
  {
    "full_name": "dselivanov/mongo-spark",
    "name": "mongo-spark",
    "description": "Example application on how to use mongo-hadoop connector with Spark",
    "language": "Scala",
    "topics": [],
    "readme": "mongo-spark\n===========\n\nExample application on how to use [mongo-hadoop][1] connector with [Apache Spark][2].\n\n[1]: https://github.com/mongodb/mongo-hadoop\n[2]: https://spark.incubator.apache.org/\n\n\nPrerequisites\n-------------\n\n* MongoDB installed and running on local or remote machine\n* Scala 2.10 and SBT installed\n* [Sbt assemly plugin](https://github.com/sbt/sbt-assembly) installed\n* [Apache spark 1.4](http://spark.apache.org/docs/1.0.0/index.html) installed. (Also should work with 1.0 + version, just adjusb build.sbt )\n\nHow to\n-------\n\nFirst of all thanks to [original repository](https://github.com/plaa/mongo-spark) and [blog post](http://codeforhire.com/2014/02/18/using-spark-with-mongodb/).\nThis example provides instructions on how to run you application on [stadalone spark cluster running on ec2](http://spark.apache.org/docs/1.0.0/ec2-scripts.html).  \n1. Because by default ec2 scipt installs standalone spark cluster with **hadoop version 2.4** you should build you mongo-hadoop connector against this version of hadoop (here the prebuilded version is located at *lib/*). Please check [mongo-hadoop-conector repository](https://github.com/mongodb/mongo-hadoop) to learn how to build mongo-hadoop connector against your version of hadoop.  \n2. The strightforward way to run your application is to make \"fat\" jar using assembly sbt plugin:  \n    **`sbt assembly`**  \nand pass it to [submit script](http://spark.apache.org/docs/latest/submitting-applications.html). Please review the **[build.sbt](https://github.com/dselivanov/mongo-spark/blob/master/build.sbt)** file and check **mergeStrategy** and **libraryDependencies**.  \n3. **Copy** your \"fat jar\" ( mongo-spark-assembly-1.0.jar ) to **spark master**:  \n `scp mongo-spark-assembly-1.0.jar root@hostSparkMaster:~/jobs/`\n\nRead and write from mongodb / bson / serialized files\n-----------------------------------------------------\nBased on my experience most of the time you have to read or write whole collection. And preferab",
    "url": "https://github.com/dselivanov/mongo-spark",
    "last_updated": "2018-05-14T01:53:49+00:00"
  },
  {
    "full_name": "google/python-fire",
    "name": "python-fire",
    "description": "Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python object.",
    "language": "Python",
    "topics": [
      "python",
      "cli"
    ],
    "readme": "# Python Fire [![PyPI](https://img.shields.io/pypi/pyversions/fire.svg?style=plastic)](https://github.com/google/python-fire)\n\n_Python Fire is a library for automatically generating command line interfaces\n(CLIs) from absolutely any Python object._\n\n-   Python Fire is a simple way to create a CLI in Python.\n    [[1]](docs/benefits.md#simple-cli)\n-   Python Fire is a helpful tool for developing and debugging Python code.\n    [[2]](docs/benefits.md#debugging)\n-   Python Fire helps with exploring existing code or turning other people's\n    code into a CLI. [[3]](docs/benefits.md#exploring)\n-   Python Fire makes transitioning between Bash and Python easier.\n    [[4]](docs/benefits.md#bash)\n-   Python Fire makes using a Python REPL easier by setting up the REPL with the\n    modules and variables you'll need already imported and created.\n    [[5]](docs/benefits.md#repl)\n\n## Installation\n\nTo install Python Fire with pip, run: `pip install fire`\n\nTo install Python Fire with conda, run: `conda install fire -c conda-forge`\n\nTo install Python Fire from source, first clone the repository and then run:\n`python setup.py install`\n\n## Basic Usage\n\nYou can call `Fire` on any Python object:<br>\nfunctions, classes, modules, objects, dictionaries, lists, tuples, etc.\nThey all work!\n\nHere's an example of calling Fire on a function.\n\n```python\nimport fire\n\ndef hello(name=\"World\"):\n  return \"Hello %s!\" % name\n\nif __name__ == '__main__':\n  fire.Fire(hello)\n```\n\nThen, from the command line, you can run:\n\n```bash\npython hello.py  # Hello World!\npython hello.py --name=David  # Hello David!\npython hello.py --help  # Shows usage information.\n```\n\nHere's an example of calling Fire on a class.\n\n```python\nimport fire\n\nclass Calculator(object):\n  \"\"\"A simple calculator class.\"\"\"\n\n  def double(self, number):\n    return 2 * number\n\nif __name__ == '__main__':\n  fire.Fire(Calculator)\n```\n\nThen, from the command line, you can run:\n\n```bash\npython calculator.py double 10  # 20\npython calculator.py double",
    "url": "https://github.com/google/python-fire",
    "last_updated": "2025-09-01T08:08:48+00:00"
  },
  {
    "full_name": "soodoku/pareto_partisan",
    "name": "pareto_partisan",
    "description": "Pareto Partisan: Are Partisans Willing to Bite Their Purse To Spite The Main Opposing Party?",
    "language": "R",
    "topics": [],
    "readme": "## Pareto Partisan: Are Partisans Willing to Bite Their Co-Partisans to Spite the Opposing Partisans?\n\nOne of the deepest concerns about group-based affect is that partisans will make policy choices motivated by spite. We fear that partisans will choose an option that is strictly worse for everyone as long as opposing party supporters get less than co-partisans. We test this on the CCES. When offered the choice between picking a highway plan that allocates $11B to co-partisan states and $12B to opposing partisan states and a plan that offers $10B to co-partisans and $9B to opposing partisans, we find that just a third of Democrats and a fourth of Republicans choose the more generous plan. The competing explanation for the results is that people do not like the government disbursing money for highways and hence opt for an option that allocates a smaller amount.\n\n### Survey Question\n\nWe showed Democrats the first image and Republicans the latter and asked them: \"Which plan do you support? Smith Plan or Williams Plan?\" We randomized the order of responses.\n\n<img src = \"data/highway_plan/Blue.png\" width = 50%>\n\n<img src = \"data/highway_plan/Red.png\" width = 50%>\n\n### Future\n\nJohn Bowen, a candidate seeking your party's nomination for Congress in another state has made a new jobs plan the centerpiece of his campaign.  His plan has received a lot of media attention. The bipartisan Congressional Budget Office has evaluated his plan and concluded it will help create jobs, but that the benefits will not be the same for all Americans. Specifically, the report predicts that the average **black (white)** family making \\\\$50,000 will see a 7 percent increase in income and the average **white (black)**family making $50,000 will see a 5 percent increase in income.\n\nAnd then c.f. with where the gains are lower but reversed.\n\nBased on this information, how likely would you be to vote for John in this primary? - Extremely Likely,\nVery Likely, Somewhat Likely, Somewhat Unlikely, Very ",
    "url": "https://github.com/soodoku/pareto_partisan",
    "last_updated": "2023-07-09T00:51:13+00:00"
  },
  {
    "full_name": "CenterForOpenScience/rpp",
    "name": "rpp",
    "description": "Repository for working on the Reproducibility project",
    "language": "PostScript",
    "topics": [],
    "readme": "# Reproducibility Project: Psychology\nRepository for working on the Reproducibility Project: Psychology. These files represent the analyses conducted in Tilburg and\nreported in the Science publication, as well as the code used to generate Figures 1-3. The Github repository also operates as the issue tracker for the analysis committee.\n\nThe data are contained in this repository and were retrieved from rpp_data.csv. The analysis script includes a function to download the .csv file directly from the Open Science Framework project page. If you would like to view the .csv file separately, you can visit it here: https://osf.io/fgjvw/. Differences might arise due to changes made subsequently to the data. The data used to generate the Figures 1-3 is downloaded from the OSF.  \n\n## Running the analyses for the RP:P project\nThere are three ways of getting the files required to reproduce all analyses in the RPP manuscript:\n\n1. Download the `masterscript.R` file, and run this in R (downloads all dependencies from the OSF)\n2. Download the zip file `rpp_reproduce.zip` and extract the folder (this is for the non-git users). You can use [this](https://github.com/centerforopenscience/rpp/raw/master/rpp_reproduce.zip) link to do that.\n3. Clone this git repository and run the `masterscript.R` (this is for the git-users. The command to do this would be `git clone https://github.com/centerforopenscience/rpp FOLDERNAME`, where FOLDERNAME is the name of the folder these files will be contained (note your working directory to know where this folder will be placed)\n\nOnce the files are downloaded, running the analyses has been made user-friendly (please make sure you have the R statistical package installed, downloadable [here](https://cran.r-project.org/)).\n\n1. Open the `masterscript.R` file in R.\n2. Run all\n3. Select the directory where you downloaded the files (i.e., the folder where `masterscript.R`, `functions.R`, `RPP_figures.R`, and `rpp_data.csv` are located)\n4. Now you can run all th",
    "url": "https://github.com/CenterForOpenScience/rpp",
    "last_updated": "2025-08-24T18:46:10+00:00"
  },
  {
    "full_name": "vpnagraj/rrefine",
    "name": "rrefine",
    "description": "R package for OpenRefine API",
    "language": "R",
    "topics": [],
    "readme": "# rrefine\n\n[![CRAN Status](http://www.r-pkg.org/badges/version/rrefine)](https://cran.r-project.org/package=rrefine)\n![](http://cranlogs.r-pkg.org/badges/rrefine)\n![](https://cranlogs.r-pkg.org/badges/grand-total/rrefine)\n\n## Introduction\n\n[**OpenRefine**](https://openrefine.org/) (formerly **Google Refine**) is a popular, open source data cleaning software. **rrefine** enables users to programmatically trigger data transfer between R and **OpenRefine**. Using the functions available in this package, you can import, export, apply data cleaning operations, or delete a project in **OpenRefine** directly from R. There are [several client libraries for automating **OpenRefine** tasks via Python, nodeJS and Ruby](https://docs.openrefine.org/technical-reference/openrefine-api#third-party-software-libraries). **rrefine** extends this functionality to R users.\n\n## Installation\n\nThe development version of **rrefine** is available on [GitHub](https://github.com/vpnagraj/rrefine) and can be installed via **devtools**:\n\n```\n# install.packages(\"devtools\")\ndevtools::install_github(\"vpnagraj/rrefine\")\nlibrary(rrefine)\n```\n\n**rrefine** is also available on [CRAN](https://cran.r-project.org/package=rrefine):\n\n```\ninstall.packages(\"rrefine\")\nlibrary(rrefine)\n```\n## Functions\n\nThe package includes the following functionality to interface with **OpenRefine** projects:\n\n- `refine_upload()`: Upload data to a project\n- `refine_export()`: Export data from a project\n- `refine_delete()`: Delete a project\n- `refine_metadata()`: Retrieve metadata from all projects\n- `refine_project_summary()`: Get project summary data\n- `refine_operations()`: Apply arbitrary operations to a project\n- `refine_remove_column()`: Remove a column from a project\n- `refine_add_column()`: Add a column to a project\n- `refine_rename_column()`: Rename an existing column in a project\n- `refine_move_column()`: Move a column to a new index\n- `refine_transform()`: Apply arbitrary text transformations\n- `refine_to_lower()`: C",
    "url": "https://github.com/vpnagraj/rrefine",
    "last_updated": "2024-02-20T17:39:19+00:00"
  },
  {
    "full_name": "lukestein/steincoresummary",
    "name": "steincoresummary",
    "description": "Luke Stein's summary notes from the Stanford graduate economics core",
    "language": "TeX",
    "topics": [],
    "readme": "# Stein Core Summary\n\nThis respository contains Luke Stein's summary notes from the Stanford graduate economics core as it was taught in 2006-7.\n\nAbbreviated sources are listed in parentheses after each topic, with fuller descriptions in the \"References\" section. Most of the content is copied verbatim (or only minimally rewritten) from the indicated sources; errors in the source materials are now in the good company of numerous additional errors I have presumably introduced in writing/compiling these notes.\n\nFormatting is largely through LaTeX's `extarticle` class, with some additions from the included `intcheetsheet.sty` package (and its dependencies).\n\n\n# Use, licensing, copyrights, etc.\n\nI (Luke Stein) am not sure exactly what legal rights (including copyrights) I have in this work, but I reserve those rights.\n\nPlease do not distribute or otherwise make available any derivative works without my permission, with the following exception:\nyou are welcome to use the `intcheetsheet.sty` package to emulate the style of my notes with your own summaries of other material.\nIf you do produce your own notes in the style of mine, I would appreciate (though don't require) that they briefly acknowledge me by name and with a link to my website ([lukestein.com](http://lukestein.com)).\nPlease also let me know; I'd love to see what you've done!\n",
    "url": "https://github.com/lukestein/steincoresummary",
    "last_updated": "2025-08-10T15:34:52+00:00"
  },
  {
    "full_name": "samuelcrane/AMNH-courseware-analysis",
    "name": "AMNH-courseware-analysis",
    "description": "Scripts for data manipulation and visualization of raw course data from the Coursera platform. ",
    "language": "Python",
    "topics": [],
    "readme": "coursera-class-analysis\n=======================\n\nI worked as data manager for a sequence of classes taught by the American Museum of Natural History on the Coursera platform in 2013. The data analysis tools provided by Coursera were rather crude, so I had to make my own. These scripts were used to gather, process, and report on the students' usage of the courseware and progess with the lessons. \n\nThis is old stuff now and likely needs updating to use anything from Coursera now and in the future. \n\nScripts for data manipulation and visualization of raw course data from the Coursera platform. \n",
    "url": "https://github.com/samuelcrane/AMNH-courseware-analysis",
    "last_updated": "2019-08-13T15:27:07+00:00"
  },
  {
    "full_name": "rapidsai/cuml",
    "name": "cuml",
    "description": "cuML - RAPIDS Machine Learning Library",
    "language": "C++",
    "topics": [
      "machine-learning-algorithms",
      "machine-learning",
      "cuda",
      "gpu",
      "nvidia"
    ],
    "readme": "# <div align=\"left\"><img src=\"img/rapids_logo.png\" width=\"90px\"/>&nbsp;cuML - GPU Machine Learning Algorithms</div>\n\ncuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other [RAPIDS](https://rapids.ai/) projects.\n\ncuML enables data scientists, researchers, and software engineers to run\ntraditional tabular ML tasks on GPUs without going into the details of CUDA\nprogramming. In most cases, cuML's Python API matches the API from\n[scikit-learn](https://scikit-learn.org).\n\nFor large datasets, these GPU-based implementations can complete 10-50x faster\nthan their CPU equivalents. For details on performance, see the [cuML Benchmarks\nNotebook](https://github.com/rapidsai/cuml/tree/branch-25.10/notebooks/tools).\n\nAs an example, the following Python snippet loads input and computes DBSCAN clusters, all on GPU, using cuDF:\n```python\nimport cudf\nfrom cuml.cluster import DBSCAN\n\n# Create and populate a GPU DataFrame\ngdf_float = cudf.DataFrame()\ngdf_float['0'] = [1.0, 2.0, 5.0]\ngdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\n\n# Setup and fit clusters\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\n\nprint(dbscan_float.labels_)\n```\n\nOutput:\n```\n0    0\n1    1\n2    2\ndtype: int32\n```\n\ncuML also features multi-GPU and multi-node-multi-GPU operation, using [Dask](https://www.dask.org), for a\ngrowing list of algorithms. The following Python snippet reads input from a CSV file and performs\na NearestNeighbors query across a cluster of Dask workers, using multiple GPUs on a single node:\n\n\nInitialize a `LocalCUDACluster` configured with [UCXX](https://github.com/rapidsai/ucxx) for fast transport of CUDA arrays\n```python\n# Initialize UCX for high-speed transport of CUDA arrays\nfrom dask_cuda import LocalCUDACluster\n\n# Create a Dask single-node CUDA cluster w/ one worker per device\ncluster = LocalCUDACluster(protocol=\"ucx\",\n                           enable_tcp",
    "url": "https://github.com/rapidsai/cuml",
    "last_updated": "2025-09-02T09:11:34+00:00"
  },
  {
    "full_name": "djsutherland/sublime-stan",
    "name": "sublime-stan",
    "description": "Syntax definition for the Stan modeling language in Sublime Text",
    "language": "",
    "topics": [],
    "readme": "Stan Syntax Package for Sublime Text\n====================================\n\nThis is a simple package defining the basic syntax for the\n[Stan](http://mc-stan.org) modeling language in Sublime and Textmate.\n\nHomepage: http://github.com/dougalsutherland/sublime-stan\n\nTo install in Sublime:\n * The easy way: install the Stan package from\n    [Package Control](http://wbond.net/sublime_packages/package_control).\n * Manually:\n    Go to your packages directory (Preferences -> Browse Packages),\n    create a folder named Stan, and copy Stan.tmLanguage into it.\n    You could also directly check out the git repository if you prefer.\n\n\nLicense\n=======\n\nThis package is released under the 3-clause BSD license:\n\nCopyright (c) 2013 Dougal J. Sutherland <dougal@gmail.com>\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n   * Redistributions of source code must retain the above copyright notice,\n     this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above copyright notice,\n     this list of conditions and the following disclaimer in the documentation\n     and/or other materials provided with the distribution.\n   * Neither the name of Carnegie Mellon University nor the names of its\n     contributors may be used to endorse or promote products derived from this\n     software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVE",
    "url": "https://github.com/djsutherland/sublime-stan",
    "last_updated": "2024-09-11T12:37:45+00:00"
  },
  {
    "full_name": "gvegayon/twitterreport",
    "name": "twitterreport",
    "description": "Out-of-the-box analysis and reporting tools for twitter",
    "language": "R",
    "topics": [
      "tweets",
      "twitter",
      "sentiment-analysis",
      "d3js",
      "leaflet",
      "wordcloud",
      "jaccard"
    ],
    "readme": "twitterreport\n=============\n\n[![Build Status](https://travis-ci.org/gvegayon/twitterreport.svg?branch=master)](https://travis-ci.org/gvegayon/twitterreport) [![Build status](https://ci.appveyor.com/api/projects/status/a7ki7jlc5qht4dmn?svg=true)](https://ci.appveyor.com/project/gvegayon/twitterreport) [![DOI](https://zenodo.org/badge/19832/gvegayon/twitterreport.svg)](https://zenodo.org/badge/latestdoi/19832/gvegayon/twitterreport)\n\nOut-of-the-box analysis and reporting tools for twitter\n\nAbout\n-----\n\nWhile there are some (very neat) R packages focused on twitter (namely `twitteR` and `stramR`), `twitterreport` is centered on providing analysis and reporting tools for twitter data. The package's current version features:\n\n-   Access to twitter API\n-   Extracting mentions/hashtags/urls from text (tweets)\n-   Gender tagging by matching user names with gender datasets included in the package (**es** and **en**)\n-   Creating (mentions) networks and visualizing them using D3js\n-   Sentiment analysis (basic, but useful) using lexicons included in the package (again, **es** and **en**)\n-   Creating time series charts of hashtags/users/etc. and visualizing them using D3js\n-   Create wordclouds (after removing stop words and processing the text)\n-   Map visualization using the leaflet package\n-   Topics identification through the Jaccard coeff (words similarity)\n\nYou can take a look at a live example at <http://www.its.caltech.edu/~gvegayon/twitter/report_example.html>, and at the source code of that example at <https://github.com/gvegayon/twitterreport/blob/master/vignettes/report_example.Rmd>\n\nSome of the functions here were firstly developed in the project *nodoschile.cl* (no longer running). You can visit the project's testimonial website <http://nodos.modularity.cl> and the website (part of nodoschile) that motivated `twitterreports` at <http://modularity.cl/presidenciales>.\n\nInstallation\n------------\n\nWhile the package is still in development, you can always use `devtoo",
    "url": "https://github.com/gvegayon/twitterreport",
    "last_updated": "2024-10-07T12:55:38+00:00"
  },
  {
    "full_name": "EleutherAI/gpt-neo",
    "name": "gpt-neo",
    "description": "An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.",
    "language": "Python",
    "topics": [
      "language-model",
      "transformers",
      "gpt",
      "gpt-2",
      "gpt-3"
    ],
    "readme": "# GPT Neo\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5297715.svg)](https://doi.org/10.5281/zenodo.5297715) [![arXiv](https://img.shields.io/badge/arXiv-2101.00027-f9f107.svg)](https://arxiv.org/abs/2101.00027)\n\n**As of August, 2021 code is no longer maintained. It is preserved here in archival form for people who wish to continue to use it.*\n\n🎉 1T or bust my dudes 🎉\n\nAn implementation of model & data parallel [GPT3](https://arxiv.org/abs/2005.14165)-like models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.\n\n**If you're just here to play with our pre-trained models, we strongly recommend you try out the [HuggingFace Transformer integration](https://huggingface.co/EleutherAI).**\n\nTraining and inference is officially supported on TPU and should work on GPU as well. This repository will be (mostly) archived as we move focus to our GPU-specific repo, [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/).\n\nIn addition to the functionality offered by GPT-3, we also offer the following:\n* [Local attention](https://arxiv.org/abs/2004.05150)\n* [Linear attention](https://arxiv.org/abs/1812.01243)\n* [Mixture of Experts](https://arxiv.org/abs/1701.06538)\n* [Axial Positional embedding](https://arxiv.org/abs/1912.12180)\n\nNB, while neo can *technically* run a training step at 200B+ parameters, it is very inefficient at those scales. This, as well as the fact that many GPUs became available to us, among other things, prompted us to move development over to [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/).\n\n# Pretrained Models\n\n**Update 21/03/2021:**\n\nWe're proud to release two pretrained GPT-Neo models trained on The Pile, the weights and configs can be freely downloaded from [the-eye.eu](https://the-eye.eu/public/AI/gptneo-release/).\n\n1.3B: https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_XL/\n\n2.7B: https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/\n\nFor more information on how to get these set up, see the colab notebook, or r",
    "url": "https://github.com/EleutherAI/gpt-neo",
    "last_updated": "2025-09-01T09:31:48+00:00"
  },
  {
    "full_name": "BuzzFeedNews/trumpworld",
    "name": "trumpworld",
    "description": "TrumpWorld data as CSV and GraphML files",
    "language": "",
    "topics": [],
    "readme": "# TrumpWorld Data\n\nThis repository contains the data behind TrumpWorld, as described in [this BuzzFeed News article](https://www.buzzfeed.com/johntemplon/help-us-map-trumpworld). In addition to the tabular files already represented in the TrumpWorld spreadsheet, this repository also includes a [GraphML](http://graphml.graphdrawing.org/) file of the data, for ease of importing into network analysis and visualization software.\n\nIf you have suggestions for expanding or improving the dataset, please email __trump@buzzfeed.com__. If you’d like to send your tip securely and anonymously, [see these instructions](https://tips.buzzfeed.com/). (Please do not submit GitHub pull requests.)\n\n---\n\n__Update, April 24, 2017:__ We’ve simplified TrumpWorld’s tabular data. Previously, the TrumpWorld data was distributed across three CSV files: person-person-connections.csv, person-org-connections.csv, and org-org-connections.csv. Now all of that data has been consolidated into a single CSV: [trumpworld.csv](data/trumpworld.csv). No information has been removed from TrumpWorld; it has just been streamlined. In addition, the GraphML representation of TrumpWorld now include each node’s type (i.e., “person” / “organization”).\n\n---\n\nLooking for more from BuzzFeed News? [Click here for a list of our open-sourced projects, data, and code](https://github.com/BuzzFeedNews/everything).\n",
    "url": "https://github.com/BuzzFeedNews/trumpworld",
    "last_updated": "2025-03-19T08:02:13+00:00"
  },
  {
    "full_name": "Kanaries/graphic-walker",
    "name": "graphic-walker",
    "description": "An open source alternative to Tableau. Embeddable visual analytic",
    "language": "TypeScript",
    "topics": [
      "tableau",
      "vega-lite",
      "visualization",
      "bi",
      "data-visualization",
      "pivot-table",
      "react",
      "data",
      "data-analysis",
      "data-mining",
      "eda",
      "low-code",
      "typescript",
      "vega",
      "k6s",
      "kanaries",
      "tableau-alternative"
    ],
    "readme": "<img src=\"https://ch-resources.oss-cn-shanghai.aliyuncs.com/images/lang-icons/icon128px.png\" width=\"22px\" /> English | [简体中文](./README.zh-CN.md) | [日本語](./README.ja-JP.md)\n\n![graphic-walker-banner](https://pub-8e7aa5bf51e049199c78b4bc744533f8.r2.dev/graphic-walker-banner202402.png)\n\n\n# Graphic Walker\n![](https://img.shields.io/github/license/Kanaries/graphic-walker?style=flat-square)\n![](https://img.shields.io/npm/v/@kanaries/graphic-walker?style=flat-square)\n![](https://img.shields.io/github/actions/workflow/status/kanaries/graphic-walker/auto-build.yml?style=flat-square)\n[![](https://img.shields.io/badge/twitter-kanaries_data-03A9F4?style=flat-square&logo=twitter)](https://twitter.com/kanaries_data)\n[![](https://img.shields.io/discord/987366424634884096?color=%237289da&label=Discord&logo=discord&logoColor=white&style=flat-square)](https://discord.gg/WWHraZ8SeV)\n[![](https://img.shields.io/badge/YouTube-red?style=flat-square&logo=youtube&logoColor=white)](https://www.youtube.com/@kanaries_data)\n[![](https://img.shields.io/badge/LinkedIn-blue?style=flat-square&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/kanaries-data/)\n\nGraphic Walker is a different open-source alternative to Tableau. It allows data scientists to analyze data and visualize patterns with simple drag-and-drop / natural language query operations. \n\n### Why is it different?\n\nIt is extremely easy to embed in your apps just as a React component 🎉! The original purpose of graphic-walker is not to be a heavy BI platform, but a easy to embed, lite, plugin.\n\n### Main features\n+ A user friendly drag and drop based interaction for exploratory data analysis with visualizations.\n+ A Data Explainer which explains why some patterns occur / what may cause them (like salesforce einstein).\n+ Using web workers to handle computational tasks which allow you to use it as a pure front-end app.\n+ A general query interface for submit data queries to your own computation service. You can have a look at ho",
    "url": "https://github.com/Kanaries/graphic-walker",
    "last_updated": "2025-08-29T23:48:08+00:00"
  },
  {
    "full_name": "nishitpatel01/Fake_News_Detection",
    "name": "Fake_News_Detection",
    "description": "Fake News Detection in Python",
    "language": "Jupyter Notebook",
    "topics": [
      "python",
      "text-classification",
      "text-analysis",
      "fake-news",
      "fakenewsdetection",
      "svm-classifier",
      "logistic-regression",
      "classification",
      "text-mining"
    ],
    "readme": "# Fake News Detection\n\nFake News Detection in Python\n\nIn this project, we have used various natural language processing techniques and machine learning algorithms to classify fake news articles using sci-kit libraries from python. \n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\n\n### Prerequisites\n\nWhat things you need to install the software and how to install them:\n\n1. Python 3.6 \n   - This setup requires that your machine has python 3.6 installed on it. you can refer to this url https://www.python.org/downloads/ to download python. Once you have python downloaded and installed, you will need to setup PATH variables (if you want to run python program directly, detail instructions are below in *how to run software section*). To do that check this: https://www.pythoncentral.io/add-python-to-path-python-is-not-recognized-as-an-internal-or-external-command/.  \n   - Setting up PATH variable is optional as you can also run program without it and more instruction are given below on this topic. \n2. Second and easier option is to download anaconda and use its anaconda prompt to run the commands. To install anaconda check this url https://www.anaconda.com/download/\n3. You will also need to download and install below 3 packages after you install either python or anaconda from the steps above\n   - Sklearn (scikit-learn)\n   - numpy\n   - scipy\n   \n  - if you have chosen to install python 3.6 then run below commands in command prompt/terminal to install these packages\n   ```\n   pip install -U scikit-learn\n   pip install numpy\n   pip install scipy\n   ```\n   - if you have chosen to install anaconda then run below commands in anaconda prompt to install these packages\n   ```\n   conda install -c scikit-learn\n   conda install -c anaconda numpy\n   conda install -c anaconda scipy\n   ```   \n\n#### Dataset used\nThe dat",
    "url": "https://github.com/nishitpatel01/Fake_News_Detection",
    "last_updated": "2025-08-26T03:23:05+00:00"
  },
  {
    "full_name": "EbookFoundation/free-programming-books",
    "name": "free-programming-books",
    "description": ":books: Freely available programming books",
    "language": "Python",
    "topics": [
      "education",
      "books",
      "list",
      "resource",
      "hacktoberfest"
    ],
    "readme": "# List of Free Learning Resources In Many Languages\n\n<div align=\"center\" markdown=\"1\">\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)&#160;\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)&#160;\n[![Hacktoberfest 2023 stats](https://img.shields.io/github/hacktoberfest/2023/EbookFoundation/free-programming-books?label=Hacktoberfest+2023)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2023-10-01..2023-10-31)\n\n</div>\n\nSearch the list at [https://ebookfoundation.github.io/free-programming-books-search/](https://ebookfoundation.github.io/free-programming-books-search/) [![https://ebookfoundation.github.io/free-programming-books-search/](https://img.shields.io/website?style=flat&logo=www&logoColor=whitesmoke&label=Dynamic%20search%20site&down_color=red&down_message=down&up_color=green&up_message=up&url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F)](https://ebookfoundation.github.io/free-programming-books-search/).\n\nThis page is available as an easy-to-read website. Access it by clicking on [![https://ebookfoundation.github.io/free-programming-books/](https://img.shields.io/website?style=flat&logo=www&logoColor=whitesmoke&label=Static%20site&down_color=red&down_message=down&up_color=green&up_message=up&url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F)](https://ebookfoundation.github.io/free-programming-books/).\n\n<div align=\"center\">\n  <form action=\"https://ebookfoundation.github.io/free-programming-books-search\">\n    <input type=\"text\" id=\"fpbSearch\" name=\"search\" required placeholder=\"Search Book or Author\"/>\n    <label for=\"submit\"> </label>\n    <input type=\"submit\" id=\"submit\" name=\"submit\" value=\"Search\" />\n  </form>\n</div>\n\n## Intro\n\nThis list was originally a clone of [Stac",
    "url": "https://github.com/EbookFoundation/free-programming-books",
    "last_updated": "2025-09-02T10:02:29+00:00"
  },
  {
    "full_name": "modular/modular",
    "name": "modular",
    "description": "The Modular Platform (includes MAX & Mojo)",
    "language": "Mojo",
    "topics": [
      "ai",
      "language",
      "machine-learning",
      "mojo",
      "modular",
      "programming-language",
      "max"
    ],
    "readme": "<div align=\"center\">\n    <img src=\"https://modular-assets.s3.amazonaws.com/images/GitHubBannerModular.png\">\n\n  [About Modular] | [Get started] | [API docs] | [Contributing] | [Changelog]\n</div>\n\n[About Modular]: https://www.modular.com/\n[Get started]: https://docs.modular.com/max/get-started\n[API docs]: https://docs.modular.com/max/api\n[Contributing]: ./CONTRIBUTING.md\n[Changelog]: https://docs.modular.com/max/changelog\n\n# Modular Platform\n\n> A unified platform for AI development and deployment, including **MAX**🧑‍🚀 and\n**Mojo**🔥.\n\nThe Modular Platform is an open and fully-integrated suite of AI libraries\nand tools that accelerates model serving and scales GenAI deployments. It\nabstracts away hardware complexity so you can run the most popular open\nmodels with industry-leading GPU and CPU performance without any code changes.\n\n![](https://docs.modular.com/images/modular-container-stack.png?20250513)\n\n## Get started\n\nYou don't need to clone this repo.\n\nYou can install Modular as a `pip` or `conda` package and then start an\nOpenAI-compatible endpoint with a model of your choice.\n\nIf we trim the ceremonial steps, you can start a local LLM endpoint with just\ntwo commands:\n\n```sh\npip install modular\n```\n\n```sh\nmax serve --model-path=modularai/Llama-3.1-8B-Instruct-GGUF\n```\n\nThen start sending the Llama 3 model inference requests using [our\nOpenAI-compatible REST API](https://docs.modular.com/max/api/serve).\n\nOr try running hundreds of other models from [our model\nrepository](https://builds.modular.com/?category=models).\n\nFor a complete walkthrough, see [the quickstart\nguide](https://docs.modular.com/max/get-started).\n\n## Deploy our container\n\nThe MAX container is our Kubernetes-compatible Docker container for convenient\ndeployment, using the same inference server you get from the `max serve`\ncommand shown above. We have separate containers for NVIDIA and AMD GPU\nenvironments, and a unified container that works with both.\n\nFor example, you can start a container for an NVI",
    "url": "https://github.com/modular/modular",
    "last_updated": "2025-09-02T08:13:08+00:00"
  },
  {
    "full_name": "justmars/cloudflare-images",
    "name": "cloudflare-images",
    "description": "Wrapper around Cloudflare Images API, usable custom Django storage class",
    "language": "Python",
    "topics": [
      "cloudflare",
      "pydantic"
    ],
    "readme": "# cloudflare-images\n\n![Github CI](https://github.com/justmars/cloudflare-images/actions/workflows/main.yml/badge.svg)\n\nWrapper around Cloudflare Images API, with instructions to create a usable custom Django storage class such wrapper.\n\n## Development\n\nSee [documentation](https://justmars.github.io/cloudflare-images).\n\n1. Run `just start`\n2. Run `just dumpenv`\n3. Run `pytest`\n\nNote: `pytest` will work only if **no** `.env` file exists with the included values. See docstrings.\n\n## Changes\n\n### Dec. 2, 2023\n\n- Compatibility: python 3.12\n- Compatibility: pydantic 2.5\n\n### Initial\n\n- Removed: _Django_ as a dependency\n- Added: Instructions to create _Django_ custom storage class\n- Added: `.enable_batch()`\n- Added: `.list_images()`\n- Added: `.get_batch_token()`\n- Added: `.get_usage_statistics()`\n- Added: `.update_image()`\n- Added: `.v2`\n- Renamed: `.base_api` to `v1`\n- Renamed: `.get()` to `.get_image_details()`\n- Renamed: `.post()` to `.upload_image()`\n- Renamed: `.delete()` to `.delete_image()`\n- Renamed: `.upsert()` to `.delete_then_upload_image()`\n- Renamed: `CloudflareImagesAPIv1` to `CloudflareImagesAPI`\n\n## Django Instructions\n\nStarting with `Django` 4.2, add a Custom `Storage` class to the `STORAGES` setting like so:\n\n```py\nSTORAGES = {  # django 4.2 and above\n    \"default\": {  # default\n        \"BACKEND\": \"django.core.files.storage.FileSystemStorage\",\n    },\n    \"staticfiles\": {  # default\n        \"BACKEND\": \"django.contrib.staticfiles.storage.StaticFilesStorage\",\n    },\n    \"cloudflare_images\": {  # add location of custom storage class\n        \"BACKEND\": \"path.to.storageclass\",\n    },\n}\n```\n\nThe path to the custom storage class should [resemble](https://docs.djangoproject.com/en/dev/howto/custom-file-storage/#django.core.files.storage._open) the following:\n\n```py\nfrom http import HTTPStatus\n\nimport httpx\nfrom django.core.files.base import File\nfrom django.core.files.storage import Storage\nfrom django.utils.deconstruct import deconstructible\n\nfrom cloudflare_imag",
    "url": "https://github.com/justmars/cloudflare-images",
    "last_updated": "2025-01-15T15:30:04+00:00"
  },
  {
    "full_name": "hrbrmstr/webhose",
    "name": "webhose",
    "description": ":hammer: Tools to Work with the 'webhose.io' 'API' in R",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-cyber",
      "webhose"
    ],
    "readme": "\nwebhose\n=======\n\nTools to Work with the 'webhose.io' 'API'\n\nDescription\n-----------\n\nThe 'webhose.io' <https://webhose.io/about> 'API' provides access to structured web data feeds across vertical content domains. Their crawlers download the web, structure the data and index save it into domain-specific repositories that can be accessed on demand. Methods are provided to query and retrieve content from this 'API'.\n\nTODO\n----\n\nCover the rest of the wehbose.io API.\n\nCovered are\n\n-   the [News/Blog/Discussions API](https://docs.webhose.io/docs/newsblogsdiscussions-api)\n-   the [Reviews API](https://docs.webhose.io/v1.0/docs/reviews-api)\n\nWhat's in the tin?\n------------------\n\nThe following functions are implemented:\n\n-   `filter_posts`: Retrieve structured posts data from news articles, blog posts and online discussions\n-   `fetch_posts`: Fetch all structured posts data from news articles, blog posts and online discussions\n-   `filter_reviews`: Retrieve structured reviews data from hundreds of review sites\n-   `fetch_reviews`: Fetch all structured reviews data from hundreds of review sites\n\nInstallation\n------------\n\n``` r\ndevtools::install_github(\"hrbrmstr/webhose\")\n```\n\nUsage\n-----\n\n``` r\nlibrary(webhose)\n\n# current verison\npackageVersion(\"webhose\")\n```\n\n    ## [1] '0.1.0'\n\nMake just one call and/or handle API pagination on your own:\n\n``` r\nres <- filter_posts(\"(China AND United) language:english site_type:news site:bloomberg.com\", ts = 1213456)\n\nstr(res)\n```\n\n    ## List of 5\n    ##  $ posts               :'data.frame':    100 obs. of  42 variables:\n    ##   ..$ uuid                             : chr [1:100] \"ea6f6084be16a50b0d4791ffa268956ca691c16d\" \"bd0ac60981ac73e2a7e71378881272eb5b6147d7\" \"3f2c2c13aa2b3c6d5fc8300f3a9876d9c86c08d1\" \"659d73d3ddba3c0a0505da8fc15862bc33ac9519\" ...\n    ##   ..$ url                              : chr [1:100] \"http://omgili.com/ri/.wHSUbtEfZQ2wVwv1GOz6Msy5xK737ZFABw8Ekw2S.t5sWsmdlgeKUSTNIqEofP1vmWYU0gcWSaL9Q_eyN8poyAr9ZMQdbU2SkzlF5VBNP",
    "url": "https://github.com/hrbrmstr/webhose",
    "last_updated": "2025-03-22T11:14:04+00:00"
  },
  {
    "full_name": "Developer-Y/cs-video-courses",
    "name": "cs-video-courses",
    "description": "List of Computer Science courses with video lectures.",
    "language": "",
    "topics": [
      "computer-science",
      "algorithms",
      "systems",
      "databases",
      "machine-learning",
      "web-development",
      "security",
      "computer-architecture",
      "bioinformatics",
      "robotics",
      "embedded-systems",
      "database-systems",
      "computer-vision",
      "quantum-computing",
      "computational-biology",
      "computational-physics",
      "deep-learning",
      "reinforcement-learning"
    ],
    "readme": "<!-- omit in toc -->\n# Computer Science courses with video lectures\n\n<!-- omit in toc -->\n## Introduction\n\n- Please check [NOTES](https://github.com/Developer-Y/cs-video-courses/blob/master/NOTES.md) for general information about this list.\n- Please refer [CONTRIBUTING.md](https://github.com/Developer-Y/cs-video-courses/blob/master/CONTRIBUTING.md) for contribution guidelines.\n- Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. \n- You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for small MOOCs, basic tutorials, or advertisements for some sites/channels.\n\n------------------------------\n\nTable of Contents\n\n------------------------------\n- [Introduction to Computer Science](#introduction-to-computer-science)\n- [Data Structures and Algorithms](#data-structures-and-algorithms)\n- [Systems Programming](#systems-programming)\n  * [Operating Systems](#operating-systems)\n  * [Distributed Systems](#distributed-systems)\n  * [Real-Time Systems](#real-time-systems) \n- [Database Systems](#database-systems)\n- [Software Engineering](#software-engineering)\n  * [Object Oriented Design](#object-oriented-design)\n  * [Software Engineering](#software-engineering)\n  * [Software Architecture](#software-architecture)\n  * [Concurrency](#concurrency)\n  * [Mobile Application Development](#mobile-application-development)\n- [Artificial Intelligence](#artificial-intelligence)\n- [Machine Learning](#machine-learning)\n  * [Introduction to Machine Learning](#introduction-to-machine-learning)\n  * [Data Mining](#data-mining)\n  * [Probabilistic Graphical Modeling](#probabilistic-graphical-modeling)\n  * [Deep Learning](#deep-learning)\n  * [Reinforcement Learning](#reinforcement-learning)\n  * [Advanced Machine Learning](#advanced-machine-learning)\n  * [Natural Language Processing](#natur",
    "url": "https://github.com/Developer-Y/cs-video-courses",
    "last_updated": "2025-09-02T09:02:25+00:00"
  },
  {
    "full_name": "benmarwick/wordcountaddin",
    "name": "wordcountaddin",
    "description": "Word counts and readability statistics in R markdown documents",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\nwordcountaddin <img src=\"inst/logo.png\" align=\"right\" height=\"130\" />\n=====================================================================\n\n[![Last-changedate](https://img.shields.io/badge/last%20change-2019--01--09-brightgreen.svg)](https://github.com/benmarwick/wordcountaddin/commits/master)\n[![minimal R\nversion](https://img.shields.io/badge/R%3E%3D-3.5.2-brightgreen.svg)](https://cran.r-project.org/)\n[![Licence](https://img.shields.io/github/license/mashape/apistatus.svg)](http://choosealicense.com/licenses/mit/)\n[![Travis-CI Build\nStatus](https://travis-ci.org/benmarwick/wordcountaddin.png?branch=master)](https://travis-ci.org/benmarwick/wordcountaddin)\n[![codecov.io](https://codecov.io/github/benmarwick/wordcountaddin/coverage.svg?branch=master)](https://codecov.io/github/benmarwick/wordcountaddin?branch=master)\n[![ORCiD](https://img.shields.io/badge/ORCiD-0000--0001--7879--4531-green.svg)](http://orcid.org/0000-0001-7879-4531)\n\nThis R package is an [RStudio\naddin](https://rstudio.github.io/rstudioaddins/) to count words and\ncharacters in text in an [R markdown](http://rmarkdown.rstudio.com/)\ndocument. It also has a function to compute readability statistics so\nyou can get an indication of how easy or difficult your document is to\nread.\n\nYou can count words in your Rmd file in three ways:\n\n-   In a selection of text in your active Rmd, by selecting some text\n    with your mouse in RStudio and using the Wordcount Addin  \n-   All the words in your active Rmd in RStudio, by using the Wordcount\n    Addin with no text selected\n-   All the words in an Rmd file, directly using the `word_count`\n    function from the console or command line (RStudio not required),\n    and specifiying the filename as an argument to the function (e.g.\n    `wordcountaddin::word_count(\"my_file.Rmd\")`). This will give you a\n    single integer result, rather than the Markdown table that the other\n    functions return.\n\nIn",
    "url": "https://github.com/benmarwick/wordcountaddin",
    "last_updated": "2025-06-27T20:01:23+00:00"
  },
  {
    "full_name": "rstudio/blogdown",
    "name": "blogdown",
    "description": "Create Blogs and Websites with R Markdown",
    "language": "R",
    "topics": [
      "hugo",
      "blogdown",
      "rstudio",
      "rmarkdown",
      "r",
      "blog-engine",
      "website-generation"
    ],
    "readme": "# blogdown <a href=\"https://pkgs.rstudio.com/blogdown/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/rstudio/blogdown/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/blogdown/actions/workflows/R-CMD-check.yaml)\n[![CRAN status](https://www.r-pkg.org/badges/version/blogdown)](https://CRAN.R-project.org/package=blogdown)\n[![Codecov test coverage](https://codecov.io/gh/rstudio/blogdown/branch/main/graph/badge.svg)](https://app.codecov.io/gh/rstudio/blogdown?branch=main)\n<!-- badges: end -->\n\nThe goal of the blogdown package is to provide a powerful and customizable website output format for [R Markdown](https://rmarkdown.rstudio.com/). Use dynamic R Markdown documents to build webpages featuring:\n\n+ R code (or other programming languages that [knitr](https://yihui.org/knitr/) supports),\n\n+ automatically rendered output such as graphics, tables, analysis results, and HTML widgets, and\n\n+ technical writing elements such as citations, footnotes, and LaTeX math, enabled by the [bookdown package](https://pkgs.rstudio.com/bookdown/).\n\nBy default, blogdown uses [Hugo](https://gohugo.io), a popular open-source static website generator, which provides a fast and flexible way to build your site content to be shared online. Other website generators like Jekyll and Hexo are also supported.\n\nA useful feature of blogdown sites, compared to other R Markdown-based [websites](https://bookdown.org/yihui/rmarkdown/rmarkdown-site.html), is that you may organize your website content (including R Markdown files) within subdirectories. This makes blogdown a good solution not just for blogging or sites about R &mdash; it can also be used to create general-purpose websites to communicate about data science, statistics, data visualization, programming, or education.\n\n## Book\n\n<a href=\"https://bookdown.org/yihui/blogdown/\"><img class=\"book\" src=\"https://bookdown.org/yihui/blogdown/images/cove",
    "url": "https://github.com/rstudio/blogdown",
    "last_updated": "2025-08-30T16:26:19+00:00"
  },
  {
    "full_name": "google/oss-fuzz-gen",
    "name": "oss-fuzz-gen",
    "description": "LLM powered fuzzing via OSS-Fuzz.",
    "language": "Python",
    "topics": [
      "ai",
      "fuzzing",
      "llm",
      "security"
    ],
    "readme": "# A Framework for Fuzz Target Generation and Evaluation\n\nThis framework generates fuzz targets for real-world `C`/`C++/Java/Python` projects with\nvarious Large Language Models (LLM) and benchmarks them via the\n[`OSS-Fuzz` platform](https://github.com/google/oss-fuzz).\n\nMore details available in [AI-Powered Fuzzing: Breaking the Bug Hunting Barrier](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html):\n![Alt text](images/Overview.png \"Overview\")\n\nCurrent supported models are:\n- Vertex AI code-bison\n- Vertex AI code-bison-32k\n- Gemini Pro\n- Gemini Ultra\n- Gemini Experimental\n- Gemini 1.5\n- OpenAI GPT-3.5-turbo\n- OpenAI GPT-4\n- OpenAI GPT-4o\n- OpenAI GPT-4o-mini\n- OpenAI GPT-4-turbo\n- OpenAI GPT-3.5-turbo (Azure)\n- OpenAI GPT-4 (Azure)\n- OpenAI GPT-4o (Azure)\n\nGenerated fuzz targets are evaluated with four metrics against the most up-to-date data from production environment:\n- Compilability\n- Runtime crashes\n- Runtime coverage\n- Runtime line coverage diff against existing human-written fuzz targets in `OSS-Fuzz`.\n\nHere is a sample experiment result from 2024 Jan 31.\nThe experiment included [1300+ benchmarks](./benchmark-sets/all) from 297 open-source projects.\n\n![image](https://github.com/google/oss-fuzz-gen/assets/759062/fa53698b-e44c-4b58-b5e7-798337c8b752)\n\nOverall, this framework manages to successfully leverage LLMs to generate valid fuzz targets (which generate non-zero coverage increase)\nfor 160 C/C++ projects. The maximum line coverage increase is 29% from the existing human-written targets.\n\nNote that these reports are not public as they may contain undisclosed vulnerabilities.\n\n## Usage\n\nCheck our detailed [usage guide](./USAGE.md) for instructions on how to run this framework and generate reports based on the results.\n\n## Independent Agent Execution and Evaluation\nYou can also execute or evaluate individual agents without running full experiments, using the integrated agent execution framework.\nSee the [framework's documentat",
    "url": "https://github.com/google/oss-fuzz-gen",
    "last_updated": "2025-09-01T20:03:07+00:00"
  },
  {
    "full_name": "ChristopherLucas/txtorg",
    "name": "txtorg",
    "description": "Software for preprocessing textual data in multiple languages for textual analysis. ",
    "language": "Python",
    "topics": [],
    "readme": "txtorg\n======\n\ntxtorg is a Python-based utility that leverages Apache Lucene to facilitate text preprocessing and management. It outputs processed text in a variety of formats for use in a wide array of analytical software, including (but not limited to) the structural topic model. It scales to large corpora and has a graphical user interface that anyone can use. With Lucene, txtorg can support a wide range of languages. \n\nFor more information, including installation instructions, see http://txtorg.org/.\n",
    "url": "https://github.com/ChristopherLucas/txtorg",
    "last_updated": "2023-05-30T03:42:43+00:00"
  },
  {
    "full_name": "notnews/cnn_transcripts",
    "name": "cnn_transcripts",
    "description": "CNN Transcripts 2000--2025",
    "language": "Python",
    "topics": [
      "news",
      "transcripts",
      "cnn-transcripts",
      "cnn"
    ],
    "readme": "## CNN Transcripts 2000--2025\n\nCNN provides transcripts for its shows at [http://edition.cnn.com/TRANSCRIPTS/](http://edition.cnn.com/TRANSCRIPTS/). \n\nThe transcripts are available for shows starting `1999/10/01`. See [http://edition.cnn.com/TRANSCRIPTS/1999.10.01.html](http://edition.cnn.com/TRANSCRIPTS/1999.10.01.html). However, we get a 'Page not found' error when we follow links until `1999/12/31`. So we started scraping the data from `2000/01/01`.\n\nCNN went through a few HTML styles of the news transcripts between `2000/01/01` and 2014. So there are two scapers to parse the different HTML styles:\n\n* [till 2002/9/17](scripts/cnn-1.py)\n* [from 2002/9/17](scripts/cnn-1.py)\n* [from 2014/6/18](scripts/cnn-2.py)\n\n### Data\n\nThe parsed data are posted at [http://dx.doi.org/10.7910/DVN/ISDPJU](http://dx.doi.org/10.7910/DVN/ISDPJU). For copyright reasons, access is restricted for research purposes only. The data are split into eight files:\n\n* `cnn-1.csv`. Data from  2000/01/01--2000/04/20. No. of transcripts = 7,017\n* `cnn-2.csv`. Data from  2000/04/21--2001/04/03. No. of transcripts = 21,381\n* `cnn-3.csv`. Data from  2001/04/04--2002/08/06. No. of transcripts = 35,269\n* `cnn-4.csv`. Data from  2002/08/07--2002/09/16. No. of transcripts = 2,343\n* `cnn-5.csv`. Data from  2002/09/17--2012/05/18. No. of transcripts = 101,336\n* `cnn-6.csv`. Data from  2012/05/19--2014/06/17. No. of transcripts = 23,536\n* `cnn-7.csv`. Data from  2014/06/18--2022/02/05. No. of transcripts = 102,458\n* `cnn-8.csv`. Data from  2022/02/01--2025/03/15. No. of transcripts = 43,562\n\nTotal number of transcripts: 336,902 \n\n### Notes\n\n* 2000-04-21 New format error\n* 2000-04-22 content within <p> and </p> tag\n* 2001-04-04 No URL prefix, subheader ==> h4, content next table <br> tag\n* Scripts from 2014\n\n## 🔗 Adjacent Repositories\n\n- [notnews/fox_news_transcripts](https://github.com/notnews/fox_news_transcripts) — Fox News Transcripts 2003--2025\n- [notnews/msnbc_transcripts](https://github.com/notnews/msnb",
    "url": "https://github.com/notnews/cnn_transcripts",
    "last_updated": "2025-05-01T05:18:11+00:00"
  },
  {
    "full_name": "soodoku/hidden",
    "name": "hidden",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "### Mis-measuring Political Knowledge? Do People Know More—or Even Less—about Politics than Commonly Thought?\n\nIt has long been widely accepted that the public knows very little about politics, but, over the past couple of decades, a number of prominent revisionist studies have claimed that it actually knows appreciably more than we have been thinking. The reasons lie in a mix of respondent behavior, the selection and design of knowledge questions, and the coding of the responses to open-ended ones. In a nutshell, incorrect and Don’t-Know responses may conceal knowledge; the questions may be too difficult, missing too much of what people do know; and the coding may count partially correct responses as incorrect. But while there may be some truth to such claims, this accounting neglects the other side of the ledger. Correct responses may reflect lucky guessing, shrewd inference, or mere belief (suspicion as distinct from knowledge), the questions may be too easy, missing too much of what people don’t know; and the respondents may be more knowledgeable than the citizenry from which they have been drawn. Here we provide a more comprehensive view.\n\n### Data\n\n* [Data](data/)\n\n### Scripts\n* [Scripts](scripts/)\n\n### Manuscript\n\n* [MS](ms/)\n\n### Authors\nRobert Luskin, Gaurav Sood, and Daniel Weitzel\n",
    "url": "https://github.com/soodoku/hidden",
    "last_updated": "2023-06-07T01:25:49+00:00"
  },
  {
    "full_name": "hrbrmstr/docxtractr",
    "name": "docxtractr",
    "description": ":scissors: Extract Tables from Microsoft Word Documents with R",
    "language": "R",
    "topics": [
      "docx",
      "r",
      "rstats",
      "microsoft-word",
      "extract-tables",
      "table-extraction"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/docxtractr.svg?branch=master)](https://travis-ci.org/hrbrmstr/docxtractr)\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/hrbrmstr/docxtractr?branch=master&svg=true)](https://ci.appveyor.com/project/hrbrmstr/docxtractr)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/hrbrmstr/docxtractr/master.svg)](https://codecov.io/github/hrbrmstr/docxtractr?branch=master)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/docxtractr)](http://cran.r-project.org/package=docxtractr)\n\n![](docxtractr-logo.png)\n\n# docxtractr\n\nExtract Data Tables and Comments from ‘Microsoft’ ‘Word’ Documents\n\n## Description\n\nAn R package for extracting tables & comments out of Word documents\n(docx). Development versions are available here and production versions\nare [on\nCRAN](https://cran.rstudio.com/web/packages/docxtractr/index.html).\n\nMicrosoft Word docx files provide an XML structure that is fairly\nstraightforward to navigate, especially when it applies to Word tables.\nThe docxtractr package provides tools to determine table count, table\nstructure and extract tables from Microsoft Word docx documents.\n\nMany tables in Word documents are in twisted formats where there may be\nlabels or other oddities mixed in that make it difficult to work with\nthe underlying data. `docxtractr` provides a\nfunction—`assign_colnames`—that makes it easy to identify a\nparticular row in a scraped (or any, really) `data.frame` as the one\ncontaining column names and have it become the column names, removing it\nand (optionally) all of the rows before it (since that’s usually what\nneeds to be done).\n\n## What’s in the tin?\n\nThe following functions are implemented:\n\n  - `read_docx`: Read in a Word document for table extraction\n  - `docx_describe_tbls`: Returns a description of all the tables in the\n    Word document\n  - `docx_describe_cmnts`: Returns",
    "url": "https://github.com/hrbrmstr/docxtractr",
    "last_updated": "2025-08-12T04:28:59+00:00"
  },
  {
    "full_name": "typst/typst",
    "name": "typst",
    "description": "A new markup-based typesetting system that is powerful and easy to learn.",
    "language": "Rust",
    "topics": [
      "typesetting",
      "compiler",
      "markup"
    ],
    "readme": "<h1 align=\"center\">\n  <img alt=\"Typst\" src=\"https://user-images.githubusercontent.com/17899797/226108480-722b770e-6313-40d7-84f2-26bebb55a281.png\">\n</h1>\n\n<p align=\"center\">\n  <a href=\"https://typst.app/docs/\">\n    <img alt=\"Documentation\" src=\"https://img.shields.io/website?down_message=offline&label=docs&up_color=007aff&up_message=online&url=https%3A%2F%2Ftypst.app%2Fdocs\"\n  ></a>\n  <a href=\"https://typst.app/\">\n    <img alt=\"Typst App\" src=\"https://img.shields.io/website?down_message=offline&label=typst.app&up_color=239dad&up_message=online&url=https%3A%2F%2Ftypst.app\"\n  ></a>\n  <a href=\"https://discord.gg/2uDybryKPe\">\n    <img alt=\"Discord Server\" src=\"https://img.shields.io/discord/1054443721975922748?color=5865F2&label=discord&labelColor=555\"\n  ></a>\n  <a href=\"https://github.com/typst/typst/blob/main/LICENSE\">\n    <img alt=\"Apache-2 License\" src=\"https://img.shields.io/badge/license-Apache%202-brightgreen\"\n  ></a>\n  <a href=\"https://typst.app/jobs/\">\n    <img alt=\"Jobs at Typst\" src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Ftypst.app%2Fassets%2Fdata%2Fshields.json&query=%24.jobs.text&label=jobs&color=%23A561FF&cacheSeconds=1800\"\n  ></a>\n</p>\n\nTypst is a new markup-based typesetting system that is designed to be as powerful\nas LaTeX while being much easier to learn and use. Typst has:\n\n- Built-in markup for the most common formatting tasks\n- Flexible functions for everything else\n- A tightly integrated scripting system\n- Math typesetting, bibliography management, and more\n- Fast compile times thanks to incremental compilation\n- Friendly error messages in case something goes wrong\n\nThis repository contains the Typst compiler and its CLI, which is everything you\nneed to compile Typst documents locally. For the best writing experience,\nconsider signing up to our [collaborative online editor][app] for free.\n\n## Example\nA [gentle introduction][tutorial] to Typst is available in our documentation.\nHowever, if you want to see the power of Typst enc",
    "url": "https://github.com/typst/typst",
    "last_updated": "2025-09-02T09:31:56+00:00"
  },
  {
    "full_name": "lukesonnet/KRLS",
    "name": "KRLS",
    "description": "R package for machine learning technique to fit flexible, interpretable functional forms for continuous and binary outcomes. ",
    "language": "R",
    "topics": [],
    "readme": "# KRLS\n[![Travis-CI Build Status](https://travis-ci.org/lukesonnet/KRLS.svg?branch=master)](https://travis-ci.org/lukesonnet/KRLS) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/lukesonnet/KRLS?branch=master&svg=true)](https://ci.appveyor.com/project/lukesonnet/KRLS) [![Coverage Status](https://coveralls.io/repos/github/lukesonnet/KRLS/badge.svg?branch=master)](https://coveralls.io/github/lukesonnet/KRLS?branch=master)\n\nThis package provides methods for fitting flexible functional forms for continuous and binary outcomes. This package is under development and may behave unexpectedly. It is intended to replace the KRLS package available on CRAN. Please email [Luke Sonnet](mailto:luke.sonnet@gmail.com) or leave an issue for Luke Sonnet or Chad Hazlett.\n\n### Install latest version\n\nYou can install the latest version by running:\n```R\ndevtools::install_github('lukesonnet/KRLS')\n```\n\n### Troubleshooting installation\n\nThis version uses `Rcpp` extensively for speed reasons. These means you need to have the right compilers on your machine.\n\n#### Windows\nIf you are on Windows, you will need to install [RTools](https://cran.r-project.org/bin/windows/Rtools/) if you haven't already. If you still are having difficulty with installing and it says that the compilation failed, try installing it without support for multiple architectures:\n```R\ndevtools::install_github('lukesonnet/KRLS', args=c('--no-multiarch'))\n```\n\n#### Mac OSX\n\nIn order to compile the `C++` in this package, `RcppArmadillo` will require you to have compilers installed on your machine. You may already have these, but you can install them by running:\n\n```bash\nxcode-select --install\n```\n\nIf you are having problems with this install on Mac OSX, specifically if you are getting errors with either `lgfortran` or `lquadmath`, then try open your Terminal and try the following:\n\n```bash\ncurl -O http://r.research.att.com/libs/gfortran-4.8.2-darwin13.tar.bz2\nsudo tar fvxz gfortran-4.8.2-darwin13.",
    "url": "https://github.com/lukesonnet/KRLS",
    "last_updated": "2025-01-16T05:29:14+00:00"
  },
  {
    "full_name": "nkotova/MLprojects",
    "name": "MLprojects",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "",
    "url": "https://github.com/nkotova/MLprojects",
    "last_updated": "2024-04-19T16:16:54+00:00"
  },
  {
    "full_name": "deepchecks/deepchecks",
    "name": "deepchecks",
    "description": "Deepchecks: Tests for Continuous Validation of ML Models & Data. Deepchecks is a holistic open-source solution for all of your AI & ML validation needs, enabling to thoroughly test your data and models from research to production.",
    "language": "Python",
    "topics": [
      "machine-learning",
      "ml",
      "model-validation",
      "data-validation",
      "mlops",
      "data-science",
      "python",
      "jupyter-notebook",
      "model-monitoring",
      "data-drift",
      "html-report",
      "deep-learning",
      "pytorch",
      "pandas-dataframe"
    ],
    "readme": "<!--\n   ~ ----------------------------------------------------------------------------\n   ~ Copyright (C) 2021-2023 Deepchecks (https://www.deepchecks.com)\n   ~\n   ~ This file is part of Deepchecks.\n   ~ Deepchecks is distributed under the terms of the GNU Affero General\n   ~ Public License (version 3 or later).\n   ~ You should have received a copy of the GNU Affero General Public License\n   ~ along with Deepchecks.  If not, see <http://www.gnu.org/licenses/>.\n   ~ ----------------------------------------------------------------------------\n   ~\n-->\n\n[![GitHub\nstars](https://img.shields.io/github/stars/deepchecks/deepchecks.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/deepchecks/deepchecks/stargazers/)\n![build](https://github.com/deepchecks/deepchecks/actions/workflows/build.yml/badge.svg)\n![pkgVersion](https://img.shields.io/pypi/v/deepchecks)\n[![Coverage\nStatus](https://coveralls.io/repos/github/deepchecks/deepchecks/badge.svg?branch=main)](https://coveralls.io/github/deepchecks/deepchecks?branch=main) \n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-44-orange.svg?style=flat-round)](#https://github.com/deepchecks/deepchecks/blob/main/CONTRIBUTING.rst)\n<!-- ALL-CONTRIBUTORS-BADGE:END --> \n\n<!---\nthis badge takes quite some space, can re-add it if wanted\n![pyVersions](https://img.shields.io/pypi/pyversions/deepchecks)\n--->\n\n\n<h1 align=\"center\">\n   Deepchecks - Continuous Validation for AI & ML: Testing, CI & Monitoring\n</h1>\n\nDeepchecks is a holistic open-source solution for all of your AI & ML validation needs,\nenabling you to thoroughly test your data and models from research to production.\n\n\n<a target=\"_blank\" href=\"https://docs.deepchecks.com/?utm_source=github.com&utm_medium=referral&utm_campaign=readme&utm_content=logo\">\n   <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/source/_static/images/readme/deepchecks_continuous_",
    "url": "https://github.com/deepchecks/deepchecks",
    "last_updated": "2025-08-31T20:16:51+00:00"
  },
  {
    "full_name": "google/tangent",
    "name": "tangent",
    "description": "Source-to-Source Debuggable Derivatives in Pure Python",
    "language": "Python",
    "topics": [
      "autodiff",
      "automatic-differentiation",
      "machine-learning",
      "deep-learning"
    ],
    "readme": "# Tangent \n\n[![Build Status](https://travis-ci.org/google/tangent.svg?branch=master)](https://travis-ci.org/google/tangent)\n[![Join the chat at https://gitter.im/google/tangent](https://badges.gitter.im/google/tangent.svg)](https://gitter.im/google/tangent?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nTangent is a new, free, and open-source Python library for automatic differentiation.  \n\n\nExisting libraries implement automatic differentiation by tracing a program's execution (at runtime, like PyTorch) or by staging out a dynamic data-flow graph and then differentiating the graph (ahead-of-time, like TensorFlow). In contrast, Tangent performs ahead-of-time autodiff on the Python source code itself, and produces Python source code as its output. Tangent fills a unique location in the space of machine learning tools.\n\n![Autodiff Tool Space](docs/toolspace.png \"Autodiff Tool Space\")\n\n\nAs a result, you can finally read your automatic derivative code just like the rest of your program. Tangent is useful to researchers and students who not only want to write their models in Python, but also read and debug automatically-generated derivative code without sacrificing speed and flexibility.\n\nTangent works on a large and growing subset of Python, provides extra autodiff features other Python ML libraries don't have, has reasonable performance, and is compatible with TensorFlow and NumPy.\n\nThis project is an experimental release, and is under active development. As we continue to build Tangent, and respond to feedback from the community, there might be API changes.\n\n## Usage\n\nNote: An interactive notebook with all the code in this page can be found [here](https://colab.research.google.com/notebook#fileId=1cjoX9GteBymbnqcikNMZP1uenMcwAGDe).\n\nTangent has a one-function API:\n```python\nimport tangent\ndf = tangent.grad(f)\n```\n\nIf you want to print out derivatives at the time Tangent generates the derivative function:\n\n```python\nimport tangent\ndf = tange",
    "url": "https://github.com/google/tangent",
    "last_updated": "2025-08-28T03:29:01+00:00"
  },
  {
    "full_name": "yinlou/mltk",
    "name": "mltk",
    "description": "Machine Learning Tool Kit",
    "language": "Java",
    "topics": [
      "machine-learning",
      "java"
    ],
    "readme": "# Machine Learning Tool Kit\n\nMLTK is a collection of various supervised machine learning algorithms, which is designed for directly training models and further development. For questions or suggestions with the code, please email <a href=\"mailto:machinelearningtoolkit@gmail.com\">machinelearningtoolkit@gmail.com</a>. \n\nSee [wiki](https://github.com/yinlou/mltk/wiki) for full documentation, examples and other information.\n",
    "url": "https://github.com/yinlou/mltk",
    "last_updated": "2025-08-13T11:12:07+00:00"
  },
  {
    "full_name": "colarusso/measured_justice",
    "name": "measured_justice",
    "description": "This repo is dedicated to the analysis of publicly available court data.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Measured Justice\n\nThis repo is dedicated to the analysis of publicly available court data. All of the work here has been done with freely available software. Mostly, it's a collection of [notebooks](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261). For people interested in replicating or expanding on this work I've put together this quick start guide for setting up [Project Jupyter](http://jupyter.org/) notebooks.\n \n## Download, Install, and Run Notebooks\n\n Download and install Anaconda (a free distro which includes everything you need to use Jupyter). See https://www.continuum.io/downloads\n+ Open up a terminal/command prompt and navigate to the directory where you want to be working (e.g., /users/yourname/documents/)\n+ At the prompt enter:\n  + ``jupyter notebook``\n+ That should open up a new tab in your web browser.\n+ You should see a dropdown menu in the upper right labeled “New.” Click on that, and choose “Python 3” under “Notebooks.”\n+ That should open a new notebook. Basically, you put your code in cells and run them as needed. As the [Nature article](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261) linked above makes clear, this is a great way to collaborate and to keep all your work in one place. \n+ Here’s are some [example notebooks](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks#introductory-tutorials) to get you started. Just cut-and-paste from these examples and start playing.  \n\n## Common Speed Bumps\n\nIf you start to play around and get an error like:\n \n``NameError: name 'pd' is not defined`` \n \nThat means that your notebook hasn’t loaded a module with the name ‘pd’. So either, you’re missing something like this:\n \n``import numpy as np``\n \nor the module isn’t installed on your system. If it’s the latter, you’ll get an error like:\n \n``ImportError: No module named 'numpy'``\n \nwhen you try to import the module. To install a module, all you have to do is go to the termi",
    "url": "https://github.com/colarusso/measured_justice",
    "last_updated": "2019-01-17T06:43:07+00:00"
  },
  {
    "full_name": "firebase/firebase-functions-python",
    "name": "firebase-functions-python",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# Cloud Functions for Firebase Python SDK\n\nThe [`firebase-functions`](https://pypi.org/project/firebase-functions/) package provides an SDK for defining Cloud Functions for Firebase in Python.\n\nCloud Functions provides hosted, private, and scalable environment where you can run server code. The Firebase SDK for Cloud Functions integrates the Firebase platform by letting you write code that responds to events and invokes functionality exposed by other Firebase features.\n\n## Learn more\n\nLearn more about the Firebase SDK for Cloud Functions in the [Firebase documentation](https://firebase.google.com/docs/functions/) or [check out our samples](https://github.com/firebase/functions-samples).\n\nHere are some resources to get help:\n\n- Start with the quickstart: <https://firebase.google.com/docs/functions/get-started>\n- Go through the guide: <https://firebase.google.com/docs/functions/>\n- Read the full API reference: <https://firebase.google.com/docs/reference/functions/2nd-gen/python>\n- Browse some examples: <https://github.com/firebase/functions-samples>\n\nIf the official documentation doesn't help, try asking through our official support channels: <https://firebase.google.com/support/>\n\n## Usage\n\n```python\n# functions/main.py\nfrom firebase_functions import db_fn\nfrom notify_users import api\n\n@db_fn.on_value_created(reference=\"/posts/{post_id}\")\ndef new_post(event):\n    print(f\"Received new post with ID: {event.params.get('post_id')}\")\n    return notifyUsers(event.data)\n```\n\n## Contributing\n\nTo contribute a change, [check out the contributing guide](.github/CONTRIBUTING.md).\n\n## License\n\n© Google, 2025. Licensed under [Apache License](LICENSE).\n\n",
    "url": "https://github.com/firebase/firebase-functions-python",
    "last_updated": "2025-08-05T09:36:20+00:00"
  },
  {
    "full_name": "PrincetonUniversity/historic_state_legislative_election_results",
    "name": "historic_state_legislative_election_results",
    "description": "Dataset of state legislative elections from 1971–2018.",
    "language": "Python",
    "topics": [],
    "readme": "_Note: The code in this repository is a bit of a mess, and the below documentation may be out of date. However, we are still maintaining the <a href=\"https://rawgit.com/PrincetonUniversity/historic_state_legislative_election_results/master/state_legislative_election_results_post1971.csv\">spreadsheet</a> that you are probably interested in. Please contact us with any questions!_\n\n# State Legislative Elections,  1971 - 2018\n### <a href=\"https://rawgit.com/PrincetonUniversity/historic_state_legislative_election_results/master/state_legislative_election_results_post1971.csv\">Download in CSV (spreadsheet) form</a>\n\nThis repo contains results of general elections to the lower house of the state legislatures over the last five decades, from 1971 to 2018.\nCandidate information from Carl Klarner's [State Legislative Election Returns dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/20401)\nwas used to compile these election results from 1971 - 2012. For each state, we only extract elections using Single Member districts (see below).\n\nThe elections dataset can be found in the output_data folder.\n\nAnalysis for elections up to 2012 (inclusive) can be re-run using the script main.py. Analysis for 2013-2017 elections can be re-run with post2013_scraper/run_scraper.py\n\nThis script and accompanying dataset are provided by the [Princeton Gerrymandering Project](http://gerrymander.princeton.edu/). Please\ndirect any feedback or questions to Brian Remlinger, brem at princeton dot edu. \nAlthough we have attempted to check the dataset for errors, accuracy cannot be guaranteed. \n\n\nThis dataset is distributed under a [CC0 license](https://creativecommons.org/publicdomain/zero/1.0/), while the generator scripts are distributed under a [GNU GPL license](https://www.gnu.org/licenses/gpl-3.0.en.html). If you use this data for research or projects, we'd love to know! Please send along an email describing your work.\n\n## Analysis Details\n### Input data\nInput data for 197",
    "url": "https://github.com/PrincetonUniversity/historic_state_legislative_election_results",
    "last_updated": "2022-09-18T22:31:18+00:00"
  },
  {
    "full_name": "rstudio/revealjs",
    "name": "revealjs",
    "description": "R Markdown Format for reveal.js Presentations",
    "language": "JavaScript",
    "topics": [],
    "readme": "R Markdown Format for reveal.js Presentations\n================\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/revealjs)](https://CRAN.R-project.org/package=revealjs)\n[![R-CMD-check](https://github.com/rstudio/revealjs/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/revealjs/actions/workflows/R-CMD-check.yaml)\n[![reveal.js](https://img.shields.io/badge/reveal.js-4.2.1-yellow)](https://github.com/rstudio/revealjs/tree/main/inst/reveal.js-4.2.1)\n<!-- badges: end -->\n\n## Overview\n\nThis repository provides an [R Markdown](https://rmarkdown.rstudio.com)\ncustom format for [reveal.js](https://revealjs.com/) HTML presentations.\nThe packages includes *reveal.js* library in version 4.2.1\n\nYou can use this format in R Markdown documents by installing this\npackage as follows:\n\n``` r\ninstall.packages(\"revealjs\")\n```\n\nTo create a [reveal.js](https://revealjs.com/) presentation from R\nMarkdown you specify the `revealjs_presentation` output format in the\nfront-matter of your document. You can create a slide show broken up\ninto sections by using the `#` and `##` heading tags (you can also\ncreate a new slide without a header using a horizontal rule (`----`).\nFor example here’s a simple slide show:\n\n``` markdown\n---\ntitle: \"Habits\"\nauthor: John Doe\ndate: March 22, 2005\noutput: revealjs::revealjs_presentation\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n```\n\n## Rendering\n\nDepending on your use case, there are 3 ways you can render the\npresentation.\n\n1.  RStudio\n2.  R console\n3.  Terminal (e.g., bash)\n\n### RStudio\n\nWhen creating the presentation in RStudio, there will be a `Knit` button\nright below the source tabs. By default, it will render the current\ndocument and place the rendered `HTML` file in the same directory as the\nsource file, with the same name.\n\n##",
    "url": "https://github.com/rstudio/revealjs",
    "last_updated": "2025-08-03T15:40:02+00:00"
  },
  {
    "full_name": "jjangsangy/ExplainToMe",
    "name": "ExplainToMe",
    "description": "Automatic Web Article Summarizer",
    "language": "Python",
    "topics": [
      "nlp",
      "python",
      "docker",
      "heroku",
      "textrank"
    ],
    "readme": "# ExplainToMe\n\n[![travis](https://travis-ci.org/jjangsangy/ExplainToMe.svg?branch=master)](https://travis-ci.org/jjangsangy/ExplainToMe)\n[![licence](https://img.shields.io/pypi/l/coverage.svg)](https://github.com/jjangsangy/ExplainToMe/blob/master/LICENSE)\n[![Quay](https://quay.io/repository/jjangsangy/explaintome/status)](https://quay.io/repository/jjangsangy/explaintome)\n\n## Automatic Web Article Summarizer\n\n![Front](https://github.com/jjangsangy/ExplainToMe/raw/master/static/front.jpg)\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy)\n\n# What is it?\n\n`ExplainToMe` is a automatic text summarizer, that utilizes\n[TextRank](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf),\na graph based algorithm to scans through the contents of a website to\nextract a concise machine generated summary. The methodology is similar\nto the way search engines return the most relevant web pages from a\nusers search query.\n\n# Support\n\nHere’s a list of Python platforms that are officially supported.\n\n* Python 2.7\n* Python 3.4\n* Python 3.5\n* pypy 2.5.0 -> 2.7.9\n\n> We are working on Python 3.6\n\n# Quickstart\n\n# Install\n\n## Clone Repository\n\n```bash\n$ git clone https://github.com/jjangsangy/ExplainToMe.git\n```\n\n## Create a virtualenv\n\n```bash\n$ virtualenv -p python venv\n```\n\n## Source Virtualenv\n\n```bash\n$ source venv/bin/activate\n```\n\n## Install Python Dependencies\n\n```bash\n$ pip install --upgrade pip setuptools wheel\n$ pip install -r requirements.txt\n```\n\n## Run Server\n\n```bash\n$ python manage.py runserver\nRunning on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n```\n\nNow go to your browser and point it towards `http://127.0.0.1:5000`\n\n# Docker\n\nRunning ExplainToMe via the [official Docker image](https://hub.docker.com/r/jjangsangy/explaintome/)\nis an easy way to start a server if you don't want to install python.\n\nWe assume here you have already installed Docker for your system.\n\nIf you are getting started on OS X, the [Docker toolbox](https://",
    "url": "https://github.com/jjangsangy/ExplainToMe",
    "last_updated": "2025-06-17T19:19:49+00:00"
  },
  {
    "full_name": "orchest/orchest",
    "name": "orchest",
    "description": "Build data pipelines, the easy way 🛠️",
    "language": "TypeScript",
    "topics": [
      "data-science",
      "machine-learning",
      "pipelines",
      "ide",
      "jupyter",
      "cloud",
      "self-hosted",
      "jupyterlab",
      "notebooks",
      "docker",
      "python",
      "data-pipelines",
      "orchest",
      "deployment",
      "kubernetes",
      "airflow",
      "dag",
      "etl",
      "etl-pipeline"
    ],
    "readme": "<p align=\"center\">\n<a href=\"https://orchest.io\">\n  <img src=\"docs/source/img/logo.png\" width=\"350px\" />\n</a>\n</p>\n\n<p align=\"center\">\n<a href=https://orchest.readthedocs.io/en/stable><img src=\"https://readthedocs.org/projects/orchest/badge/?version=stable&style=flat\"></a>\n</p>\n\n<p align=center><i>Notice: we’re no longer actively developing Orchest. We could not find a way to make building a workflow orchestrator commercially viable. Check out Apache Airflow for a robust workflow solution.</i></p>\n\n\n## Build data pipelines, the easy way 🙌\n\nNo frameworks. No YAML. Just write your data processing code directly in **Python**, **R** or\n**Julia**.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://user-images.githubusercontent.com/1309307/191785568-ce4857c3-e71f-4b71-84ce-dfa5d65a98f9.gif\">\n</p>\n\n<p align=\"center\">\n  <i>💡 Watch the <a target=\"_blank\" href=\"https://vimeo.com/764866337\">full narrated video</a> to learn more about building data pipelines in Orchest.</i>\n </p>\n\n> **Note**: Orchest is in **beta**.\n\n## Features\n\n- **Visually construct pipelines** through our user-friendly UI\n- **Code in Notebooks** and scripts\n  ([quickstart](https://docs.orchest.io/en/stable/getting_started/quickstart.html))\n- Run any subset of a pipelines directly or periodically\n  ([jobs](https://docs.orchest.io/en/stable/fundamentals/jobs.html))\n- Easily define your dependencies to run on **any machine**\n  ([environments](https://docs.orchest.io/en/stable/fundamentals/environments.html))\n- Spin up services whose lifetime spans across the entire pipeline run\n  ([services](https://docs.orchest.io/en/stable/fundamentals/services.html))\n- Version your projects using git\n  ([projects](https://docs.orchest.io/en/stable/fundamentals/projects.html))\n\n**When to use Orchest?** Read it in the\n[docs](https://docs.orchest.io/en/stable/getting_started/when_to_use_orchest.html).\n\n👉 Get started with our\n[quickstart](https://docs.orchest.io/en/stable/getting_started/quickstart.html) tutorial or have a loo",
    "url": "https://github.com/orchest/orchest",
    "last_updated": "2025-08-31T15:48:11+00:00"
  },
  {
    "full_name": "wikimedia-research/memo",
    "name": "memo",
    "description": "An analysis of referer traffic in January-August 2015",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/wikimedia-research/memo",
    "last_updated": "2015-08-13T22:28:56+00:00"
  },
  {
    "full_name": "hrbrmstr/madhttr",
    "name": "madhttr",
    "description": "🎩Tidy Helper Methods for Many Types of Unkempt Internet Metadata and Content",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "httr",
      "openssl",
      "har",
      "web-scraping"
    ],
    "readme": "\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/madhttr.svg?branch=master)](https://travis-ci.org/hrbrmstr/madhttr)\n[![Coverage\nStatus](https://codecov.io/gh/hrbrmstr/madhttr/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/madhttr)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/madhttr)](https://cran.r-project.org/package=madhttr)\n\n# madhttr\n\nTidy Helper Methods for Many Types of Unkempt Internet Metadata and\nContent\n\n## Description\n\nThe ‘httr’, ‘openssl’, and ‘HARtools’ packages provide methods to\nretrieve rich metadata and content from internet hosts but their return\nobjects are quite unkempt. Methods are provided to turn these objects\ninto tidy data frames along with other useful helper methods which\naugment functionality in these packages.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `download_file`: Download file from the Internet (cache-aware)\n  - `download_ssl_cert`: Doewnload X.509 certificates\n  - `GET`: GET a URL\n  - `nslookup`: Lookup a hostname\n  - `POST`: POST file to a server\n  - `read_har`: Read HAR objects\n  - `tidy_cert`: Turn an openssl downloaded SSL certificate into a tidy\n    data frame\n  - `tidy_har`: Tidy HAR entries\n  - `tidy_response`: Turn an httr response object into a tidy data frame\n  - `write_har`: Write HAR objects\n\n## Installation\n\n``` r\ninstall.packages(\"madhttr\", repos = \"https://cinc.rud.is\")\n# or\ndevtools::install_git(\"https://git.rud.is/hrbrmstr/madhttr.git\")\n# or\ndevtools::install_git(\"https://git.sr.ht/~hrbrmstr/madhttr\")\n# or\ndevtools::install_gitlab(\"hrbrmstr/madhttr\")\n# or\ndevtools::install_bitbucket(\"hrbrmstr/madhttr\")\n# or\ndevtools::install_github(\"hrbrmstr/madhttr\")\n```\n\n## Usage\n\n``` r\nlibrary(madhttr)\nlibrary(tibble) # for printing\n\n# current version\npackageVersion(\"madhttr\")\n## [1] '0.1.0'\n```\n\n### Certifiable\n\n``` r\ntidy_cert(download_ssl_cert(\"r-project.org\"))\n## # A tibble: 4 x 9\n##   subject               issuer               algorithm   signature ",
    "url": "https://github.com/hrbrmstr/madhttr",
    "last_updated": "2025-03-22T10:55:28+00:00"
  },
  {
    "full_name": "gojiplus/blogsky",
    "name": "blogsky",
    "description": "Blogspot to bsky Github Action",
    "language": "Python",
    "topics": [],
    "readme": "# BlogSky. Blogspot Atom → Bluesky\n\n**A GitHub Action** to automatically fetch the latest post from a Blogspot Atom/RSS feed and publish it to Bluesky Social every morning.\n\n## Features\n- Fetches the newest entry from your Blogspot feed.\n- Posts the title and link to Bluesky via the official `atproto` Python client.\n- Configurable feed URL, schedule, and post content template.\n\n## Usage\n1. Copy this action into your repository under `.github/actions/post-blogspot-to-bsky/`.\n2. Add the following workflow in `.github/workflows/post-blogspot.yml`:\n\n```yaml\nname: Post Blogspot to Bluesky\n\non:\n  schedule:\n    # Every day at 7 AM Pacific Time\n    - cron: '0 14 * * *'\n  workflow_dispatch:\n\njobs:\n  post:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install feedparser atproto grapheme\n\n      - name: Post latest blogspot entry to Bluesky\n        uses: ./github/actions/post-blogspot-to-bsky\n        with:\n          feed-url: ${{ inputs.feed-url }}\n        env:\n          BSKY_HANDLE: ${{ secrets.BSKY_HANDLE }}\n          BSKY_PASSWORD: ${{ secrets.BSKY_PASSWORD }}\n```\n\n## Action Inputs\n| Input     | Required | Description                                        |\n|-----------|:--------:|----------------------------------------------------|\n| `feed-url`|   ✅     | The URL of your Blogspot Atom/RSS feed.            |\n\n## Environment Variables (via `env:`)\n- `BSKY_HANDLE` **(required)** — Your Bluesky handle (username).\n- `BSKY_PASSWORD` **(required)** — Your Bluesky password.\n\n## How It Works\n1. The action fetches the feed specified by `feed-url`.\n2. Parses the Atom entries and identifies the newest post.\n3. Formats a short message: **title** + link.\n4. Logs in to Bluesky and creates a post using the `atproto` client.\n\n## Customization",
    "url": "https://github.com/gojiplus/blogsky",
    "last_updated": "2025-04-29T19:15:11+00:00"
  },
  {
    "full_name": "thomasp85/ggraph",
    "name": "ggraph",
    "description": "Grammar of Graph Graphics",
    "language": "R",
    "topics": [
      "graph-visualization",
      "ggplot2",
      "visualization",
      "network-visualization",
      "r",
      "ggplot-extension"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# ggraph <img src=\"man/figures/logo.png\" align=\"right\" />\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/thomasp85/ggraph/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/thomasp85/ggraph/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Release_Badge](http://www.r-pkg.org/badges/version-ago/ggraph)](https://CRAN.R-project.org/package=ggraph)\n[![CRAN_Download_Badge](http://cranlogs.r-pkg.org/badges/ggraph)](https://CRAN.R-project.org/package=ggraph)\n[![Codecov test\ncoverage](https://codecov.io/gh/thomasp85/ggraph/branch/main/graph/badge.svg)](https://app.codecov.io/gh/thomasp85/ggraph?branch=main)\n<!-- badges: end -->\n\n*/dʒiː.dʒɪˈrɑːf/* (or g-giraffe)\n\n## A grammar of graphics for relational data\n\nggraph is an extension of [`ggplot2`](https://ggplot2.tidyverse.org)\naimed at supporting relational data structures such as networks, graphs,\nand trees. While it builds upon the foundation of `ggplot2` and its API\nit comes with its own self-contained set of geoms, facets, etc., as well\nas adding the concept of *layouts* to the grammar.\n\n### An example\n\n``` r\nlibrary(ggraph)\n#> Loading required package: ggplot2\nlibrary(tidygraph)\n#> \n#> Attaching package: 'tidygraph'\n#> The following object is masked from 'package:stats':\n#> \n#>     filter\n\n# Create graph of highschool friendships\ngraph <- as_tbl_graph(highschool) |> \n    mutate(Popularity = centrality_degree(mode = 'in'))\n\n# plot using ggraph\nggraph(graph, layout = 'kk') + \n    geom_edge_fan(aes(alpha = after_stat(index)), show.legend = FALSE) + \n    geom_node_point(aes(size = Popularity)) + \n    facet_edges(~year) + \n    theme_graph(foreground = 'steelblue', fg_text_colour = 'white')\n```\n\n![](man/figures/README-unnamed-chunk-2-1.png)<!-- -->\n\n### The core concepts\n\n`ggraph` builds upon three core concepts that are quite easy to\nunderstand:\n\n1.  [**The\n    Layout**](https://www.data-imaginist.com/2017/ggraph-introduction-layout",
    "url": "https://github.com/thomasp85/ggraph",
    "last_updated": "2025-09-02T05:19:22+00:00"
  },
  {
    "full_name": "lancedb/lance",
    "name": "lance",
    "description": "Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, and PyTorch with more integrations coming..",
    "language": "Rust",
    "topics": [
      "machine-learning",
      "computer-vision",
      "data-format",
      "deep-learning",
      "python",
      "apache-arrow",
      "duckdb",
      "mlops",
      "data-analysis",
      "data-analytics",
      "data-science",
      "dataops",
      "data-centric",
      "embeddings",
      "rust",
      "llms"
    ],
    "readme": "<div align=\"center\">\n<p align=\"center\">\n\n<img width=\"257\" alt=\"Lance Logo\" src=\"https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png\">\n\n**Modern columnar data format for ML. Convert from Parquet in 2-lines of code for 100x faster random access, zero-cost schema evolution, rich secondary indices, versioning, and more.<br/>**\n**Compatible with Pandas, DuckDB, Polars, Pyarrow, and Ray with more integrations on the way.**\n\n<a href=\"https://lancedb.github.io/lance/\">Documentation</a> •\n<a href=\"https://blog.lancedb.com/\">Blog</a> •\n<a href=\"https://discord.gg/zMM32dvNtd\">Discord</a> •\n<a href=\"https://x.com/lancedb\">X</a>\n\n[CI]: https://github.com/lancedb/lance/actions/workflows/rust.yml\n[CI Badge]: https://github.com/lancedb/lance/actions/workflows/rust.yml/badge.svg\n[Docs]: https://lancedb.github.io/lance/\n[Docs Badge]: https://img.shields.io/badge/docs-passing-brightgreen\n[crates.io]: https://crates.io/crates/lance\n[crates.io badge]: https://img.shields.io/crates/v/lance.svg\n[Python versions]: https://pypi.org/project/pylance/\n[Python versions badge]: https://img.shields.io/pypi/pyversions/pylance\n\n[![CI Badge]][CI]\n[![Docs Badge]][Docs]\n[![crates.io badge]][crates.io]\n[![Python versions badge]][Python versions]\n\n</p>\n</div>\n\n<hr />\n\nLance is a modern columnar data format that is optimized for ML workflows and datasets. Lance is perfect for:\n\n1. Building search engines and feature stores.\n2. Large-scale ML training requiring high performance IO and shuffles.\n3. Storing, querying, and inspecting deeply nested data for robotics or large blobs like images, point clouds, and more.\n\nThe key features of Lance include:\n\n* **High-performance random access:** 100x faster than Parquet without sacrificing scan performance.\n\n* **Vector search:** find nearest neighbors in milliseconds and combine OLAP-queries with vector search.\n\n* **Zero-copy, automatic versioning:** manage versions of your data without needing extra infrastructure.\n",
    "url": "https://github.com/lancedb/lance",
    "last_updated": "2025-09-02T09:11:27+00:00"
  },
  {
    "full_name": "gojiplus/lost_years",
    "name": "lost_years",
    "description": "Join to Human Life Table Data ",
    "language": "Jupyter Notebook",
    "topics": [
      "life-tables",
      "coronavirus",
      "mortality",
      "covid-19",
      "who",
      "hld",
      "ssa"
    ],
    "readme": "Lost Years: Expected Number of Years Lost\n-----------------------------------------\n\n.. image:: https://img.shields.io/pypi/v/lost_years.svg\n    :target: https://pypi.python.org/pypi/lost_years\n.. image:: https://readthedocs.org/projects/lost-years/badge/?version=latest\n    :target: http://lost-years.readthedocs.io/en/latest/?badge=latest\n.. image:: https://static.pepy.tech/badge/lost_years\n    :target: https://pepy.tech/project/lost-years\n\nThe mortality rate is puzzling to mortals. A better number is the expected number of years lost. (A yet better number would be quality-adjusted years lost.) To make it easier to calculate the expected years lost, `lost_years` provides a convenient way to join to the `SSA actuarial data <https://www.ssa.gov/oact/STATS/table4c6.html>`__, `HLD data <https://www.lifetable.de/cgi-bin/data.php>`__, and `WHO life table data <https://apps.who.int/gho/data/node.main.LIFECOUNTRY?lang=en>`__.\n\nThe package exposes three functions: ``lost_years_ssa``, ``lost_years_hld``, and ``lost_years_who``:\n\n* ``lost_years_ssa``: Joins to the final SSA dataset stored `here <https://github.com/gojiplus/lost_years/blob/master/lost_years/data/ssa.csv>`__. The data are from `SSA actuarial data <https://www.ssa.gov/oact/STATS/table4c6.html>`__\n\n    * **Inputs:**\n\n        * The function expects 4 inputs: ``age, sex, and year``. If any of the inputs are not available, it errors out.\n        * **Closest Year and Age Matching** By default, we match to the closest year; The year we match to is stored as ``ssa_year.`` Same for age. If the age provided is not available, we match to the closest age and store the matched age in the ``ssa_age`` column.\n\n    * **What the function does**\n\n        * While ``lost_years_ssa`` is technically only applicable for the US, we make it so that the function ignores the ``country`` argument and gives you the counterfactual of what the expected years lost would be if the person who died (or is predicted to die) was in the US. (You can",
    "url": "https://github.com/gojiplus/lost_years",
    "last_updated": "2025-03-14T02:02:00+00:00"
  },
  {
    "full_name": "ropensci/elastic",
    "name": "elastic",
    "description": "R client for the Elasticsearch HTTP API",
    "language": "R",
    "topics": [
      "elasticsearch",
      "http",
      "database",
      "database-wrapper",
      "json",
      "rstats",
      "r",
      "r-package",
      "data-science",
      "etl"
    ],
    "readme": "elastic\n=======\n\n\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![R-check](https://github.com/ropensci/elastic/workflows/R-check/badge.svg)](https://github.com/ropensci/elastic/actions?query=workflow%3AR-check)\n[![cran checks](https://cranchecks.info/badges/worst/elastic)](https://cranchecks.info/pkgs/elastic)\n[![rstudio mirror downloads](https://cranlogs.r-pkg.org/badges/elastic?color=E664A4)](https://github.com/r-hub/cranlogs.app)\n[![cran version](https://www.r-pkg.org/badges/version/elastic)](https://cran.r-project.org/package=elastic)\n<!-- [![codecov.io](https://codecov.io/github/ropensci/elastic/coverage.svg?branch=master)](https://codecov.io/github/ropensci/elastic?branch=master) -->\n\n**A general purpose R interface to [Elasticsearch](https://www.elastic.co/elasticsearch/)**\n\n\n## Elasticsearch info\n\n* [Elasticsearch home page](https://www.elastic.co/elasticsearch/)\n* [API docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)\n\n\n## Compatibility\n\nThis client is developed following the latest stable releases, currently `v7.10.0`. It is generally compatible with older versions of Elasticsearch. Unlike the [Python client](https://github.com/elastic/elasticsearch-py#compatibility), we try to keep as much compatibility as possible within a single version of this client, as that's an easier setup in R world.\n\n## Security\n\nYou're fine running ES locally on your machine, but be careful just throwing up ES on a server with a public IP address - make sure to think about security.\n\n* Elastic has paid products - but probably only applicable to enterprise users\n* DIY security - there are a variety of techniques for securing your Elasticsearch installation. A number of resources are collected in a [blog post](https://recology.info/2015/02/secure-elasticsearch/) - tools include putting your ES beh",
    "url": "https://github.com/ropensci/elastic",
    "last_updated": "2025-09-01T02:03:16+00:00"
  },
  {
    "full_name": "hughperkins/pytorch",
    "name": "pytorch",
    "description": "Python wrappers for torch and lua",
    "language": "Python",
    "topics": [],
    "readme": "# pytorch\nWrappers to use torch and lua from python\n\n# What is pytorch?\n\n- create torch tensors, call operations on them\n- instantiate `nn` network modules, train them, make predictions\n- create your own lua class, call methods on that\n\n## Create torch tensors\n\n```\nimport PyTorch\na = PyTorch.FloatTensor(2,3).uniform()\na += 3\nprint('a', a)\nprint('a.sum()', a.sum())\n```\n\n## Instantiate nn network modules\n\n```\nimport PyTorch\nfrom PyTorchAug import nn\n\nnet = nn.Sequential()\nnet.add(nn.SpatialConvolutionMM(1, 16, 5, 5, 1, 1, 2, 2))\nnet.add(nn.ReLU())\nnet.add(nn.SpatialMaxPooling(3, 3, 3, 3))\n\nnet.add(nn.SpatialConvolutionMM(16, 32, 3, 3, 1, 1, 1, 1))\nnet.add(nn.ReLU())\nnet.add(nn.SpatialMaxPooling(2, 2, 2, 2))\n\nnet.add(nn.Reshape(32 * 4 * 4))\nnet.add(nn.Linear(32 * 4 * 4, 150))\nnet.add(nn.Tanh())\nnet.add(nn.Linear(150, 10))\nnet.add(nn.LogSoftMax())\nnet.float()\n\ncrit = nn.ClassNLLCriterion()\ncrit.float()\n\nnet.zeroGradParameters()\ninput = PyTorch.FloatTensor(5, 1, 28, 28).uniform()\nlabels = PyTorch.ByteTensor(5).geometric(0.9).icmin(10)\noutput = net.forward(input)\nloss = crit.forward(output, labels)\ngradOutput = crit.backward(output, labels)\ngradInput = net.backward(input, gradOutput)\nnet.updateParameters(0.02)\n```\n\n# Write your own lua class, call methods on it\n\nExample lua class:\n```\nrequire 'torch'\nrequire 'nn'\n\nlocal TorchModel = torch.class('TorchModel')\n\nfunction TorchModel:__init(backend, imageSize, numClasses)\n  self:buildModel(backend, imageSize, numClasses)\n  self.imageSize = imageSize\n  self.numClasses = numClasses\n  self.backend = backend\nend\n\nfunction TorchModel:buildModel(backend, imageSize, numClasses)\n  self.net = nn.Sequential()\n  local net = self.net\n\n  net:add(nn.SpatialConvolutionMM(1, 16, 5, 5, 1, 1, 2, 2))\n  net:add(nn.ReLU())\n  net:add(nn.SpatialMaxPooling(3, 3, 3, 3))\n  net:add(nn.SpatialConvolutionMM(16, 32, 3, 3, 1, 1, 1, 1))\n  net:add(nn.ReLU())\n  net:add(nn.SpatialMaxPooling(2, 2, 2, 2))\n  net:add(nn.Reshape(32 * 4 * 4))\n  net:add(nn.Linear(32 *",
    "url": "https://github.com/hughperkins/pytorch",
    "last_updated": "2025-08-20T14:58:40+00:00"
  },
  {
    "full_name": "kbroman/PearsonData",
    "name": "PearsonData",
    "description": "Data extracted from Pearson and Lee (1903) On the Laws of Inheritance in Man: I. Inheritance of Physical Characters",
    "language": "R",
    "topics": [],
    "readme": "# Pearson and Lee (1903) data\n\nData extracted from Karl Pearson and Alice Lee (1903)\n[On the laws of inheritance in man: I. Inheritance of physical characters](http://www.jstor.org/stable/2331507).\nBiometrika 2: 357-462\n\nThe `.txt` files are transcribed versions of various tables from the\npaper.\n\nThe `.R` file contains a function, `interp_pearson()` for converting\nthe data to interpolated (and slightly randomized) values for pairs of\nindividuals. It reads a `.txt` file and writes a `.csv` file.\n\nThe `.csv` files are files derived from `interp_pearson()`.\n",
    "url": "https://github.com/kbroman/PearsonData",
    "last_updated": "2019-07-24T18:56:41+00:00"
  },
  {
    "full_name": "fhamborg/newstsc",
    "name": "newstsc",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# Announcement\nWe recommend using our newer, improved [model and dataset](https://github.com/fhamborg/NewsMTSC), which have various advantages, most importantly: \n\n1. larger dataset (11k sentences rather than 3k),\n2. improved labels in the dataset, e.g., including more realistic cases of sentiment in news articles, such as through subtle word choice, and\n3. improved model that achieves better classification performance.\n\nThe repository you are currently viewing is not maintained any longer.\n\n# [Click here to go to NewsMTSC](https://github.com/fhamborg/NewsMTSC)\n\n# Target-dependent Sentiment Classification in News Articles (NewsTSC)\nCode and dataset for our [paper](https://www.gipp.com/wp-content/papercite-data/pdf/hamborg2021.pdf) \"Towards Target-dependent Sentiment Classification in News Articles\" accepted at the iConference 2021.\n\nThis readme consists of two main parts: installation of NewsTSC and how to use it. For both, there are instructions describing two use cases of the system: \n\n1. How to easily classify your own data (using our best performing model).\n2. How to train your own models (using the NewsTSC dataset or any other dataset).\n\nIf you only want to download the dataset, please click [here](https://github.com/fhamborg/newstsc/raw/master/newstsc-dataset.zip).\n\n# Installation\nTo setup the required environment, we use Anaconda. If you haven't installed Anaconda yet, first follow their [installation instructions](https://docs.anaconda.com/anaconda/install/). NewsTSC was tested on MacOS and Ubuntu. Other OS may work, too. Let us know :-)\n\n## Core installation\n```bash\nconda create --yes -n ctsacuda python=3.7\nconda activate ctsacuda\n\n# choose either of both: the first is recommended if you have an NVIDIA GPU that supports CUDA\n# with CUDA 10.0\nconda install --yes pytorch torchvision cudatoolkit=10.1 -c pytorch \n# without CUDA (calculations will be performed on your CPU, not recommended for training your own model but should be okay if you only classify sentim",
    "url": "https://github.com/fhamborg/newstsc",
    "last_updated": "2023-06-03T18:16:59+00:00"
  },
  {
    "full_name": "gojiplus/get-weather-data",
    "name": "get-weather-data",
    "description": "Get weather data for a list of zip codes for a range of dates",
    "language": "Python",
    "topics": [
      "weather-data",
      "noaa"
    ],
    "readme": "Get Weather Data\n~~~~~~~~~~~~~~~~\n.. image:: https://travis-ci.com/gojiplus/get-weather-data.svg?branch=master\n    :target: https://travis-ci.com/github/gojiplus/get-weather-data\n.. image:: https://ci.appveyor.com/api/projects/status/07gx3u2opf5i4dor?svg=true\n    :target: https://ci.appveyor.com/project/soodoku/get-weather-data/\n.. image:: https://img.shields.io/pypi/v/get-weather-data.svg?maxAge=2592000\n    :target: https://pypi.python.org/pypi/get-weather-data\n.. image:: https://pepy.tech/badge/get-weather-data\n   :target: https://pepy.tech/project/get-weather-data\n\nScripts for finding out the weather in a particular zip code on a\nparticular day (or a set of dates). You can also find weather stations\nnear a zip code, and vice-versa.\n\n\nBackground\n^^^^^^^^^^\n\nFrom `Bad Weather: Getting weather data by zip and\ndate <http://gbytes.gsood.com/2013/06/27/bad-weather-getting-weather-data-by-zip-and-date/>`__:\n\nSome brief ground clearing before we begin. Weather data come from\nweather stations, which can belong to any of the five or more\n'networks,' each of which collect somewhat different data, sometimes\nlabel the same data differently, and have different reporting protocols.\nThe only geographic information that typically comes with weather\nstations is their latitude and longitude. By “weather”, we may mean\ntemperature, rain, wind, snow, etc. and we may want data on these for\nevery second, minute, hour, day, month etc. It is good to keep in mind\nthat not all weather stations report data for all units of time, and\nthere can be a fair bit of missing data. Getting data at coarse time\nunits like day, month, etc. typically involves making some decisions\nabout what particular statistic is the most useful. So for instance, you\nmay want, minimum and maximum (for daily temperature), or totals (for\nrainfall and snow). With that primer, let’s begin.\n\nWe begin with what not to do. Do not use the `NOAA web\nservice <http://www.ncdc.noaa.gov/cdo-web/webservices>`__. The API\nprovides a s",
    "url": "https://github.com/gojiplus/get-weather-data",
    "last_updated": "2025-05-13T23:05:42+00:00"
  },
  {
    "full_name": "justinelliotmeyers/India_Locality_Polygons",
    "name": "India_Locality_Polygons",
    "description": "",
    "language": "",
    "topics": [],
    "readme": "",
    "url": "https://github.com/justinelliotmeyers/India_Locality_Polygons",
    "last_updated": "2025-01-20T15:45:59+00:00"
  },
  {
    "full_name": "apoorvalal/llm-ocr-tex",
    "name": "llm-ocr-tex",
    "description": "ocr using llm to typeset old books",
    "language": "TeX",
    "topics": [],
    "readme": "# `llm-ocr-tex`: convert old books/papers into LaTeX\n\nPfanzagl (1982) is a classic text in semiparametric statistics but is difficult to read due to its archaic typesetting. This repo processes the text using OCR (via gemini 2.0) and converts it into latex, which can then be post-processed into a reasonable output format.\n\nThis can be adapted to OCR and typeset your own old books/papers. The process is as follows:\n\n1. Scan the text using a high-resolution scanner (300 dpi or higher), preferably in grayscale, output to pdf.\n2. Use `0_extract.py` to convert the pdf into images (this populates the `pdf_images` directory).\n  - python requirements in `requirements.txt`\n  - also requires GhostScript installed (e.g. via `brew install ghostscript` on MacOS, or `apt install ghostscript` on Ubuntu)\n3. Use `1_ocr.py` to run OCR on the images (this populates the `raw_tex` directory).\n  - uses `google-genai` to call gemini 2.0. Requires `GEMINI_API_KEY` in your environment variables. You can get this from [google ai studio](https://aistudio.google.com/prompts/new_chat)\n4. Tinker with the output in `raw_tex` to get the text into a readable\n   format. This is the most time-consuming part of the process, but is\n   hard to automate. An example of a successful output is shown below, which is generated from page 51 of the text and is `1_tex_paper.tex` in the `raw_tex` directory.\n\n![](demo_pg_51.png)\n\n",
    "url": "https://github.com/apoorvalal/llm-ocr-tex",
    "last_updated": "2025-04-07T23:11:43+00:00"
  },
  {
    "full_name": "nyukat/breast_cancer_classifier",
    "name": "breast_cancer_classifier",
    "description": "Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening",
    "language": "Jupyter Notebook",
    "topics": [
      "pytorch",
      "breast-cancer",
      "classification",
      "pretrained-models",
      "deep-learning",
      "neural-network",
      "breast-cancer-diagnosis",
      "medical-imaging",
      "medical-image-analysis",
      "tensorflow"
    ],
    "readme": "# DISCLAIMER: this model is not used clinically at NYU Langone Health. As it was created in 2019, its accuracy is far behind the strongest model we trained since then. If you are interested in discussing our recent models in any capacity, please email [Krzysztof J. Geras](mailto:krzysztof.geras@nyulangone.org).\n\n# Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening\n\n## Introduction\nThis is an implementation of the model used for breast cancer classification as described in our paper [Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening](https://ieeexplore.ieee.org/document/8861376). The implementation allows users to get breast cancer predictions by applying one of our pretrained models: a model which takes images as input (*image-only*) and a model which takes images and heatmaps as input (*image-and-heatmaps*). \n\n* Input images: 2 CC view mammography images of size 2677x1942 and 2 MLO view mammography images of size 2974x1748. Each image is saved as 16-bit png file and gets standardized separately before being fed to the models.\n* Input heatmaps: output of the patch classifier constructed to be the same size as its corresponding mammogram. Two heatmaps are generated for each mammogram, one for benign and one for malignant category. The value of each pixel in both of them is between 0 and 1.\n* Output: 2 predictions for each breast, probability of benign and malignant findings: `left_benign`, `right_benign`, `left_malignant`, and `right_malignant`.\n\nBoth models act on screening mammography exams with four standard views (L-CC, R-CC, L-MLO, R-MLO). As a part of this repository, we provide 4 sample exams (in `sample_data/images` directory and exam list stored in `sample_data/exam_list_before_cropping.pkl`). Heatmap generation model and cancer classification models are implemented in PyTorch. \n\n**Update (2019/10/26)**: [Our paper](https://ieeexplore.ieee.org/document/8861376) will be published in the IEEE Trans",
    "url": "https://github.com/nyukat/breast_cancer_classifier",
    "last_updated": "2025-08-21T21:58:41+00:00"
  },
  {
    "full_name": "marcotcr/lime",
    "name": "lime",
    "description": "Lime: Explaining the predictions of any machine learning classifier",
    "language": "JavaScript",
    "topics": [],
    "readme": "# lime\n\n[![Build Status](https://travis-ci.org/marcotcr/lime.svg?branch=master)](https://travis-ci.org/marcotcr/lime)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/marcotcr/lime/master)\n\nThis project is about explaining what machine learning classifiers (or models) are doing.\nAt the moment, we support explaining individual predictions for text classifiers or classifiers that act on tables (numpy arrays of numerical or categorical data) or images, with a package called lime (short for local interpretable model-agnostic explanations).\nLime is based on the work presented in [this paper](https://arxiv.org/abs/1602.04938) ([bibtex here for citation](https://github.com/marcotcr/lime/blob/master/citation.bib)). Here is a link to the promo video:\n\n<a href=\"https://www.youtube.com/watch?v=hUnRCxnydCc\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/video_screenshot.png\" width=\"450\" alt=\"KDD promo video\"/></a>\n\nOur plan is to add more packages that help users understand and interact meaningfully with machine learning.\n\nLime is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a probability for each class. Support for scikit-learn classifiers is built-in.\n\n## Installation\n\nThe lime package is on [PyPI](https://pypi.python.org/pypi/lime). Simply run:\n\n```sh\npip install lime\n```\n\nOr clone the repository and run:\n\n```sh\npip install .\n```\n\nWe dropped python2 support in `0.2.0`, `0.1.1.37` was the last version before that.\n\n## Screenshots\n\nBelow are some screenshots of lime explanations. These are generated in html, and can be easily produced and embedded in ipython notebooks. We also support visualizations using matplotlib, although they don't look as nice as these ones.\n\n#### Two class case, text\n\nNegative (blue) words indicate atheism, while positive (orange) words indicate chri",
    "url": "https://github.com/marcotcr/lime",
    "last_updated": "2025-09-01T15:49:23+00:00"
  },
  {
    "full_name": "egnha/valaddin",
    "name": "valaddin",
    "description": "Functional input validation to make R functions more readable and robust",
    "language": "R",
    "topics": [
      "input-validation",
      "data-validation",
      "type-safety",
      "r"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n**Development has moved to [rong](https://github.com/egnha/rong)**\n\n# valaddin\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/egnha/valaddin/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/egnha/valaddin/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/valaddin)](https://cran.r-project.org/package=valaddin)\n[![stability-frozen](https://img.shields.io/badge/stability-frozen-blue.svg)](https://github.com/emersion/stability-badges#frozen)\n\n<!-- badges: end -->\n\nDealing with invalid function inputs is a chronic pain for R users,\ngiven R’s weakly typed nature. *valaddin* provides pain relief—a\nlightweight R package that enables you to transform an existing function\ninto a function with input validation checks, *in situ*, in a manner\nsuitable for both programmatic use and interactive sessions.\n\n## Installation\n\nInstall from [CRAN](https://cran.r-project.org/package=valaddin)\n\n``` r\ninstall.packages(\"valaddin\")\n```\n\nor get the development version from GitHub using the\n[devtools](https://github.com/r-lib/devtools) package\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"egnha/valaddin\", ref = \"dev\", build_vignettes = TRUE)\n```\n\n## Why use valaddin\n\n### Fail fast—save time, spare confusion\n\nYou can be more confident your function works correctly, when you know\nits arguments are well-behaved. But when they aren’t, its better to stop\nimmediately and bring them into line, than to let them pass and wreak\nhavoc, exposing yourself to breakages or, worse, silently incorrect\nresults. Validating the inputs of your functions is good defensive\nprogramming practice.\n\nSuppose you have a function `secant()`\n\n``` r\nsecant <- function(f, x, dx) (f(x + dx) - f(x)) / dx\n```\n\nand you want to ensure that the user (or some code) supplies numerical\ninputs for `x` and `dx`. Typically, you’d rewrite `secant()` so that it\nstops if this c",
    "url": "https://github.com/egnha/valaddin",
    "last_updated": "2025-02-25T14:13:06+00:00"
  },
  {
    "full_name": "washingtonpost/data-homicides",
    "name": "data-homicides",
    "description": "The Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities.",
    "language": "",
    "topics": [],
    "readme": "[This data is published under an [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license](https://creativecommons.org/licenses/by-nc-sa/4.0/)]\n\n# How The Post mapped unsolved murders\n\n## Data Collection\n\nThe Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities.\n\nThe data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.\n\nReporters received data in many formats, including paper, and worked for months to clean and standardize it, comparing homicide counts and aggregate closure rates with FBI data to ensure the records were as accurate as possible.\n\nIn some cases, departments provided only partial information about the homicides, so reporters consulted public records, including death certificates, court records and medical examiner reports, to fill in the gaps. The data is more specific than the federal homicide data gathered annually by the FBI from police agencies nationwide.\n\nThe Post mapped each homicide, identifying arrest rates by geography in each city, sharing the analysis with the local police department prior to publication.\n\n## Definitions\n\nWhen possible, The Post followed definitions used in the FBI’s Uniform Crime Reporting Program. In that program, homicides include murder and non-negligent manslaughter but exclude suicides, accidents, justifiable homicides and deaths caused by negligence.\n\nThe Post considered a homicide to be closed by arrest when police reported that to be the case.\n\nCases were counted as closed without arrest if they were reported by police to be “exceptionally cleared.” Those are cases in which there is sufficient evidence but an arrest is not possible, for example, if the suspect has died.\n\nAll other cases were classified as having no arrest.\n\nMass shootings or terrorist attacks in the cities of Las Vegas, Dallas, the District and San Bernardino, Calif",
    "url": "https://github.com/washingtonpost/data-homicides",
    "last_updated": "2025-06-24T08:11:26+00:00"
  },
  {
    "full_name": "PsyTeachR/webex",
    "name": "webex",
    "description": "RMarkdown tools for creating self-guided web exercises",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<link href=\"inst/reports/default/webex.css\" rel=\"stylesheet\" />\n\n# The `{webexercises}` package\n\n<img src=\"https://raw.githubusercontent.com/PsyTeachR/misc/master/img/webexercises.001.png\" style=\"float:right; max-width:280px; width: 25%;\" />\n\nThe goal of `{webexercises}` is to enable instructors to easily create\ninteractive web pages that students can use in self-guided learning.\nAlthough `{webexercises}` has fewer features than RStudio’s\n[learnr](https://rstudio.github.io/learnr/) package, it is more\nlightweight: whereas `{learnr}` tutorials must be either hosted on a\nshiny server or run locally, `{webexercises}` creates standalone HTML\nfiles that require only a JavaScript-enabled browser. It is also\nextremely simple to use.\n\n## Installation\n\nYou can install `{webexercises}` from CRAN using:\n\n``` r\ninstall.packages(\"webexercises\")\n```\n\nYou can install the development version from\n[GitHub](https://github.com/PsyTeachR/webexercises) with:\n\n``` r\ndevtools::install_github(\"psyteachr/webexercises\")\n```\n\n## Creating interactive widgets with inline code\n\nThe webexercises package provides functions that create HTML widgets\nusing [inline R\ncode](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf).\nThese functions are:\n\n| function                | widget         | description                    |\n| :---------------------- | :------------- | :----------------------------- |\n| `fitb()`                | text box       | fill-in-the-blank question     |\n| `mcq()`                 | pull-down menu | multiple choice question       |\n| `torf()`                | pull-down menu | TRUE or FALSE question         |\n| `longmcq()`             | radio buttons  | MCQs with long answers         |\n| `hide()` and `unhide()` | button         | solution revealed when clicked |\n| `total_correct()`       | text           | updating total correct         |\n\nThe appearance of the text box and pull-down menu widg",
    "url": "https://github.com/PsyTeachR/webex",
    "last_updated": "2025-04-18T07:36:38+00:00"
  },
  {
    "full_name": "MangoTheCat/franc",
    "name": "franc",
    "description": "Detect the Language of Text",
    "language": "R",
    "topics": [],
    "readme": "\n\n\n# franc\n\n> Detect the Language of Text\n\n[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![Linux Build Status](https://travis-ci.org/MangoTheCat/franc.svg?branch=master)](https://travis-ci.org/MangoTheCat/franc)\n[![Windows Build\nstatus](https://ci.appveyor.com/api/projects/status/github/mangothecat/franc?svg=true)](https://ci.appveyor.com/project/gaborcsardi/franc)\n[![](http://www.r-pkg.org/badges/version/franc)](http://www.r-pkg.org/pkg/franc)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/franc)](http://www.r-pkg.org/pkg/franc)\n\nFranc has no external dependencies and supports 310 languages; all\nlanguages spoken by more than one million speakers. Franc is a port\nof the JavaScript project of the same name, see\nhttps://github.com/wooorm/franc.\n\n## Installation\n\n\n```r\ndevtools::install_github(\"mangothecat/franc\")\n```\n\n## Usage\n\n\n```r\nlibrary(franc)\n```\n\nSimply supply the text, and franc detects its language:\n\n\n```r\nfranc(\"Alle menslike wesens word vry\")\n```\n\n```\n#> [1] \"afr\"\n```\n\n```r\nfranc(\"এটি একটি ভাষা একক IBM স্ক্রিপ্ট\")\n```\n\n```\n#> [1] \"ben\"\n```\n\n```r\nfranc(\"Alle mennesker er født frie og\")\n```\n\n```\n#> [1] \"nno\"\n```\n\n```r\nhead(franc_all(\"O Brasil caiu 26 posições\"))\n```\n\n```\n#>   language     score\n#> 1      por 1.0000000\n#> 2      src 0.8800937\n#> 3      glg 0.8702576\n#> 4      snn 0.8637002\n#> 5      bos 0.8168618\n#> 6      hrv 0.8103044\n```\n\n`und` is the `undefined` language, this is returned if the input is\ntoo short (shorter than 10 characters by default).\n\n\n```r\nfranc(\"the\")\n```\n\n```\n#> [1] \"und\"\n```\n\n```r\nfranc(\"the\", min_length = 3)\n```\n\n```\n#> [1] \"sco\"\n```\n\nYou can provide a whitelist or a blacklist:\n\n\n```r\nfranc_all(\"O Brasil caiu 26 posições\",\n    whitelist = c(\"por\", \"src\", \"glg\", \"spa\"))\n```\n\n```\n#>   language     score\n#> 1      por 1.0000000\n#> 2      src 0.8800937\n#> 3      glg",
    "url": "https://github.com/MangoTheCat/franc",
    "last_updated": "2024-11-04T13:36:49+00:00"
  },
  {
    "full_name": "ropensci/handlr",
    "name": "handlr",
    "description": "convert among citation formats",
    "language": "R",
    "topics": [
      "citations",
      "doi",
      "digital-object-identifier",
      "metadata",
      "bibtex",
      "citeproc",
      "crosscite",
      "ris",
      "r",
      "r-package",
      "rstats"
    ],
    "readme": "# handlr\n\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![codecov.io](https://codecov.io/github/ropensci/handlr/coverage.svg?branch=master)](https://app.codecov.io/github/ropensci/handlr?branch=master)\n[![rstudio mirror\ndownloads](https://cranlogs.r-pkg.org/badges/handlr)](https://github.com/r-hub/cranlogs.app)\n[![cran\nversion](https://www.r-pkg.org/badges/version/handlr)](https://cran.r-project.org/package=handlr)\n\na tool for converting among citation formats.\n\nheavily influenced by, and code ported from the Ruby gem `bolognese`\n\nsupported readers:\n\n-   citeproc\n-   ris\n-   bibtex\n-   codemeta\n-   cff\n\nsupported writers:\n\n-   citeproc\n-   ris\n-   bibtex\n-   schemaorg\n-   rdfxml\n-   codemeta\n-   cff\n\nnot supported yet, but plan to:\n\n-   crosscite\n\n## Installation\n\nstable version\n\n    install.packages(\"handlr\")\n\ndev version\n\n    remotes::install_github(\"ropensci/handlr\")\n\n    library(\"handlr\")\n\n## Meta\n\n-   Please [report any issues or\n    bugs](https://github.com/ropensci/handlr/issues).\n-   License: MIT\n-   Get citation information for `handlr` in R doing\n    `citation(package = 'handlr')`\n-   Please note that this package is released with a [Contributor Code\n    of Conduct](https://ropensci.org/code-of-conduct/). By contributing\n    to this project, you agree to abide by its terms.\n\n[![ropensci\\_footer](https://ropensci.org/public_images/github_footer.png)](https://ropensci.org)\n",
    "url": "https://github.com/ropensci/handlr",
    "last_updated": "2025-04-12T01:49:32+00:00"
  },
  {
    "full_name": "datamade/chicago-openelex",
    "name": "chicago-openelex",
    "description": "Chicago Open Election Scraper",
    "language": "Python",
    "topics": [],
    "readme": "# chicago-openelex\nChicago Open Election Scraper\n\nTo run:\n\n1. set up [datamade/openelections-core](https://github.com/datamade/openelections-core/)\n2. set up this repo\n  ```\n  git clone git@github.com:datamade/chicago-openelex.git\n  cd chicago-openelex\n  python setup.py develop\n  ```\n\n3. run the scraper\n  ```\n  openelex scrape --state=il --place=chicago\n  ```\n",
    "url": "https://github.com/datamade/chicago-openelex",
    "last_updated": "2016-11-07T23:03:13+00:00"
  },
  {
    "full_name": "notnews/rainbow",
    "name": "rainbow",
    "description": "Racial Diversity of News Coverage and the Newsroom",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "### Rainbow: Racial Diversity of People Referred to in the News and the Newsroom\n\nWe use the [Top News Data](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZNAKK6) that collates news articles from news feeds from ABC, CBS, CNN, NBC, LA Times, NBC, NYT, Politico, USAT, and WaPo, and [ethnicolr](https://github.com/appeler/ethnicolr) to estimate the racial diversity of the people mentioned in the news and the newsroom.\n\nData suggest that African Americans and Hispanics are grossly underrepresented in the newsroom. We see a more attenuated pattern for the mentions.\n\nRace/ethnicity of authors across all outlets.\n\n| Race/Ethnicity | Percentage |\n|--------------|----------|\n| nh_white     | 78%   |\n| nh_black     | 5.7%   |\n| asian        | 8.5%   |\n| hispanic     | 7.3%   |\n| other        | .4%   |\n\n\nRace/ethnicity of people mentioned in the news across all outlets.\n\n| Race/Ethnicity | Percentage |\n|--------------|----------|\n| nh_white     | 73.5%   |\n| nh_black     | 9.5%   |\n| asian        | 8.6%   |\n| hispanic     | 8%    |\n| other        | .4%   |\n\nFor channel-wise breakdown, please refer to the notebooks.\n\n### Notebooks\n\n* [Get News Data](notebooks/01_news_grabber.ipynb)\n* [Extract Names](notebooks/02_extract_names_from_news.ipynb)\n* [Analyze Authors](notebooks/03_news_authors_lstm_infer.ipynb)\n* [Analyze People Mentioned](notebooks/04_news_mentioned_names_lstm_infer.ipynb)\n\n### Authors\n\nRajashekar Chintalapati and Gaurav Sood\n",
    "url": "https://github.com/notnews/rainbow",
    "last_updated": "2023-07-03T20:58:58+00:00"
  },
  {
    "full_name": "osmdatar/osmdatar",
    "name": "osmdatar",
    "description": "R package for downloading OSM data",
    "language": "C++",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/osmdatar/osmdatar.svg?branch=master)](https://travis-ci.org/osmdatar/osmdatar) [![codecov](https://codecov.io/gh/osmdatar/osmdatar/branch/master/graph/badge.svg)](https://codecov.io/gh/osmdatar/osmdatar) [![Project Status: Abandoned](http://www.repostatus.org/badges/0.1.0/abandoned.svg)](http://www.repostatus.org/#abandoned)\n\n![](./figure/map.png)\n\n`osmdatar` has been merged with [`overpass`](https://github.com/hrbrmstr/overpass) to form [`osmdata`](https://github.com/osmdatar/osmdata).\n\nThis repository ceased development on 19th Oct., 2016.\n",
    "url": "https://github.com/osmdatar/osmdatar",
    "last_updated": "2025-03-22T11:20:21+00:00"
  },
  {
    "full_name": "soodoku/adult",
    "name": "adult",
    "description": "Consumption of Pornography Online Using Passively Observed Browsing Data",
    "language": "Jupyter Notebook",
    "topics": [
      "adult-content",
      "moral",
      "partisanship",
      "pornography"
    ],
    "readme": "## Holier Than Thou? No Large Partisan Gaps in the Consumption of Pornography Online\n\n![retractions](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/soodoku/adult/main/.github/badges/retraction_status.json)\n\n\nConsumption of pornography has been blamed for a variety of societal ills, including the rise in misogyny, sex crimes, and the coarsening of the culture. Using passively collected browsing data from YouGov, we investigate how much pornography Americans consume online. We find that there is a sharp positive skew in the consumption of pornography, with a small number of users consuming lots of pornography and most consuming small amounts. Only about 32 percent of respondents consumed pornography online during the month-long observation period. Of the people who consumed pornography, the median consumer spent about three-quarters of an hour consuming pornography, and 95 percent of the consumers spent less than five and a half hours. Lastly, in line with previous research (MacInnis and Hodson, 2015; Edelman, 2009), which was based on aggregated data, we find that Republicans consume slightly more pornography online than Democrats. Adjusting for immutable characteristics like age and gender makes the differences go away.\n\n```tex\n@article{sood2024holier,\n  author       = {Sood, Gaurav and Shen, Lucas},\n  title        = {Holier Than Thou? No Large Partisan Gaps in the Consumption of Pornography Online},\n  journal      = {Journal of Quantitative Description: Digital Media},\n  volume       = {4},\n  year         = {2024},\n  month        = {April},\n  doi          = {10.51685/jqd.2024.011},\n  url          = {https://doi.org/10.51685/jqd.2024.011}\n}\n```\n\n### Key Results\n\n**Distribution of Partisan Differences in Hours Spent on Pornographic Sites**\n<p align=\"center\"><img width=\"55%\" src=\"./figs/quantile_reg_duration_adult.png\"></p>\n\n**Distribution of Partisan Differences in Percentage of Time Spent on Pornographic Sites**\n<p align=\"center\"><img width=\"5",
    "url": "https://github.com/soodoku/adult",
    "last_updated": "2025-04-14T22:45:41+00:00"
  },
  {
    "full_name": "alexmojaki/outdated",
    "name": "outdated",
    "description": "Check if a version of a PyPI package is outdated",
    "language": "Python",
    "topics": [
      "python",
      "pypi",
      "version-checker",
      "version",
      "versioning",
      "version-check",
      "versioncheck"
    ],
    "readme": "# outdated\n\n[![Build Status](https://travis-ci.org/alexmojaki/outdated.svg?branch=master)](https://travis-ci.org/alexmojaki/outdated) [![Coverage Status](https://coveralls.io/repos/github/alexmojaki/outdated/badge.svg?branch=master&uncache)](https://coveralls.io/github/alexmojaki/outdated?branch=master) [![Supports Python versions 2.7 and 3.5+](https://img.shields.io/pypi/pyversions/outdated.svg)](https://pypi.python.org/pypi/outdated)\n\nThis is a mini-library which, given a package name and a version, checks if it's the latest version available on PyPI.\n\nTo install:\n\n    pip install outdated\n\n## Quickstart:\n\n    from outdated import warn_if_outdated\n\n    warn_if_outdated('my-package-name', '1.2.3')\n\nThis will:\n\n- Show a warning if the given version is not the latest. The warning includes the package name, the given version, and the latest version.\n- Perform the check in a background thread (so it doesn't delay anything)\n- Make at most one HTTP call (unless there is an HTTP error, in which case it will try 3 times) to the PyPI server for that specific package\n- Cache the result of the HTTP call on disk for 24 hours\n- Show a warning if any exception occurs during the check\n\nThis will *not* check what version is currently installed, it will only use the given version. Library authors must make sure that the version in their `setup.py` matches the version here.\n\nThe package name argument must be exactly the name used on PyPI, so that e.g. https://pypi.python.org/pypi/my-package-name is a valid URL.\n\nOptional arguments:\n\n- `background` (default `True`): run the check in a separate thread. Set to `False` to run immediately.\n- `raise_exceptions` (default: `False`): if `True`, allow exceptions to bubble to the top. Otherwise, show a warning including the exception message. If `background` is `True` and this is `True` then this will result in a full traceback showing but the process continuing.\n\n## Lower level API\n\n    from outdated import check_outdated\n\n    is_outdated, la",
    "url": "https://github.com/alexmojaki/outdated",
    "last_updated": "2024-02-24T04:27:16+00:00"
  },
  {
    "full_name": "18F/domain-scan",
    "name": "domain-scan",
    "description": "A lightweight pipeline, locally or in Lambda, for scanning things like HTTPS, third party service use, and web accessibility.",
    "language": "Python",
    "topics": [],
    "readme": "[![Code Climate](https://codeclimate.com/github/18F/domain-scan/badges/gpa.svg)](https://codeclimate.com/github/18F/domain-scan)\n\n## A simple scanning system for the cloud\n\nA **lightweight scan pipeline** for orchestrating third party tools, at scale and (optionally) using serverless infrastructure.\n\nThe point of this project is to make it easy to **coordinate and parallelize** third party tools with a **simple scanning interface** that produces **consistent and database-agnostic output**.\n\nOutputs aggregate CSV for humans and machines, and detailed JSON for machines.\n\nCan scan websites and domains for data on their HTTPS and email configuration, third party service usage, accessibility, and other things. [Adding new scanners](#developing-new-scanners) is relatively straightforward.\n\nAll scanners can be run locally using **native Python multi-threading**.\n\nSome scanners can be executed **inside Amazon Lambda** for much higher levels of parallelization.\n\nMost scanners work by using **specialized third party tools**, such as [`SSLyze`](https://github.com/nabla-c0d3/sslyze) or [`trustymail`](https://github.com/dhs-ncats/trustymail). Each scanner in this repo is meant to add the smallest wrapper possible around the responses returned from these tools.\n\nThere is also built-in support for using **headless Chrome** to efficiently measure sophisticated properties of web services. Especially powerful when **combined with Amazon Lambda**.\n\n### Requirements\n\n`domain-scan` requires **Python 3.6 or 3.7**.\n\nTo install **core dependencies**:\n\n```bash\npip install -r requirements.txt\n```\n\nYou can install scanner- or gatherer-specific dependencies yourself. Or, you can \"quick start\" by just **installing all dependencies for all scanners and/or all gatherers**:\n\n```bash\npip install -r requirements-scanners.txt\npip install -r requirements-gatherers.txt\n```\n\nIf you plan on **developing/testing domain-scan itself**, install development requirements:\n\n```bash\npip install -r requirements-d",
    "url": "https://github.com/18F/domain-scan",
    "last_updated": "2025-08-25T16:29:38+00:00"
  },
  {
    "full_name": "udacity/self-driving-car",
    "name": "self-driving-car",
    "description": "The Udacity open source self-driving car project",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "> This repository is deprecated. Currently enrolled learners, if any, can utilize the https://knowledge.udacity.com/ forum for help on specific issues.\n>\n\n<img src=\"images/cover.png\" alt=\"Self-Driving Car\" width=\"800px\">\n\n## We’re Building an Open Source Self-Driving Car\n#### And we want your help!\n\nAt [Udacity](https://udacity.com), we believe in democratizing education. How can we provide opportunity to everyone on the planet? We also believe in teaching really amazing and useful subject matter. When we decided to build the [Self-Driving Car Nanodegree program](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013), to teach the world to build autonomous vehicles, we instantly knew we had to tackle our own self-driving car too.\n\nTogether with [Google](https://google.com) Self-Driving Car founder and [Udacity](https://udacity.com) President Sebastian Thrun, we formed our core Self-Driving Car Team. One of the first decisions we made? **Open source code, written by hundreds of students from across the globe!**\n\n**[You can read more about our plans for this project](https://medium.com/udacity/were-building-an-open-source-self-driving-car-ac3e973cd163#.bm5c5chek).**\n\n## Contributions\nHere's a list of the projects we've open sourced:\n\n* [**Deep Learning Steering Models**](https://github.com/udacity/self-driving-car/tree/master/steering-models) – Many different neural networks trained to predict steering angles of the car. More information [here](https://medium.com/p/d73217f2492c).\n* [**Camera Mount**](https://github.com/udacity/camera-mount) by [@spartanhaden](https://twitter.com/spartanhaden) – A mount to support a lens and camera body that can be mounted using standard GoPro hardware\n* [**Annotated Driving Datasets**](https://github.com/udacity/self-driving-car/tree/master/annotations) – Many hours of labelled driving data\n* [**Driving Datasets**](https://github.com/udacity/self-driving-car/tree/master/datasets) – Over 10 hours of driving data (L",
    "url": "https://github.com/udacity/self-driving-car",
    "last_updated": "2025-08-31T13:58:36+00:00"
  },
  {
    "full_name": "alisorkuncuk/wikiseriesasorkunlib",
    "name": "wikiseriesasorkunlib",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "====================\nwikiseriesasorkunlib\n====================\n\nThis my project\n\n\n* Documentation: https://wikiseriesasorkunlib.readthedocs.org/en/latest\n\n\nDevelopment Workflow\n====================\n\nThe workflow supports the following steps\n\n * lint\n * test\n * build\n * document\n * upload\n * graph\n\nThese actions are supported out of the box by the corresponding scripts under _CI/scripts directory with sane defaults based on best practices.\nSourcing setup_aliases.ps1 for windows powershell or setup_aliases.sh in bash on Mac or Linux will provide with handy aliases for the shell of all those commands prepended with an underscore.\n\nThe bootstrap script creates a .venv directory inside the project directory hosting the virtual environment. It uses pipenv for that.\nIt is called by all other scripts before they do anything. So one could simple start by calling _lint and that would set up everything before it tried to actually lint the project\n\nOnce the code is ready to be delivered the _tag script should be called accepting one of three arguments, patch, minor, major following the semantic versioning scheme.\nSo for the initial delivery one would call\n\n    $ _tag --minor\n\nwhich would bump the version of the project to 0.1.0 tag it in git and do a push and also ask for the change and automagically update HISTORY.rst with the version and the change provided.\n\n\nSo the full workflow after git is initialized is:\n\n * repeat as necessary (of course it could be test - code - lint :) )\n\n   * code\n   * lint\n   * test\n * commit and push\n * develop more through the code-lint-test cycle\n * tag (with the appropriate argument)\n * build\n * upload (if you want to host your package in pypi)\n * document (of course this could be run at any point)\n\n\nImportant Information\n=====================\n\nThis template is based on pipenv. In order to be compatible with requirements.txt so the actual created package can be used by any part of the existing python ecosystem some hacks were needed.\nSo when bui",
    "url": "https://github.com/alisorkuncuk/wikiseriesasorkunlib",
    "last_updated": "2025-01-15T15:28:59+00:00"
  },
  {
    "full_name": "tidyverse/multidplyr",
    "name": "multidplyr",
    "description": "A dplyr backend that partitions a data frame over multiple processes",
    "language": "R",
    "topics": [
      "dplyr",
      "multiprocess"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# multidplyr\n\n<!-- badges: start -->\n\n[![Lifecycle:\nexperimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)\n[![R-CMD-check](https://github.com/tidyverse/multidplyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/multidplyr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/multidplyr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/multidplyr?branch=main)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/multidplyr)](https://cran.r-project.org/package=multidplyr)\n<!-- badges: end -->\n\n## Overview\n\nmultidplyr is a backend for dplyr that partitions a data frame across\nmultiple cores. You tell multidplyr how to split the data up with\n`partition()` and then the data stays on each node until you explicitly\nretrieve it with `collect()`. This minimises the amount of time spent\nmoving data around, and maximises parallel performance. This idea is\ninspired by [partools](https://github.com/matloff/partools) by Norm\nMatloff and [distributedR](https://github.com/vertica/DistributedR) by\nthe Vertica Analytics team.\n\nDue to the overhead associated with communicating between the nodes, you\nwon’t see much performance improvement with simple operations on less\nthan ~10 million observations, and you may want to instead try\n[dtplyr](https://dtplyr.tidyverse.org/), which uses\n[data.table](https://R-datatable.com/). multidplyr’s strength is found\nparallelising calls to slower and more complex functions.\n\n(Note that unlike other packages in the tidyverse, multidplyr requires R\n3.5 or greater. We hope to relax this requirement [in the\nfuture](https://github.com/traversc/qs/issues/11).)\n\n## Installation\n\nYou can install the released version of multidplyr from\n[CRAN](https://CRAN.R-project.org) with:\n\n``` r\ninstall.packages(\"multidplyr\")\n```\n\nAnd",
    "url": "https://github.com/tidyverse/multidplyr",
    "last_updated": "2025-04-30T21:34:07+00:00"
  },
  {
    "full_name": "wrathematics/lptools",
    "name": "lptools",
    "description": "Linear Programming Tools",
    "language": "R",
    "topics": [],
    "readme": "<!-- Warning! Do not directly edit this file; see README.Rmd -->\n# lptools \n\n* **Version:** 0.1\n* **Status:** [![Build Status](https://travis-ci.org/wrathematics/lptools.png)](https://travis-ci.org/wrathematics/lptools)\n* **License:** [![License](http://img.shields.io/badge/license-BSD%202--Clause-orange.svg?style=flat)](http://opensource.org/licenses/BSD-2-Clause)\n* **Author:** Drew Schmidt\n\n\nLinear programming tools to make doing my homework easier.\n\n\n## Example\n\nSay we have the system inspired from a linear program:\n\n\n```r\nlibrary(lptools)\n\nA <- matrix(c(1, 1, 1, 1, 0, -1, 1, 2, 0 , 1), byrow=TRUE, nrow=2)\nb <- c(5, 6)\n```\n\nTODO\n\n\n```r\nlatex(A)\n```\n\n```\n## \\begin{align*}\n## A &= \\left[\n## \\begin{array}{rrrrr}\n## 1 &1 &1 &1 &0 \\\\\n## -1 &1 &2 &0 &1 \\\\\n## \\end{array}\\right]\n## \\end{align*}\n```\n\nTODO\n\n\n```r\nbfs <- find.bfs(A, b)\nlatex(bfs)\n```\n\n```\n## \\begin{align*}\n## B^{-1}b = [a_1, a_2]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## -0.5 &5.5 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_1, a_3]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 1.333 &3.667 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_1, a_4]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## -6 &11 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_1, a_5]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 5 &11 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_2, a_3]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 4 &1 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_2, a_4]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 6 &-1 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_2, a_5]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 5 &1 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_3, a_4]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 3 &2 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_3, a_5]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 5 &-4 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## B^{-1}b = [a_4, a_5]^{-1}b &=   \\left[\n## \\begin{array}{rr}\n## 5 &6 \\\\\n## \\end{array}\\right]^T\n## \\\\\n## \\end{align*}\n```\n\nTODO\n\n\n```r\nep <- find.ep(bfs)\nlatex(ep)\n``",
    "url": "https://github.com/wrathematics/lptools",
    "last_updated": "2024-03-28T18:46:33+00:00"
  },
  {
    "full_name": "blakeblackshear/frigate",
    "name": "frigate",
    "description": "NVR with realtime local object detection for IP cameras",
    "language": "TypeScript",
    "topics": [
      "rtsp",
      "realtime",
      "tensorflow",
      "google-coral",
      "mqtt",
      "nvr",
      "camera",
      "home-assistant",
      "object-detection",
      "ai",
      "homeautomation",
      "home-automation"
    ],
    "readme": "<p align=\"center\">\n  <img align=\"center\" alt=\"logo\" src=\"docs/static/img/frigate.png\">\n</p>\n\n# Frigate - NVR With Realtime Object Detection for IP Cameras\n\n<a href=\"https://hosted.weblate.org/engage/frigate-nvr/\">\n<img src=\"https://hosted.weblate.org/widget/frigate-nvr/language-badge.svg\" alt=\"Translation status\" />\n</a>\n\n\\[English\\] | [简体中文](https://github.com/blakeblackshear/frigate/blob/dev/README_CN.md)\n\nA complete and local NVR designed for [Home Assistant](https://www.home-assistant.io) with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.\n\nUse of a GPU or AI accelerator such as a [Google Coral](https://coral.ai/products/) or [Hailo](https://hailo.ai/) is highly recommended. AI accelerators will outperform even the best CPUs with very little overhead.\n\n- Tight integration with Home Assistant via a [custom component](https://github.com/blakeblackshear/frigate-hass-integration)\n- Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary\n- Leverages multiprocessing heavily with an emphasis on realtime over processing every frame\n- Uses a very low overhead motion detection to determine where to run object detection\n- Object detection with TensorFlow runs in separate processes for maximum FPS\n- Communicates over MQTT for easy integration into other systems\n- Records video with retention settings based on detected objects\n- 24/7 recording\n- Re-streaming via RTSP to reduce the number of connections to your camera\n- WebRTC & MSE support for low-latency live view\n\n## Documentation\n\nView the documentation at https://docs.frigate.video\n\n## Donations\n\nIf you would like to make a donation to support development, please use [Github Sponsors](https://github.com/sponsors/blakeblackshear).\n\n## Screenshots\n\n### Live dashboard\n\n<div>\n<img width=\"800\" alt=\"Live dashboard\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/5e713cb9-9db5-41dc-947a-6937c",
    "url": "https://github.com/blakeblackshear/frigate",
    "last_updated": "2025-09-02T07:48:57+00:00"
  },
  {
    "full_name": "yhat/ggpy",
    "name": "ggpy",
    "description": "ggplot port for python",
    "language": "Python",
    "topics": [],
    "readme": "# ggplot\n\n<img src=\"./examples/example-34d773b9-ec68-40b1-999b-7bb07c208be9.png\" width=\"400px\" />\n<img src=\"./examples/example-8f4fbffe-2999-42b0-9c34-de6f0b205733.png\" width=\"400px\" />\n\n### What is it?\n`ggplot` is a Python implementation of the grammar of graphics. It is not intended\nto be a feature-for-feature port of [`ggplot2 for R`](https://github.com/hadley/ggplot2)--though \nthere is much greatness in `ggplot2`, the Python world could stand to benefit \nfrom it. So there __will be feature overlap__, but not neccessarily mimicry \n(after all, R is a little weird).\n\nYou can do cool things like this:\n\n```python\nggplot(diamonds, aes(x='price', color='clarity')) + \\\n    geom_density() + \\\n    scale_color_brewer(type='div', palette=7) + \\\n    facet_wrap('cut')\n```\n![](./docs/example.png)\n\n### Installation\n```bash\n$ pip install -U ggplot\n# or \n$ conda install -c conda-forge ggplot\n# or\npip install git+https://github.com/yhat/ggplot.git\n```\n\n### Examples\nExamples are the best way to learn. There is a Jupyter Notebook full of them. \nThere are also notebooks that show how to do particular things with ggplot \n(i.e. [make a scatter plot](./docs/how-to/Making%20a%20Scatter%20Plot.ipynb) or [make a histogram](./docs/how-to/Making%20a%20Scatter%20Plot.ipynb)).\n\n- [docs](./docs)\n- [gallery](./docs/Gallery.ipynb)\n- [various examples](./examples.md)\n\n\n### What happened to the old version that didn't work?\nIt's gone--the windows, the doors, [everything](https://www.youtube.com/watch?v=YuxCKv_0GZc). \nJust kidding, [you can find it here](https://github.com/yhat/ggplot/tree/v0.6.6), though I'm not sure why you'd want to look at it. The data grouping and manipulation bits were re-written\n(so they actually worked) with things like facets in mind.\n\n### Contributing\nThanks to all of the ggplot [contributors](./contributors.md#contributors)!\nSee *[contributing.md](./contributing.md)*.\n",
    "url": "https://github.com/yhat/ggpy",
    "last_updated": "2025-08-18T02:43:56+00:00"
  },
  {
    "full_name": "alexhanna/unm-text-as-data",
    "name": "unm-text-as-data",
    "description": "UNM Text as Data workshop -- 2018-04-09",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# UNM Text as Data workshop\n\nApril 9, 2018\n\nHead over to the [Jupyter Notebook](https://github.com/alexhanna/unm-text-as-data/blob/master/Introduction%20to%20Text%20as%20Data.ipynb) for all the fun!\n",
    "url": "https://github.com/alexhanna/unm-text-as-data",
    "last_updated": "2023-02-25T20:29:19+00:00"
  },
  {
    "full_name": "ChandlerLutz/starpolishr",
    "name": "starpolishr",
    "description": "Post-polishing of stagazer output",
    "language": "R",
    "topics": [
      "latex-table",
      "stargazer",
      "r"
    ],
    "readme": "\r\n<!-- README.md is generated from README.Rmd. Please edit that file -->\r\nstarpolishr\r\n===========\r\n\r\nThis package allows for easy post polishing of latex [stargazer](https://cran.r-project.org/web/packages/stargazer/index.html) output. See [this vignette](https://github.com/ChandlerLutz/starpolishr/blob/master/vignettes/pdf/starpolishr-intro.pdf) for a quick introduction. For examples, see the `Tables` section of my papers [here](https://chandlerlutz.github.io/pdf/california-foreclosure-prevention-laws.pdf) and [here](https://chandlerlutz.github.io/pdf/local-labor-markets-canada-us.pdf). All of these tables were generated exclusively within R using `stargazer` and `starpolishr`.\r\n\r\nInstallation\r\n------------\r\n\r\nYou can install starpolishr from github with:\r\n\r\n``` r\r\n# install.packages(\"devtools\")\r\ndevtools::install_github(\"ChandlerLutz/starpolishr\")\r\n```\r\n\r\nQuick Tips\r\n----------\r\n\r\n-   The first argument in all `starpolishr` functions is a `stargazer` table, allowing for easy compatibility with the [magrittr](https://cran.r-project.org/web/packages/magrittr/index.html) `%>%`.\r\n-   All of the functions in the `starpolishr` package begin with `star_` for easy `tab` completion in emacs or rstudio.\r\n\r\nKey Functions\r\n-------------\r\n\r\nHere is a list of key `starpolishr` functions in order of how often I use them. See their help files for more details.\r\n\r\n-   `star_tex_notes` -- Adds custom notes using either the latex [caption](https://www.ctan.org/pkg/caption) or the latex [threeparttable](https://www.ctan.org/pkg/threeparttable).\r\n-   `star_tex_write` -- writes latex tables to a file and optionally adds header files and common packages\r\n-   `star_lhs_names` and `star_rhs_names` -- updates variable names using regular expressions. The advantage of these functions is that they allow for variable names to span more than one line.\r\n-   `star_panel` -- panels multiple related `stargazer` tables into a single latex table.\r\n-   `star_insert_row` -- insert a row after a given",
    "url": "https://github.com/ChandlerLutz/starpolishr",
    "last_updated": "2025-02-18T16:47:52+00:00"
  },
  {
    "full_name": "facebook/prophet",
    "name": "prophet",
    "description": "Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth.",
    "language": "Python",
    "topics": [
      "forecasting",
      "r",
      "python"
    ],
    "readme": "\n# Prophet: Automatic Forecasting Procedure\n\n![Build](https://github.com/facebook/prophet/workflows/Build/badge.svg)\n\n[![PyPI Version](https://img.shields.io/pypi/v/prophet.svg)](https://pypi.python.org/pypi/prophet)\n[![PyPI Downloads Monthly](https://pepy.tech/badge/prophet/month)](https://pepy.tech/project/prophet)\n[![PyPI Downloads All](https://pepy.tech/badge/prophet)](https://pepy.tech/project/prophet)\n\n[![CRAN Version](https://www.r-pkg.org/badges/version/prophet)](https://CRAN.R-project.org/package=prophet)\n[![CRAN Downloads Monthly](https://cranlogs.r-pkg.org/badges/prophet?color=brightgreen)](https://cran.r-project.org/package=prophet)\n[![CRAN Downloads All](https://cranlogs.r-pkg.org/badges/grand-total/prophet?color=brightgreen)](https://cranlogs.r-pkg.org/badges/grand-total/prophet)\n\n[![Conda_Version](https://anaconda.org/conda-forge/prophet/badges/version.svg)](https://anaconda.org/conda-forge/prophet/)\n\n-----\n\n**2023 Update:** We discuss our plans for the future of Prophet in this blog post: [facebook/prophet in 2023 and beyond](https://medium.com/@cuongduong_35162/facebook-prophet-in-2023-and-beyond-c5086151c138)\n\n-----\n\nProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nProphet is [open source software](https://code.facebook.com/projects/) released by Facebook's [Core Data Science team](https://research.fb.com/category/data-science/). It is available for download on [CRAN](https://cran.r-project.org/package=prophet) and [PyPI](https://pypi.python.org/pypi/prophet/).\n\n## Important links\n\n- Homepage: https://facebook.github.io/prophet/\n- HTML documentation: https://facebook.github.io/prophet/docs/quick_start.html\n- Issue tracke",
    "url": "https://github.com/facebook/prophet",
    "last_updated": "2025-09-02T09:42:23+00:00"
  },
  {
    "full_name": "asyml/texar",
    "name": "texar",
    "description": "Toolkit for Machine Learning, Natural Language Processing, and Text Generation, in TensorFlow.  This is part of the CASL project: http://casl-project.ai/",
    "language": "Python",
    "topics": [
      "machine-learning",
      "natural-language-processing",
      "tensorflow",
      "deep-learning",
      "text-generation",
      "python",
      "machine-translation",
      "dialog-systems",
      "texar",
      "bert",
      "gpt-2",
      "xlnet",
      "text-data",
      "data-processing",
      "casl-project"
    ],
    "readme": "<div align=\"center\">\n   <img src=\"./docs/_static/img/logo_h_035.png\"><br><br>\n</div>\n \n-----------------\n\n\n[![pypi](https://img.shields.io/pypi/v/texar.svg)](https://pypi.python.org/pypi/texar)\n[![Build Status](https://travis-ci.org/asyml/texar.svg?branch=master)](https://travis-ci.org/asyml/texar)\n[![codecov](https://codecov.io/gh/asyml/texar/branch/master/graph/badge.svg)](https://codecov.io/gh/asyml/texar)\n[![Documentation Status](https://readthedocs.org/projects/texar/badge/?version=latest)](https://texar.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/asyml/texar/blob/master/LICENSE)\n \n\n**Texar** is a toolkit aiming to support a broad set of machine learning, especially natural language processing and text generation tasks. Texar provides a library of easy-to-use ML modules and functionalities for composing whatever models and algorithms. The tool is designed for both researchers and practitioners for fast prototyping and experimentation.\n\nTexar was originally developed and is \nactively contributed by [Petuum](https://petuum.com/) and [CMU](https://www.cmu.edu/) in collaboration with other institutes.\nA mirror of this repository is maintained by [Petuum Open Source](https://github.com/petuum).\n\n### Key Features\n* **Two Versions, (Mostly) Same Interfaces**. Texar-TensorFlow (this repo) and **[Texar-PyTorch](https://github.com/asyml/texar-pytorch)** have mostly the same interfaces. Both further combine the best design of TF and PyTorch:\n  - Interfaces and variable sharing in *PyTorch convention*\n  - Excellent factorization and rich functionalities in *TF convention*.\n* **Rich Pre-trained Models, Rich Usage with Uniform Interfaces**. BERT, GPT2, XLNet, etc, for encoding, classification, generation, and composing complex models with other Texar components!\n* **Fully Customizable** at multiple abstraction level -- both novice-friendly and expert-friendly. \n  - Free to plug in whateve",
    "url": "https://github.com/asyml/texar",
    "last_updated": "2025-08-28T07:06:48+00:00"
  },
  {
    "full_name": "rasmusab/distribution_diagrams",
    "name": "distribution_diagrams",
    "description": "R-script for generating canonical diagrams of distributions to be used to describe Bayesian hierarchical models.",
    "language": "HTML",
    "topics": [],
    "readme": "Diagrams of distributions in the style of Kruschke (2011)\n=====================================================\n\nThe aim of the script `plot_dist.R` is to create diagrams of distribution to be used when illustrating Bayesian hierarchical models in the style of John K. Kruschke's [*Doing Bayesian Data Analysis*](http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/). The image below shows an example taken from DBDA and Kruschke describes the advantages of this style of diagram compared to DoodleBUGS style diagrams in this [blog post](http://doingbayesiandataanalysis.blogspot.se/2012/05/graphical-model-diagrams-in-doing.html).\n\n![DBDA diagram](https://raw.github.com/rasmusab/distribution_diagrams/master/dbda_diagram.jpg)\n\nIn order to create Kruschke style diagrams you need pretty pictures of the different distribution you have in your model, pictures that you later can stitch together in some drawing program (for example [Libre Office Draw](http://www.libreoffice.org/features/draw/) or [Inkscape](http://inkscape.org/)). The script `plot_dist.R` (written in [R](http://www.r-project.org/)) helps with this and to create a diagram of the normal distribution you would run the following code in your current R session.\n\n\n```r\n# Reads in the functions plot_dist, plot_dist_svg, plot_dist_png and a\n# list of predefined distributions called dists.\nsource(\"plot_dist.R\")\nplot_dist(dists$normal)\n```\n\n![plot of chunk unnamed-chunk-1](https://raw.github.com/rasmusab/distribution_diagrams/master/figure/unnamed-chunk-1.png) \n\nIf you want to you can fill in the parameters yourself in the drawing program later but you can also make `plot_dist` draw the parameters by supplying a character or expression vector.\n\n\n```r\nplot_dist(dists$normal, labels = c(mean = expression(mu[j]), right_sd = expression(tau)))\nplot_dist(dists$normal, labels = c(mean = expression(M[0]), right_sd = expression(T[0])))\nplot_dist(dists$normal, labels = c(mean = expression(M[1]), right_sd = expression(T[1])))\n",
    "url": "https://github.com/rasmusab/distribution_diagrams",
    "last_updated": "2025-08-10T09:14:58+00:00"
  },
  {
    "full_name": "rturn/parseTweetFiles",
    "name": "parseTweetFiles",
    "description": "Processing code in R package format",
    "language": "R",
    "topics": [],
    "readme": "This is the code for handling tweet files downloaded using the streamR package. This package prepares this files for future topic modeling, and removes undesired variables. If the time stamps are included, they are converted into an R friendly format. If coordinates are included, this package can also label each tweet with zip codes. Finally, this package can also\nbe used to fit a series of topic models to a dataframe of time stamped Tweets.\n\n### Installation\n\nThis package can be downloaded directly from github as follows:\n\ninstall.packages(c(\"qtl\", \"htmlwidgets\", \"devtools\"))  \nlibrary(devtools)  \ninstall_github(\"rturn/parseTweetFiles\")  \n\n### Examples\n\nFor most uses, this package is easy to use. Just place all tweets you want to edit in one folder, and run the process.files functions. If necessary, clean.tweets and locate.tweets can be used independently.\n\n```{r, eval = FALSE}\nprocess.files(\"~/Documents/RawTweets\", \"~/Documents/EditedTweets\")\nprocess.files(\"~/Documents/RawTweets\", \"~/Documents/EditedTweets\", loc = TRUE, \nvars = c(\"text\", \"time_zone\"), tz = c(\"Jerusalem\", \"NA\"), stoplist = stoplist)\n```\n\nFor fitting topics models first a maptpx_model and maptpx_vis folder have to be created in the current directory. The data\nframe to fit models to must have a created_at column with time stamps in the posixct format. After that the model fitting\nis one command, and the output is saved to the created directories.\n\n```{r, eval = FALSE}\nstep = 1.5\ntime = as.POSIXct('2015-04-24 12:11', tz = \"GMT\")\nmodel_time_points(english.tweets.df, time, step, 96, 2, 20)\n```\n",
    "url": "https://github.com/rturn/parseTweetFiles",
    "last_updated": "2023-07-14T18:46:50+00:00"
  },
  {
    "full_name": "nisaacson/pdf-extract",
    "name": "pdf-extract",
    "description": "Node PDF Extract",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Node PDF\nNode PDF is a set of tools that takes in PDF files and converts them to usable formats for data processing. The library supports both extracting text from searchable pdf files as well as performing OCR on pdfs which are just scanned images of text\n\n[![Build Status](https://travis-ci.org/nisaacson/pdf-extract.png)](https://travis-ci.org/nisaacson/pdf-extract)\n\n## Installation\n\nTo begin install the module.\n\n`npm install pdf-extract`\n\nAfter the library is installed you will need the following binaries accessible on your path to process pdfs.\n\n- pdftk\n    - pdftk splits multi-page pdf into single pages.\n- pdftotext\n    - pdftotext is used to extract text out of searchable pdf documents\n- ghostscript\n    - ghostscript is an ocr preprocessor which convert pdfs to tif files for input into tesseract\n- tesseract\n    - tesseract performs the actual ocr on your scanned images\n\n\n### OSX\nTo begin on OSX, first make sure you have the homebrew package manager installed.\n\n**pdftk** is not available in Homebrew. However a gui install is available here.\n[http://www.pdflabs.com/docs/install-pdftk/](http://www.pdflabs.com/docs/install-pdftk/)\n\n**pdftotext** is included as part of the **poppler** utilities library. **poppler** can be installed via homebrew\n\n``` bash\nbrew install poppler\n```\n\n**ghostscript** can be install via homebrew\n``` bash\nbrew install gs\n```\n\n**tesseract** can be installed via homebrew as well\n\n``` bash\nbrew install tesseract\n```\n\nAfter tesseract is installed you need to install the alphanumeric config and an updated trained data file\n``` bash\ncd <root of this module>\ncp \"./share/eng.traineddata\" \"/usr/local/Cellar/tesseract/3.02.02_3/share/tessdata/eng.traineddata\"\ncp \"./share/dia.traineddata\" \"/usr/local/Cellar/tesseract/3.02.02_3/share/tessdata/dia.traineddata\"\ncp \"./share/configs/alphanumeric\" \"/usr/local/Cellar/tesseract/3.02.02_3/share/tessdata/configs/alphanumeric\"\n```\n\n### Ubuntu\n**pdftk** can be installed directly via apt-get\n```bash\napt-get ins",
    "url": "https://github.com/nisaacson/pdf-extract",
    "last_updated": "2025-08-31T16:33:46+00:00"
  },
  {
    "full_name": "first20hours/google-10000-english",
    "name": "google-10000-english",
    "description": "This repo contains a list of the 10,000 most common English words in order of frequency, as determined by n-gram frequency analysis of the Google's Trillion Word Corpus.",
    "language": "",
    "topics": [],
    "readme": "[![Not Maintained](https://img.shields.io/badge/Maintenance%20Level-Not%20Maintained-yellow.svg)](https://gist.github.com/cheerfulstoic/d107229326a01ff0f333a1d3476e068d)\n\nAbout This Repo\n===============\n\nThis repo contains a list of the 10,000 most common English words in order of frequency, as determined by [n-gram](https://en.wikipedia.org/wiki/N-gram) [frequency analysis](https://en.wikipedia.org/wiki/Frequency_analysis) of the [Google's Trillion Word Corpus](https://books.google.com/ngrams/info).\n\nAccording to the [Google Machine Translation Team](https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html):\n\n>Here at Google Research we have been using word n-gram models for a variety of R&D projects, such as statistical machine translation, speech recognition, spelling correction, entity detection, information extraction, and others. While such models have usually been estimated from training corpora containing at most a few billion words, we have been harnessing the vast power of Google's datacenters and distributed processing infrastructure to process larger and larger training corpora. We found that there's no data like more data, and scaled up the size of our data by one order of magnitude, and then another, and then one more - resulting in a training corpus of one trillion words from public Web pages.\n>\n>We believe that the entire research community can benefit from access to such massive amounts of data. It will advance the state of the art, it will focus research in the promising direction of large-scale, data-driven approaches, and it will allow all research groups, no matter how large or small their computing resources, to play together. That's why we decided to share this enormous dataset with everyone. We processed 1,024,908,267,229 words of running text and are publishing the counts for all 1,176,470,663 five-word sequences that appear at least 40 times. There are 13,588,391 unique words, after discarding words that appear less than 200 ",
    "url": "https://github.com/first20hours/google-10000-english",
    "last_updated": "2025-09-02T05:51:30+00:00"
  },
  {
    "full_name": "hrbrmstr/censys",
    "name": "censys",
    "description": "R interface to the Censys \"cyber\"/scans search engine • https://www.censys.io/tutorial",
    "language": "R",
    "topics": [
      "censys-data",
      "censys-api",
      "r",
      "rstats",
      "r-cyber"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n[![Travis-CI Build Status](https://travis-ci.org/hrbrmstr/censys.svg?branch=master)](https://travis-ci.org/hrbrmstr/censys) [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/censys)](https://cran.r-project.org/package=censys)\n\n`censys` is an R package interface to the [Censys API](https://censys.io/api)\n\nCensys is a search engine that enables researchers to ask questions about the hosts and networks that compose the Internet. Censys collects data on hosts and websites through daily [ZMap](https://zmap.io/) and [ZGrab](https://github.com/zmap/zgrab) scans of the IPv4 address space, in turn maintaining a database of how hosts and websites are configured. Researchers can interact with this data through a search interface, report builder, and SQL engine.\n\n[Censys tutorial](https://www.censys.io/tutorial).\n\nThe following functions are implemented:\n\n- `censys_export_download`:\tDownload export job files to a specified directory\n- `censys_export_job_status`:\tGet status of a Censys export job\n- `censys_get_job_result`:\tGet results of completed Censys SQL query job\n- `censys_get_job_status`:\tGet status of a Censys SQL query job\n- `censys_query`:\tIssue SQL Queries against the Censys API\n- `censys_report`:\tCreate aggregate reports on the breakdown of a field in the result set of a query\n- `censys_search`:\tPerform queries against Censys data\n- `censys_series`:\tList all series that can be queried from the SQL interface\n- `censys_series_details`:\tGet details about a series, including the list of tables and schema for the series\n- `censys_start_export`:\tExport large datasets and structured records from Censys to JSON or CSV files\n- `get_series`:\tRetrieve data on the types of scans Censys regularly performs (\"series\").\n- `view_document`:\tRetrieve data that Censys has about a specific host, website, or certificate.\n- `view_result`:\tRetrieve data on a particular scan \"result\"\n- `view_series`:\tRetrieve d",
    "url": "https://github.com/hrbrmstr/censys",
    "last_updated": "2025-03-22T11:19:37+00:00"
  },
  {
    "full_name": "OrlandoSentinel/TabsOnTallahassee",
    "name": "TabsOnTallahassee",
    "description": "Tabs on Tallahassee",
    "language": "Python",
    "topics": [],
    "readme": "Tabs on Tallahassee\n===================\n\n[![Build Status](https://travis-ci.org/OrlandoSentinel/TabsOnTallahassee.svg)](https://travis-ci.org/OrlandoSentinel/TabsOnTallahassee)\n\nDocumentation: http://tabs-on-tallahassee.readthedocs.org/en/latest/\n",
    "url": "https://github.com/OrlandoSentinel/TabsOnTallahassee",
    "last_updated": "2020-10-18T21:27:46+00:00"
  },
  {
    "full_name": "me-shaon/GLWTPL",
    "name": "GLWTPL",
    "description": "\"Good Luck With That\" Public License",
    "language": "",
    "topics": [],
    "readme": "# GLWTPL (Good Luck With That Public License)\n\n[![GLWTPL](https://img.shields.io/badge/GLWT-Public_License-red.svg)](https://github.com/me-shaon/GLWTPL)\n\nIf you have *that* feeling about your code...\n\n```\nWhen I wrote this, only God and I understood what I was doing.\nNow, only God knows.\n```\n\n...think no further and include [this LICENSE](./LICENSE) to your project!\n\nAnd, wish a very **Good Luck** to your future self, or a fellow human being or an alien or an AI bot (who can code and will destroy human race) - literally anyone who will dare to dive into your project.\n\n![Good luck GIF](./good-luck.gif)\n\nIt has a [NSFW version](./NSFW_LICENSE) too. Cheers!\n\n### Possible Use Cases\n- You wrote some code but not quite proud of it, yet want to release it.\n- You want to make free your code in the wild and want no responsibility for it.\n- \"Whatever done is done\", and you don't have time/intention to fix, modify or improve your code any further\n- Built an awesome project in a Hackathon/Code-competition? Want to license it? This is the license for you.\n- Your college/university course/lab work and this license are a match made in heaven.\n\n### Contribution Guideline\nSee the [Contribution Guideline](https://github.com/me-shaon/GLWTPL/wiki#contribution-guideline) before bother me with some fancy PR.\n\n### Translations\n* [Albanian - Shqip](translations/LICENSE_al-AL)\n* [Arabic - العربية](translations/LICENSE_ar-AR)\n* [Bangla - বাংলা](translations/LICENSE_bn-BN)\n* [Cantonese - 廣東話](translations/LICENSE_zh-HK)\n* [Catalan - Català](translations/LICENSE_cat-CAT)\n* [Croatian - Hrvatski](translations/LICENSE_hr-HR)\n* [Czech - Čeština](translations/LICENSE_cs-CZ)\n* [Danish - Dansk](translations/LICENSE_da-DK)\n* [Dutch - Nederlands](translations/LICENSE_nl-NL)\n* [French - Français](translations/LICENSE_fr-FR)\n* [Galician - Galego](translations/NSFW_LICENSE_gl-GL)\n* [Georgian - ქართული](translations/LICENSE_ka-GE)\n* [German - Deutsch](translations/LICENSE_de-DE)\n* [Greek - Ελληνικά](transl",
    "url": "https://github.com/me-shaon/GLWTPL",
    "last_updated": "2025-08-30T01:20:46+00:00"
  },
  {
    "full_name": "hrbrmstr/nominatim",
    "name": "nominatim",
    "description": ":earth_asia: Tools for Working with the 'Nominatim' API in R",
    "language": "R",
    "topics": [
      "nominatim",
      "openstreetmap",
      "bb-lookup",
      "rstats",
      "r"
    ],
    "readme": "![](nominatim.png)\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\nnominatim is an R package to interface to the [OpenStreeMap Nominatim API](http://wiki.openstreetmap.org/wiki/Nominatim).\n\nFrom the wiki:\n\n> Nominatim (from the Latin, 'by name') is a tool to search OSM data by name and address and to generate synthetic addresses of OSM points (reverse geocoding). It can be found at nominatim.openstreetmap.org.\n>\n> Nominatim is also used as one of the sources for the search box on the OpenStreetMap home page. Several companies provide hosted instances of Nominatim that you can query via an API, for example see MapQuest Open Initiative, PickPoint or the OpenCage Geocoder.\n\nMost functions hit the [MapQuest Nominatim API](http://open.mapquestapi.com/nominatim/) as recommended by OpenStreetMap.\n\nThe following functions are implemented:\n\n-   `address_lookup`: Lookup the address of one or multiple OSM objects like node, way or relation.\n-   `osm_geocode`: Search for places by address\n-   `osm_search`: Search for places\n-   `osm_search_spatial`: Search for places, returning a list of 'SpatialPointsDataFrame', 'SpatialLinesDataFrame' or a 'SpatialPolygonsDataFrame'\n-   `reverse_geocode_coords`: Reverse geocode based on lat/lon\n-   `reverse_geocode_osm`: Reverse geocode based on OSM Type & Id\n-   `bb_lookup`: Bounding box (and other metadata) lookup\n\n### News\n\n-   Version 0.2.1.9000 released : bb\\_lookup can also take an `sp::bbox`-like matrix as value to `viewbox`\n-   Version 0.2.0.9000 released : bb\\_lookup\n-   Version 0.1.1.9000 released : address lookup, switch API server, API timeout watch\n-   Version 0.1.0.9000 released : \"spatial\" stuff\n-   Version 0.0.0.9000 released\n\n### NOTE\n\n-   Data © OpenStreetMap contributors, ODbL 1.0. <http://www.openstreetmap.org/copyright>\n-   Nominatim Usage Policy: <http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy>\n-   MapQuest Nominatim Terms of Use: <http://info.mapquest.com/terms-of-use/>\n\n### TODO\n\n-   ",
    "url": "https://github.com/hrbrmstr/nominatim",
    "last_updated": "2025-09-02T01:21:06+00:00"
  },
  {
    "full_name": "facebookresearch/Detectron",
    "name": "Detectron",
    "description": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.",
    "language": "Python",
    "topics": [],
    "readme": "**Detectron is deprecated. Please see [detectron2](https://github.com/facebookresearch/detectron2), a ground-up rewrite of Detectron in PyTorch.**\n\n# Detectron\n\nDetectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.\n\nAt FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440), [DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434), and [Group Normalization](https://arxiv.org/abs/1803.08494).\n\n<div align=\"center\">\n  <img src=\"demo/output/33823288584_1d21cf0a26_k_example_output.jpg\" width=\"700px\" />\n  <p>Example Mask R-CNN output.</p>\n</div>\n\n## Introduction\n\nThe goal of Detectron is to provide a high-quality, high-performance\ncodebase for object detection *research*. It is designed to be flexible in order\nto support rapid implementation and evaluation of novel research. Detectron\nincludes implementations of the following object detection algorithms:\n\n- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*\n- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*\n- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n- [RPN](https://arxiv.org/abs/1506.01497)\n- [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n- [R-FCN](https://arxiv.org/abs/1605.06409",
    "url": "https://github.com/facebookresearch/Detectron",
    "last_updated": "2025-09-02T08:23:30+00:00"
  },
  {
    "full_name": "rawls238/react-experiments",
    "name": "react-experiments",
    "description": "React components for implementing UI experiments",
    "language": "JavaScript",
    "topics": [
      "react",
      "react-experiments",
      "planout",
      "ab-testing"
    ],
    "readme": "# react-experiments\n[![Build Status](https://travis-ci.org/rawls238/react-experiments.svg?branch=master)](https://travis-ci.org/rawls238/react-experiments)\n[![npm downloads](https://img.shields.io/npm/dm/react-experiments.svg?style=flat-square)](https://www.npmjs.com/package/react-experiments)\n\n\n### This library is no longer actively maintained. ###\n\nreact-experiments is a set of React components for implementing UI experiments.\n\nFor additional context behind why we built this, check out the accompanying [blog post](http://product.hubspot.com/blog/product-experimentation-with-planout-and-react.js)\n\n# Installation\n\n```\nnpm install react-experiments\n```\n\n# Usage\n\n**If you are using React 0.14 the latest supported version is 4.1.0**\n\nreact-experiments was built to work with [PlanOut.js](https://www.github.com/HubSpot/PlanOut.js) and most of its constructs are inspired by the structure of PlanOut.js. This library will work out of the box if you pass it an instantiated PlanOut Namespace or Experiment class, but if you want to use your own methods of assigning experiment parameters and logging exposure then you can extend the base [experiment class](https://github.com/HubSpot/react-experiments/blob/master/src/Experiment.js) and pass that as the experiment class prop.\n\n## Implementing a simple experiment\n\nThis library serves as a way to declaratively implement UI experiments that are defined via PlanOut. The standard usage of this library is as follows:\n\n1) Define experiment via PlanOut script / API. The PlanOut parameters that you set should map to the props on which you want to run an experiment. Let's use the [sample PlanOut.js experiment](https://github.com/HubSpot/PlanOut.js/blob/master/examples/sample_planout_es5.js#L41) as an example, which is effectively:\n\n```js\nsignupText = uniformChoice(choices=['Signup', 'Join now'])\n```\n\n2) Wrap the component where you want to implement your UI experiment with the parametrize function provided by the library along with an insta",
    "url": "https://github.com/rawls238/react-experiments",
    "last_updated": "2024-02-28T12:45:20+00:00"
  },
  {
    "full_name": "cpsievert/LDAvis",
    "name": "LDAvis",
    "description": "R package for web-based interactive topic model visualization.",
    "language": "JavaScript",
    "topics": [
      "topic-modeling",
      "r",
      "javascript",
      "visualization",
      "text-mining"
    ],
    "readme": "## LDAvis\n\n[![Build Status](https://travis-ci.org/cpsievert/LDAvis.png?branch=master)](https://travis-ci.org/cpsievert/LDAvis)\n\nR package for interactive topic model visualization.\n\n![LDAvis icon](http://www.kennyshirley.com/figures/ldavis-pic.png)\n\n**LDAvis** is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n\n### Installing the package\n\n* Stable version on CRAN:\n\n```r\ninstall.packages(\"LDAvis\")\n```\n\n* Development version on GitHub (with [devtools](https://cran.r-project.org/package=devtools)):\n\n```r\ndevtools::install_github(\"cpsievert/LDAvis\")\n```\n\n### Getting started\n\nOnce installed, we recommend a visit to the main help page:\n\n```r\nlibrary(LDAvis)\nhelp(createJSON, package = \"LDAvis\")\n```\n\nThe documentation and example on the bottom of that page should provide a quick sense of how to create (and share) your own visualizations. If you want more details about the technical specifications of the visualization, see the vignette:\n\n```r\nvignette(\"details\", package = \"LDAvis\")\n```\n\nNote that **LDAvis** itself does not provide facilities for *fitting* the model (only *visualizing* a fitted model). If you want to perform LDA in R, there are several packages, including [mallet](https://cran.r-project.org/package=mallet), [lda](https://cran.r-project.org/package=lda), and [topicmodels](https://cran.r-project.org/package=topicmodels).\n\nIf you want to perform LDA with the R package **lda** and visualize the result with **LDAvis**, our example of a [20-topic model fit to 2,000 movie reviews](https://ldavis.cpsievert.me/reviews/reviews.html) may be helpful.\n\n**LDAvis** does not limit you to topic modeling facilities in R. If you use other tools ([MALLET](http://mallet.cs.umass.edu/) and [gensim](https://radimrehurek.com/gensim/) are popular), we recommend that you visit our [Twenty Newsgroups](https://ldavis.cp",
    "url": "https://github.com/cpsievert/LDAvis",
    "last_updated": "2025-08-21T21:57:25+00:00"
  },
  {
    "full_name": "erikgahner/PolData",
    "name": "PolData",
    "description": "A dataset with political datasets",
    "language": "R",
    "topics": [
      "political-datasets",
      "elections",
      "democracy",
      "national-election-studies",
      "political-institutions",
      "international-relations",
      "political-parties",
      "survey"
    ],
    "readme": "A dataset with political datasets\n---\n\n### Description\n\nA collection of political datasets. The datasets are listed below within specific categories: cabinets, citizens, constitutions, political institutions, parties and politicians, democracy, economics, elections, international relations, media, policy, political elites, political speeches and debates. All datasets are listed in the datasets in the repository (`.xlsx`, `.csv`) with detailed information on the topics, coverage and availability of the respective datasets.\n\n## Variables in the dataset\n\n- `name` = Name of dataset\n- `category` = Dataset category (`cabinets`, `citizens` etc.)\n- `link` = URL to dataset\n- `topics` = Specific topics of interest in the dataset\n- `country` = Name of country (ISO 3166-1 alpha-2)\n- `region_africa` = Country in African Group (United Nations Regional Groups of Member States)\n- `region_asia` = Country in Asia-Pacific Group (United Nations Regional Groups of Member States)\n- `region_easteurope` = Country in Eastern European Group (United Nations Regional Groups of Member States)\n- `region_latinamerica` = Country in Latin American and Caribbean Group (United Nations Regional Groups of Member States)\n- `region_westeurope` = Country in Western European and Others Group (United Nations Regional Groups of Member States)\n- `year_start` = Time coverage, year start\n- `year_end` = Time coverage, year end\n- `availability` = Availability of dataset\n- `registration` = Requirements for data access\n- `license`= License (identifiers from [SPDX License List](https://spdx.org/licenses/))\n- `file_codebook` = URL to documentation (usually `.pdf`)\n- `file_csv` = URL to dataset (`.csv`)\n- `file_dta` = URL to dataset (`.dta`)\n- `file_sav` = URL to dataset (`.sav`)\n- `file_excel` = URL to dataset (Excel)\n- `file_r` = URL to dataset (`.Rdata`)\n- `file_zip` = URL to compressed dataset (`.zip`)\n- `variable_country` = Name of country string variable\n- `variable_year` = Name of year variable\n- `variable_cow`",
    "url": "https://github.com/erikgahner/PolData",
    "last_updated": "2025-08-27T01:51:07+00:00"
  },
  {
    "full_name": "wesm/pydata-book",
    "name": "pydata-book",
    "description": "Materials and IPython notebooks for \"Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Python for Data Analysis, 3rd Edition\n\nMaterials and IPython notebooks for \"Python for Data Analysis, 3rd\nEdition\" by Wes McKinney, published by O'Reilly Media. Book content\nincluding updates and errata fixes can be [found for free on my\nwebsite][6].\n\n[Buy the book on Amazon][1]\n\nFollow Wes on Twitter: [![Twitter Follow](https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&label=Follow)](https://twitter.com/wesmckinn)\n\n# 2nd Edition Readers\n\nIf you are reading the 2nd Edition (published in 2017), please find the\nreorganized book materials on the [`2nd-edition` branch][5].\n\n# 1st Edition Readers\n\nIf you are reading the 1st Edition (published in 2012), please find the\nreorganized book materials on the [`1st-edition` branch][2].\n\n## IPython Notebooks:\n\n* [Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch02.ipynb)\n* [Chapter 3: Built-in Data Structures, Functions, and Files](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch03.ipynb)\n* [Chapter 4: NumPy Basics: Arrays and Vectorized Computation](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch04.ipynb)\n* [Chapter 5: Getting Started with pandas](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch05.ipynb)\n* [Chapter 6: Data Loading, Storage, and File Formats](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch06.ipynb)\n* [Chapter 7: Data Cleaning and Preparation](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch07.ipynb)\n* [Chapter 8: Data Wrangling: Join, Combine, and Reshape](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch08.ipynb)\n* [Chapter 9: Plotting and Visualization](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition/ch09.ipynb)\n* [Chapter 10: Data Aggregation and Group Operations](http://nbviewer.ipython.org/github/pydata/pydata-book/blob/3rd-edition",
    "url": "https://github.com/wesm/pydata-book",
    "last_updated": "2025-09-02T08:32:18+00:00"
  },
  {
    "full_name": "Reproducible-Science-Curriculum/2015-05-14-reproducible-science-duke",
    "name": "2015-05-14-reproducible-science-duke",
    "description": "Website for the Reproducible Science Workshop to be taught at Duke University, May 14-15, 2015",
    "language": "HTML",
    "topics": [],
    "readme": "## Reproducible Science Workshop\n\nTo be held at Duke University (EDGE workshop room, Bostock Library)\n\nMay 14-15 2015\n\nIf you are interested in attending, check out the\n[website](http://reproducible-science-curriculum.github.io/2015-05-14-reproducible-science-duke/)\nand sign up to attend through\n[Eventbrite](http://reproducible-science-bootcamp-duke-2015-05.eventbrite.com/)\n",
    "url": "https://github.com/Reproducible-Science-Curriculum/2015-05-14-reproducible-science-duke",
    "last_updated": "2024-06-20T19:56:20+00:00"
  },
  {
    "full_name": "Experience-Monks/math-as-code",
    "name": "math-as-code",
    "description": "a cheat-sheet for mathematical notation in code form",
    "language": "",
    "topics": [],
    "readme": "# math-as-code\n\n>[Chinese translation (中文版)](./README-zh.md)  \n>[Python version (English)](./PYTHON-README.md)\n\nThis is a reference to ease developers into mathematical notation by showing comparisons with JavaScript code.\n\nMotivation: Academic papers can be intimidating for self-taught game and graphics programmers. :) \n\nThis guide is not yet finished. If you see errors or want to contribute, please [open a ticket](https://github.com/Jam3/math-as-code/issues) or send a PR.\n\n> **Note**: For brevity, some code examples make use of [npm packages](https://www.npmjs.com/). You can refer to their GitHub repos for implementation details.\n\n\n\n# foreword\n\nMathematical symbols can mean different things depending on the author, context and the field of study (linear algebra, set theory, etc). This guide may not cover *all* uses of a symbol. In some cases, real-world references (blog posts, publications, etc) will be cited to demonstrate how a symbol might appear in the wild.\n\nFor a more complete list, refer to [Wikipedia - List of Mathematical Symbols](https://en.wikipedia.org/wiki/List_of_mathematical_symbols). \n\nFor simplicity, many of the code examples here operate on floating point values and are not numerically robust. For more details on why this may be a problem, see [Robust Arithmetic Notes](https://github.com/mikolalysenko/robust-arithmetic-notes) by Mikola Lysenko.\n\n# contents\n\n- [variable name conventions](#variable-name-conventions)\n- [equals `=` `≈` `≠` `:=`](#equals-symbols)\n- [square root and complex numbers `√` *`i`*](#square-root-and-complex-numbers)\n- [dot & cross `·` `×` `∘`](#dot--cross)\n  - [scalar multiplication](#scalar-multiplication)\n  - [vector multiplication](#vector-multiplication)\n  - [dot product](#dot-product)\n  - [cross product](#cross-product)\n- [sigma `Σ`](#sigma) - *summation*\n- [capital Pi `Π`](#capital-pi) - *products of sequences*\n- [pipes `||`](#pipes)\n  - [absolute value](#absolute-value)\n  - [Euclidean norm](#euclidean-norm)\n  - [determ",
    "url": "https://github.com/Experience-Monks/math-as-code",
    "last_updated": "2025-08-30T07:31:29+00:00"
  },
  {
    "full_name": "stefano-meschiari/latex2exp",
    "name": "latex2exp",
    "description": "Use LaTeX in R graphics.",
    "language": "R",
    "topics": [
      "latex",
      "r",
      "plotmath-expressions",
      "latex-formulas",
      "cran",
      "ggplot2"
    ],
    "readme": "<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/stefano-meschiari/latex2exp/workflows/R-CMD-check/badge.svg)](https://github.com/stefano-meschiari/latex2exp/actions)\n<!-- badges: end -->\n\n# latex2exp <img src=\"man/figures/logo.png\" align=\"right\" width=\"200px\">\n\n**latex2exp** is an R package that lets you use LaTeX in plots. It\nparses and converts LaTeX to R’s custom [plotmath\nexpressions](http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/plotmath.html).\nYou can read the full documentation on the [package’s\nwebsite](https://stefano-meschiari.github.io/latex2exp).\n\nExpressions returned by `latex2exp` can be used to create formatted text\nand mathematical formulas and symbols to be rendered as axis labels,\nannotations, legends, titles, etc. throughout R’s plotting system.\n\n## Installation\n\nInstall this package from CRAN:\n\n``` r\ninstall.packages('latex2exp')\n```\n\nYou can also install the development version from GitHub using\n[devtools](http://cran.r-project.org/web/packages/devtools/index.html):\n\n``` r\ndevtools::install_github('stefano-meschiari/latex2exp')\n```\n\n## Usage\n\nThe `TeX` function takes a LaTeX string, parses it, and returns the\nclosest [plotmath\nexpression](http://stat.ethz.ch/R-manual/R-patched/library/grDevices/html/plotmath.html)\nsuitable for use in graphics. The return value of `TeX()` can be used\nanywhere a plotmath expression is accepted, including plot labels,\nlegends, and text for both base graphics and ggplot2.\n\nHere’s a simple example:\n\n``` r\n# Use raw strings, no need to escape backslashes.\nTeX(r\"(\\textbf{Euler's identity} is $e^{i\\pi} + 1 = 0$.)\")\n```\n\nIn this example, `\\textbf{}` is used to mark a fragment of text as bold,\n`$` introduces inline math mode, `^{}` typesets its contents as\nsuperscript, and `\\pi` typesets the letter *π*.\n\nStarting with R 4.0, it is recommended to use the new raw string literal\nsyntax (see `?Quotes`). The syntax looks like `r\"(...)\"`, where `...`\ncan contain any character sequence, including `\\`.\n",
    "url": "https://github.com/stefano-meschiari/latex2exp",
    "last_updated": "2025-06-02T05:38:36+00:00"
  },
  {
    "full_name": "lllyasviel/ControlNet",
    "name": "ControlNet",
    "description": "Let us control diffusion models!",
    "language": "Python",
    "topics": [],
    "readme": "# News: A nightly version of ControlNet 1.1 is released!\n\n[ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly) is released. Those new models will be merged to this repo after we make sure that everything is good.\n\n# Below is ControlNet 1.0\n\nOfficial implementation of [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions.\n\n![img](github_page/he.png)\n\nIt copys the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. \n\nThe \"trainable\" one learns your condition. The \"locked\" one preserves your model. \n\nThanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models.\n\nThe \"zero convolution\" is 1×1 convolution with both weight and bias initialized as zeros. \n\nBefore training, all zero convolutions output zeros, and ControlNet will not cause any distortion.\n\nNo layer is trained from scratch. You are still fine-tuning. Your original model is safe. \n\nThis allows training on small-scale or even personal devices.\n\nThis is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.\n\n### FAQ\n\n**Q:** But wait, if the weight of a conv layer is zero, the gradient will also be zero, and the network will not learn anything. Why \"zero convolution\" works?\n\n**A:** This is not true. [See an explanation here](docs/faq.md).\n\n# Stable Diffusion + ControlNet\n\nBy repeating the above simple structure 14 times, we can control stable diffusion in this way:\n\n![img](github_page/sd.png)\n\nIn this way, the ControlNet can **reuse** the SD encoder as a **deep, strong, robust, and powerful backbone** to learn diverse controls. Many evidences (like [this](https://jerryxu.net/ODISE/) and [this](https://vpd.ivg-research.xyz/)) validate that the SD encoder is an excellent backbone.\n\nNote that the way we connect layers is computational efficient. The original",
    "url": "https://github.com/lllyasviel/ControlNet",
    "last_updated": "2025-09-02T05:38:49+00:00"
  },
  {
    "full_name": "schemaorg/schemaorg",
    "name": "schemaorg",
    "description": "Schema.org - schemas and supporting software",
    "language": "HTML",
    "topics": [
      "structured-data",
      "json-ld",
      "rdf"
    ],
    "readme": "Welcome to Schema.org\n=====================\n\n[![CI Tests Shield](https://github.com/schemaorg/schemaorg/actions/workflows/ci_tests.yml/badge.svg)](https://github.com/fthobe/schemaorg/actions)\n\nThis is the Schema.org project repository. It contains all the schemas, examples and software used to publish schema.org. For the site itself, please see [Schema.org](https://schema.org/) instead.\n\nNote: Much of the supporting software is imported from a sub module: 'sdopythonapp'\n\nIssues and proposals are managed here by collaborators around the project, especially participants of the [W3C Schema.org Community Group](https://www.w3.org/community/schemaorg/). If you are interested to participate please\njoin the group at the [W3C](https://www.w3.org/community/schemaorg/), introduce yourself and find or file issues here that engage your interest. If you are new to Git and GitHub, there's a useful [introduction to GitHub](https://www.w3.org/2006/tools/wiki/Github) in the W3C Wiki.\n\nThere are also continuous integration tests to check incoming pull requests.\n\n[Issue #1](https://github.com/schemaorg/schemaorg/issues/1) in GitHub is an entry point for release planning. It\nshould provide an overview of upcoming work, in terms of broad themes, specific issues and release milestones.\n\n[Issue #1](https://github.com/schemaorg/schemaorg/issues/1) will link to per-release entry points, or else navigate issues via label or milestone within GitHub.\nEvery change to the site comes via discussions here. Substantive changes are recorded in our [release notes](https://schema.org/docs/releases.html).\nA preview of the [draft new release notes](https://staging.schema.org/docs/releases.html) can be found as part of the test site for our next release.\nEvery month or so, after final review by the Schema.org Steering Group and wider community, we make a formal release.\n\nRegarding CC and opensource licenses for documents and software, see our [FAQ entry](https://schema.org/docs/faq.html#18).\n\nWe have a [",
    "url": "https://github.com/schemaorg/schemaorg",
    "last_updated": "2025-09-01T22:27:12+00:00"
  },
  {
    "full_name": "ajmcoqui/webAPIsR",
    "name": "webAPIsR",
    "description": "Example scripts for \"Using web APIs in R\" presentation, rstudio::conf 2017",
    "language": "HTML",
    "topics": [],
    "readme": "# webAPIsR\nThese are example scripts for my \"Using web APIs in R\" presentation from rstudio::conf 2017.\n\n## Presentation\n#### WebAPIsInR_Expanded.Rmd\nSlides from my rstudio::conf presentation.\n\n## Scripts\n#### omdb.R\nGet data from http://www.omdbapi.com/, an open-source movie database.  Explore the GET response; parse it with jsonlite or xml2, depending on the response format.\n\n#### swapi.R\nGet data from https://swapi.co/, an open-source Star Wars database.  Explore the GET response, parse with jsonlite, create a function to parse automatically, write a loop to process a paged response.\n\n#### twitter.R\nGet data from https://api.twitter.com using httr's convenience functions for OAuth 1.0 authentication.\n\n#### github.R\nGet data from https://api.github.com using httr's convenience functions for OAuth 2.0 authentication.\n\n## More resources\n#### On APIs and HTTP: \nhttps://zapier.com/learn/apis/chapter-1-introduction-to-apis/\n\nhttps://www.ntu.edu.sg/home/ehchua/programming/webprogramming/HTTP_Basics.html\n\n#### On the packages:\n`help(package=httr)`\n\nhttps://cran.r-project.org/web/packages/httr/vignettes/quickstart.html\n\nhttp://github.com/hadley/httr/tree/master/demo\n\nhttps://cran.r-project.org/web/packages/jsonlite/vignettes/json-apis.html\n\nhttps://cran.r-project.org/web/packages/httr/vignettes/api-packages.html\n",
    "url": "https://github.com/ajmcoqui/webAPIsR",
    "last_updated": "2025-03-22T11:19:19+00:00"
  },
  {
    "full_name": "amaas/stanford_dl_ex",
    "name": "stanford_dl_ex",
    "description": "Programming exercises for the Stanford Unsupervised Feature Learning and Deep Learning Tutorial",
    "language": "",
    "topics": [],
    "readme": "stanford_dl_ex\n==============\n\nProgramming exercises for the Stanford Unsupervised Feature Learning and Deep Learning Tutorial\n\nThis repository contains starter code for the tutorial at http://ufldl.stanford.edu/tutorial\n\nIf you have suggestions, questions, or bug reports, please submit contact:\n\nAndrew Maas ( amaas AT cs dot stanford.edu)\nSameep Tandon (sameep AT stanford dot edu)\n",
    "url": "https://github.com/amaas/stanford_dl_ex",
    "last_updated": "2025-08-31T16:07:02+00:00"
  },
  {
    "full_name": "jmarshallnz/adsmooth",
    "name": "adsmooth",
    "description": "Adaptive kernel density estimation with boundary correction",
    "language": "C++",
    "topics": [],
    "readme": "Adsmooth for R.\n\nAdaptive kernel density estimation with boundary correction.\n\nNote: This code has not been cleaned up for public consumption. Explore at your own risk.\nAny and all contributions welcome.\n\nJonathan Marshall\nMassey University",
    "url": "https://github.com/jmarshallnz/adsmooth",
    "last_updated": "2025-03-16T12:46:00+00:00"
  },
  {
    "full_name": "matthewjdenny/SpeedReader",
    "name": "SpeedReader",
    "description": "High Performance Text Processing in R",
    "language": "R",
    "topics": [],
    "readme": "# SpeedReader [![Travis-CI Build Status](https://travis-ci.org/matthewjdenny/SpeedReader.svg?branch=master)](https://travis-ci.org/matthewjdenny/SpeedReader)\nAn R package that provides functions to facilitate high performance text analysis in R.\n\n## Overview\nThis package provides a number of functions to:\n\n  * A front end for Stanford's CoreNLP libraries for POS tagging and finding named entities.\n  * Term-category association analyses including PMI and TF-IDF, with various forms of weighting.\n  * A front end for topic modeling using MALLET, that also reads the results back into R and presents them in a series of data.frames. \n  * A set of methods to compare documents and document versions using sequences of n-grams, and ensembles of Dice coefficients.\n  * An implementation of the informed Dirichlet model from Monroe et al. (2008), along with publication quality funnel plots.\n  * Functions for forming complex contingency tables.\n  * Functions of displaying text in LaTeX tables.\n  * Functionality to read in a preprocess text data into a document-term matrix.\n\nThe unifying theme of these functions is that they are designed to be easy\nto use, and to operate on up to tens of billions of tokens over hundreds of millions of \ndocuments without requiring a massive map-reduce cluster with terabytes of RAM. I have decided\nto produce an R package since these are functions I use quite frequently andthey have been replicated\nin several projects. **Check out the early version of the package vignette, [avialable here!](http://www.mjdenny.com/getting_started_with_SpeedReader.html)**\n\n## Installation\n\n### Requirements for using C++ code with R\n\nNote that if you are using a Mac, you will need to start by making sure you have Xcode + developer tools installed or you will not be able to compile the C++ code that is used in the samplers for this package. You will need to go here: <https://developer.apple.com/xcode/downloads/> and then select the link to the additional downloads page whi",
    "url": "https://github.com/matthewjdenny/SpeedReader",
    "last_updated": "2025-06-24T10:31:51+00:00"
  },
  {
    "full_name": "lukesonnet/KRLS.jl",
    "name": "KRLS.jl",
    "description": "Implements the KRLS method in Julia",
    "language": "Julia",
    "topics": [],
    "readme": "# KRLS in Julia\n\nThis script (very much under construction) implements Kernel Regularized Least Squares [(paper here)](http://www.stanford.edu/~jhain/Paper/PA2014a.pdf) in the Julia language. While much more is on the way, it is currently mostly a transliteration from the R package [(found here)](https://cran.r-project.org/web/packages/KRLS/). It is also currently about 10 times faster!\n\n## Functions\n\n### `krls`\n\n#### Arguments\n\nRequired\n* `Xinit` - your data matrix, observations are the first dimension, features are the second dimension\n* `yinit` - your response vector\n\nOptional\n* `lambda` - default is to fit using leave-one-out-cross-validation, but the user can specify any number greater than 0\n\n#### Returns\n\nThe method only outputs one object, a `KRLS` object, from which the following can be retrieved:\n* `K` - the kernel matrix\n* `coeffs` - the choice coefficients\n* `Looe` - the final leave-one-out error\n* `fitted` - fitted `y` values\n* `X` - original `X` matrix\n* `y` - original `y` vector\n* `sigma`\n* `lambda`\n* `R2` - R-squared\n* `derivatives` - the pointwise marginal effects\n* `avgderivatives` - the average pointwise marginal effects\n* `var_avgderivatives` - variance of the average pointwise marginal effects\n* `vcov_c` - variance covariance matrix of the choice coefficients\n* `vcov_fitted` - variance covariance matrix of the fitted values\n\n### `predict`\n\nThis can be used to predict outcomes for new data given a new data matrix with the same number of columns.\n\n#### Arguments\n* `k` - a KRLS object\n* `newmatint` a new data matrix with the same number of columns\n\n#### Returns\nReturns a 3-tuple with the following objects (this will be changed soon):\n* `yfitted` - predicted `y` values\n* `sefit` - standard errors of the predicted values\n* `vcovfit` - variance covariance matrix of the predicted values\n\n## Example\n\n```julia\nX = randn(1000, 3)\nX = hcat(X, vcat(repmat([1], 500, 1), repmat([0], 500, 1)))\ny = X * [1,2,3, -2] + randn(1000)\nk = krls(X, y)\n```\n\n\n\n\n    KRLS r",
    "url": "https://github.com/lukesonnet/KRLS.jl",
    "last_updated": "2016-09-20T01:12:00+00:00"
  },
  {
    "full_name": "fsolt/dotwhisker",
    "name": "dotwhisker",
    "description": "Dot-and-Whisker Plots of Regression Results",
    "language": "BibTeX Style",
    "topics": [
      "rstats",
      "r-package",
      "graphics",
      "plot",
      "regression-models"
    ],
    "readme": "<img src=\"https://user-images.githubusercontent.com/58319029/113226393-a7b68700-92c2-11eb-9221-a4a10fcd7112.png\" align=\"right\" alt=\"\" width=\"120\" />\r\n\r\n[![CRAN version](http://www.r-pkg.org/badges/version/dotwhisker)](https://cran.r-project.org/web/packages/dotwhisker/index.html) ![](http://cranlogs.r-pkg.org/badges/grand-total/dotwhisker) [![Travis-CI Build Status](https://travis-ci.org/fsolt/dotwhisker.svg?branch=master)](https://travis-ci.org/fsolt/dotwhisker)\r\n\r\n------------------------------------------------------------------------\r\ndotwhisker\r\n=========\r\n\r\n`dotwhisker` is an R package for quickly and easily generating dot-and-whisker plots of regression results, either directly from model objects or from tidy data frames. It provides a convenient way to create highly customizable plots for presenting and comparing statistics. It can be used to plot coefficients or other estimates (e.g., predicted probabilities) within a model or compare them across different models. The estimates are presented as dots with confidence interval whiskers, and predictors can be grouped in brackets.\r\n\r\nTo install:\r\n\r\n* the latest release version: `install.packages(\"dotwhisker\")`.\r\n* the latest development version: `if (!require(\"remotes\")) install.packages(\"remotes\"); remotes::install_github(\"fsolt/dotwhisker\")`.\r\n\r\n\r\nMore details are available at here:\r\n\r\nhttps://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html\r\n\r\n\r\nPlease note that this project is released with a [Contributor Code of Conduct](https://github.com/fsolt/dotwhisker/blob/master/CONDUCT.md). By participating in this project you agree to abide by its terms.\r\n",
    "url": "https://github.com/fsolt/dotwhisker",
    "last_updated": "2025-06-23T03:53:24+00:00"
  },
  {
    "full_name": "NoahFinberg/google_kg_movie_scraper",
    "name": "google_kg_movie_scraper",
    "description": "Uses Apify to scrape Google Search Engine Results pages for all American Films between 1950-2020 and then parses the knowledge panel structured data.",
    "language": "Python",
    "topics": [
      "movies",
      "serp",
      "apify",
      "knowledge-panel"
    ],
    "readme": "## Movie Google Knowledge Panel Dataset: 1950--2020\n\nWe iterate through the Wikipedia list of movies from 1950--2020 and then scrape Google Knowledge Panel for these movies using [Apify's Google Search Results Scraper](https://apify.com/apify/google-search-scraper). The final dataset including the raw html of the SERP pages as well as the parsed Knowledge Panels is posted [here on Harvard Dataverse](https://dataverse.harvard.edu/dataverse/americanmovies). Feel free to directly explore on [Kaggle](https://www.kaggle.com/noahfinberg/movies) too.\n\nWe use the dataset to estimate the correlation between [average reviews across different platforms](https://github.com/soodoku/mixed_signals).\n\n### Scripts\n\n* [Get Movie List](scripts/get_movie_list.py)\n* [Google Knowledge Panel Scraper](scripts/google_kg_scraper.py)\n\n### Future\n- It'd be nice (and not super difficult) to generalize this code to be able to automatically parse any google knowledge panel beyond movies for all of the structured data.\n\n### Author \nNoah Finberg\n",
    "url": "https://github.com/NoahFinberg/google_kg_movie_scraper",
    "last_updated": "2021-04-29T02:47:23+00:00"
  },
  {
    "full_name": "trinker/textplot",
    "name": "textplot",
    "description": "Plotting for text data",
    "language": "R",
    "topics": [
      "react",
      "textplot",
      "speech-gantt-charts",
      "wordcloud",
      "lexical-dispersion-plot",
      "text-hilighting",
      "text-mining"
    ],
    "readme": "textplot   \n============\n\n\n[![Build\nStatus](https://travis-ci.org/trinker/textplot.svg?branch=master)](https://travis-ci.org/trinker/textplot)\n[![Coverage\nStatus](https://coveralls.io/repos/trinker/textplot/badge.svg?branch=master)](https://coveralls.io/r/trinker/textplot?branch=master)\n\n![](tools/textplot_logo/r_textplot.png)\n\n**textplot** is a suite of text plotting tools that enable the user to\nanalyze text data via serveral common text plotting methods. Methods\ninclude lexical dispersion plots, word trees, speech networks,\nco-occurrence networks, speech Gantt charts, text hilighting, and word\nclouds.\n\n\nTable of Contents\n============\n\n-   [Functions](#functions)\n-   [Installation](#installation)\n-   [Examples](#examples)\n    -   [Lexical Dispersion](#lexical-dispersion)\n    -   [Word Trees](#word-trees)\n    -   [Speech Networks](#speech-networks)\n    -   [Co-occurrence Networks](#co-occurrence-networks)\n    -   [Speech Gantt Charts](#speech-gantt-charts)\n    -   [Text Hilighting](#text-hilighting)\n        -   [Regular Expresion Terms](#regular-expresion-terms)\n        -   [Token Matching](#token-matching)\n        -   [Sentence Matching](#sentence-matching)\n    -   [Word Clouds](#word-clouds)\n-   [Contact](#contact)\n\nFunctions\n============\n\n\nInstallation\n============\n\nTo download the development version of **textplot**:\n\nDownload the [zip\nball](https://github.com/trinker/textplot/zipball/master) or [tar\nball](https://github.com/trinker/textplot/tarball/master), decompress\nand run `R CMD INSTALL` on it, or use the **pacman** package to install\nthe development version:\n\n    if (!require(\"pacman\")) install.packages(\"pacman\")\n    pacman::p_load_current_gh(\"trinker/textplot\")\n\nExamples\n========\n\n    if (!require(\"pacman\")) install.packages(\"pacman\")\n    pacman::p_load(dplyr, magrittr, textplot)\n    pacman::p_load_current_gh('trinker/numform')\n\nLexical Dispersion\n------------------\n\n    sam_i_am %>%\n        lexical_dispersion(c(' not ', ' eat ', ' sam ', ' (sam|eat) '))",
    "url": "https://github.com/trinker/textplot",
    "last_updated": "2025-06-22T19:48:28+00:00"
  },
  {
    "full_name": "rust-lang/rust-by-example",
    "name": "rust-by-example",
    "description": "Learn Rust with examples (Live code editor included)",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Rust By Example\n\n[![Build Status](https://github.com/rust-lang/rust-by-example/actions/workflows/rbe.yml/badge.svg)](https://github.com/rust-lang/rust-by-example/actions)\n\nLearn Rust with examples (Live code editor included)\n\n## Using\n\nIf you'd like to read Rust by Example, you can visit <https://doc.rust-lang.org/rust-by-example/>\nto read it online.\n\nIf you'd like to read it locally, [install Rust], and then:\n\n```bash\ngit clone https://github.com/rust-lang/rust-by-example\ncd rust-by-example\ncargo install mdbook\nmdbook build\nmdbook serve\n```\n\n[install Rust]: https://www.rust-lang.org/tools/install\n\nTo be able to run the examples, you must be connected to the internet; you can\nread all content offline, however!\n\n**The following warnings can be ignored safely.**\n\n```text\n[WARN] (mdbook::preprocess::cmd): The command wasn't found, is the \"gettext\" preprocessor installed?\n[WARN] (mdbook::preprocess::cmd):   Command: mdbook-gettext\n```\n\n### Using translated version\n\nIf there is a translated resource in `po/` directory, it can be specified through `MDBOOK_BOOK__LANGUAGE` like below:\n\n```bash\ngit clone https://github.com/rust-lang/rust-by-example\ncd rust-by-example\ncargo install mdbook\nMDBOOK_BOOK__LANGUAGE=ja mdbook build\nMDBOOK_BOOK__LANGUAGE=ja mdbook serve\n```\n\n## Contributing\n\nPlease see the [CONTRIBUTING.md] file for more details.\n\n[CONTRIBUTING.md]: https://github.com/rust-lang/rust-by-example/blob/master/CONTRIBUTING.md\n\n## Translating\n\nPlease see the [TRANSLATING.md] file for more details.\n\n[TRANSLATING.md]: https://github.com/rust-lang/rust-by-example/blob/master/TRANSLATING.md\n\n### Translating guide for each languages\n\n* Japanese/日本語: [TRANSLATING_JA.md]\n\n[TRANSLATING_JA.md]: https://github.com/rust-lang/rust-by-example/blob/master/TRANSLATING_JA.md\n\n* Chinese/中文: [TRANSLATING_ZH.md]\n\n[TRANSLATING_ZH.md]: https://github.com/rust-lang/rust-by-example/blob/master/TRANSLATING_ZH.md\n\n## Translations to other languages\n\n* [Bulgarian](https://github.com/kberov/rust-",
    "url": "https://github.com/rust-lang/rust-by-example",
    "last_updated": "2025-09-02T09:06:50+00:00"
  },
  {
    "full_name": "Prooffreader/chorogrid",
    "name": "chorogrid",
    "description": "A python script to produce choropleths and colored square- and hex-grid maps",
    "language": "Python",
    "topics": [],
    "readme": "# chorogrid\nA python script to produce choropleths and colored square- and hex-grid maps\n\nExamples:\n\n![Examples](https://raw.githubusercontent.com/Prooffreader/chorogrid/master/examples.png \"Examples\")\n\nThere are two classes, Chorobin to assign colors to quantities, and Chorogrid to make the maps.\n\nMaps are made in SVG format; if you need to convert them to PNG you can do so in an external application like Inkscape or Adobe Illustrator, or in python with CairoSVG among other solutions.\n\nPro tip, because I made this mistake for a long time: it's \"choropleth\", not \"chloropleth\"; there's no L. The word comes from ancient Greek χώρα (khṓra, “location”) and πλῆθος (plêthos, “a great number”, same root as *plethora*), not χλωρός (khlōrós, “pale green”) as in *chlorine* and *chlorophyll*. Luckily Google knows enough to suggest the right one.\n",
    "url": "https://github.com/Prooffreader/chorogrid",
    "last_updated": "2025-06-13T15:44:49+00:00"
  },
  {
    "full_name": "MarkEdmondson1234/googleAuthR",
    "name": "googleAuthR",
    "description": "Google API Client Library for R. Easy authentication and help to build Google API R libraries with OAuth2. Shiny compatible. ",
    "language": "R",
    "topics": [
      "googleauthr",
      "shiny",
      "oauth2-flow",
      "r",
      "google",
      "api",
      "authentication"
    ],
    "readme": "# googleAuthR - Google API R Client\n\n## gargle backend\n\nAs of version `googleAuthR>=1.0.0` the OAuth2 and service JSON authentication is provided by [gargle](https://gargle.r-lib.org/index.html).  Refer to that documentation for details.\n\nThe plan is to migrate as much functionality to `gargle` from `googleAuthR`, but backward compatibility will be maintained for all packages depending on `googleAuthR` in the meantime. \n\nOnce there is feature parity, client packages can then migrate totally to `gargle`.  At time of writing some of the major features not in `gargle` yet are:\n\n* Shiny authentication flows\n* Paging\n* Caching\n* Batching\n\nIf you are not using the above then you can use `gargle` directly now.  Otherwise you can still use `googleAuthR` that will use the features of `gargle` and wait for more features to be migrated.\n\n## Overview\n\nThis library allows you to authenticate easily via local use in an OAuth2 flow; within a Shiny app; or via service accounts. \n\nThe main two functions are `gar_auth()` and `gar_api_generator()`.\n\n### `gar_auth`\n\nThis takes care of getting the authentication token, storing it and refreshing. \nUse it before any call to a Google library.\n\n### `gar_api_generator`\n\nThis creates functions for you to use to interact with Google APIs.\nUse it within your own function definitions, to query the Google API you want.\n\n## Summary\n\nAuto-build libraries for Google APIs with OAuth2 for both local and Shiny app use.\n\nGet more details at the [googleAuthR website](https://code.markedmondson.me/googleAuthR/)\n\nThe [`googleAuthRverse`](https://googleauthrverse.slack.com/) Slack team has been setup for support for using `googleAuthR` and the libraries it helps create.  Sign up via this [Google form](https://goo.gl/forms/d541yrJoDFMrrSJp1) to get access. \n\n## R Google API libraries using googleAuthR\n\nHere is a list of [available Google APIs](https://developers.google.com/apis-explorer/#p/) to make with this library.\nThe below libraries are all cross-compat",
    "url": "https://github.com/MarkEdmondson1234/googleAuthR",
    "last_updated": "2025-08-27T12:07:44+00:00"
  },
  {
    "full_name": "fstpackage/fst",
    "name": "fst",
    "description": "Lightning Fast Serialization of Data Frames for R",
    "language": "R",
    "topics": [
      "r",
      "data-frame",
      "compression",
      "data-storage"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<img src=\"man/figures/fst.png\" align=\"right\" height=\"196\" width=\"196\" />\n\n[![Build\nStatus](https://github.com/fstpackage/fst/actions/workflows/R-CMD-check.yaml/badge.svg?branch=develop)](https://github.com/fstpackage/fst/actions/workflows/R-CMD-check.yaml)\n[![License: AGPL\nv3](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/fst)](https://cran.r-project.org/package=fst)\n[![fastverse status\nbadge](https://fastverse.r-universe.dev/badges/fst)](https://fastverse.r-universe.dev/ui#package:fastverse)\n[![codecov](https://codecov.io/gh/fstpackage/fst/branch/develop/graph/badge.svg)](https://app.codecov.io/gh/fstpackage/fst)\n[![downloads](https://cranlogs.r-pkg.org/badges/fst)](http://cran.rstudio.com/web/packages/fst/index.html)\n[![total_downloads](https://cranlogs.r-pkg.org/badges/grand-total/fst)](http://cran.rstudio.com/web/packages/fst/index.html)\n\n## Overview\n\nThe [*fst* package](https://github.com/fstpackage/fst) for R provides a\nfast, easy and flexible way to serialize data frames. With access speeds\nof multiple GB/s, *fst* is specifically designed to unlock the potential\nof high speed solid state disks that can be found in most modern\ncomputers. Data frames stored in the *fst* format have full random\naccess, both in column and rows.\n\nThe figure below compares the read and write performance of the *fst*\npackage to various alternatives.\n\n| Method        | Format  | Time (ms) | Size (MB) | Speed (MB/s) | N       |\n|:--------------|:--------|:----------|:----------|:-------------|:--------|\n| readRDS       | bin     | 1577      | 1000      | 633          | 112     |\n| saveRDS       | bin     | 2042      | 1000      | 489          | 112     |\n| fread         | csv     | 2925      | 1038      | 410          | 232     |\n| fwrite        | csv     | 2790      | 1038      | 358          | 241     |",
    "url": "https://github.com/fstpackage/fst",
    "last_updated": "2025-08-05T21:53:25+00:00"
  },
  {
    "full_name": "greta-dev/greta",
    "name": "greta",
    "description": "simple and scalable statistical modelling in R",
    "language": "C++",
    "topics": [],
    "readme": "![](logos/top_banner.png)\n\n### greta is an R package for writing statistical models and fitting them by MCMC and optimisation\n\ngreta lets you write your own model like in BUGS, JAGS and Stan, except\nthat you write models right in R, it scales well to massive datasets,\nand it’s easy to extend and build on.\n\n### See the [website](https://greta-stats.org/) for more information, including [tutorials](https://greta-stats.org/articles/get_started.html), [examples](https://greta-stats.org/articles/example_models.html), [package documentation](https://greta-stats.org/reference/index.html), and the [greta forum](https://forum.greta-stats.org).\n\nYou can install the current release version of the package from\nCRAN:\n\n``` r\ninstall.packages(\"greta\")\n```\n\nOr install the development version of `greta` from [r-universe](http://greta-dev.r-universe.dev/):\n\n```r\ninstall.packages(\"greta\", repos = c(\"https://greta-dev.r-universe.dev\", \"https://cloud.r-project.org\"))\n```\n\n(Note - installing from r-universe is just like installing from CRAN, and should be faster and more convenient than installing from GitHub)\n\nYou can also install the development version of `greta` via GitHub:\n\n``` r\ndevtools::install_github(\"greta-dev/greta\")\n```\n\n# Installing Python Dependencies\n\nThe `install_greta_deps()` function helps install the Python dependencies (Google's [TensorFlow](https://www.tensorflow.org/) and [tensorflow-probability](https://github.com/tensorflow/probability)). \n\nBy default, `install_greta_deps()` installs versions TF 2.15.0, and TFP version 0.23.0, using python 3.10. To change the versions of TF, TFP, or python that you want to use, you specify the `deps` argument of `install_greta_deps()`, which used `greta_deps_spec()`. See `?install_greta_deps()` or `?greta_deps_spec()` for more information.\n\nThis helper function, `install_greta_deps()`, installs the exact pythons package versions needed. It also places these inside a conda environment, \"greta-env-tf2\". This isolates these exact pyt",
    "url": "https://github.com/greta-dev/greta",
    "last_updated": "2025-09-01T14:33:52+00:00"
  },
  {
    "full_name": "sid-devic/multicalibration",
    "name": "multicalibration",
    "description": "A python package for multicalibration post-processing.",
    "language": "Python",
    "topics": [
      "fairness-ml",
      "post-processing"
    ],
    "readme": "# Multicalibration Post-Processing Python Package\nMulticalibration is a Python package that implements a model post-processing method of the [same name](https://arxiv.org/abs/1711.08513).\nThe goal of multicalibration post-processing algorithms are to improve the calibration of a model not only overall, but also on specified subpopulations (or \"groups\"/\"subgroups\") given as input.\nMulticalibration originated in the field of algorithmic fairness, and was suggested in order to provide better performance of machine learning models on protected subpopulations of the data.\nThis package provides implementations of two multicalibration algorithms: [HKRR](https://arxiv.org/abs/1711.08513) (from the original multicalibration paper), and [HJZ](https://arxiv.org/abs/2302.10863).\n\nThe package can be installed via pip:\n```bash\npip install multicalibration\n```\nThe package can also be installed by cloning the git repository:\n```bash\ngit clone https://github.com/sid-devic/multicalibration.git\ncd multicalibration/\npip install .\n```\n\n## Example Usage\nMulticalibration post-processing takes as input a set of _probabilistic predictions_, true labels for those predictions, and a list of subgroups membership lists.\nThe goal is to improve the calibration of the predictions conditioned on each subgroup list.\nThis is done in a black-box and post-hoc manner: the multicalibration algorithm only operates on and modifies the _predictions_ of the model, and not the model itself.\nImportantly, datapoints may belong to multiple subgroups: that is, subgroups can potentially be both complex and overlapping.\nIn `examples/basic_usage.py`, we give a short example of applying the HKRR algorithm on some synthetic data, summarized here.\n```python\n# Generate some synthetic data\nprobs, labels, subgroups = generate_correlated_subgroup_data(n_samples=1000)\nn_groups = len(subgroups)\n\n# Hyperparams for HKRR predictor\nhkrr_params = {\n    'alpha': 0.1,           # Permitted subgroup calibration violation\n    'lambda",
    "url": "https://github.com/sid-devic/multicalibration",
    "last_updated": "2025-04-13T21:45:39+00:00"
  },
  {
    "full_name": "johnmchambers/XRPython",
    "name": "XRPython",
    "description": "XR-style Interface to Python (from \"Extending R\") ",
    "language": "R",
    "topics": [],
    "readme": "# XRPython - An Interface from R to Python\n\nThis package provides an interface from R to Python, based on the XR\nstructure, as implemented in the XR package, in this repository.\n\nThe interface is designed as a basis for computations in R that use\nfunctions, objects and classes in Python.\nIn particular, the design caters to programmers developing application\npackages.\nThe XR structure encourages definition of proxy functions and classes\nin \\R{}, which users of the package can treat essentially as they\nwould in R, without special programming imposed by the interface.\n\nThe interface structure is described in the book\n*Extending R* (John M. Chambers, 2016, Chapman & Hall).\nA pdf version of the XRPython chapter from the book is included with the\ndocumentation of this package.  To open the pdf file from R:\n\n  `RShowDoc(\"Chapter_XRPython\", package = \"XRPython\")`\n\nAs of version 0.8, the package uses package 'reticulate' for its\nlow-level interface to Python, replacing 'rpython'.\n\nTo test whether a working Python interface can be opened on this\nmachine:\n  okPython(TRUE)\nwhich will return TRUE or FALSE and report the cause of a failure.\n",
    "url": "https://github.com/johnmchambers/XRPython",
    "last_updated": "2024-04-07T20:30:42+00:00"
  },
  {
    "full_name": "CERT-Polska/ursadb",
    "name": "ursadb",
    "description": "Trigram database written in C++, suited for malware indexing",
    "language": "C++",
    "topics": [
      "yara",
      "malware",
      "database",
      "security-tools",
      "security-automation"
    ],
    "readme": "# UrsaDB\n\nA 3gram search engine for querying terabytes of data in milliseconds. Optimized for working with binary files (for example, malware dumps).\n\nCreated in [CERT.PL](https://cert.pl). Originally by Jarosław Jedynak ([tailcall.net](https://tailcall.net)), extended and improved by Michał Leszczyński.\n\n**This repository is only for UrsaDB project (ngram database). See [CERT-Polska/mquery](https://github.com/CERT-Polska/mquery) for more user friendly UI.**\n\n## Installation\n\nSee [installation instructions](./INSTALL.md)\n\n## Quickstart\n\n1. Create new database:\n```\nmkdir /opt/ursadb\nursadb_new /opt/ursadb/db.ursa\n```\n\n2. Run UrsaDB server:\n```\nursadb /opt/ursadb/db.ursa\n```\n\n3. Connect with UrsaCLI:\n```\n$ ursacli\n[2020-04-13 18:16:36.511] [info] Connected to UrsaDB v1.3.0 (connection id: 006B8B4571)\nursadb>\n```\n\n4. [Index some files](./docs/indexing.md):\n```\nursadb> index \"/opt/samples\" with [gram3, text4, wide8, hash4];\n```\n\n5. Now you can perform queries. For example, match all files with three null bytes:\n```\nursadb> select {00 00 00};\n```\n\nRead the [syntax](./docs/syntax.md) documentation to learn more about available commands.\n\n## Learn more\n\nMore documentation can be found in the [docs](./docs/) directory.\n\nYou can also read the hosted version here:\n[cert-polska.github.io/ursadb](https://cert-polska.github.io/ursadb).\n\n## Contact\nIf you have any problems, bugs or feature requests related to UrsaDB, you're encouraged to create a GitHub issue.\n\n## Funding acknowledgement\n![Co-financed by the Connecting Europe Facility by of the European Union](https://www.cert.pl/wp-content/uploads/2019/02/en_horizontal_cef_logo-1.png)\n",
    "url": "https://github.com/CERT-Polska/ursadb",
    "last_updated": "2025-08-14T18:29:09+00:00"
  },
  {
    "full_name": "microsoft/CNTK",
    "name": "CNTK",
    "description": "Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit",
    "language": "C++",
    "topics": [
      "cognitive-toolkit",
      "cntk",
      "deep-learning",
      "machine-learning",
      "deep-neural-networks",
      "neural-network",
      "distributed",
      "python",
      "c-plus-plus",
      "c-sharp",
      "java"
    ],
    "readme": "## CNTK\n\n| **Chat** | **Windows build status** | **Linux build status** |\n|-------------|-------------|---------------|\n| [![Join the chat at https://gitter.im/Microsoft/CNTK](https://badges.gitter.im/Microsoft/CNTK.svg)](https://gitter.im/Microsoft/CNTK?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/126/badge)](https://cntk.ai/nightly-windows.html) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/127/badge)](https://cntk.ai/nightly-linux.html) |\n\nThe Microsoft Cognitive Toolkit (https://cntk.ai) is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph. In this directed graph, leaf nodes represent input values or network parameters, while other nodes represent matrix operations upon their inputs. CNTK allows users to easily realize and combine popular model types such as feed-forward DNNs, convolutional nets (CNNs), and recurrent networks (RNNs/LSTMs). It implements stochastic gradient descent (SGD, error backpropagation) learning with automatic differentiation and parallelization across multiple GPUs and servers. CNTK has been available under an open-source license since April 2015. It is our hope that the community will take advantage of CNTK to share ideas more quickly through the exchange of open source working code.\n\n## Installation\n\n* [Setup CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-your-machine)\n    * Windows ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-manual))\n    * Linux ([Python-only](https://docs.microsoft.com/en-us/cognitive-t",
    "url": "https://github.com/microsoft/CNTK",
    "last_updated": "2025-08-30T03:34:16+00:00"
  },
  {
    "full_name": "esphome/dashboard",
    "name": "dashboard",
    "description": "ESPHome dashboard and ESPHome Web",
    "language": "TypeScript",
    "topics": [
      "esphome",
      "esp8266",
      "esp32",
      "hacktoberfest",
      "lit-element"
    ],
    "readme": "# ESPHome Dashboard\n\nThe ESPHome Device Builder is a user facing dashboard embedded in ESPHome. It allows users to easily create and manage their configurations.\n\nThis repository contains the JavaScript frontend and is embedded in ESPHome releases.\n\n## Development\n\nCheck out this repository, run `npm install` and then run\n\n```\nnpm run develop\n```\n\nIt will start the dev server and will automatically re-bundle updated JavaScript (except for the `static` folder).\n\nThen run ESPHome in dashboard dev mode by defining the relative path from the ESPHome repository to this dashboard repository as the environment variable `ESPHOME_DASHBOARD_DEV`.\n\n```\nESPHOME_DASHBOARD_DEV=../dashboard esphome dashboard ./\n```\n",
    "url": "https://github.com/esphome/dashboard",
    "last_updated": "2025-08-27T23:29:00+00:00"
  },
  {
    "full_name": "jstat/jstat",
    "name": "jstat",
    "description": "JavaScript Statistical Library",
    "language": "JavaScript",
    "topics": [],
    "readme": "[jStat](https://jstat.github.io/all.html) - JavaScript Statistical Library\n===============================================================\n\n[![npm version](https://badge.fury.io/js/jstat.svg)](https://badge.fury.io/js/jstat)\n\njStat provides native javascript implementations of statistical functions.\nFull details are available in the [docs](https://jstat.github.io/all.html).\njStat provides more functions than most libraries, including the weibull,\ncauchy, poisson, hypergeometric, and beta distributions.  For most\ndistributions, jStat provides the pdf, cdf, inverse, mean, mode, variance, and\na sample function, allowing for more complex calculations.\n\n**NOTICE:** The previous case sensitive `jStat` module will no longer be\nupdated. Instead use the all lowercase `jstat` when doing an `npm install` or\nsimilar.\n\nUsing jStat in a Browser\n------------------------\n\njStat can be used in the browser. The `jStat` object will be added to the\nwindow. For example:\n\n```\n<script src=\"components/jstat.js\"></script> <!-- include jStat, from the CDN or otherwise -->\n\n<script>\n...\nvar jstat = this.jStat(dataset); // jStat will be added to the window\n...\ndata[i]['cum'] = jstat.normal(jstat.mean(), jstat.stdev()).cdf(data[i].x);\n...\n</script>\n\n```\n\nCDN [![jsDelivr Hits](https://data.jsdelivr.com/v1/package/npm/jstat/badge?style=rounded)](https://www.jsdelivr.com/package/npm/jstat)\n---\n\nThe library is hosted on [jsDelivr](http://www.jsdelivr.com/) using the following\nurl:\n```\n//cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js\n```\nNote that `'latest'` can be replaced with any released verion of jStat.\n\nModule Loaders\n--------------\n\nCurrently jStat is exposed as `j$` and `jStat` inside an object, rather than\nexported directly. This may confuse some module loaders, however should be\neasily remedied with the correct configuration.\n\nNodeJS & NPM\n------------\nTo install via npm:\n\n```\nnpm install --save jstat\n```\n\nWhen loading under Node be sure to reference the child object.\n\n```\nvar { jStat",
    "url": "https://github.com/jstat/jstat",
    "last_updated": "2025-08-27T00:12:01+00:00"
  },
  {
    "full_name": "r-three/common-pile",
    "name": "common-pile",
    "description": "Code for collecting, processing, and preparing datasets for the Common Pile",
    "language": "Python",
    "topics": [],
    "readme": "# The Common Pile\n\nThis repository tracks the code used to collect, process, and prepare the datasets for the Common Pile.\nThe code used for the preparation of each source in the Common Pile can be found in the `sources/` subdirectory.\nSource-agnostic utility code and scripts are provided in the `common_pile` package.\nIf you are looking for the data itself or our trained models, please see [our Hugging Face organization](https://huggingface.co/common-pile/).\n\n## Installation\n\nThe majority of packages required for dataset creation can be installed with `pip install -r requirements.txt`.\nTo make use of the shared functionality in the `common_pile` pckage, run `pip install -e .`.\nIf you are on a system that doesn't support automatic installation of pandoc with `pypandoc_binary`, change it to `pypandoc` in the `requirements.txt` and and install pandoc manually.\n\n## Contributing\n\nIf you'd like to contribute a new source to the Common Pile, please [start an issue](https://github.com/r-three/common-pile/issues/new) to share details of the source.\nGenerally, we expect each source to include code that 1) downloads the data, 2) processes it appropriately to retain primarily plain text, and 3) write out the results in the Dolma format (gzipped jsonl).\nYou can find utilities to help with each of these steps in the `common_pile` library.\nAlternatively, you can look at our existing sources for ideas as to how to prepare a source.\nWe use git pre-commit hooks to format code and keep style consistent.\nYou can install the pre-commit libraries with `pip install pre-commit` and insert the pre-commit hooks with `pre-commit install` from the repository root.\n\n## Tips\n\nThe [scripts subdirectory](https://github.com/r-three/common-pile/tree/main/common_pile/scripts) has various scripts that can be helpful for inspecting or computing statistics over data.\nAlternatively, the Dolma-formatted files can be inspected with [`jq`](https://jqlang.org/) by running\n\n```\ncat ${file}.jsonl.gz | gunzip |",
    "url": "https://github.com/r-three/common-pile",
    "last_updated": "2025-08-28T14:36:28+00:00"
  },
  {
    "full_name": "kotlasaicharanreddy/pravega-client-rust",
    "name": "pravega-client-rust",
    "description": "Rust based Pravega client.",
    "language": "Rust",
    "topics": [],
    "readme": "![CIbuild](https://github.com/pravega/pravega-client-rust/workflows/CIbuild/badge.svg)\n[![codecov](https://codecov.io/gh/pravega/pravega-client-rust/branch/master/graph/badge.svg?token=XEjqMkINCV)](https://codecov.io/gh/pravega/pravega-client-rust)\n\n# Rust client for Pravega\n\nThis is a native Rust client for [Pravega](https://www.pravega.io/). \n\nNote: Pravega 0.9.0+ is required.\n\n## Status\n\nUp to date status can be seen on [the wiki](https://github.com/pravega/pravega-client-rust/wiki/Design-plan).\n\n## Goals\n\nThe goal is to allow for clients to be written in Rust, as well as provide a common implementation for clients in higher level languages including Python and nodejs. \n\nSee the wiki for the [status of each language](https://github.com/pravega/pravega-client-rust/wiki/Supported-APIs).\n\n## Approach\n\nThe approach is to write a common native implementation of the internals of the client in Rust. Then use a C ABI to provide an interface for other languages to link against.\n\nFinally for each supported language the low level API is translated into a high level API that is idiomatic for the language.\n\n## Book\n\nCheck out the Pravega Rust client [book](https://pravega.github.io/pravega-client-rust/) for more details.",
    "url": "https://github.com/kotlasaicharanreddy/pravega-client-rust",
    "last_updated": "2025-01-15T15:30:13+00:00"
  },
  {
    "full_name": "hrbrmstr/ggalt",
    "name": "ggalt",
    "description": ":earth_americas: Extra Coordinate Systems, Geoms,  Statistical Transformations & Scales for 'ggplot2'",
    "language": "R",
    "topics": [
      "geom",
      "ggplot2",
      "r",
      "rstats",
      "ggplot-extension",
      "ggplot2-geom",
      "ggplot2-scales"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/ggalt.svg?branch=master)](https://travis-ci.org/hrbrmstr/ggalt)\n[![AppVeyor Build\nStatus](https://ci.appveyor.com/api/projects/status/github/hrbrmstr/ggalt?branch=master&svg=true)](https://ci.appveyor.com/project/hrbrmstr/ggalt)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/ggalt)](https://CRAN.R-project.org/package=ggalt)\n![downloads](http://cranlogs.r-pkg.org/badges/grand-total/ggalt)\n\n`ggalt` : Extra Coordinate Systems, Geoms, Statistical Transformations,\nScales & Fonts for ‘ggplot2’\n\nA compendium of ‘geoms’, ‘coords’, ‘stats’, scales and fonts for\n‘ggplot2’, including splines, 1d and 2d densities, univariate average\nshifted histograms, a new map coordinate system based on the\n‘PROJ.4’-library and the ‘StateFace’ open source font ‘ProPublica’.\n\nThe following functions are implemented:\n\n  - `geom_ubar` : Uniform width bar charts\n\n  - `geom_horizon` : Horizon charts (modified from\n    <https://github.com/AtherEnergy/ggTimeSeries>)\n\n  - `coord_proj` : Like `coord_map`, only better (prbly shld use this\n    with `geom_cartogram` as `geom_map`’s new defaults are ugh)\n\n  - `geom_xspline` : Connect control points/observations with an\n    X-spline\n\n  - `stat_xspline` : Connect control points/observations with an\n    X-spline\n\n  - `geom_bkde` : Display a smooth density estimate (uses\n    `KernSmooth::bkde`)\n\n  - `geom_stateface`: Use ProPublica’s StateFace font in ggplot2 plots\n\n  - `geom_bkde2d` : Contours from a 2d density estimate. (uses\n    `KernSmooth::bkde2D`)\n\n  - `stat_bkde` : Display a smooth density estimate (uses\n    `KernSmooth::bkde`)\n\n  - `stat_bkde2d` : Contours from a 2d density estimate. (uses\n    `KernSmooth::bkde2D`)\n\n ",
    "url": "https://github.com/hrbrmstr/ggalt",
    "last_updated": "2025-08-14T07:49:26+00:00"
  },
  {
    "full_name": "codelucas/yelpcrawl",
    "name": "yelpcrawl",
    "description": "Crawl and scrape Yelp's restaurant data for every zip code in the United States (or a specified zipcode). Yelp Crawler.",
    "language": "Python",
    "topics": [],
    "readme": "YelpCrawl: Exhaustive Yelp! Scraper\n===================================\n\nExample usage for `yelp`_ extraction.\n\nExtract all restaurant data from a specific zipcode.\n\n::\n\n    $ python2.7 crawler.py -z 98029\n\n    ===== Attempting extraction for zipcode < 98029 >=====\n    \n    title: Issaquah Coffee Company\n    categories: Coffee & Tea\n    rating: 4.0 star rating\n    ...\n\n\nExtract all restaurant data from America (all American zipcodes).\n\n::\n\n    $ python2.7 crawler.py\n\n    **We are attempting to extract all zipcodes in Amerrica!**\n\n    ===== Attempting extraction for zipcode < 35004 >=====\n\n    title: Brasher Sam Tire &amp; Auto Service Inc\n    categories: Tires\n    rating: 5.0 star rating\n    ...\n\n\nInstallation:\n-------------\n\n::\n\n    $ git clone https://github.com/codelucas/yelpcrawl\n    $ cd yelpcrawl\n    $ pip install -r requirements.txt\n\nAnd now you can begin!\n\n::\n\n    $ python2.7 crawler.py -z 98029\n\nFeel free to send in pull requests. We need some test cases please :)\n\nThis code was written when the two of us were still relatively new at python \nso excuse the shittyness. This was open sourced just for keepsake, it's nothing\nfancy and there are definitely better scraping solutions out there.\n\nWe used slower parsers like `beautifulsoup`_ and no multithreading\nbecause `yelp`_ would've rate limited us anyways :)\n\nBy: `Lucas`_, `Mathew`_\n\n.. _`yelp`: http://www.yelp.com\n.. _`beautifulsoup`: http://www.crummy.com/software/BeautifulSoup/\n.. _`Lucas`: http://codelucas.com\n.. _`Mathew`: https://www.facebook.com/matsprehn\n",
    "url": "https://github.com/codelucas/yelpcrawl",
    "last_updated": "2025-04-26T09:20:48+00:00"
  },
  {
    "full_name": "myeomans/politeness",
    "name": "politeness",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# Politeness\n\n[![](https://www.r-pkg.org/badges/version/politeness?color=blue)](https://cran.r-project.org/package=politeness)\n[![](http://cranlogs.r-pkg.org/badges/grand-total/politeness?color=green)](https://cran.r-project.org/package=politeness)\n[![](http://cranlogs.r-pkg.org/badges/last-month/politeness?color=green)](https://cran.r-project.org/package=politeness)\n\nPoliteness is a universal dimension of langauge (Lakoff, 1973; Brown &\nLevinson, 1987). In practically all communication, a speaker can choose\nto be more or less polite to their audience. In this package, we provide\ntools for researchers to measure the markers and effects of politeness\nin natural language.\n\n## Installation\n\nYou can install politeness directly, like so:\n\n``` r\n install.packages(\"politeness\")\n```\n\nMany of the politeness features containted in this package use\ndependency parsing. We rely on the popular python library\n[SpaCy](https://spacy.io/), which is simple to install, although the\nprocedure can vary for different machines. Our software depends on a\nconvenient wrapper function, [spacyr](https://spacyr.quanteda.io/), that\nalso includes several intallation tools. If you do not have SpaCy\ninstalled, a reduced set of features is provided (i.e. those that do not\nrequire dependency tags) but this only recommended for initial tests,\nrather than final analyses.\n\nPlease confirm that your machine can run SpaCy and spacyr first! This\nstep can be difficult for some machines, and we defer to the existing\ndocumentation for that software as a guide.\n\n<!---\nIf you want to try out this package without having to configure SpaCy on\nyour own machine, I have written a tutorial that will let you analyse\nthe data on\n[Colab](https://colab.research.google.com/drive/1EmVhqlPLUIlFjYw73nzydtfT1PQ8QU2_?usp=sharing).\nThis will install everything you need automatically (though it does take\n~ 20 minutes for everything to run). However, for peopl",
    "url": "https://github.com/myeomans/politeness",
    "last_updated": "2025-05-07T01:28:47+00:00"
  },
  {
    "full_name": "philipus/Simple-FTRL-in-R",
    "name": "Simple-FTRL-in-R",
    "description": "Simple ftrl implementation in R for the AVAZU CTR Competition in R",
    "language": "R",
    "topics": [],
    "readme": "#simple ftrl implementation in R\n\nThis is a simple implementation of the ftrl algo in order to take part of the kaggle competition\n\nhttp://www.kaggle.com/c/avazu-ctr-prediction\n\nFirst I tried to use old fashion learning with lm  in R but it didn't work out because of the dimesnion of the data itself but also the matrix which was made by lm was simple to big (>10GB) even for a sample of 100k datasets.\n\nI was faszinated by online learning algo in any case. so I tried a published implementation in python. I know some python and wanted to get a better experience in R programming.\n\nso here is my first more serious code in R...\n\n* avazu-ctr-train-predict.R\n* function_4ftrl.R\n\nbefore I do the introduction I should give some references\n\n1) the original paper\n\nhttp://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n\n2) implementaion in python where I made a copy from\n\nhttp://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory\n\n\n## introduction\n\n* \"mkdir data\" in the directory where the R-Files are\n* download the data from kaggle webpage http://www.kaggle.com/c/avazu-ctr-prediction/data into the data directory\n* gunzip train.gz and test.gz\n* run the program avazu-ctr-train-predict.R\n\nI got a score of 0.3981133\n\n##Comments on my experiences with R\n\nReading and writing files is extremely slow in R. In python reading the file must be must faster at least in comparison to R. In the python Implementation from above uses for \"t, row in enumerate(DictReader(open(path))):\" which is a realy nice construct.\n\nI also tried parallelization using doSnow and foreach. After some tests I came to the conclusion that it would speedup the training.\n\nso at last I implemented a random Sample while reading the train file in order to get result in time (< 4h). predicting the data in the test file took also quite long because of slow reading and writing in R\n\n",
    "url": "https://github.com/philipus/Simple-FTRL-in-R",
    "last_updated": "2019-01-09T09:42:03+00:00"
  },
  {
    "full_name": "satwikkansal/wtfpython",
    "name": "wtfpython",
    "description": "What the f*ck Python? 😱",
    "language": "Python",
    "topics": [
      "python",
      "wats",
      "snippets",
      "wtf",
      "gotchas",
      "documentation",
      "pitfalls",
      "interview-questions",
      "python-interview-questions"
    ],
    "readme": "<!-- markdownlint-disable MD013 -->\n<p align=\"center\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"/images/logo_dark_theme.svg\">\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"/images/logo.svg\">\n      <img alt=\"Shows a wtfpython logo.\" src=\"/images/logo.svg\">\n    </picture>\n</p>\n<h1 align=\"center\">What the f*ck Python! 😱</h1>\n<p align=\"center\">Exploring and understanding Python through surprising snippets.</p>\n\nTranslations: [Chinese 中文](https://github.com/leisurelicht/wtfpython-cn) |\n[Vietnamese Tiếng Việt](https://github.com/vuduclyunitn/wtfptyhon-vi) |\n[Spanish Español](https://web.archive.org/web/20220511161045/https://github.com/JoseDeFreitas/wtfpython-es) |\n[Korean 한국어](https://github.com/buttercrab/wtfpython-ko) |\n[Russian Русский](https://github.com/satwikkansal/wtfpython/tree/master/translations/ru-russian) |\n[German Deutsch](https://github.com/BenSt099/wtfpython) |\n[Persian فارسی](https://github.com/satwikkansal/wtfpython/tree/master/translations/fa-farsi) |\n[Add translation](https://github.com/satwikkansal/wtfpython/issues/new?title=Add%20translation%20for%20[LANGUAGE]&body=Expected%20time%20to%20finish:%20[X]%20weeks.%20I%27ll%20start%20working%20on%20it%20from%20[Y].)\n\nOther modes: [Interactive Website](https://wtfpython-interactive.vercel.app) | [Interactive Notebook](https://colab.research.google.com/github/satwikkansal/wtfpython/blob/master/irrelevant/wtf.ipynb)\n\nPython, being a beautifully designed high-level and interpreter-based programming language,\nprovides us with many features for the programmer's comfort.\nBut sometimes, the outcomes of a Python snippet may not seem obvious at first sight.\n\nHere's a fun project attempting to explain what exactly is happening under the hood for some counter-intuitive snippets\nand lesser-known features in Python.\n\nWhile some of the examples you see below may not be WTFs in the truest sense,\nbut they'll reveal some of the interesting parts of Python that you might be unaw",
    "url": "https://github.com/satwikkansal/wtfpython",
    "last_updated": "2025-09-02T10:16:43+00:00"
  },
  {
    "full_name": "ShanEllis/datasharing_TAS",
    "name": "datasharing_TAS",
    "description": "How to share data for collaboration",
    "language": "TeX",
    "topics": [],
    "readme": "\\begin{thebibliography}{xx}\n\n\\harvarditem{Baggerly}{2010}{baggerly2010disclose}\nBaggerly, K.  \\harvardyearleft 2010\\harvardyearright , `Disclose all data in\n  publications', {\\em Nature} {\\bf 467}(7314),~401.\n\n\\harvarditem{Broman \\harvardand\\ Woo}{2017}{broman_data_2017}\nBroman, K.~W. \\harvardand\\ Woo, K.~H.  \\harvardyearleft 2017\\harvardyearright ,\n  Data organization in spreadsheets, Technical Report e3183v1, PeerJ Preprints.\n\\newblock DOI: 10.7287/peerj.preprints.3183v1.\n\\newline\\harvardurl{https://peerj.com/preprints/3183}\n\n\\harvarditem{{\\em {JSON}: {The} {Fat}-{Free} {Alternative} to\n  {XML}}}{n.d.}{noauthor_json:_nodate}\n{\\em {JSON}: {The} {Fat}-{Free} {Alternative} to {XML}}  \\harvardyearleft\n  n.d.\\harvardyearright .\n\\newline\\harvardurl{http://www.json.org/xml.html}\n\n\\harvarditem{Leek \\harvardand\\ Peng}{2015}{leek2015opinion}\nLeek, J.~T. \\harvardand\\ Peng, R.~D.  \\harvardyearleft 2015\\harvardyearright ,\n  `Opinion: Reproducible research can still be wrong: Adopting a prevention\n  approach', {\\em Proceedings of the National Academy of Sciences} {\\bf\n  112}(6),~1645--1646.\n\n\\harvarditem{Newman \\harvardand\\ Klyne}{n.d.}{newman_date_nodate}\nNewman, C. \\harvardand\\ Klyne, G.  \\harvardyearleft n.d.\\harvardyearright ,\n  `Date and {Time} on the {Internet}: {Timestamps}'.\n\\newline\\harvardurl{https://tools.ietf.org/html/rfc3339.html}\n\n\\harvarditem{Peng}{2011}{peng_reproducible_2011}\nPeng, R.~D.  \\harvardyearleft 2011\\harvardyearright , `Reproducible {Research}\n  in {Computational} {Science}', {\\em Science} {\\bf 334}(6060),~1226--1227.\n\\newline\\harvardurl{http://science.sciencemag.org/content/334/6060/1226}\n\n\\harvarditem{{\\em R: {The} {R} {Project} for {Statistical}\n  {Computing}}}{n.d.}{_r}\n{\\em R: {The} {R} {Project} for {Statistical} {Computing}}  \\harvardyearleft\n  n.d.\\harvardyearright .\n\\newline\\harvardurl{https://www.r-project.org/}\n\n\\harvarditem[White et~al.]{White, Baldridge, Brym, Locey, McGlinn \\harvardand\\\n  Supp}{2013}{white_nine_2013}\nWhite, E.~P., Baldri",
    "url": "https://github.com/ShanEllis/datasharing_TAS",
    "last_updated": "2017-09-10T18:03:47+00:00"
  },
  {
    "full_name": "wallarm/libdetection",
    "name": "libdetection",
    "description": "Signature-free approach library to detect injection and commanding attacks",
    "language": "C",
    "topics": [],
    "readme": "# libdetection\n\nExtendable library for detection syntaxes by formal notations.\nCan be used to detect injections and commanding attacks such as SQLi and others.\nDoes not require attacks samples to learn.\n\n\n## Requirements\n\n- cmake\n- cunit\n- bison\n- re2c\n- [libwallarmmisc](https://github.com/wallarm/libwallarmmisc)\n\n## Installation\n\nTo build, run\n\n```sh\n$ ./config\n```\nor, to build with optimizations\n\n```sh\n$ CFLAGS=\"-Ofast -mtune=native -march=native\" ./configure\n```\n\nthen run make\n\n```sh\n$ make -C build\n```\n\nAfter that, you will get\n\n1. The library `build/lib/libdetection.so`\n2. Example executable: `build/perf/libdetection_perf`\n\n\n## Extend syntaxes\n\nTo add your own module to the library, you have to\n\n1. Add the module code into a new directory under `lib/`. See\n   `lib/sqli/sqli.c` code which implements libdetection interface\n   for sqli module (look at `detect_parser_sqli` global variable).\n2. Make libdetection to load the module statically, see\n   `TRYLOAD` in `detect_parser_init()`. Dynamic loading is not\n   implemented yet.\n3. Include your module name in `lib/CMakeLists.txt` for automatic building.\n\n\n## Formal model\n\nFor attacks detection, a formal model is used which allows to make a decision based on the type of attack. This approach allows us to implement the library without having to specify precedents of attacks (without a signature of each specific attack). Here you will not find the files with fingerprints or static rules with regular expressions.\n\nDetermination of the attack - the user input (string coming to the input of the library) can be processed as a sequence of data, among which there will be at least one syntax instruction.\n\nFor example, in the string `123 union select` there are one data token and two instructions (commands). And the string is considered to be an attack. But it’s not complete as there no names of column and table used.\n\nWithin the library, each parser state (where line processing begins) is called a context. Contexts are formula",
    "url": "https://github.com/wallarm/libdetection",
    "last_updated": "2025-08-11T07:09:07+00:00"
  },
  {
    "full_name": "winstonlarson/brfss",
    "name": "brfss",
    "description": "Analyzing health data from the CDC",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# brfss\nInsights into health and behavior using data from the CDC\n\n### BRFSS Introduction\n\nEvery year since 1984, government employees have called around the country and asked the people who pick up a series of probing questions about their health and what they're doing about it. While it may sound annoying, the CDC's Behavioral Risk Factor Surveillance System (BRFSS) provides a wealth of information about health and health-related behaviors in the United States. It is the largest and longest running health survey system in the world, and in its current incarnation, it covers over 400,000 adult interviews from all 50 states, the District of Columbia, and three territories. For more information about the survey itself, you should check out the [CDC BRFSS site](http://www.cdc.gov/brfss/).\n\nThe BRFSS is a rich source of information on how demographics, behaviors, and other risk factors can correlate with health. Many important population health studies and measures use the BRFSS as a key data source. For example, it is the source of the CDC's \"Healthy Days\" measurement, a key performance metric for the healthcare industry. \n\nUnfortunately, BRFSS data isn't exactly easy to deal with. Its breadth and structure have changed considerably over the years, and there are important sampling considerations that must be taken into account when using the data to draw conclusions. My goal is to build this repository over time to demonstrate how to use BRFSS data and some of the interesting correlations and associations that can be drawn from this data set using machine learning and statistical techniques.\n\n### Getting BRFSS data\n\nTo get started, you'll want to download the data from the [BRFSS Annual Survey Data](http://www.cdc.gov/brfss/annual_data/annual_data.htm) page. There you can find links to each year the survey has been conducted. The data is available in `.XPT` (SAS Transport Format) or in `.ASCII` files. You should be warned that these are pretty big files, especially in",
    "url": "https://github.com/winstonlarson/brfss",
    "last_updated": "2025-07-15T07:46:06+00:00"
  },
  {
    "full_name": "poliquin/pylegiscan",
    "name": "pylegiscan",
    "description": "Python interface for LegiScan API",
    "language": "Python",
    "topics": [
      "laws",
      "government"
    ],
    "readme": "Python Interface to LegiScan API\n================================\n\nPython module for interacting with [LegiScan API](http://legiscan.com/legiscan)\nto download information about bills in state legislatures.  This module works\nwith the \"pull API\".  You must have an API key to use this module. Register for\na key at [LegiScan API](http://legiscan.com/legiscan).\n\nFunctions essentially follow the format described in the \n[API User Manual](http://legiscan.com/misc/LegiScan_API_User_Manual.pdf).\n\n\nExamples\n--------\n\nRead the API User Manual and start by typing `help(LegiScan)` to familiarize\nyourself with the available functions.  Here are some examples to get you\nstarted using the module.\n\nTell the API who you are and search for abortion legislation in the Texas state\nlegislature for current year...\n\n    from pylegiscan import LegiScan\n    legis = LegiScan('MY_API_KEY_GOES_HERE')\n    bills = legis.search(state='tx', query='abortion')\n    bills['summary']  # how many results did we get?\n    \n    # print the bill titles\n    for b in bills['results']:\n        print b['title']\n\nGet more detailed information for the first search result...\n\n    bill_id = bills['results'][0]['bill_id']\n    bill_detail = legis.get_bill(bill_id=bill_id)\n\nFigure out who the first sponsor of the bill is...\n\n    people_id = bill_detail['sponsors'][0]['people_id']\n    sponsor = legis.get_sponsor(people_id)\n    print sponsor['name']\n\nYou can put your API key in the `LEGISCAN_API_KEY` environment variable and\ncreate a new instance of `LegiScan` without passing a key.\n\nLicense\n-------\n\nYour interaction with the LegiScan API and use of the data is subject to the\n[LegiScan Terms of Service](http://legiscan.com/terms-of-service).\n\nThis code was written by Chris Poliquin and provided under the MIT License.\nChris Poliquin is not associated with LegiScan Inc.  LegiScan Inc. has not\nendorsed this module.\n\n### The MIT License (MIT)\n\nCopyright (c) 2014 Chris Poliquin <cpoliquin@hbs.edu>\n\nPermission is hereby grant",
    "url": "https://github.com/poliquin/pylegiscan",
    "last_updated": "2025-06-24T15:59:30+00:00"
  },
  {
    "full_name": "nvidia-riva/tutorials",
    "name": "tutorials",
    "description": "NVIDIA Riva runnable tutorials",
    "language": "Jupyter Notebook",
    "topics": [
      "notebook",
      "nvidia",
      "riva"
    ],
    "readme": "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_all_tutorials-readme/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n\n# Riva Speech Skills Tutorials\n\nThe best way to get started with Riva is to start with the tutorials.\n\n## Tutorials\n\n| Domain | Tutorial | Key Words | Github URL |\n|--------|----------|-----------|------------|\n| ASR    | How to use Riva ASR APIs with Out-Of-The-Box Models | ASR, API Basics | [Riva ASR - API Basics](asr-basics.ipynb) |\n| ASR    | How to Customize Riva ASR Vocabulary and Pronunciation with Lexicon Mapping | ASR, Customization, Custom Vocab, Lexicon Mapping | [Riva ASR - Customization - Vocab and Lexicon Mapping](asr-customize-vocabulary-and-lexicon.ipynb) |\n| ASR    | How To Train, Evaluate, and Fine-Tune an n-gram Language Model | ASR, Customization, Fine-tuning, Interpolation, n-gram | [Riva ASR - Customization - Training, Fine-tuning and Deploying n-gram Language Model in NeMo](asr-python-advanced-nemo-ngram-training-and-finetuning.ipynb) |\n| ASR    | How to Deploy a Custom Language Model (n-gram) Trained with NVIDIA NeMo on Riva | ASR, Customization, Custom Language Model Deployment, n-gram | [Riva ASR - Customization - Custom Language Model (n-gram) Deployment on Riva](asr-deploy-am-and-ngram-lm.ipynb) |\n| ASR    | How to Deploy a Custom Acoustic Model (Citrinet) Trained with NVIDIA NeMo on Riva | ASR, Customization, Acoustic Model Deployment, Citrinet | [Riva ASR - Customization - Acoustic Model (Citrinet) Deployment on Riva](asr-deploy-citrinet.ipynb) |\n| ASR    | How to Deploy a custom Acoustic Model (Conformer-CTC) Trained with NVIDIA NeMo on Riva | ASR, Customization, Acoustic Model Deployment, Conformer-CTC | [Riva ASR - Customization - Acoustic Model (Conformer-CTC) Deployment on Riva](asr-deploy-conformer-ctc.ipynb) |\n| ASR    | How to Deploy a custom Acoustic Model (Conformer-CTC) Trained with NVIDIA NeMo on Riva with WFST Decoder | ASR, Customization, Acoustic Model Deployment, Conforme",
    "url": "https://github.com/nvidia-riva/tutorials",
    "last_updated": "2025-08-11T13:55:38+00:00"
  },
  {
    "full_name": "bart6114/jug",
    "name": "jug",
    "description": "jug: A Simple Web Framework for R",
    "language": "R",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/Bart6114/jug.svg)](https://travis-ci.org/Bart6114/jug)\n[![codecov](https://codecov.io/gh/Bart6114/jug/branch/master/graph/badge.svg)](https://codecov.io/gh/Bart6114/jug)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/jug)](http://cran.r-project.org/web/packages/jug)\n[![Downloads](http://cranlogs.r-pkg.org/badges/jug)](http://cran.rstudio.com/package=jug)\n\n# jug: A Simple Web Framework for R\n\n<img src=\"https://github.com/Bart6114/jug/blob/master/var/beer_jug.png?raw=true\" width=\"64\" alt=\"jug\">\n\njug is a small web development framework for R which relies heavily upon the ```httpuv``` package. It’s main focus is to make building APIs for your code as easy as possible.\n\njug is not supposed to be either an especially performant nor an uber stable web framework. Other tools (and languages) might be more suited for that. It focuses on maximizing the ease with which you can create web APIs for your R code. However, the flexibility of jug means that, in theory, you could built an extensive web framework with it.\n\nCheck out [http://bart6114.github.io/jug/articles/jug.html](http://bart6114.github.io/jug/articles/jug.html) for the vignette documentation.\n\nPlugins:\n\n- [`jug.parallel`](https://github.com/Bart6114/jug.parallel)\n\n## Changes\n\n### v0.1.7.902\n- Introduction of events and event listeners (check e.g. the `on` function)\n- Introduction of the `logger`, a predefined event listener based on futile.logger\n- Enabled ByteCompile on installation\n\n### v0.1.7.901\n- Adjustment to `serve_static_files`: add `pdf` to binary types\n\n### v0.1.7.900\n- Adjustment to `serve_static_files`: removed link between `path` and file served\n\n### v0.1.7\n\n- Fixed CORS preflight request bug (issue #15)\n- Fixed masking of base::get\n\n### v0.1.6\n\n- Ability to specify `auto-unbox` value for json responses\n- Added `strict_params` argument to `decorate`\n\n### v0.1.5\n\n- Added basic authentication functionality through `auth_basic`\n\n### v0.1.4\n\n- Fixed bug wh",
    "url": "https://github.com/bart6114/jug",
    "last_updated": "2025-08-05T22:14:05+00:00"
  },
  {
    "full_name": "dselivanov/text2vec",
    "name": "text2vec",
    "description": "Fast vectorization, topic modeling, distances and GloVe word embeddings in R.",
    "language": "R",
    "topics": [
      "word2vec",
      "text-mining",
      "natural-language-processing",
      "glove",
      "vectorization",
      "topic-modeling",
      "word-embeddings",
      "latent-dirichlet-allocation"
    ],
    "readme": "**text2vec** is an R package which provides an efficient framework with a concise API for text analysis and natural language processing (NLP). \n\nGoals which we aimed to achieve as a result of development of `text2vec`:\n\n* **Concise** - expose as few functions as possible\n* **Consistent** - expose unified interfaces, no need to explore new interface for each task\n* **Flexible** - allow to easily solve complex tasks\n* **Fast** - maximize efficiency per single thread, transparently scale to multiple threads on multicore machines\n* **Memory efficient** - use streams and iterators, not keep data in RAM if possible\n\nSee [API](http://text2vec.org/api.html) section for details.\n\n# Performance\n\n![htop](http://text2vec.org/images/htop.png)\n\nThis package is efficient because it is carefully written in C++, which also means that text2vec is memory friendly. Some parts are fully parallelized using OpenMP. \n\nOther emrassingly parallel tasks (such as vectorization) can use any fork-based parallel backend on UNIX-like machines. They can achieve near-linear scalability with the number of available cores. \n\nFinally, a streaming API means that  users do not have to load all the data into RAM. \n\n\n# Contributing\n\nThe package has [issue tracker on GitHub](https://github.com/dselivanov/text2vec/issues) where I'm filing feature requests and notes for future work. Any ideas are appreciated.\n\nContributors are welcome. You can help by: \n\n- testing and leaving feedback on the [GitHub issuer tracker](https://github.com/dselivanov/text2vec/issues) (preferably) or directly by e-mail\n- forking and contributing (check [code our style guide](https://github.com/dselivanov/text2vec/wiki/Code-style-guide)). Vignettes, docs, tests, and use cases are very welcome\n- by giving me a star on [project page](https://github.com/dselivanov/text2vec) :-)\n\n# License\n\nGPL (>= 2)\n\n",
    "url": "https://github.com/dselivanov/text2vec",
    "last_updated": "2025-08-25T17:41:57+00:00"
  },
  {
    "full_name": "typesense/showcase-recipe-search",
    "name": "showcase-recipe-search",
    "description": "Instantly search 2M cooking recipes using Typesense Search (an open source alternative to Algolia / ElasticSearch)  ⚡ 🥘 🔍",
    "language": "JavaScript",
    "topics": [
      "typesense",
      "typesense-instantsearch-adapter",
      "typesense-showcase",
      "instantsearch"
    ],
    "readme": "#  🥘 Instant Recipe Search, powered by Typesense\n\nThis is a demo that showcases some of [Typesense's](https://github.com/typesense/typesense) features using a 2 Million database of recipes.\n\nView it live here: [recipe-search.typesense.org](https://recipe-search.typesense.org/)\n\n## Tech Stack\n\nThis search experience is powered by <a href=\"https://typesense.org\" target=\"_blank\">Typesense</a> which is\na blazing-fast, <a href=\"https://github.com/typesense/typesense\" target=\"_blank\">open source</a> typo-tolerant\nsearch-engine. It is an open source alternative to Algolia and an easier-to-use alternative to ElasticSearch.\n\nThe recipe dataset is from <a href=\"https://github.com/Glorf/recipenlg\" target=\"_blank\">Glorf/recipenlg</a> 🙏!\n\nThe dataset is 2.2 GB on disk, with ~2.2 million rows. It took 8 minutes to index this dataset on a 3-node Typesense cluster with 4vCPUs per node and the index was 2.7GB in RAM.\n\nThe app was built using the <a href=\"https://github.com/typesense/typesense-instantsearch-adapter\" target=\"_blank\">\nTypesense Adapter for InstantSearch.js</a> and is hosted on S3, with CloudFront for a CDN.\n\nThe search backend is powered by a geo-distributed 3-node Typesense cluster running on <a href=\"https://cloud.typesense.org\" target=\"_blank\">Typesense Cloud</a>,\nwith nodes in Oregon, Frankfurt and Mumbai.\n\n## Repo structure\n\n- `src/` and `index.html` - contain the frontend UI components, built with <a href=\"https://github.com/typesense/typesense-instantsearch-adapter\" target=\"_blank\">Typesense Adapter for InstantSearch.js</a>\n- `scripts/indexer` - contains the script to index the recipe data into Typesense.\n- `scripts/data` - contains a 1K sample subset of the recipes database. But you can download the full dataset from the link above.\n\n## Development\n\nTo run this project locally, install the dependencies and run the local server:\n\n```shell\nyarn\nbundle # JSON parsing takes a while to run using JS when indexing, so we're using Ruby just for indexing\n```\n\nIn a separ",
    "url": "https://github.com/typesense/showcase-recipe-search",
    "last_updated": "2025-08-29T10:36:46+00:00"
  },
  {
    "full_name": "trinker/rnltk",
    "name": "rnltk",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "rnltk\n============\n\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)\n[![Build\nStatus](https://travis-ci.org/trinker/rnltk.svg?branch=master)](https://travis-ci.org/trinker/rnltk)\n[![Coverage\nStatus](https://coveralls.io/repos/trinker/rnltk/badge.svg?branch=master)](https://coveralls.io/r/trinker/rnltk?branch=master)\n<a href=\"https://img.shields.io/badge/Version-0.0.1-orange.svg\"><img src=\"https://img.shields.io/badge/Version-0.0.1-orange.svg\" alt=\"Version\"/></a>\n</p>\n<img src=\"inst/rnltk_logo/r_rnltk_logo.png\" width=\"200\" alt=\"rnltk Logo\">\n\n**rnltk** is a natural language tool kit for R. It is inspired by\nPython's [NLTK](http://www.nltk.org/) porting and extending much of\nPython's functionality with some R flavor. The packages/functions that\nare included are designed to be more intuitive wrappers or optimized for\nperformance. Currently, the package calls the following packages\nmaintained by me:\n\n1.  [entity](https://github.com/trinker/entity)\n2.  [formality](https://github.com/trinker/formality)\n3.  [gofastr](https://github.com/trinker/gofastr)\n4.  [hclustext](https://github.com/trinker/hclustext)\n5.  [lexr](https://github.com/trinker/lexr)\n6.  [parsent](https://github.com/trinker/parsent)\n7.  [qdapRegex](https://github.com/trinker/qdapRegex)\n8.  [readability](https://github.com/trinker/readability)\n9.  [sentimentr](https://github.com/trinker/sentimentr)\n10. [stansent](https://github.com/trinker/stansent)\n11. [syllable](https://github.com/trinker/syllable)\n12. [tagger](https://github.com/trinker/tagger)\n13. [termco](https://github.com/trinker/termco)\n14. [textproj](https://github.com/trinker/textproj)\n15. [textreadr](https://github.com/trinker/textreadr)\n16. [textreport](https://github.com/trinker/textreport)\n17. [textshape](https://github.com/trinker/textshape)\n\nThere are plans to add to this list including outside pack",
    "url": "https://github.com/trinker/rnltk",
    "last_updated": "2017-01-15T09:47:32+00:00"
  },
  {
    "full_name": "apache/superset",
    "name": "superset",
    "description": "Apache Superset is a Data Visualization and Data Exploration Platform",
    "language": "TypeScript",
    "topics": [
      "superset",
      "apache",
      "apache-superset",
      "data-visualization",
      "data-viz",
      "analytics",
      "business-intelligence",
      "data-science",
      "data-engineering",
      "asf",
      "bi",
      "business-analytics",
      "data-analytics",
      "data-analysis",
      "python",
      "react",
      "sql-editor",
      "flask"
    ],
    "readme": "<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n\n# Superset\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/license/apache-2-0)\n[![Latest Release on Github](https://img.shields.io/github/v/release/apache/superset?sort=semver)](https://github.com/apache/superset/releases/latest)\n[![Build Status](https://github.com/apache/superset/actions/workflows/superset-python-unittest.yml/badge.svg)](https://github.com/apache/superset/actions)\n[![PyPI version](https://badge.fury.io/py/apache_superset.svg)](https://badge.fury.io/py/apache_superset)\n[![Coverage Status](https://codecov.io/github/apache/superset/coverage.svg?branch=master)](https://codecov.io/github/apache/superset)\n[![PyPI](https://img.shields.io/pypi/pyversions/apache_superset.svg?maxAge=2592000)](https://pypi.python.org/pypi/apache_superset)\n[![Get on Slack](https://img.shields.io/badge/slack-join-orange.svg)](http://bit.ly/join-superset-slack)\n[![Documentation](https://img.shields.io/badge/docs-apache.org-blue.svg)](https://superset.apache.org)\n\n<picture width=\"500\">\n  <source\n    width=\"600\"\n    media=\"(prefers-color-scheme: dark)\"\n    src=\"https://superset.apache.org/img/superset-logo-horiz-dark.svg\"\n    alt=\"Superset logo (dark)\"\n  />\n  <img\n    width=\"600\"\n  ",
    "url": "https://github.com/apache/superset",
    "last_updated": "2025-09-02T09:27:04+00:00"
  },
  {
    "full_name": "ctesta01/QualtricsTools",
    "name": "QualtricsTools",
    "description": "Using R, Shiny, Pandoc, JSON, CSVs and more to automate processing Qualtrics surveys",
    "language": "R",
    "topics": [
      "shiny",
      "pandoc",
      "qualtrics",
      "reporting"
    ],
    "readme": "# QualtricsTools \n\nThis is no longer the main repository for the QualtricsTools project. Check out the continued work on this project in the [emmamorgan-tufts/QualtricsTools](https://github.com/emmamorgan-tufts/QualtricsTools/) repository. \n\nQualtricsTools is an [R](https://www.r-project.org/) package that automatically processes Qualtrics survey data into \nreports breaking down the responses to each question. The package creates \nreports that summarize the results of closed-ended questions, compiles appendices of open-ended text responses, and generates question dictionaries that describe the details of each survey question. It also can generate reports for subsets of respondents based on their response data. \nThis package uses the R web-application framework [Shiny](https://shiny.rstudio.com/), \nthe universal document converter [Pandoc](http://pandoc.org/), \n[Roxygen2](https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html) documentation, \nand much more. \n\nThis package was developed for Tufts University's [Office of Institutional Research and Evaluation](http://provost.tufts.edu/institutionalresearch/). \nAnyone is welcome to use it.\n\n![A slideshow of the QualtricsTools web app](https://github.com/ctesta01/QualtricsTools/blob/master/pics/animation.gif?raw=true)\n\n## Installing and Running the Shiny App\nBefore installing, you must install [R or Rstudio](https://www.rstudio.com/), \n[devtools](https://github.com/hadley/devtools), \n[Rtools](https://cran.r-project.org/bin/windows/Rtools/) (if you're on Windows), \nand [Pandoc](http://pandoc.org/). For Rtools with Windows, \n[please make sure Rtools is added to the `Path` environment variable](http://stackoverflow.com/a/29480538/3161979). You need to `install.packages('devtools')` or have already installed the devtools package in R. After installing each of the prerequisites, to install QualtricsTools run the following in R:\n\n    devtools::install_github(\"ctesta01/QualtricsTools\")\n    \nThe QualtricsTools p",
    "url": "https://github.com/ctesta01/QualtricsTools",
    "last_updated": "2025-04-29T15:10:08+00:00"
  },
  {
    "full_name": "brian-bot/githubr",
    "name": "githubr",
    "description": "Making it easy to talk to GitHub from R",
    "language": "R",
    "topics": [],
    "readme": "## githubr\n##### Making it easy to talk to GitHub from R\n\n-----\n[![Travis-CI Build Status](https://travis-ci.org/brian-bot/githubr.svg?branch=master)](https://travis-ci.org/brian-bot/githubr)\n\n##### Installing `githubr`:\n\nCurrently, `githubr` is not available via CRAN, but can be installed directly from GitHub using the `devtools` package. A little bit meta, huh?\n\n```r\ninstall.packages(\"devtools\")\nrequire(devtools)\ninstall_github(\"brian-bot/githubr\")\n```\n\n-----\n\n##### Overview\n\nThe purpose is to allow users to:\n* Use GitHub as a version control system for analysis code in addition to enterprise software development\n* Store code centrally for sourcing into an analysis environment\n* Allow easy sharing and forking of code that may be useful for others\n\nCurrent status:\n* For access to private repositories and/or to have an increased limit on the number of API calls, users should register a personal access token with the client via the `setGithubToken` function. Personal access tokens can be generated on your [GitHub settings page](https://github.com/settings/tokens). Once you have called `setGithubToken`, the token passed to this function is then used for all subsequent calls to GitHub API for the current R session.\n* Currently staying away from downloading files -- sticking with meta-information and ability to directly source files\n* `sourceRepoFile` sources code into the global environment, but allows users to pass optional arguments as specified in the `base::source()` function\n\n-----\n\n##### Quickstart Guide\n\n```r\n## LOAD THE PACKAGE\nrequire(githubr)\n\n## USE TOKEN TO AUTHENTICATE FOR YOUR CURRENT SESSION\nsetGithubToken(\"myTokenFromGithub12345678\")\n\n#####\n## ACCESSING META-INFORMATION ABOUT A GITHUB REPOSITORY\n#####\n## FOR HEAD OF MASTER BRANCH OF A GITHUB REPOSITORY\nrepoHead <- getRepo('brian-bot/githubr')\n\n## OR HEAD OF A SPECIFIC BRANCH (dev)\nrepoBranch <- getRepo('brian-bot/githubr', ref='branch', refName='dev')\n\n## OR A SPECIFIC COMMIT\nrepoCommit <- getRepo('bria",
    "url": "https://github.com/brian-bot/githubr",
    "last_updated": "2024-10-27T22:30:52+00:00"
  },
  {
    "full_name": "huggingface/smolagents",
    "name": "smolagents",
    "description": "🤗 smolagents: a barebones library for agents that think in code.",
    "language": "Python",
    "topics": [],
    "readme": "<!---\nCopyright 2024 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n<p align=\"center\">\n    <!-- Uncomment when CircleCI is set up\n    <a href=\"https://circleci.com/gh/huggingface/accelerate\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/master\"></a>\n    -->\n    <a href=\"https://github.com/huggingface/smolagents/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/github/license/huggingface/smolagents.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/smolagents\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/smolagents/index.html.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/smolagents/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/smolagents.svg\"></a>\n    <a href=\"https://github.com/huggingface/smolagents/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n</p>\n\n<h3 align=\"center\">\n  <div style=\"display:flex;flex-direction:row;\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/smolagents.png\" alt=\"Hugging Face mascot as James Bond\" width=400px>\n    <p>Agents that think in code!</p>\n  </div>\n</h3>\n\n`smolagents` is a library that enables you to run powerful agents in a few lines of code. It",
    "url": "https://github.com/huggingface/smolagents",
    "last_updated": "2025-09-02T09:52:27+00:00"
  },
  {
    "full_name": "WZBSocialScienceCenter/pdftabextract",
    "name": "pdftabextract",
    "description": "A set of tools for extracting tables from PDF files helping to do data mining on (OCR-processed) scanned documents.",
    "language": "Python",
    "topics": [
      "pdf",
      "data-mining",
      "python",
      "image-processing",
      "tables",
      "ocr"
    ],
    "readme": "# pdftabextract - A set of tools for data mining (OCR-processed) PDFs\n\nJuly 2016 / Feb. 2017, Markus Konrad <markus.konrad@wzb.eu> <post@mkonrad.net> / [Berlin Social Science Center](https://www.wzb.eu/en)\n\n**This project is currently not maintained.**\n\n**IMPORTANT INITIAL NOTES**\n\nFrom time to time I receive emails from people trying to extract tabular data from PDFs. I'm fine with that and I'm glad to help. However, some people think that *pdftabextract* is some kind of magic wand that automatically extracts the data they want by simply running one of the provided examples on *their* documents. This, in the very most cases, won't work. I want to clear up a few things that you should consider before using this software and before writing an email to me:\n\n1. pdftabextract is **not an OCR (optical character recognition) software**. It requires scanned pages *with OCR information*, i.e. a \"sandwich PDF\" that contains both the scanned images and the recognized text. You need software like tesseract or ABBYY Finereader for OCR. In order to check if you have a \"sandwich PDF\", open your PDF and press \"select all\". This usually reveals the OCR-processed text information. \n2. pdftabextract is some kind of **last resort** when all other things fail for extracting tabular data from PDFs. Before trying this out, you should ask yourself the following questions:\n  * Is there *really* no other way / no other format for which the data is available?\n  * Can a special OCR software like ABBYY Finereader detect and extract the tables (you need to try this with a large sample of pages -- I found the table recognition in Finereader often unreliable)?\n  * Is it possible to extract the recognized text as-is from the PDFs and parse it? Try using the `pdftotext` tool from **poppler-utils**, a package which is part of most Linux distributions and is also available for OSX via Homebrew or MacPorts: `pdftotext -layout yourdocument.pdf`. This will create a file `yourdocument.txt` containing the",
    "url": "https://github.com/WZBSocialScienceCenter/pdftabextract",
    "last_updated": "2025-08-28T07:06:17+00:00"
  },
  {
    "full_name": "yaringal/BayesianRNN",
    "name": "BayesianRNN",
    "description": "Code for the paper \"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\"",
    "language": "Lua",
    "topics": [],
    "readme": "This is the code used for the experiments in the paper [\"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\"](http://mlg.eng.cam.ac.uk/yarin/publications.html#Gal2015Theoretically). The [sentiment analysis experiment](Sentiment_analysis_code/) relies on a [fork of keras](https://github.com/yaringal/keras/tree/BayesianRNN) which implements Bayesian LSTM, Bayesian GRU, embedding dropout, and MC dropout. The [language model experiment](LM_code/) extends [wojzaremba's lua code](https://github.com/wojzaremba/lstm).\n\n## Update 1 (Feb 22): \n[Keras](https://keras.io/layers/recurrent/) now supports dropout in RNNs following the implementation above. A simplified example of the [sentiment analysis experiment](Sentiment_analysis_code/) using the latest keras implementation is given in [here](Example/).\n\n## Update 2 (March 28): \nThe script [main_new_dropout_SOTA](LM_code/main_new_dropout_SOTA.lua) implements Bayesian LSTM (Gal, 2015) for the large model of Zaremba et al. (2014). In the setting of Zaremba et al. the states are not reset and the testing is done with a single pass through the test set. The only changes I've made to the setting of Zaremba et al. are:\n\n1. dropout technique (using a Bayesian LSTM)\n2. weight decay (which was chosen to be zero in Zaremba et al.)\n3. a slightly smaller network was used to fit on my GPU (1250 units per layer instead of 1500)\n\nAll other hypers being identical to Zaremba et al.: learning rate decay was not tuned for my setting and is used following Zaremba et al., and the sequences are initialised with the previous state following Zaremba et al. (unlike in main_dropout.lua). Dropout parameters were optimised with grid search (tying dropout_x & dropout_h and dropout_i & dropout_o) over validation perplexity (optimal values are 0.3 and 0.5 compared Zaremba et al.'s 0.6).\n\nSingle model validation perplexity is improved from Zaremba et al.'s 82.2 to 79.1. Test perplexity is reduced from 78.4 to 76.5, see [log](LM_code/",
    "url": "https://github.com/yaringal/BayesianRNN",
    "last_updated": "2025-08-18T20:15:38+00:00"
  },
  {
    "full_name": "mimno/TwelveMedievalGhostStories",
    "name": "TwelveMedievalGhostStories",
    "description": "Stories transcribed by M.R. James from a manuscript from Byland Abbey",
    "language": "",
    "topics": [],
    "readme": "# TwelveMedievalGhostStories\n\n[Latin Text](/latin.md)\n\n[English Translation](/english.md)\n\nStories transcribed by M.R. James from a manuscript from Byland Abbey.\nThis collection is a wonderful perspective on James' own ghost stories, along with his edition of Walter Map's \"Courtly Trifles\".\nIf you haven't read anything by James, \"Casting the Runes\" will give you an entirely new perspective on blind peer review.\n\nThe stories were published in the English Historical Review in July 1922.\nA scan of the article is available at [Google Books](https://books.google.com/books?id=d6LRAAAAMAAJ&pg=PA414#v=onepage&q&f=false).\nI corrected the OCR from this edition, which was remarkably accurate given the rather obscure medieval Latin.\nI removed running page headers and marginal references to folio page breaks; if these are of interest they can be readily restored from the scan.\nThe encoding is in Markdown format with UTF-8 character encoding. Anna Waymack identified an obscure character (Eboꝝ) as an \"r-rotunda with cut\" and pointed me to the Unicode codepoint for it. This character usually represents *-rum* but can represent any syllable starting with *r*. In this case it seems to mean to *Ebo-racum*, or York. York is the closest city to Byland Abbey so it seems likely to me that the scribe would have a ready abbreviation.\n\nA recent discussion on the content and background of the work by Maik Hildebrandt can be found at [JSTOR](https://www.jstor.org/stable/j.ctv2t4d7f.5).\nThe stories appear to be genuinely old -- this is not merely the familiar \"so I found this old manuscript\" frame story.\nWaymack points to [an image from the British Library](https://www.bl.uk/catalogues/illuminatedmanuscripts/ILLUMIN.ASP?Size=mid&IllID=41133) of a page from the manuscript, which I can recognize as the material transcribed, but which would require considerable palaeographic experience to transcribe by itself.\n\nThe Latin is fairly straightforward, with a few medieval spellings, notably *-e* for *-",
    "url": "https://github.com/mimno/TwelveMedievalGhostStories",
    "last_updated": "2019-06-22T05:09:30+00:00"
  },
  {
    "full_name": "public-bible/website",
    "name": "website",
    "description": "Website for America's Public Bible",
    "language": "HTML",
    "topics": [],
    "readme": "Website, visualizations, and data export for [America's Public Bible](http://americaspublicbible.org).\n\nAll code and data are licensed MIT. All text and visualizations are copyright Lincoln Mullen, all rights reserved. (But that's just a temporary state of things until I decide the best long term license.)\n\n",
    "url": "https://github.com/public-bible/website",
    "last_updated": "2023-10-14T22:07:25+00:00"
  },
  {
    "full_name": "stanfordnlp/GloVe",
    "name": "GloVe",
    "description": "Software in C and data files for the popular GloVe model for distributed word representations, a.k.a. word vectors or embeddings",
    "language": "C",
    "topics": [],
    "readme": "## GloVe: Global Vectors for Word Representation\n\n\n| nearest neighbors of <br/> <em>frog</em> | Litoria             |  Leptodactylidae | Rana | Eleutherodactylus |\n| --- | ------------------------------- | ------------------- | ---------------- | ------------------- |\n| Pictures | <img src=\"https://nlp.stanford.edu/projects/glove/images/litoria.jpg\"></img> | <img src=\"https://nlp.stanford.edu/projects/glove/images/leptodactylidae.jpg\"></img> | <img src=\"https://nlp.stanford.edu/projects/glove/images/rana.jpg\"></img> | <img src=\"https://nlp.stanford.edu/projects/glove/images/eleutherodactylus.jpg\"></img> |\n\n| Comparisons | man -> woman             |  city -> zip | comparative -> superlative |\n| --- | ------------------------|-------------------------|-------------------------|\n| GloVe Geometry | <img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman_small.jpg\"></img>  | <img src=\"https://nlp.stanford.edu/projects/glove/images/city_zip_small.jpg\"></img> | <img src=\"https://nlp.stanford.edu/projects/glove/images/comparative_superlative_small.jpg\"></img> |\n\nWe provide an implementation of the GloVe model for learning word representations, and describe how to download web-dataset vectors or train your own. See the [project page](https://nlp.stanford.edu/projects/glove/) or the [paper](https://nlp.stanford.edu/pubs/glove.pdf) for more information on glove vectors. For documentation and analysis of 2024 vectors, please see the [report](https://arxiv.org/abs/2507.18103)\n\n## Download pre-trained word vectors \\*\\***NEW 2024 VECTORS**\\*\\*\nThe links below contain word vectors obtained from the respective corpora. If you want word vectors trained on massive web datasets, you need only download one of these text files! Pre-trained word vectors are made available under the <a href=\"https://opendatacommons.org/licenses/pddl/\">Public Domain Dedication and License</a>.\n<div class=\"entry\">\n<ul style=\"padding-left:0px; margin-top:0px; margin-bottom:0px\">\n  <li> **NEW!!** 20",
    "url": "https://github.com/stanfordnlp/GloVe",
    "last_updated": "2025-08-31T10:37:47+00:00"
  },
  {
    "full_name": "soodoku/distortions",
    "name": "distortions",
    "description": "Replication Data and Scripts for Deliberative Distortions",
    "language": "R",
    "topics": [
      "deliberation"
    ],
    "readme": "## Replication Data and Scripts for Deliberative Distortions\n\nPaper: http://gsood.com/research/papers/DeliberativeDistortions.pdf\n\n### Data\n\n* [Data](data/polardata.csv)\n* [Metadata on Polls](data/poll_indices.csv)\n\n### Scripts and Outputs\n\n1. [DP Summary](scripts/01_summary_dp_data_table_1.R)\n    * Produces [Table 1](tabs/01_table_1_dp_summary.csv)\n\n2. [Homogenization and Polarization by Poll and Aggregate](scripts/02_hom_pol_table_2_3.R)\n    * [s.e.](scripts/05a_hp_se.R)\n    * Produces [Table 2 Rows](tabs/02_table_2_hom_pol.csv)\n\n3. Domination\n    - [Education](scripts/03a_dom_educ.R)\n    - [Gender](scripts/03b_dom_gender.R)\n    - [Income](scripts/03c_dom_income.R)\n    - [Education, Income, and Gender Combined](scripts/03d_dom_men_income_ed.R)\n    * Produces Table 3 Rows\n        * Underlying tables: \n            * [Female](tabs/04_table_4a_toward_female.csv)\n            * [Male](tabs/04_table_4a_toward_male.csv)\n            * [Lower Education](tabs/04_table_4b_toward_lowed.csv)\n            * [Higher Education](tabs/04_table_4b_toward_highed.csv)\n            * [Lower Income](tabs/04_table_4c_toward_lowinc.csv)\n            * [Higher Income](tabs/04_table_4c_toward_highinc.csv)\n            * [Triple Disadv.](tabs/04_table_4d_toward_triple.csv)\n            * [Triple Adv.](tabs/04_table_4d_toward_triple_disadv.csv)\n    * [s.e.](scripts/05b_dom_se.R)\n\n4. [Correlation Between HPD](scripts/04_corr_hpd.R)\n    * Produces [tabs/05_corr_hom_pol.csv](tabs/05_corr_hom_pol.csv)\n\n5. [Figures](scripts/06_figs.R)\n    - Uses output tables from steps 1, 2, and 3 to produce all the figures except those produced in step 7. \n\n6. [Parsing Domination](scripts/07_parsing_domination.R) produces in-text numbers for the section on parsing domination.\n\n7. [Description of Groups](scripts/08_appendix_sample_description.R)---Not in the Paper---produces figures that show the distribution of proportion male, better educated, higher income across groups.\n\n8. [Attitude Change](scripts/09_attitude_cha",
    "url": "https://github.com/soodoku/distortions",
    "last_updated": "2023-02-04T12:38:29+00:00"
  },
  {
    "full_name": "DavyLandman/csvtools",
    "name": "csvtools",
    "description": "GNU-alike tools for parsing RFC 4180 CSVs at high speed.",
    "language": "C",
    "topics": [],
    "readme": "# csvtools, fast processing of CSV streams\n[![Build Status](https://travis-ci.org/DavyLandman/csvtools.svg?branch=master)](https://travis-ci.org/DavyLandman/csvtools)\n[![Coverity Scan Build Status](https://img.shields.io/coverity/scan/5024.svg)](https://scan.coverity.com/projects/5024)\n[![codecov.io](https://codecov.io/github/DavyLandman/csvtools/coverage.svg?branch=master)](https://codecov.io/github/DavyLandman/csvtools?branch=master)\n\n\nAs our data gets bigger, CSV files grow in size.\nThe CSV format is not exactly pipe-friendly due to embedded newlines and quoted separators.\n[onyxfish/csvkit](https://github.com/onyxfish/csvkit) offers a great set of utilties for most tasks you would want to perform on CSV's in a gnu toolset kind of way.\nHowever, it is not fast. For reasonable data sets, this doesn't matter, but for CSVs of more than a few MBs, you start to feel the pain.\n\nThis repository contains gnu-alike tools for parsing [RFC 4180](https://tools.ietf.org/html/rfc4180) CSVs at high speed.\n\n## Tools\n\n- `csvcut` a `cut(1)` equivalent to drop columns from a csv file\n- `csvgrep` a `grep(1)` equivalent to match on one or more collumns per row, and only keep the rows matching all or any of the patterns. (it uses PRCE for regular expression goodness)\n- `csvawk` a wrapper for `awk(1)` which correctly recognizes rows and cells (even across newlines). This is comparable to [geoffroy-aubry/awk-csv-parser](https://github.com/geoffroy-aubry/awk-csv-parser), except that it also supports embedded newlines.\n- `csvpipe` and `csvunpipe` translate the newlines separating rows to `\\0` such that `sort -z` and `uniq -z` and other null-terminated-line based tools can be used more correctly.\n\n## Performance\n\nBenchmarking is complicated, the primary goal is to measure only that of interest, by reducing the impact of other factors. Originally csvtools was benchmarked on the [Canada 2011 census](http://www12.statcan.gc.ca/census-recensement/2011/dp-pd/prof/details/download-telecharger/comp",
    "url": "https://github.com/DavyLandman/csvtools",
    "last_updated": "2025-07-08T09:10:49+00:00"
  },
  {
    "full_name": "filterbubbler/filterbubbler-web-ext",
    "name": "filterbubbler-web-ext",
    "description": "The FilterBubbler WebExtension turns your browser into a collaborative text classification lab.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# FilterBubbler\n\nThis is an early sketch of a tool for performing collaborative text\nanalysis and classification of web page text content.\n\n## Screencasts\n\nWe have created some screencasts to introduce FilterBubbler and to demonstrate how to build the software.\n\n * [FilterBubbler functionality demo](https://www.youtube.com/watch?v=-1W9dEkNHN8)\n * [Checking out FilterBubbler from GIT and building it](https://youtu.be/Pnq_pwpR_ww)\n\n## Getting started\n\n### Setup\n\n * Firefox: You will need Firefox version 49 or higher, in order to support [temporary add-on installation](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Temporary_Installation_in_Firefox).\n\n * [Install Node.js and npm](https://docs.npmjs.com/getting-started/installing-node)\n\n * [Install web-ext](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext) with: `npm install --global web-ext`\n\n### Building\n\nTo build the extension you will need to clone this repository, change into the directory and run:\n\n```npm install```\n\n### Running locally\n\nChange to the `extension` subdirectory of this repository, and use web-ext to start Firefox with InfoBubbles.\n\n```sh\ncd extension\nweb-ext run\n```\n",
    "url": "https://github.com/filterbubbler/filterbubbler-web-ext",
    "last_updated": "2022-11-03T08:53:05+00:00"
  },
  {
    "full_name": "Kilo-Org/kilocode",
    "name": "kilocode",
    "description": "Open Source AI coding assistant for planning, building, and fixing code. We frequently merge features from open-source projects like Roo Code and Cline, while building our own vision. Follow us: kilocode.ai/social",
    "language": "TypeScript",
    "topics": [
      "ai-coding",
      "vscode",
      "vscode-extension",
      "ai-developer-tools",
      "chatgpt",
      "claude",
      "gemini",
      "sonnet"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://marketplace.visualstudio.com/items?itemName=kilocode.Kilo-Code\"><img src=\"https://img.shields.io/visual-studio-marketplace/v/kilocode.Kilo-Code.svg?label=VS%20Code%20Marketplace\" alt=\"VS Code Marketplace\"></a>\n  <a href=\"https://x.com/kilo_code\"><img src=\"https://img.shields.io/twitter/follow/kilo_code?style=flat&logo=x&color=555\" alt=\"X (Twitter)\"></a>\n  <a href=\"https://blog.kilocode.ai\"><img src=\"https://img.shields.io/badge/Blog-555?style=flat&logo=substack&logoColor=white\" alt=\"Substack Blog\"></a>\n  <a href=\"https://kilocode.ai/discord\"><img src=\"https://img.shields.io/discord/1349288496988160052?style=flat&logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  <a href=\"https://www.reddit.com/r/kilocode/\"><img src=\"https://img.shields.io/reddit/subreddit-subscribers/kilocode?style=flat&logo=reddit&logoColor=white\" alt=\"Reddit\"></a>\n</p>\n\n# 🚀 Kilo Code\n\n> Open-source VS Code AI agent. We frequently merge features from open-source projects, such as [Roo Code](https://github.com/RooVetGit/Roo-Code) and [Cline](https://github.com/cline/cline), while building our own vision.\n\n- ✨ Generate code from natural language\n- ✅ Checks its own work\n- 🧪 Run terminal commands\n- 🌐 Automate the browser\n- 🤖 Latest AI models\n- 🎁 API keys optional\n- 💡 **Get $25 in free credits: $5 when you sign up, $20 when you top-up for the first time** Credits can be used with 400+ models like Gemini 2.5 Pro, Claude 4 Sonnet & Opus, and GPT-5\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Kilo-Org/kilocode/refs/heads/main/kilo.gif\" width=\"100%\" />\n</p>\n\n- [VS Code Marketplace](https://kilocode.ai/vscode-marketplace?utm_source=Readme) (download)\n- [Official KiloCode.ai Home page](https://kilocode.ai) (learn more)\n\n## Key Features\n\n- **Code Generation:** Generate code using natural language.\n- **Task Automation:** Automate repetitive coding tasks.\n- **Automated Refactoring:** Refactor and improve existing code.\n- **MCP Server Marketplace**: Easil",
    "url": "https://github.com/Kilo-Org/kilocode",
    "last_updated": "2025-09-02T10:08:24+00:00"
  },
  {
    "full_name": "facebookresearch/watermark-anything",
    "name": "watermark-anything",
    "description": "Official implementation of the paper \"Watermark Anything with Localized Messages\"",
    "language": "Jupyter Notebook",
    "topics": [
      "image",
      "watermarking",
      "image-watermarking"
    ],
    "readme": "# 🐤 Watermark Anything\n\nImplementation and pretrained models for the paper [**Watermark Anything**](https://arxiv.org/abs/2411.07231). \nOur approach allows for embedding (possibly multiple) localized watermarks into images.\n\n<!-- [[`Webpage`](...)] -->\n[[`arXiv`](https://arxiv.org/abs/2411.07231)]\n[[`Colab`](https://colab.research.google.com/github/facebookresearch/watermark-anything/blob/main/notebooks/colab.ipynb)]\n[[`HF Demo`]](https://huggingface.co/spaces/xiaoyao9184/watermark-anything)\n[[`Podcast`](https://notebooklm.google.com/notebook/6c69b3f8-b1a6-41c4-92fb-c416903ceb49/audio)]\n[[`Hacker News`](https://news.ycombinator.com/item?id=42113674)]\n[[`Video`](https://youtu.be/gwnYmoVzJCo)]\n\n![Watermark Anything Overview](assets/splash_wam.jpg)\n\n## 📰 News\n\n### [January 30, 2025] - Watermark Anything is [accepted](https://openreview.net/forum?id=IkZVDzdC8M) at ICLR 2025!\n\n### [December 12, 2024] - New WAM Model Released Under MIT License!\n- 📢 We are excited to announce the release of the weights for our new model, trained on a subset of the [SA-1B](https://ai.meta.com/datasets/segment-anything/) dataset, now available under the MIT License.\n- We've also enhanced the model's robustness, particularly in handling moving watermarked objects in images, and for the rest it should yield similar results than the model in the publication. \n\n\n## Requirements\n\n\n### Installation\n\nThis repos was tested with Python 3.10.14, PyTorch 2.5.1, CUDA 12.4, Torchvision 0.20.1:\n```cmd\nconda create -n \"watermark_anything\" python=3.10.14\nconda activate watermark_anything\nconda install pytorch torchvision pytorch-cuda=12.4 -c pytorch -c nvidia\n```\n\nInstall the required packages:\n```cmd\npip install -r requirements.txt\n```\n\n### Weights\n\nDownload the latest pre-trained model weights - trained on [SA-1B](https://ai.meta.com/datasets/segment-anything/) and under MIT license - [here](https://dl.fbaipublicfiles.com/watermark_anything/wam_mit.pth), or via command line:\n```cmd\nwget https://dl.fbaipub",
    "url": "https://github.com/facebookresearch/watermark-anything",
    "last_updated": "2025-09-01T13:47:32+00:00"
  },
  {
    "full_name": "tisjune/whitehouse-transcripts",
    "name": "whitehouse-transcripts",
    "description": "Utilities for retrieving whitehouse.gov transcripts and matching news quotes to them",
    "language": "Python",
    "topics": [],
    "readme": "whitehouse-transcripts\n======================\n\nUtilities for retrieving whitehouse.gov transcripts and matching news quotes to them\n",
    "url": "https://github.com/tisjune/whitehouse-transcripts",
    "last_updated": "2025-05-13T13:19:44+00:00"
  },
  {
    "full_name": "mbostock/stack",
    "name": "stack",
    "description": "A presentation library with intuitive, scroll-based navigation.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# stack.js\n\nA presentation library with intuitive, scroll-based navigation.\n",
    "url": "https://github.com/mbostock/stack",
    "last_updated": "2025-08-18T11:46:48+00:00"
  },
  {
    "full_name": "pierredavidbelanger/ekmeans",
    "name": "ekmeans",
    "description": "A Java K-means Clustering implementation",
    "language": "Java",
    "topics": [],
    "readme": "## About\nA Java K-means Clustering implementation with an optional special equal option that apply an equal cardinality constraint on the clusters while remaining as spatially cohesive as possible.\n\nFor the impatients, please go the the Quick start section.\n\n## License\nI used the GNU General Public License Version 3. So you are free to use/modify/redistribute this code, as long as it is for an open source project.\n\nIf you plan to use this code in a commercial application, please let me know. I will probably gladly accept, but I just want to know.\n\n## Motivation\nI was searching the Internet to find a clustering algorithm that can produce equal size clusters, without success.\n\nI was not alone:\n\n* K-means algorithm variation with equal cluster size\n* Clustering procedure where each cluster has an equal number of points?\n\nSo I decided to take a look at the problem myself. I took a fairly simple and fast clustering algorithm (K-means), and I changed it a little to meet my requirements.\n\n## Known bugs\nThe equal option is somewhat experimental. You should know that, when this option is enabled, the algorithm do its best, in a reasonable time, to respect the equal cardinality constraint, but can sometime produce clusters that are not as spatially cohesive as the original K-means algorithm (keep in mind that this is a NP-hard problem). See issue 1. Please let me know if you want to contribute your great idea to fix this bug.\n\n## Help\nYou want to help? Especially with the [issue 1](https://github.com/pierredavidbelanger/ekmeans/issues/1)?\n\nGo ahead, clone and make some changes. Do not forget to add your name into the 'Contributor(s)' comment section of the files you edit :)\n\n## Quick start (demo)\nDownload the [latest compiled version](http://repo1.maven.org/maven2/ca/pjer/ekmeans/), and run the demo by double-clicking the jar file or by running it in a terminal with\n\n```bash\n$ java -jar ekmeans.jar\n```\n\nThe demo supports importing data from a CSV file. Each line terminated by",
    "url": "https://github.com/pierredavidbelanger/ekmeans",
    "last_updated": "2025-04-24T07:16:05+00:00"
  },
  {
    "full_name": "mthorrell/gbnet",
    "name": "gbnet",
    "description": "Gradient Boosting Modules for pytorch",
    "language": "Python",
    "topics": [],
    "readme": "# GBNet\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.08047/status.svg)](https://doi.org/10.21105/joss.08047)\n\n\n\nPytorch Modules for XGBoost and LightGBM\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Install and Docs](#install-and-docs)\n3. [Pytorch Modules](#pytorch-modules)\n   - [Conceptually, how can Pytorch be used to fit XGBoost or LightGBM models?](#conceptually-how-can-pytorch-be-used-to-fit-xgboost-or-lightgbm-models)\n   - [Is training a `gbnet` model closer to training a neural network or to training a GBM?](#is-training-a-gbnet-model-closer-to-training-a-neural-network-or-to-training-a-gbm)\n   - [Basic training of a GBM for comparison to existing gradient boosting packages](#basic-training-of-a-gbm-for-comparison-to-existing-gradient-boosting-packages)\n   - [Training XGBoost and LightGBM together](#training-xgboost-and-lightgbm-together)\n4. [Models](#models)\n   - [Forecasting](#forecasting)\n   - [Ordinal Regression](#ordinal-regression)\n5. [Contributing](#contributing)\n6. [Cite this work](#cite-this-work)\n\n\n\n## Introduction\n\nXGBoost and LightGBM are industry-standard gradient boosting packages used to solve tabular data machine learning problems. Users of these packages wishing to define custom loss functions, novel architectures, or other advanced modeling scenarios, however, may face substantial difficulty due to potentially complex gradient and Hessian calculations required by both XGBoost and LightGBM. GBNet provides PyTorch Modules wrapping XGBoost and LightGBM so that users can construct and fit nearly arbitrary model architectures involving XGBoost or LightGBM without requiring users to provide gradient and Hessian calculations. PyTorch's autograd system calculates derivative information automatically; GBNet orchestrates delivery of that information back to the boosting algorithms. GBNet, by linking XGBoost and LightGBM to PyTorch, expands the set of applications for gradient boosting models.\n\nThere are two main components of `gbn",
    "url": "https://github.com/mthorrell/gbnet",
    "last_updated": "2025-08-18T21:53:07+00:00"
  },
  {
    "full_name": "automeris-io/digitizeR",
    "name": "digitizeR",
    "description": "R package to extract data from plots and other images. Hosts WebPlotDigitizer locally.",
    "language": "JavaScript",
    "topics": [],
    "readme": "digitizeR\n---------\n\n[WebPlotDigitizer](https://automeris.io/WebPlotDigitizer) powered R package for data extraction from images of plots, maps etc.\n\nInstall\n-------\n\nThis package is under heavy development, but uses a stable version of WebPlotDigitizer. If you want to give this a try, then follow these instructions:\n\n\n1) If you don't already have devtools, then install using:\n\n        install.packages(\"devtools\")\n    \n2) Install digitizeR (Linux/Mac/Windows):\n    \n        devtools::install_github(\"ankitrohatgi/digitizeR\")\n        \nUse\n---\n\nNo real-time communication has been implemented at the moment, but you can launch and close WPD using the following:\n\nLoad library:\n\n    library('digitizeR')\n\nLaunch a local instance of WebPlotDigitizer:\n\n    app <- wpd_launch()\n    \nThis starts a httpuv based server on your machine. This should also open the local URL in a browser window.\n\nClose server instance:\n\n    wpd_close(app)\n\nChange default server location and port:\n\n    app <- wpd_launch(location=\"192.168.1.100\", port=8080) # for example\n\nGoals\n-----\n\nAt the moment, this package only lets you start (and stop) WebPlotDigitizer, but eventually, I would like to add R functions that can communicate with the app in real-time (using WebSockets). A few examples of what is possible in the future are as follows:\n\n    ds <- wpd_getDatasets(app)                  # fetch all digitized data as a data frame.\n    wpd_loadImage(app, 'my_plot.jpg')           # load an image file programmatically.\n    wpd_loadPDF(app, 'thesis.pdf', page=5)      # load a specific page from a PDF file.\n    wpd_calibrate(app, 'calibration_data.json') # align the axes to pixels using some calibration data.\n    \n    # and so on.\n\nContact\n-------\n\nAnkit Rohatgi <ankitrohatgi@hotmail.com>\n",
    "url": "https://github.com/automeris-io/digitizeR",
    "last_updated": "2024-03-27T23:09:55+00:00"
  },
  {
    "full_name": "pymc-devs/pymc",
    "name": "pymc",
    "description": "Bayesian Modeling and Probabilistic Programming in Python",
    "language": "Python",
    "topics": [
      "python",
      "statistical-analysis",
      "bayesian-inference",
      "mcmc",
      "variational-inference",
      "probabilistic-programming",
      "pytensor"
    ],
    "readme": ".. image:: https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg\n    :height: 100px\n    :alt: PyMC logo\n    :align: center\n\n|Build Status| |Coverage| |NumFOCUS_badge| |Binder| |Dockerhub| |DOIzenodo| |Conda Downloads|\n\nPyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling\nfocusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI)\nalgorithms. Its flexibility and extensibility make it applicable to a\nlarge suite of problems.\n\nCheck out the `PyMC overview <https://docs.pymc.io/en/latest/learn/core_notebooks/pymc_overview.html>`__,  or\none of `the many examples <https://www.pymc.io/projects/examples/en/latest/gallery.html>`__!\nFor questions on PyMC, head on over to our `PyMC Discourse <https://discourse.pymc.io/>`__ forum.\n\nFeatures\n========\n\n-  Intuitive model specification syntax, for example, ``x ~ N(0,1)``\n   translates to ``x = Normal('x',0,1)``\n-  **Powerful sampling algorithms**, such as the `No U-Turn\n   Sampler <http://www.jmlr.org/papers/v15/hoffman14a.html>`__, allow complex models\n   with thousands of parameters with little specialized knowledge of\n   fitting algorithms.\n-  **Variational inference**: `ADVI <http://www.jmlr.org/papers/v18/16-107.html>`__\n   for fast approximate posterior estimation as well as mini-batch ADVI\n   for large data sets.\n-  Relies on `PyTensor <https://pytensor.readthedocs.io/en/latest/>`__ which provides:\n    *  Computation optimization and dynamic C or JAX compilation\n    *  NumPy broadcasting and advanced indexing\n    *  Linear algebra operators\n    *  Simple extensibility\n-  Transparent support for missing value imputation\n\n\nLinear Regression Example\n==========================\n\n\nPlant growth can be influenced by multiple factors, and understanding these relationships is crucial for optimizing agricultural practices.\n\nImagine we conduct an experiment to predict the growth of a plant based on different environmental variables.\n\n.. code-block:: python\n\n   import py",
    "url": "https://github.com/pymc-devs/pymc",
    "last_updated": "2025-09-02T08:07:01+00:00"
  },
  {
    "full_name": "gojiplus/captr",
    "name": "captr",
    "description": "R Client for the Captricity API",
    "language": "R",
    "topics": [
      "captricity",
      "captricity-api",
      "ocr",
      "cran",
      "r"
    ],
    "readme": "## captr: R Client for the Captricity API\n\n[![Build Status](https://travis-ci.org/soodoku/captr.svg?branch=master)](https://travis-ci.org/soodoku/captr)\n[![Build status](https://ci.appveyor.com/api/projects/status/ck34qnr03mpbuke7?svg=true)](https://ci.appveyor.com/project/soodoku/captr)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/captr)](https://cran.r-project.org/package=captr)\n![](http://cranlogs.r-pkg.org/badges/grand-total/captr)\n[![Coverage Status](https://img.shields.io/codecov/c/github/soodoku/captr/master.svg)](https://codecov.io/github/soodoku/captr?branch=master)\n[![Research software impact](http://depsy.org/api/package/cran/captr/badge.svg)](http://depsy.org/package/r/captr)\n[![Github Stars](https://img.shields.io/github/stars/soodoku/captr.svg?style=social&label=Github)](https://github.com/soodoku/captr)\n\nOCR text and handwritten forms using [Captricity](https://captricity.com/). Captricity's big advantage over [Abbyy Cloud OCR](https://github.com/soodoku/abbyyR) is that it allows the user to easily specify the position of text-blocks that want to OCR; they have a simple web-based UI. The quality of the OCR can be checked using `compare_txt` from [recognize](https://github.com/soodoku/recognize). \n\n\n### Installation\n\nTo get the latest version on CRAN:\n```r\ninstall.packages(\"captr\")\n```\n\nTo get the current development version from GitHub:\n\n```r\ninstall.packages(\"devtools\")\ndevtools::install_github(\"soodoku/captr\", build_vignettes = TRUE)\n```\n\n-------------------\n### Using captr\n\nRead the vignette:\n```r\nvignette(\"using_captr\", package = \"captr\")\n```\n\nor follow the overview below.\n\nStart by getting an application token and setting it using:\n\n```r\nset_token(\"token\")\n```\n\nThen, create a batch using:\n\n```r\ncreate_batch(\"batch_name\")\n```\n\nOnce you have created a batch, you need to get the template ID (it tells Captricity what data to pull from where). Captricity requires a template. These templates can be created using the [Web UI](https://shredd",
    "url": "https://github.com/gojiplus/captr",
    "last_updated": "2024-11-04T13:36:09+00:00"
  },
  {
    "full_name": "yonicd/carbonate",
    "name": "carbonate",
    "description": "carbon.js for R",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/carbonate)](https://cran.r-project.org/package=carbonate)\n[![](https://cranlogs.r-pkg.org/badges/carbonate)](https://cran.r-project.org/package=carbonate)\n[![Travis-CI Build\nStatus](https://travis-ci.org/yonicd/carbonate.svg?branch=master)](https://travis-ci.org/yonicd/carbonate)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/yonicd/carbonate/master.svg)](https://codecov.io/github/yonicd/carbonate?branch=master)\n[![Covrpage\nSummary](https://img.shields.io/badge/covrpage-Last_Build_2022_08_21-yellowgreen.svg)](https://goo.gl/gNRcCb)\n\n“[carbon.js](https://carbon.now.sh/about) is the easiest way to create\nbeautiful images of your source code.”\n\nThis package uses an `R6` api to interact with carbon.js and create\ndirectly from the console carbon images.\n\nLike the image below:\n\n![](man/figures/unnamed-chunk-3-1.png)<!-- -->\n\n## Installation\n\n``` r\n#remotes::install_github('yonicd/carbonate')\ninstall.packages('carbonate')\n```\n\n## Usage\n\n### Initialize new carbon object\n\n``` r\nlibrary(carbonate)\n```\n\nThe default code in the carbon object is taken from the clipboard.\n\n``` r\nx <- carbon$new()\n```\n\nBut can also be defined inline. Code can be a character object of any\nlength.\n\n``` r\nx <- carbon$new(readLines('DESCRIPTION'))\n```\n\nThe code is kept in the object and can be changed at any time.\n\n``` r\nx$code\n#>  [1] \"Package: carbonate\"                                        \n#>  [2] \"Title: Interact with 'carbon.js'\"                          \n#>  [3] \"Version: 0.1.4\"                                            \n#>  [4] \"Authors@R: \"                                               \n#>  [5] \"    person(given = \\\"Jonathan\\\",\"                          \n#>  [6] \"           family = \\\"Sidi\\\",\"                             \n#>  [7] \"           role = c(\\\"aut\\\", \\\"cre\\\"),\"                    \n#>  [8] \"           email = \\\"yonicd@gmail.com\\\",\"         ",
    "url": "https://github.com/yonicd/carbonate",
    "last_updated": "2025-04-03T12:36:40+00:00"
  },
  {
    "full_name": "tiimgreen/github-cheat-sheet",
    "name": "github-cheat-sheet",
    "description": "A list of cool features of Git and GitHub.",
    "language": "",
    "topics": [
      "awesome",
      "awesome-list",
      "list",
      "github",
      "git"
    ],
    "readme": "# GitHub Cheat Sheet [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\nA collection of cool hidden and not so hidden features of Git and GitHub. This cheat sheet was inspired by [Zach Holman](https://github.com/holman)'s [Git and GitHub Secrets](http://confreaks.tv/videos/aloharuby2012-git-and-github-secrets) talk at Aloha Ruby Conference 2012 ([slides](https://speakerdeck.com/holman/git-and-github-secrets)) and his [More Git and GitHub Secrets](https://vimeo.com/72955426) talk at WDCNZ 2013 ([slides](https://speakerdeck.com/holman/more-git-and-github-secrets)).\n\n*Shortlink: [`http://git.io/sheet`](http://git.io/sheet)*\n\n*Read this in other languages: [English](README.md), [한국어](README.ko.md), [日本語](README.ja.md), [简体中文](README.zh-cn.md), [正體中文](README.zh-tw.md).*\n\nGitHub Cheat Sheet is sponsored by [Snapshot: create interactive professional-quality product photos using AI](https://www.snapshotapp.io/)\n\n## Table of Contents\n  - [GitHub](#github)\n    - [Ignore Whitespace](#ignore-whitespace)\n    - [Adjust Tab Space](#adjust-tab-space)\n    - [Commit History by Author](#commit-history-by-author)\n    - [Cloning a Repository](#cloning-a-repository)\n    - [Branch](#branch)\n      - [Compare all Branches to Another Branch](#compare-all-branches-to-another-branch)\n      - [Comparing Branches](#comparing-branches)\n      - [Compare Branches across Forked Repositories](#compare-branches-across-forked-repositories)\n    - [Gists](#gists)\n    - [Git.io](#gitio)\n    - [Keyboard Shortcuts](#keyboard-shortcuts)\n    - [Line Highlighting in Repositories](#line-highlighting-in-repositories)\n    - [Closing Issues via Commit Messages](#closing-issues-via-commit-messages)\n    - [Cross-Link Issues](#cross-link-issues)\n    - [Locking Conversations](#locking-conversations)\n    - [CI Status on Pull Requests](#ci-status-on-pull-requests)\n    - [Filters](#filters)\n    - [Syntax Highlighting ",
    "url": "https://github.com/tiimgreen/github-cheat-sheet",
    "last_updated": "2025-09-02T08:18:37+00:00"
  },
  {
    "full_name": "Deleetdk/OKCubot",
    "name": "OKCubot",
    "description": "A Scrapy scraper to scrape OKCupid.",
    "language": "Python",
    "topics": [],
    "readme": "# OKCubot\n\nScraping people for science and stuff.\n\n## Instructions\n\n1. Install Python 2.7\n2. Install pip (you could use python get-pip.py, but don't trust anyone)\n3. Add pip to your path\n4. python -m pip install -r requirements.txt\n\n## Troubleshooting\n\nI don't know. Add an issue.\n",
    "url": "https://github.com/Deleetdk/OKCubot",
    "last_updated": "2020-06-04T11:11:03+00:00"
  },
  {
    "full_name": "hiyali/apple-store-scraper",
    "name": "apple-store-scraper",
    "description": "Single API ☝ App Store Review Scraper 🧹",
    "language": "",
    "topics": [],
    "readme": "![build](https://img.shields.io/github/workflow/status/hiyali/apple-store-scraper/Build)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/hiyali/apple-store-scraper/pulls)\n[![PyPI](https://img.shields.io/pypi/v/apple-store-scraper)](https://pypi.org/project/apple-store-scraper/)\n![downloads](https://img.shields.io/pypi/dm/apple-store-scraper)\n![license](https://img.shields.io/pypi/l/apple-store-scraper)\n![code style](https://img.shields.io/badge/code%20style-black-black)\n\n```\n   ___                _____ _                   _____\n  / _ \\              /  ___| |                 /  ___|\n / /_\\ \\_ __  _ __   \\ `--.| |_ ___  _ __ ___  \\ `--.  ___ _ __ __ _ _ __   ___ _ __\n |  _  | '_ \\| '_ \\   `--. \\ __/ _ \\| '__/ _ \\  `--. \\/ __| '__/ _` | '_ \\ / _ \\ '__|\n | | | | |_) | |_) | /\\__/ / || (_) | | |  __/ /\\__/ / (__| | | (_| | |_) |  __/ |\n \\_| |_/ .__/| .__/  \\____/ \\__\\___/|_|  \\___| \\____/ \\___|_|  \\__,_| .__/ \\___|_|\n       | |   | |                                                    | |\n       |_|   |_|                                                    |_|\n```\n\n# Quickstart\n\nInstall:\n```console\npip3 install apple-store-scraper\n```\n\nScrape reviews for an app:\n\n```python\nfrom apple_store_scraper import AppStore\nfrom pprint import pprint\n\nminecraft = AppStore(country=\"nz\", app_name=\"minecraft\")\nminecraft.review(how_many=20)\n\npprint(minecraft.reviews)\npprint(minecraft.reviews_count)\n```\n\nScrape reviews for a podcast:\n\n```python\nfrom apple_store_scraper import Podcast\nfrom pprint import pprint\n\nsysk = Podcast(country=\"nz\", app_name=\"stuff you should know\")\nsysk.review(how_many=20)\n\npprint(sysk.reviews)\npprint(sysk.reviews_count)\n```\n\n# Extra Details\n\nLet's continue from the code example used in [Quickstart](#quickstart).\n\n\n## Instantiation\n\nThere are two required and one positional parameters:\n\n- `country` (required)\n  - two-letter country code of [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) standard\n- ",
    "url": "https://github.com/hiyali/apple-store-scraper",
    "last_updated": "2025-01-15T15:28:07+00:00"
  },
  {
    "full_name": "jupyterlab/jupyter-ai",
    "name": "jupyter-ai",
    "description": "A generative AI extension for JupyterLab",
    "language": "Python",
    "topics": [
      "generative-ai",
      "jupyter",
      "jupyterlab",
      "jupyterlab-extension"
    ],
    "readme": "# Jupyter AI\n\n**Jupyter AI is under incubation as part of the JupyterLab organization.**\n\nJupyter AI connects generative AI with Jupyter notebooks. Jupyter AI provides a user-friendly\nand powerful way to explore generative AI models in notebooks and improve your productivity\nin JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers:\n\n* An `%%ai` magic that turns the Jupyter notebook into a reproducible generative AI playground.\n  This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, Kaggle, VSCode, etc.).\n* A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant.\n* Support for a wide range of generative model providers, including AI21, Anthropic, AWS, Cohere,\n  Gemini, Hugging Face, MistralAI, NVIDIA, and OpenAI.\n* Local model support through GPT4All and Ollama, enabling use of generative AI models on consumer grade machines\n  with ease and privacy.\n\nDocumentation is available on [ReadTheDocs](https://jupyter-ai.readthedocs.io/en/latest/).\n\n![A screenshot of Jupyter AI showing the chat interface and the magic commands](docs/source/_static/jupyter-ai-screenshot.png)\n\n## Requirements\n\nYou will need to have installed the following software to use Jupyter AI:\n\n- Python 3.9 - 3.12\n- JupyterLab 4 or Notebook 7\n\nIn addition, you will need access to at least one model provider.\n\n> [!IMPORTANT]\n> JupyterLab 3 reached its end of maintenance date on May 15, 2024. As a result, we will not backport new features to the v1 branch supporting JupyterLab 3. Fixes for critical issues will still be backported until December 31, 2024. If you are still using JupyterLab 3, we strongly encourage you to **upgrade to JupyterLab 4 as soon as possible**. For more information, see [JupyterLab 3 end of maintenance](https://blog.jupyter.org/jupyterlab-3-end-of-maintenance-879778927db2) on the Jupyter Blog.\n\n## Setting Up Model Providers in a Notebook\n\nTo use any AI model provider within this not",
    "url": "https://github.com/jupyterlab/jupyter-ai",
    "last_updated": "2025-09-02T07:39:05+00:00"
  },
  {
    "full_name": "clauswilke/dataviz",
    "name": "dataviz",
    "description": "A book covering the fundamentals of data visualization",
    "language": "HTML",
    "topics": [],
    "readme": "# Fundamentals of Data Visualization\nA guide to making visualizations that accurately reflect the data, tell a story, and look professional.\n\nClaus O. Wilke\n\nThis repository holds the R Markdown source for the book \"Fundamentals of Data Visualization\" to be published with O’Reilly Media, Inc. A rendered version of the completed book chapters is [available here.](https://clauswilke.com/dataviz/) The book requires a supporting R package [available here.](https://github.com/clauswilke/dviz.supp)\n\nThe book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. Over the years, I have noticed that the same issues arise over and over. I have attempted to collect my accumulated knowledge from these interactions in the form of this book.\n\nIf you notice typos or other issues, feel free to open an issue on GitHub or send me a pull request. If you do the latter, in your commit message, please add the sentence \"I assign the copyright of this contribution to Claus O. Wilke,\" so that I can maintain the option of publishing this book in other forms.\n\nThis work is licensed under the [Attribution-NonCommercial-NoDerivatives 4.0 International](https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode) License. \n\n## Frequently Asked Questions\n\n**1\\. Can you include the R code for the figures as part of the book?**\n\nNo. The book is very purposefully designed to not be a programming book. The moment a book contains even one line of computer code, some people who don't use the chosen language will dismiss the book as not relevant to them. That's why the famous Numerical Recipes books had to be rewritten in so many different versions: Numerical Recipes in C, Numerical Recipes in Fortran, Numerical Recipes in C++. The recipes were always the same, but the C users didn't want to read the Fortran book and v",
    "url": "https://github.com/clauswilke/dataviz",
    "last_updated": "2025-09-02T00:40:43+00:00"
  },
  {
    "full_name": "lukesonnet/brazilNames",
    "name": "brazilNames",
    "description": "Names and Sexes of Brazilian Politicians",
    "language": "Python",
    "topics": [],
    "readme": "# Brazilian Politician's Names and Sex\n\nThis repository contains the raw data from the Brazilian Electoral Tribunal's website [here](http://www.tse.jus.br/hotSites/pesquisas-eleitorais/candidatos.html). Using this raw data, I build a data set with a list of ~76,500 Brazilian first names and how often each name is used for a male or female candidates in the 2000, 2004, 2008, and 2012 municipal elections in Brazil. Thus it effectively works as a tool to classify the gender of any Brazilian (and perhaps Portuguese) name.\n\nIn total, there are 1,758,134 candidates that ran in municipal and state elections from 1998-2014 and the sex of 1,652,685 of those candidates was reported in the official data. I take the first characters in the reported names that precede a space and designate that as the candidate's first name. This yielded 76,597 unique first names which make up this dataset. I then record the sex of each candidate and sum how many males and females were associated with each unique first name. There are errors in the names from the source dataset and there are probably errors in the reported sex as well. However, if you are matching names from your own dataset that does NOT have errors in the name, the dataset I provide here should work fairly well to predict the sex of those in your dataset.\n\n## Important Caveats\n\n- The data have not been cleaned thoroughly. If there were typos or errors in the original data uploaded by the Brazilian TSE then those errors carry forward in to this dataset with a few exceptions (leading punctuation has been removed)\n- Candidates that have run more than once may be repeated in this dataset. Future work could only use one name per individual, but this may be more work than it is worth.\n- Accents are maintained in their original encoding, which was ISO-8859-1.\n- Due to disparities in which\n\n## To do\n\n- Expand data source\n- Cleanly document workflow in README.md\n",
    "url": "https://github.com/lukesonnet/brazilNames",
    "last_updated": "2022-08-17T23:39:04+00:00"
  },
  {
    "full_name": "bowenbaker/metaqnn",
    "name": "metaqnn",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "MetaQNN Codebase\n========\n\nMetaQNN is a codebase used for automatically designing convolutional neural network architectures outlined in the paper: \n\n**[Designing Neural Network Architectures Using Reinforcement Learning](https://arxiv.org/pdf/1611.02167.pdf)**   \nBowen Baker, Otkrist Gupta, Nikhil Naik, Ramesh Raskar  \n*International Conference on Learning Representations*, 2017\n\nIf our software or paper helps your research or project, please cite us using:\n\n    @article{baker2017designing,\n      title={Designing Neural Network Architectures using Reinforcement Learning},\n      author={Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},\n      journal={International Conference on Learning Representations},\n      year={2017}\n    }\n\n# Installation\nAll code was only tested on ubuntu 16.04, python 2.7, with caffe at commit [d208b71](https://github.com/BVLC/caffe/tree/d208b714abb8425f1b96793e04508ad21724ae3f)\n\n1. Install caffe using these [instructions](https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide) with CUDA 8 and cuDNN 5.1.\n2. ```pip install -r requirements.txt```\n\n# Quick Example (CIFAR-10)\n1. Create CIFAR-10 LMDB's on each server you plan to use for training  \n\n    ``` bash\n    python libs/input_modules/lmdb_creator.py cifar10 /path/to/data/directory/cifar10 -gcn True -v 5000\n    ```\n    \n2. Modify `models/cifar10/hyper_parameters.py`  \n  2a. set `TRAIN_FILE = '/path/to/data/directory/cifar10/train.lmdb'`  \n  2b. set `VAL_FILE = '/path/to/data/directory/cifar10/val.lmdb'`  \n  2c. set `CAFFE_ROOT = '/path/to/caffe/installation/directory'`  \n  2d. (optional) set `CHECKPOINT_DIR = '/path/to/model/snapshot/directory/'`  \n3. Create directory `cifar10_logs` to store Q-values and replay database\n4. Start Q-Learning Server\n\n    ```bash \n    python q_server.py cifar10 cifar10_logs\n    ```\n    \n5. On each server you want to use for training start a Q-Learning Client\n\n    ```bash\n    python caffe_client.py cifar10 unique_client_id",
    "url": "https://github.com/bowenbaker/metaqnn",
    "last_updated": "2025-06-04T03:30:22+00:00"
  },
  {
    "full_name": "lmullen/dh-r",
    "name": "dh-r",
    "description": "Computational Historical Thinking: With Applications in R",
    "language": "TeX",
    "topics": [
      "digital-history",
      "computational-history",
      "history",
      "r"
    ],
    "readme": "[![Travis-CI Build Status](https://travis-ci.org/lmullen/dh-r.svg?branch=master)](https://travis-ci.org/lmullen/dh-r)\n\n# Computational Historical Thinking: With Applications in R\n\nThis is a textbook in progress on using computational methods for historical research. The [book is available online](https://dh-r.lincolnmullen.com).\n\n## License\n\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.\n",
    "url": "https://github.com/lmullen/dh-r",
    "last_updated": "2024-11-21T16:29:02+00:00"
  },
  {
    "full_name": "hrbrmstr/rstudioconf2017",
    "name": "rstudioconf2017",
    "description": "Slides/code/data from rstudio:: conf 2017",
    "language": "ASP",
    "topics": [],
    "readme": "\n### Slides, data and some code from rstudio::conf 2017\n\n[Official Conference Video](https://www.rstudio.com/resources/videos/writing-readable-code-with-pipes/)\n\nNOTE: the \"satcat\" code snippets which were intended solely to show how to pipe are now in a small [API pkg](https://github.com/hrbrmstr/satcat)\n\n![](readme.png)\n",
    "url": "https://github.com/hrbrmstr/rstudioconf2017",
    "last_updated": "2025-03-22T11:19:21+00:00"
  },
  {
    "full_name": "openai/human-eval",
    "name": "human-eval",
    "description": "Code for the paper \"Evaluating Large Language Models Trained on Code\"",
    "language": "Python",
    "topics": [],
    "readme": "# HumanEval: Hand-Written Evaluation Set \n\nThis is an evaluation harness for the HumanEval problem solving dataset\ndescribed in the paper \"[Evaluating Large Language Models Trained on\nCode](https://arxiv.org/abs/2107.03374)\".\n\n## Installation\n\nMake sure to use python 3.7 or later:\n```\n$ conda create -n codex python=3.7\n$ conda activate codex\n```\n\nCheck out and install this repository:\n```\n$ git clone https://github.com/openai/human-eval\n$ pip install -e human-eval\n```\n\n## Usage\n\n**This program exists to run untrusted model-generated code. Users are strongly\nencouraged not to do so outside of a robust security sandbox. The [execution\ncall](https://github.com/openai/human-eval/blob/master/human_eval/execution.py#L48-L58)\nin `execution.py` is deliberately commented out to ensure users read this\ndisclaimer before running code in a potentially unsafe manner. See the comment in\n`execution.py` for more information and instructions.**\n\nAfter following the above instructions to enable execution, generate samples\nand save them in the following JSON Lines (jsonl) format, where each sample is\nformatted into a single line like so:\n```\n{\"task_id\": \"Corresponding HumanEval task ID\", \"completion\": \"Completion only without the prompt\"}\n```\nWe provide `example_problem.jsonl` and `example_solutions.jsonl` under `data`\nto illustrate the format and help with debugging.\n\nHere is nearly functional example code (you just have to provide\n`generate_one_completion` to make it work) that saves generated completions to\n`samples.jsonl`.\n```\nfrom human_eval.data import write_jsonl, read_problems\n\nproblems = read_problems()\n\nnum_samples_per_task = 200\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n    for task_id in problems\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl(\"samples.jsonl\", samples)\n```\n\nTo evaluate the samples, run\n```\n$ evaluate_functional_correctness samples.jsonl\nReading samples...\n32800it [00:01, 23787.50it/s]\nRunni",
    "url": "https://github.com/openai/human-eval",
    "last_updated": "2025-09-02T07:40:40+00:00"
  },
  {
    "full_name": "johnjosephhorton/latexlog2html",
    "name": "latexlog2html",
    "description": "This is a script for parsing a latex log file and turning it into a reasonably looking HTML page w/ nav links. ",
    "language": "Python",
    "topics": [],
    "readme": "latexlog2html \n=============\n\nThis is script for turning a LaTeX log file into a more-pleasant-to-read [HTML file](http://dl.dropboxusercontent.com/u/420874/permanent/sample.html).\nIt lists all the errors and warnings from the log file as ordered lists at the top of the HTML file, with internal hyperlinks to the actual in situ warnings/errors.                                   \n\nInstall\n-------\n    \n\tgit clone git@github.com:johnjosephhorton/latexlog2html.git\n\tcd latexlog2html \n\tsudo python setup.py install \n\nUsage\n-----\nTo create the HTML file, simply run: \n\n\tlatexlog2html LOGFILE\n\t\nWhere `LOGFILE` is the name of the LaTeX log file. \nThe HTML file will open automatically in a browser tab. \n\nLicense \n-------\n\n\tThis program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "url": "https://github.com/johnjosephhorton/latexlog2html",
    "last_updated": "2023-02-26T16:09:26+00:00"
  },
  {
    "full_name": "rich-iannone/unidecoder",
    "name": "unidecoder",
    "description": "Replace Unicode characters with sensible US-ASCII equivalents",
    "language": "R",
    "topics": [
      "transliteration",
      "text-based",
      "r",
      "ascii-equivalents",
      "unicode-characters",
      "language"
    ],
    "readme": "<img src=\"man/figures/unidecoder.png\" width=\"100%\">\n\nWhile Unicode characters are undeniably important, there can be occasions where you need text only in US-ASCII format. This is where the **R** package **unidecoder** can help. If you provide text to **unidecoder** along with the input language, it will replace accented letters, ligatures, and other Unicode characters with US-ASCII equivalents.\n\n### Installation\n\nInstall **unidecoder** from GitHub using the **devtools** package.\n\n```r\ndevtools::install_github(\"rich-iannone/unidecoder\")\n```\n\n### How to Use **unidecoder**\n\nUse the **unidecoder** package's function `unidecode()` to transform text to ASCII. The function takes in a vector of strings and replaces Unicode characters with their best equivalents. Knowing which equivalents are best depends on providing the source language for the input text. Transliterations can be accomplished for several languages: Armenian, Bulgarian, Czech, Danish, French, Georgian, German, Greek, Norwegian, Polish, Romanian, Russian, and Slovenian.\n\nTake, for example, Goethe's *Totentanz* (1813):\n\n```\n\nDer Türmer, der schaut zu Mitten der Nacht\nHinab auf die Gräber in Lage;\nDer Mond, der hat alles ins Helle gebracht;\nDer Kirchhof, er liegt wie am Tage.\nDa regt sich ein Grab und ein anderes dann:\nSie kommen hervor, ein Weib da, ein Mann,\nIn weißen und schleppenden Hemden.\n\nDas reckt nun, es will sich ergetzen sogleich,\nDie Knöchel zur Runde, zum Kranze,\nSo arm und so jung, und so alt und so reich;\nDoch hindern die Schleppen am Tanze.\nUnd weil hier die Scham nun nicht weiter gebeut,\nSie schütteln sich alle, da liegen zerstreut\nDie Hemdlein über den Hügeln.\n\nNun hebt sich der Schenkel, nun wackelt das Bein,\nGebärden da gibt es vertrackte;\nDann klippert's und klappert's mitunter hinein,\nAls schlüg' man die Hölzlein zum Takte.\nDas kommt nun dem Türmer so lächerlich vor;\nDa raunt ihm der Schalk, der Versucher, ins Ohr:\nGeh! hole dir einen der Laken.\n\nGetan wie gedacht! und er flüchtet sich sc",
    "url": "https://github.com/rich-iannone/unidecoder",
    "last_updated": "2025-05-05T14:45:44+00:00"
  },
  {
    "full_name": "tpq/exprso",
    "name": "exprso",
    "description": "[DEPRECATED] An R package to rapidly build and deploy supervised machine learning workflows",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\nQuick start\n-----------\n\nWelcome to the `exprso` GitHub page! Let's get started.\n\n``` r\nlibrary(devtools)\ndevtools::install_github(\"tpq/exprso\")\nlibrary(exprso)\n```\n\nImporting data\n--------------\n\nTo import data, we use the `exprso` function. This function has two arguments.\n\n``` r\ndata(iris)\narray <- exprso(iris[1:80, 1:4], iris[1:80, 5])\n```\n\n    ## [1] \"Preparing data for binary classification.\"\n\nPre-processing data\n-------------------\n\nFunctions with a `mod` prefix pre-process the data.\n\n``` r\narray <- modTransform(array)\narray <- modNormalize(array, c(1, 2))\n```\n\nSplit data\n----------\n\nFunctions with a `split` prefix split the data into training and test sets.\n\n``` r\narrays <- splitSample(array, percent.include = 67)\narray.train <- arrays$array.train\narray.test <- arrays$array.valid\n```\n\nSelect features\n---------------\n\nFunctions with a `fs` prefix select features.\n\n``` r\narray.train <- fsStats(array.train, top = 0, how = \"t.test\")\n```\n\nBuild models\n------------\n\nFunctions with a `build` prefix build models.\n\n``` r\nmach <- buildSVM(array.train,\n                 top = 50,\n                 kernel = \"linear\",\n                 cost = 1)\n```\n\n    ## Setting probability to TRUE (forced behavior, cannot override)...\n    ## Setting cross to 0 (forced behavior, cannot override)...\n\n``` r\npred <- predict(mach, array.train)\n```\n\n    ## Individual classifier performance:\n    ## Arguments not provided in an ROCR AUC format. Calculating accuracy outside of ROCR...\n    ## Classification confusion table:\n    ##          actual\n    ## predicted Control Case\n    ##   Control      29    0\n    ##   Case          0   25\n    ##   acc sens spec\n    ## 1   1    1    1\n\n``` r\npred <- predict(mach, array.test)\n```\n\n    ## Individual classifier performance:\n    ## Arguments not provided in an ROCR AUC format. Calculating accuracy outside of ROCR...\n    ## Classification confusion table:\n    ##          actual\n    ## p",
    "url": "https://github.com/tpq/exprso",
    "last_updated": "2023-07-25T05:25:14+00:00"
  },
  {
    "full_name": "themiurgo/twitterstream-downloader",
    "name": "twitterstream-downloader",
    "description": "Twitter stream and social network crawling tools",
    "language": "Python",
    "topics": [],
    "readme": "twitterstream-downloader\n------------------------\n\nTwitterstream-downloader (in the following `twsd`) is a Python script that allows to download streams of tweets using the Twitter Streaming API.\n\nDumping all the tweets related to Arcade Fire and Radiohead is as simple as:\n\n    twsd filter music_news -p track='Arcade Fire,Radiohead'\n    \nThis will download the Streaming messages to daily files\n`music_news_20130801.json`, `music_news_20130802.json`, ...\n\nInstallation\n------------\n\n\tpip install git+git://github.com/themiurgo/twitterstream-downloader\n\n\nUsage\n-----\n\n    twsd ENDPOINT FILE_PREFIX [-p PARNAME=PARVAL]\n\n* `ENDPOINT` can be either `sample` or `filter` or `firehose` (read more on [Twitter Streaming API docs (Public Streams)](https://dev.twitter.com/docs/streaming-apis/streams/public). The fictious `authorize` endpoint is used to authorize a new client.\n* `FILE_PREFIX` is the path+prefix of the files related the stream.\n\nFeatures\n--------\n* Download streams from Twitter Streaming API\n* Rotate files every day\n* Automated OAuth authentication and key management.\n* Log error messages\n\nExamples\n--------\nDownload some data from the Twitter Sample endpoint to files `~/tweets/sample_YYYYMMDD.json`. (This will also create a logfile `~/tweets/sampledump.log` containing events related to the stream (crawling start, stop, limit messages).\n    \n    twsd sample ~/tweets/sampledump\n    \nDownload all the geolocated tweets in UK and Ireland to files `~/project_ukir/tweets_YYYYMMDD.json` (and create a log file, as specified above).\n\n\ttwsd filter ~/project_ukir/tweets -p locations=\"-180,-90,180,90\"\n\nConfiguration\n-------------\n\nTwsd will attempt to locate a configuration file `~/.twsd` with authentication credentials. When this fails, it will ask OAuth credentials.\n",
    "url": "https://github.com/themiurgo/twitterstream-downloader",
    "last_updated": "2020-06-09T13:50:29+00:00"
  },
  {
    "full_name": "justin/SilencedBots",
    "name": "SilencedBots",
    "description": "A collection of regex filters that are useful for muting stuff in Tweetbot.",
    "language": "",
    "topics": [],
    "readme": "SilencedBots\n============\n\nA collection of regex filters that are useful for muting stuff in Tweetbot.",
    "url": "https://github.com/justin/SilencedBots",
    "last_updated": "2025-08-15T21:13:41+00:00"
  },
  {
    "full_name": "CamDavidsonPilon/PyconCanada2015",
    "name": "PyconCanada2015",
    "description": "My scrapers, data and analysis for PyCon Canada 2015 Keynote",
    "language": "Python",
    "topics": [],
    "readme": "# PyConCanada2015\n\n\n\n#### My scrapers + data + analysis for PyConCanada2015 Keynote\n\n(sorry github)\n\n\n\n\n## Frequency of libraries in `requirements.txt` files in Github Python repositories\n\n\nThis was done by scraping 10k+ Python repositories on Github that contain a `requirements.txt` file. This file is commonly used to store dependencies of the repository. \n\n![freq_libs](http://i.imgur.com/Kft8vUl.png)\n\nIt's clear that the majority of repositories on Python are web development related, or web developers are most likely to include a proper requirements.txt file in their repositories. \n\n\n## Relationships between libraries\n\nUsing the data in `requirements.txt' files, we can find common co-occurences of libraries. For example, it's not hard to imagine that whenever django is a requirement, so is psycopg2. In fact, in the dataset I had, 41% of all django apps also included psycopg2. These relationships can be mined using a simple algorithm called the apriori algorithm. It's history goes back to large department stores that were interested in what products were commonly bought together. The naive solution, compare all possible pairs, results in a quadratic algorithm - and if you have thousands of products, this becomes inefficient quickly. The apriori algorithm intelligently cuts through this massive space. \n\nHere are the other common libraries paired with django:\n\n(confidence is defined as \n```\nconfidence = P(ending_with | starting_with)\n           = P(starting_with and ending_with) / P(starting_with)\n           = #{requirement.txts with both} / #{requirement.txts with starting_with}\n\n```\n\n\n|starting_with | ending_with      |  starting_with_occurrences |confidence     | occurrences |ending_with_occurrences |\n|------------|-----------------|-------------------------|---------------|-------------|-----------------------|\n|django      | requests        |  2714                   |0.243920412675 | 662         |2463                   |\n|django      | wheel           |  2714   ",
    "url": "https://github.com/CamDavidsonPilon/PyconCanada2015",
    "last_updated": "2023-09-08T17:01:22+00:00"
  },
  {
    "full_name": "lmcinnes/enstop",
    "name": "enstop",
    "description": "Ensemble topic modelling with pLSA",
    "language": "Python",
    "topics": [
      "topic-modeling",
      "plsa",
      "dimensionality-reduction",
      "matrix-factorization"
    ],
    "readme": "======\nEnsTop\n======\n\nEnsTop provides an ensemble based approach to topic modelling using pLSA. It makes\nuse of a high performance numba based pLSA implementation to run multiple\nbootstrapped topic models in parallel, and then clusters the resulting outputs to\ndetermine a set of stable topics. It can then refit the document vectors against\nthese topics embed documents into the stable topic space.\n\n---------------\nWhy use EnsTop?\n---------------\n\nThere are a number of advantages to using an ensemble approach to topic modelling.\nThe most obvious is that it produces better more stable topics. A close second,\nhowever, is that, by making use of HDBSCAN for clustering topics, it can learn a\n\"natural\" number of topics. That is, while the user needs to specify an estimated\nnumber of topics, the *actual* number of topics produced will be determined by how\nmany stable topics are produced over many bootstrapped runs. In practice this can\neither be more, or less, than the estimated number of topics.\n\nDespite all of these extra features the ensemble topic approach is still very\nefficient, especially in multi-core environments (due the the embarrassingly parallel\nnature of the ensemble). A run with a reasonable size ensemble can be completed in\naround the same time it might take to fit an LDA model, and usually produces superior\nquality results.\n\nIn addition to this EnsTop comes with a pLSA implementation that can be used\nstandalone (and not as part of an ensemble). So if all you are loosing for is a good\nfast pLSA implementation (that can run considerably faster than many LDA\nimplementations) then EnsTop is the library for you.\n\n-----------------\nHow to use EnsTop\n-----------------\n\nEnsTop follows the sklearn API (and inherits from sklearn base classes), so if you\nuse sklearn for LDA or NMF then you already know how to use Enstop. General usage is\nvery straightforward. The following example uses EnsTop to model topics from the\nclassic 20-Newsgroups dataset, using sklearn's Count",
    "url": "https://github.com/lmcinnes/enstop",
    "last_updated": "2025-05-11T15:37:56+00:00"
  },
  {
    "full_name": "emitanaka/ninja-theme",
    "name": "ninja-theme",
    "description": "A xaringan / remark js ninja theme ",
    "language": "",
    "topics": [
      "xaringan",
      "remarkjs",
      "presentation-ninja",
      "rmarkdown",
      "rmarkdown-slide"
    ],
    "readme": "\nxaringan / remark js ninja themes\n======\n\n\nYou can find the R markdown for the source file for [these slides](https://emitanaka.github.io/ninja-theme) in `docs/themes/kunoichi/kunoichi-theme-example.Rmd`.\n\n\n![](docs/themes/kunoichi/images/kunoichi-showcase.gif)\n\n# Featured Ninja and their Ninja Slides\n\n* [Sarah Romanes](https://twitter.com/sarah_romanes) - [Machine Learning 101](http://bit.ly/rladies-sydney-ML-1) ([source files](https://github.com/sarahromanes/r-ladies-ML-1))\n* [William Chase](https://twitter.com/W_R_Chase) - [What I Wish I Knew When I Started R](https://www.williamrchase.com/slides/intro_r_anthropology_2018) ([source files](https://github.com/will-r-chase/blog/tree/master/static/slides))\n* [Eric Nantz](https://twitter.com/thercast) - [The officer package - making PowerPoint slides from R](https://rpodcast.github.io/officer-advrmarkdown) ([source_files](https://github.com/rpodcast/officer-advrmarkdown))\n",
    "url": "https://github.com/emitanaka/ninja-theme",
    "last_updated": "2024-05-17T16:47:55+00:00"
  },
  {
    "full_name": "montera34/pageonex",
    "name": "pageonex",
    "description": "PageOneX. Analyzing front pages",
    "language": "Ruby",
    "topics": [],
    "readme": "PageOneX in Ruby on Rails\n=========================\n\n*PageOneX is an open source tool to code, analyze and visualize the evolution of stories on newspaper front pages.* PageOneX is an free software/open source software tool designed to aid the coding, analysis, and visualization of front page newspaper coverage of major stories and media events. Newsrooms spend massive time and effort deciding what stories make it to the front page.\nIn the past, this approach involved obtaining copies of newspapers, measurement by hand (with a physical ruler), and manual input of measurements into a spreadsheet or database, followed by calculation and analysis. Some of these steps can now be automated, while others can be simplified; some can be easily shared by distributed teams of investigators working with a common dataset hosted online.\n\n*How is your project different from what already exists?* Communication scholars have long used column-inches of print newspaper coverage as an important indicator of mass media attention. PageOneX simplifies, digitizes, and distributes the process across the net.\n\nMore Info\n---------\n\nYou can find more information and examples at http://pageonex.com or at [the blog](http://montera34.org/pageonex/)\n\nRunning pageonex\n----------------\n\nPageonex is a Free/Libre and Open Source Software (F/LOSS) project. If you don't want to use the hosted version of pageonex at http://pageonex.com you have a few options to run it yourself. We've prepared a set of soltuions that use [docker](http://docker.com/), a containerized version of the tools that allows you to use the tool without the need to handle all the dependencies of Ruby on Rails.\n\n1. For any of the following options with you need to download or clone [this repository](https://github.com/montera34/pageonex). If you are familiar with git you can clone it git clone with `git clone https://github.com/montera34/pageonex.git`. Otherwise you can [download the entire code in a .zip](https://github.com/montera",
    "url": "https://github.com/montera34/pageonex",
    "last_updated": "2025-03-22T11:13:59+00:00"
  },
  {
    "full_name": "Lasagne/Recipes",
    "name": "Recipes",
    "description": "Lasagne recipes: examples, IPython notebooks, ...",
    "language": "Python",
    "topics": [],
    "readme": "Recipes\n=========\n\nThis is a collection of recipes for using [Lasagne](https://github.com/Lasagne/Lasagne). Code snippets, IPython notebooks, tutorials and useful extensions are welcome here.\n\n! Please note - the AWS S3 bucket used in many of these examples has been made [Requester Pays](https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html). Direct links or downloading with `wget` will no longer work, but you can still download these files using [`aws cli`](https://github.com/aws/aws-cli) or a client such as [S3 Browser](http://s3browser.com/).\n\nOrganization\n------------\n\n* Examples: short examples demonstrating how to accomplish something interesting with Lasagne.\n* Tutorials: longer examples covering a range of topics.\n* Papers: code implementing a new technique or replicating results of a specific paper.\n* Model Zoo: a collection of pretrained models.\n* Utils: helper functions which can be imported.\n* Stale: things that break due to api changes will live here until they can be updated. Hopefully empty.\n",
    "url": "https://github.com/Lasagne/Recipes",
    "last_updated": "2025-08-04T19:43:37+00:00"
  },
  {
    "full_name": "fozziethebeat/TopicModelComparison",
    "name": "TopicModelComparison",
    "description": "Scripts and codes for replicating experiments published in Exploring Topic Coherence over many models and many topics",
    "language": "Scala",
    "topics": [],
    "readme": "# Detailed Setup for Exploring Topic Coherence over many models and many topics\n\nWe (Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler)\npublished the paper [Exploring Topic Coherence over many models and many topics][1]\n(link to appear soon) which compares several topic models using a variety of\nmeasures in an attempt to determine which model should be used in which\napplication.  This evaluation secondly compares automatic coherence measures as\na quick, task free method for comparing a variety of models.  Below is a\ndetailed series of steps on how to replicate the results from the paper.  \n\nThe evaluation setup breaks down into the following steps:\n\n1. Select a corpus and pre-process.\n2. Remove stop words, infrequent words, and format the corpus.\n3. Perform topic modelling on all documents\n4. Compute topic coherence measures for induced topics\n5. Compute word similarities using semantic pairing tests\n6. Compute Classifier accuracy using induced topics\n\nEach of these steps are automated in the bash scripts provided in this\nrepository.  To run those scripts read the last section for downloading the\nneeded components, setting parameters, and then watching the scripts blaze\nthrough the setup.\n\nThe rest of this writeup explains each step in more detail than was permitted in\nthe published paper.\n\n## Selecting the corpus\n\nThe evaluation requires the use of a semantically labeled corpus that has a\nrelatively cohesive focus.  The original paper used all articles from 2003 of\nthe [New York Times Annotated Corpus][2] provided by the [Linguistics Data Consortium][3].  \nAny similarly structured corpus should work.  \n\nThe New York Times corpus requires some pre-processing before it can be easily\nused in the evaluation.  The original corpus comes in a series of tarballed xml\nfiles where each file looks something like this:\n\n``` xml\n<nitf change.date=\"month day, year\" change.time=\"HH:MM\" version=\"-//IPTC//DTD NITF 3.3//EN\">\n<head>\n  <title>Article Title</title",
    "url": "https://github.com/fozziethebeat/TopicModelComparison",
    "last_updated": "2025-05-23T03:05:06+00:00"
  },
  {
    "full_name": "NYPL-publicdomain/data-and-utilities",
    "name": "data-and-utilities",
    "description": "Snapshot of Item and Collection data for public domain materials in NYPL Digital Collections, as part of NYPL's January 2016 public domain release. ",
    "language": "Shell",
    "topics": [],
    "readme": "# Digital Collections Public Domain Item Data and Tools\n\nDid you know that nearly one-third of the items in our Digital Collections are in the public domain -- that is, they have been designated as having no known U.S. copyright restrictions? This means that everyone has the freedom to enjoy and reuse these materials in almost limitless ways. To help you explore, visualize, and repurpose these items, we've gathered all of their metadata into a single data release. (Based on feedback from this release, we'll be considering regular update possibilities, but at this time the data is a snapshot of our data from 12/30/15. See the [NYPL Digital Collections Metadata API](http://api.repo.nypl.org/) for updated information and for data about the non-public domain portions of our Digital Collections.)\n\nThis dataset is organized by [Items](#items) and [Collections](#collections) in both CSV and JSON formats. Our descriptive metadata is normally stored in the MODS schema (which is what you'll find in our Digital Collections API), but for this release we've simplified and flattened the metadata structure for CSV to make it easier to navigate with spreadsheet tools. The JSON versions include a bit more metadata, including URIs for many names and subjects and links to the full-size images comprising each item. \n\nNYPL has been digitizing collections since 1999, so our metadata reflects an evolution of standards, practices, and workflows. We are actively refining our metadata creation and quality control processes and exploring ways to improve the consistency and accuracy of our legacy metadata, but in the meantime, you may find some idiosyncracies and curiosities in our data. If you'd like to bring certain issues to our attention, we welcome your feedback through our [Digital Collections](http://digitalcollections.nypl.org) feedback form.\n\n- [Items](#items)\n- [Collections](#collections)\n- [Attribution](#attribution)\n- [Code Examples](#code-examples)\n- [Pull Requests and Issues](#pu",
    "url": "https://github.com/NYPL-publicdomain/data-and-utilities",
    "last_updated": "2025-08-21T14:11:05+00:00"
  },
  {
    "full_name": "dmilo75/ai-zoning",
    "name": "ai-zoning",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# NYU AI-Zoning Project\n\nThe Advent of Large Language Models (LLMs) has transformed many facets of society, enabling groundbreaking applications across diverse fields. This project aims to leverage LLMs to analyze and study the zoning landscape in the United States. This current repository offers a demo model that is tested with zoning ordinances extracted from Wheaton, Illinois. \n\nFor the latest paper please see [here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4627587). \n\nFor the latest data please see [here](https://www.dropbox.com/scl/fo/7ujwxl4fbzor65vu7zjku/ABezF48kL_THI_nfA35PIGQ?rlkey=0aip2l0c0hq2dvplou040hqhu&st=99ow05cq&dl=0). \n\nAll are included in the repository besides the .env file, which you will need to create in the main AI-Zoning directory (same directory as config.yaml). A guide on what to include in you .env file, as well as documentation for all the key components of the repository, can be found in the readme folder of this repository.  \n\nThe code in this repository was containerized using Docker. To demo the code, please refer to the following guide on running containerized code: https://drive.google.com/file/d/1BiEs74T4dKHhyQI2Je3EUJxNfzEcvsD0/view?usp=sharing\n\n# AI and Zoning: Using Large Language Models for Regulatory Analysis\n\nThis repository is dedicated to the use of Large Language Models (LLMs) for parsing zoning documents. We introduce a generative regulatory measurement approach to decode and interpret statutes and administrative documents. This project leverages LLMs to construct a detailed assessment of U.S. zoning regulations and examines the correlation between these regulations, housing costs, and construction. \n\nThe work demonstrates the reliability of LLMs in analyzing complex regulatory datasets.\n\nFor the latest paper, please see [here](https://static1.squarespace.com/static/56086d00e4b0fb7874bc2d42/t/653b143abbdc5f5bfacf947a/1698370623319/AI_Zoning.pdf).\n\n## Table of Contents\n1. [Setup](#setup)\n2. [Overview](#overview)",
    "url": "https://github.com/dmilo75/ai-zoning",
    "last_updated": "2025-08-27T22:23:09+00:00"
  },
  {
    "full_name": "google-research/bert",
    "name": "bert",
    "description": "TensorFlow code and pre-trained models for BERT",
    "language": "Python",
    "topics": [
      "nlp",
      "google",
      "natural-language-processing",
      "natural-language-understanding",
      "tensorflow"
    ],
    "readme": "# BERT\n\n**\\*\\*\\*\\*\\* New March 11th, 2020: Smaller BERT Models \\*\\*\\*\\*\\***\n\nThis is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).\n\nWe have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.\n\nOur goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.\n\nYou can download all 24 from [here][all], or individually from the table below:\n\n|   |H=128|H=256|H=512|H=768|\n|---|:---:|:---:|:---:|:---:|\n| **L=2**  |[**2/128 (BERT-Tiny)**][2_128]|[2/256][2_256]|[2/512][2_512]|[2/768][2_768]|\n| **L=4**  |[4/128][4_128]|[**4/256 (BERT-Mini)**][4_256]|[**4/512 (BERT-Small)**][4_512]|[4/768][4_768]|\n| **L=6**  |[6/128][6_128]|[6/256][6_256]|[6/512][6_512]|[6/768][6_768]|\n| **L=8**  |[8/128][8_128]|[8/256][8_256]|[**8/512 (BERT-Medium)**][8_512]|[8/768][8_768]|\n| **L=10** |[10/128][10_128]|[10/256][10_256]|[10/512][10_512]|[10/768][10_768]|\n| **L=12** |[12/128][12_128]|[12/256][12_256]|[12/512][12_512]|[**12/768 (BERT-Base)**][12_768]|\n\nNote that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model.\n\nHere are the corresponding GLUE scores on the test set:\n\n|Model|Score|CoLA|SST-2|MRPC|STS-B|QQP|MNLI-m|MNLI-mm|QNLI(v2)|RTE|WNLI|AX|\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|",
    "url": "https://github.com/google-research/bert",
    "last_updated": "2025-09-02T03:45:55+00:00"
  },
  {
    "full_name": "OHDSI/PatientLevelPrediction",
    "name": "PatientLevelPrediction",
    "description": "An R package for performing patient level prediction  in an observational database in the OMOP Common Data Model.",
    "language": "R",
    "topics": [
      "hades"
    ],
    "readme": "PatientLevelPrediction\n======================\n\n\n[![Build status](https://github.com/OHDSI/PatientLevelPrediction/actions/workflows/R_CMD_check_Hades.yaml/badge.svg?branch=main)](https://github.com/OHDSI/PatientLevelPrediction/actions/workflows/R_CMD_check_Hades.yaml)\n[![codecov.io](https://codecov.io/github/OHDSI/PatientLevelPrediction/coverage.svg?branch=main)](https://app.codecov.io/github/OHDSI/PatientLevelPrediction?branch=main)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/PatientLevelPrediction)](https://cran.r-project.org/package=PatientLevelPrediction)\n[![CRAN_Status_Badge](https://cranlogs.r-pkg.org/badges/PatientLevelPrediction)](https://cran.r-project.org/package=PatientLevelPrediction)\n\nPatientLevelPrediction is part of [HADES](https://ohdsi.github.io/Hades/).\n\nIntroduction\n============\n\nPatientLevelPrediction is an R package for building and validating patient-level predictive models using data in the OMOP Common Data Model format.  \n\nReps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. [Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data.](https://doi.org/10.1093/jamia/ocy032) J Am Med Inform Assoc. 2018;25(8):969-975.\n\nThe figure below illustrates the prediction problem we address. Among a population at risk, we aim to predict which patients at a defined moment in time (t = 0) will experience some outcome during a time-at-risk. Prediction is done using only information about the patients in an observation window prior to that moment in time.\n\n![](vignettes/images/Figure1.avif)\n\nTo define a prediction problem we have to define t=0 by a Target Cohort (T), the outcome we like to predict by an outcome cohort (O), and the time-at-risk (TAR). Furthermore, we  have to make design choices for the model we like to develop, and determine the observational datasets to perform internal and external validation. This conceptual framework works for all type",
    "url": "https://github.com/OHDSI/PatientLevelPrediction",
    "last_updated": "2025-08-31T19:19:01+00:00"
  },
  {
    "full_name": "bahadasx/newsname-match",
    "name": "newsname-match",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "NewsName-Match\n==============\n\n![Newspaper Image](https://upload.wikimedia.org/wikipedia/commons/8/84/The_San_Francisco_Call_newspaper_front_page%2C_thursday%2C_feb_8_1906%2C_featuring_Thomas_B._Bishop_closer_crop.png)\n\nBy [Mercurywoodrose](https://commons.wikimedia.org/wiki/User:Mercurywoodrose) , via Wikimedia Commons\n\n\nAbout\n-----\n\nA tool for finding variations of person names in OCR'ed news articles.  This project is part of the District Data Research Labs on Entity Resolution\n\nData\n----\n\nThis dataset contains OCR text extracted from the front page of newspapers in 1961.  It has been gathered from http://chroniclingamerica.loc.gov\n\nGoal\n----\n\nThe problem with these newspaper text files is that a person can be mentioned in a variety of ways in an article other than by using their full name.\n\nThe goal of this project is first, to create a method to identify all variations of a person's name that can be used and to create a map of those variations that can be used to perform further analytics such as how frequently a person's name appears per article/newspaper.\n\nThe goal of this project is the create a method of identifying mentions of a person's name, regardless of how they are mentioned.  This can then be used to perform further analytics such as how frequently a person's name appears per article/newspaper.\n\nApproach\n--------\n\nThe approach to solving this problem involves the use of a couple different libraries.  First, NLTK is used to tokenize sentences in each file and separate them into parts of speech.  Below is a diagram of the NLTK approach:\n\n![NLTK Approach](/docs/ie-architecture.png)\n\nBy identifying all the person mentions in the files, you can then compare that to a canonical list of persons you are interested in(in this case it is presidential candidates from the 1916 election) to create a map.  For instance, Woodrow Wilson could be mapped to a number of variants used to describe him:\n\n```\n\"Woodrow Wilson\" -> [\"President Wilson\",\"W. Wilson\", \"Woodrow\"]\n",
    "url": "https://github.com/bahadasx/newsname-match",
    "last_updated": "2019-08-10T06:38:59+00:00"
  },
  {
    "full_name": "benhoyt/pybktree",
    "name": "pybktree",
    "description": "Python BK-tree data structure to allow fast querying of \"close\" matches",
    "language": "Python",
    "topics": [
      "python",
      "data-structures",
      "tree",
      "levenshtein-distance"
    ],
    "readme": "pybktree\n========\n\npybktree is a generic, pure Python implementation of a `BK-tree`_ data\nstructure, which allows fast querying of \"close\" matches (for example, matches\nwith small hamming distance or Levenshtein distance). This module is based on\nthe algorithm by Nick Johnson in his `blog article on BK-trees`_.\n\nThe library is `on the Python Package Index (PyPI)`_ and works on both Python\n3 and Python 2.7. To install it, fire up a command prompt, activate your\nvirtual environment if you're using one, and type:\n\n::\n\n    pip install pybktree\n\nExample usage:\n\n.. code:: python\n\n    >>> tree = pybktree.BKTree(pybktree.hamming_distance, [0, 4, 5, 14])\n    >>> tree.add(15)              # add element 15\n    >>> sorted(tree)              # BKTree instances are iterable\n    [0, 4, 5, 14, 15]\n    >>> sorted(tree.find(13, 1))  # find elements at most 1 bit away from element 13\n    [(1, 5), (1, 15)]\n\nIf you need to track the ID, key, or filename of the original item, use a\ntuple or namedtuple. Repeating the above example with an ``Item`` namedtuple:\n\n.. code:: python\n\n    >>> import collections\n    >>> Item = collections.namedtuple('Item', 'bits id')\n    >>> def item_distance(x, y):\n    ...     return pybktree.hamming_distance(x.bits, y.bits)\n    >>> tree = pybktree.BKTree(item_distance, [Item(0, 'a'), Item(4, 'b'),\n                                               Item(5, 'c'), Item(14, 'd')])\n    >>> tree.add(Item(15, 'e'))\n    >>> sorted(tree.find(Item(13, 'x'), 1))\n    [(1, Item(bits=5, id='c')), (1, Item(bits=15, id='e'))]\n\nFor large trees and fairly small N when calling ``find()``, using a BKTree is\nmuch faster than doing a linear search. This is especially good when you're\nde-duping a few hundred thousand photos -- with a linear search that would\nbecome a very slow, O(N) for every photo, so O(N²) in total.\n\nA lookup in a BKTree is much faster than linear for small distance thresholds --\nthough it goes up to O(N) far large distance thresholds, so won't be valuable in\nthose ca",
    "url": "https://github.com/benhoyt/pybktree",
    "last_updated": "2025-07-11T20:20:00+00:00"
  },
  {
    "full_name": "NoahFinberg/insight_extractor",
    "name": "insight_extractor",
    "description": "A package of a simple version of the Considdr (2014-2020) ML model used to extract insights from full text documents.",
    "language": "Jupyter Notebook",
    "topics": [
      "cnn-classification",
      "summarization",
      "nlp"
    ],
    "readme": "Insight Extractor\n=================\n\n\n.. image:: https://pepy.tech/badge/insight-extractor\n    :target: https://pepy.tech/project/insight-extractor\n.. image:: https://travis-ci.com/NoahFinberg/insight_extractor.svg?branch=main\n    :target: https://travis-ci.com/NoahFinberg/insight_extractor\n\nThe Insight Extractor was the ML model that `Considdr <https://medium.com/considdr-history>`_ used to identify abstractive sentences in full text documents on the web. Considdr closed in the summer of 2020 and now we're making our model freely available to all. We'd love to hear the interesting ways people apply this model. All we ask is that you cite this repo.\n\nAbstractive sentences are of particular value when it comes to understanding the key insights in adjacent documents. For more on this summarization approach see `\"Summarization by Adjacent Document.\" <https://medium.com/considdr-history/summarization-by-adjacent-document-a-new-way-to-extract-insights-450ab9b60c41?source=collection_home---4------2----------------------->`_\n\nInstall\n-------\n\n.. code-block::\n\n   pip install insight_extractor\n\n\n**Notes:** \n\n\n* We use Tensorflow 2.X and recommend using Python 3.6 or higher.\n* Python 2 is not supported\n\nUsing insight_extractor\n-----------------------\n\nv0.1.1 of insight_extractor exposes one primary function --\\ ``extract_insights`` -- which takes a list of candidate insight sentences and returns  a list of prediction scores signifying the probability that our model thinks a given sentence is an insight.\n\n**Input:**\n\n.. code-block::\n\n   # given a list of input sentences\n   sentences = [\n       'According to the most recent statistics, more than a million people a year are arrested for simple drug possession in the United States -- and more than half a million of those arrests are for marijuana possession.',\n       'One study found that for cancer patients considering experimental chemotherapy, trust in their physician was one of the most important reasons they enrolled in a cl",
    "url": "https://github.com/NoahFinberg/insight_extractor",
    "last_updated": "2023-01-22T05:22:51+00:00"
  },
  {
    "full_name": "bbc/bbplot",
    "name": "bbplot",
    "description": "R package that helps create and export ggplot2 charts in the style used by the BBC News data team",
    "language": "R",
    "topics": [],
    "readme": "## BBPLOT\n\nThis repo contains the functions of the `bbplot` package, which once installed locally, provides helpful functions for creating and exporting  graphics made in ggplot in the style used by the BBC News data team.\n\n![Example of graphics created using the bbplot package](chart_examples/bbplot_example_plots.png)\n\n## Installing bbplot\n\n`bbplot` is not on CRAN, so you will have to install it directly from Github using `devtools`. \n\nIf you do not have the `devtools` package installed, you will have to run the first line in the code below as well. \n\n```\n# install.packages('devtools')\ndevtools::install_github('bbc/bbplot')\n```\n\n## Using the functions\n\nThe package has two functions for plots: `bbc_style()` and `finalise_plot`.\n\nDetailed examples on how to use the functions included within the `bbplot` package to produce graphics are included in the [R cookbook](https://bbc.github.io/rcookbook/), as well as a more general reference manual for working with `ggplot2`.\n\nA basic explanation and summary here:\n\n### `bbc_style()`\n\n1. `bbc_style()`: has no arguments and is added to the ggplot chain after you have created a plot. What it does is generally makes text size, font and colour, axis lines, axis text and many other standard chart components into BBC style, which has been formulated together with the Visual Journalism design team. \n\nThe function is pretty basic and does not change or adapt based on the type of chart you are making, so in some cases you will need to make additional `theme` arguments in your ggplot chain if you want to make any additions or changes to the style, for example to add or remove gridlines etc. Also note that colours for lines in the case of a line chart or bars for a bar chart, do not come out of the box from the `bbc_style` function, but need to be explicitly set in your other standard `ggplot` chart functions.\n\nExample of how it is used in a standard workflow:\n\n```\nline <- ggplot(line_df, aes(x = year, y = lifeExp)) +\ngeom_line(colour = ",
    "url": "https://github.com/bbc/bbplot",
    "last_updated": "2025-08-26T01:37:39+00:00"
  },
  {
    "full_name": "friendly/matlib",
    "name": "matlib",
    "description": "Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics",
    "language": "R",
    "topics": [
      "linear-equations",
      "matrix-functions",
      "matrix",
      "vector",
      "vignette",
      "matrix-visualizer",
      "diagrams"
    ],
    "readme": "<!-- badges: start -->\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/matlib)](https://cran.r-project.org/package=matlib)\n[![R-universe](https://friendly.r-universe.dev/badges/matlib)](https://friendly.r-universe.dev)\n[![Downloads](http://cranlogs.r-pkg.org/badges/grand-total/matlib)](https://cran.r-project.org/package=matlib)\n[![downloads](http://cranlogs.r-pkg.org/badges/matlib)](https://cran.r-project.org/package=matlib)\n[![pkgdown](https://img.shields.io/badge/documentation-blue)](https://friendly.github.io/matlib)\n<!-- [![Dependencies](https://tinyverse.netlify.com/badge/matlib)](https://cran.r-project.org/package=matlib) -->\n\n<!-- badges: end -->\n\n# matlib <img src=\"man/figures/logo.png\" align=\"right\" height=\"200px\" />\n\n**Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics**, http://friendly.github.io/matlib/\n\nVersion 1.0.1\n\nThese functions were originally designed for tutorial purposes in teaching & learning matrix algebra\nideas using R. In some cases, functions are provided for concepts or computations available\nelsewhere in R, but where the name is not obvious, e.g., `R()` for the rank of a matrix,\nor `tr()` for matrix trace.\n\nIn other\ncases, we provide cover functions to show or demonstrate an algorithm in more detail, sometimes\nproviding a `verbose =` argument to print the details of computations, e.g., `Det()` for a\nmatrix determinant, `Inverse()` for a matrix inverse, using `gaussianElimination()` to show the\nsteps.\n\nIn addition, a collection of functions are provided for drawing vector diagrams in 2D and 3D, illustrating\nvarious concepts of linear algebra more concretely than has been available before.\nFor example, \n\n* `showEqn(A, b)` shows the matrix equations $\\mathbf{A x} = \\mathbf{b}$ in text or LaTeX form, while\n`plotEqn(A, b)` and `plotEqn3d(A, b)` plots those equations in 2D or 3D space.\n\n* `matrix2latex()`, `latexMatrix()`, `Eqn()` and friends facilitate writing matrix equations in LaTeX.\n\n* `vector",
    "url": "https://github.com/friendly/matlib",
    "last_updated": "2025-06-29T07:11:33+00:00"
  },
  {
    "full_name": "notnews/archive_news_cc",
    "name": "archive_news_cc",
    "description": "Closed Caption Transcripts of News Videos from archive.org 2014--2023",
    "language": "HTML",
    "topics": [
      "news",
      "closed-captions",
      "transcript",
      "closed-caption-transcripts",
      "archive-org",
      "transcripts",
      "news-videos"
    ],
    "readme": "## Closed Captions of News Videos from Archive.org\n\nThe repository provides scripts for downloading the data, and link to two datasets that were built using the scripts:\n\n* [Scripts](https://github.com/notnews/archive_news_cc#downloading-the-data-from-archiveorg)\n* [Data](https://github.com/notnews/archive_news_cc#data)\n\n-------------\n\n### Downloading the Data from Archive.org\n\nDownload closed caption transcripts of nearly 1.3M news shows from [http://archive.org](http://archive.org). \n\nThere are three steps to downloading the transcripts:\n\n1. We start by searching [https://archive.org/advancedsearch.php](https://archive.org/advancedsearch.php) with collection `collection:\"tvarchive\"`. This gets us unique identifiers for each of the news shows. An identifier is a simple string that combines channel_name, show_name, time, and date. The current final list of identifiers (2009--Nov. 2017) is posted [here](data/search.csv). \n\n2. Next, we use the identifier to build a URL where the metadata file and HTML file with the closed captions is posted. The general base URL is http://archive.org/download followed by the identifier.\n\n3. The third script parses the downloaded metadata and HTML closed caption files and creates a CSV along with the meta data.\n\nFor instance, we will go http://archive.org/download/CSPAN_20090604_230000 for identifier `CSPAN_20090604_230000` And from http://archive.org/download/CSPAN_20090604_230000/CSPAN_20090604_230000_meta.xml, we read the link http://archive.org/details/CSPAN_20090604_230000, from which we get the text from HTML file. We also store the meta data from the META XML file.\n\n#### Scripts\n\n1. **Get Show Identifiers**  \n    - [Get Identifiers For Each Show (Channel, Show, Date, Time)](scripts/get_news_identifiers.py)\n    - Produces [data/search.csv](data/search.csv)\n\n2. **Download Metadata and HTML Files**  \n    - [Download the Metadata and HTML Files](scripts/scrape_archive_org.py)\n    - Saves the metadata and HTML files to two separate f",
    "url": "https://github.com/notnews/archive_news_cc",
    "last_updated": "2025-08-06T00:57:15+00:00"
  },
  {
    "full_name": "matthewjdenny/REmail",
    "name": "REmail",
    "description": "R package for Email Data Processing",
    "language": "R",
    "topics": [],
    "readme": "# REmail\n\nAn R package for email data ingestion. \n\n## Installation\n\nTo install this package from git, you will need to Hadley Wickham's devtools package installed.\n\n    install.packages(\"devtools\")\n    library(\"devtools\")\n    \nNow we can install from Github using the following line:\n\n    devtools::install_github(\"matthewjdenny/REmail\")\n\nI have  had success installing with R 3.2.0+ installed but if you do not have the latest \nversion of R installed, it should work as long as you install the dependencies first with\nthe following block of code:\n\n    install.packages( pkgs = c(\"R.methodsS3\",\"rPython\",\"stringr\"), dependencies = TRUE)\n\n## Examples\n\nThis package only implements a function `Read_In_Email()` which will extract \nthe To, From, Cc, Date, Subject, and Message fields from an email using Pythons email libraries and handle exceptions if \nany are missing. This function will return an object of class `Email` that has the following attributes:\n\n* `@from` The email address of the sender (if provided) otherwise will simple store the value included in that field (such as \"John Smith\"\n* `@to` A vector of email address (or strings, handled the same way as the `@from` field) of recipients included in the \"To:\" field of the email.\n* `@CC` A vector of email address (or strings, handled the same way as the `@from` field) of recipients included in the \"CC:\" field of the email. If the field was left empty in the email, this simply stores \"\".\n* `@subject` A string containing the plain text of the subject line.\n* `@subject_tokenized` A string vector containing the lowercased plain text of the subject line with all punctuation removed except for \"?\" and \"!\".\n* `@message` A string containing the plain text of the message body with newlines and blank lines removed.\n* `@message_tokenized` A string vector containing the lowercased plain text of the message body with all punctuation removed except for \"?\" and \"!\".\n* `@date` The date the message was sent in is raw format (will require fu",
    "url": "https://github.com/matthewjdenny/REmail",
    "last_updated": "2025-03-22T10:59:12+00:00"
  },
  {
    "full_name": "TaddyLab/MBAcourse",
    "name": "MBAcourse",
    "description": "Material from the Big Data course at Chicago Booth",
    "language": "TeX",
    "topics": [],
    "readme": "# MBA class materials\n\nExamples and lectures for the Big Data course at Chicago Booth.  This material was expanded, improved, and developed into the Business Data Science book. \n",
    "url": "https://github.com/TaddyLab/MBAcourse",
    "last_updated": "2025-07-22T03:50:00+00:00"
  },
  {
    "full_name": "Reproducible-Science-Curriculum/2015-06-01-reproducible-science-idigbio",
    "name": "2015-06-01-reproducible-science-idigbio",
    "description": "Website for the Reproducible Science Workshop to be taught at iDigBio, June 1-2, 2015",
    "language": "HTML",
    "topics": [],
    "readme": "## Reproducible Science Workshop\n\nTo be held at iDigBio (105 building, teaching room (3rd floor))\n\nJune 1-2, 2015\n\nIf you are interested in attending, check out the\n[website](http://reproducible-science-curriculum.github.io/2015-06-01-reproducible-science-idigbio/).\n",
    "url": "https://github.com/Reproducible-Science-Curriculum/2015-06-01-reproducible-science-idigbio",
    "last_updated": "2019-08-20T22:53:12+00:00"
  },
  {
    "full_name": "wush978/Rython",
    "name": "Rython",
    "description": "Embed python in R",
    "language": "C++",
    "topics": [],
    "readme": "# Rython\n\nEmbed python in R\n\n# Environment\n\nUbuntu with python 2.7\n\nPlease install the following ubuntu packages:\n\n- python-dev\n- libboost-python-dev\n\n# Usage\n\n```r\nlibrary(Rython)\npy(\"python script\")\n```\n\n# Type Mapping\n\nTo pass object from *R* to *python*, use the following functions:\n\n<!-- html table generated in R 2.15.3 by xtable 1.7-1 package -->\n<!-- Wed Apr  3 17:40:22 2013 -->\n<TABLE border=1>\n<TR> <TH>  </TH> <TH> function name </TH> <TH> R type </TH> <TH> python type </TH>  </TR>\n  <TR> <TD align=\"right\"> 1 </TD> <TD> pydict </TD> <TD> list </TD> <TD> dictionary </TD> </TR>\n  <TR> <TD align=\"right\"> 2 </TD> <TD> pylong </TD> <TD> integer </TD> <TD> list of long </TD> </TR>\n  <TR> <TD align=\"right\"> 3 </TD> <TD> pyfloat </TD> <TD> numeric </TD> <TD> list of float </TD> </TR>\n  <TR> <TD align=\"right\"> 4 </TD> <TD> pybool </TD> <TD> logical </TD> <TD> list of bool </TD> </TR>\n  <TR> <TD align=\"right\"> 5 </TD> <TD> pystr </TD> <TD> character </TD> <TD> list of str </TD> </TR>\n   </TABLE>\n\n\n## API\n\n\n\n# Demo \n\n## cql 1.4\n\nThe following script query the Cassandra with python-cql 1.4 and pass the value back to R\n\n```r\nlibrary(Rython)\npy(\"import cql\")\npy(\"con = cql.connect(host, port, key_space, cql_version='3.0.0')\")\npy(\"cursor = con.cursor()\")\npy(\"cursor.execute('select * from column_family limit 1')\")\npy(\"result = cursor.fetchall()\")\npy(\"print result\")\npy(\"result1 = result[0]\")\nresult1 <- pyobj(obj_name=\"result1\", module_name=\"\")\npywrap(result1)\n```\n\n## Demo for R package with python\n\nPut your python script under `inst/python` and install it.\n\n```r\n#'@export\n.onLoad <- function(libname, pkgname) {\n  import_pkg_module(pkgname, \"foo.py\")\n}\n```\n\n# NEWS\n\n- 2013-04-03\n    - add conversion functions(See `R/conversion.R`)\n    - add import_pkg_module\n",
    "url": "https://github.com/wush978/Rython",
    "last_updated": "2023-03-12T10:16:35+00:00"
  },
  {
    "full_name": "hoffmangroup/pdfcomments",
    "name": "pdfcomments",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# pdfcomments: extract comments from PDF\n\npdfcomments extracts comments from a PDF and puts them in a text file.\nIt is most useful for turning text comments and sticky notes into a list of comments with page numbers.\n\nBy default, all comments are minor.\nYou can specify a major comment by adding an asterisk at its beginning.\n\n## Example output\n\nSay you have `test.pdf` and you add the following comments with Acrobat Reader, PDFExpert, or your annotation tool of choice:\n- page 1: a text comment, `Text is unclear.`\n- page 2: a sticky note, `Sticky note.`\n- page 2: a text comment, `* Important comment.`\n\nRun `pdfcomments test.pdf` and it will produce the following output file `test.txt`:\n\n```text\nMajor comments:\n\np2: Important comment.\n\nMinor comments:\n\np1: Text is unclear.\np2: Sticky note.\n```\n\n## Prerequisites\n\n- Python >=3.6\n- PyPDF2 (installed automatically by `pip`)\n\n## Installation\n\n```sh\npython -m pip install pdfcomments\n```\n\nReplace `python` with whatever command runs a version that is of Python 3.6 or greater.\n\n## Usage\n\n```\nusage: pdfcomments [-h] [--version] infile [outfile]\n\nextract comments from PDF\n\npositional arguments:\n  infile      input PDF file\n  outfile     output text file (default: infile with extension changed to 'txt')\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --version   show program's version number and exit\n```\n\n## License\n\nGNU General Public License v3.\n\n## Support\n\nYou are welcome to post issues but no guarantee of support is provided.\nPull requests are welcome.\n",
    "url": "https://github.com/hoffmangroup/pdfcomments",
    "last_updated": "2025-04-20T13:22:40+00:00"
  },
  {
    "full_name": "jayjacobs/ggcal",
    "name": "ggcal",
    "description": "Generate simple calendar plots using ggplot2",
    "language": "R",
    "topics": [],
    "readme": "\n`ggcal` is a simple package to generate a familiar calendar plot from a vector of dates and fill values.\n\nInstallation\n------------\n\n``` r\ndevtools::install_github(\"jayjacobs/ggcal\")\n```\n\nGenerating a simple calendar plot\n---------------------------------\n\n``` r\nlibrary(ggplot2)\nlibrary(ggcal)\n\nmydate <- seq(as.Date(\"2017-02-01\"), as.Date(\"2017-07-22\"), by=\"1 day\")\nmyfills <- rnorm(length(mydate))\n\nprint(ggcal(mydate, myfills))\n```\n\n![](README_files/figure-markdown_github/unnamed-chunk-2-1.png)\n\nThe function returns as base ggplot object, which can be modified with `scale_fill_*`\n\n``` r\ngg <- ggcal(mydate, myfills) +\n  scale_fill_gradient2(low=\"#4575b4\", mid=\"#ffffbf\", high=\"#d73027\", midpoint=0)\n\nprint(gg)\n```\n\n![](README_files/figure-markdown_github/unnamed-chunk-3-1.png)\n\nNotice how the last few days in July are missing in the original data and are set to a dark gray by default. This can be overridden in the `scale_fill_*` function.\n\n``` r\nmydate2 <- sample(mydate, 100) \nmyfills2 <- rnorm(length(mydate2))\n\ngg <- ggcal(mydate2, myfills2) +\n  scale_fill_gradient2(low=\"#4575b4\", mid=\"#ffffbf\", high=\"#d73027\", midpoint=0,\n                       na.value=\"gray95\")\n\nprint(gg)\n```\n\n![](README_files/figure-markdown_github/unnamed-chunk-4-1.png)\n\nThe fill values also do not have to be a continuous variable.\n\n``` r\nmydate <- seq(as.Date(\"2017-02-01\"), as.Date(\"2017-07-31\"), by=\"1 day\")\nmyfills <- ifelse(format(mydate, \"%w\") %in% c(0,6), \"weekend\" ,\"weekday\")\n\nggcal(mydate, myfills) + scale_fill_manual(values=c(\"weekday\"=\"steelblue\", \"weekend\"=\"lightsteelblue\"))\n```\n\n![](README_files/figure-markdown_github/unnamed-chunk-5-1.png)\n\nThis will also span multiple years...\n\n``` r\nmydate <- seq(as.Date(\"2016-09-01\"), as.Date(\"2017-02-28\"), by=\"1 day\")\nmyfills <- rnorm(length(mydate))\n\nggcal(mydate, myfills) + \n    scale_fill_gradient2(low=\"#4575b4\", mid=\"#ffffbf\", high=\"#d73027\", midpoint=0)\n```\n\n![](README_files/figure-markdown_github/unnamed-chunk-6-1.png)\n",
    "url": "https://github.com/jayjacobs/ggcal",
    "last_updated": "2024-05-04T05:33:03+00:00"
  },
  {
    "full_name": "rlabbe/statistical-rethinking",
    "name": "statistical-rethinking",
    "description": "Notebooks containing R code from Richard McElreath's Statistical Rethinking",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "Statistical Rethinking Notebooks\n================================\n\nThese notebooks contain the R code written by Richard McElreath,\navailable for download on his website xcelab.net/rm/statistical-rethnking\nin the file code.txt\n\nI broke each chapter into a separate Jupyter notebook, and devoted\none cell to each code fragment.\n\nRequirements\n------------\n\nYou will need Richard's R package rethinking installed. Installation\ninstructions and source are available on his GitHub repository:\n\nhttps://github.com/rmcelreath/rethinking\n\nAs with the book, you will need to have RStan installed. McElreath's rethinking\nrepository gives instructions for installing it. You will also need\nto have Jupyter's R kernel installed, along with, of course, Jupyter\nNotebook and R.\n\n\n\n",
    "url": "https://github.com/rlabbe/statistical-rethinking",
    "last_updated": "2025-02-20T05:07:41+00:00"
  },
  {
    "full_name": "propublica/guides",
    "name": "guides",
    "description": "ProPublica's News App and Data Style Guides",
    "language": "",
    "topics": [],
    "readme": "# ProPublica's Nerd Guides\n\n## Table of Contents\n\n1. [Intro: The Design and Structure of a News Application](design-structure.md)\n2. [Data Style Guide](news-apps.md)\n3. [Data Bulletproofing Guide](data-bulletproofing.md)\n\n## License\n\nThese guides are copyright 2018 Pro Publica, Inc. and are licensed under a [Creative Commons BY-NC 3.0](http://creativecommons.org/licenses/by-nc/3.0/) license. You are free to share and to remix them, though not to use them commercially without permission.\n\n## Discuss\n\nYou can submit issues using [Github Issues](https://github.com/propublica/guides/issues).\n\n## Contributing\n\n1. Fork it\n2. Create your branch (`git checkout -b my-guide`)\n3. Commit your changes (`git commit -am \"Proposed Change\"`)\n4. Push your branch (`git push origin my-guide`)\n5. Send a pull request\n",
    "url": "https://github.com/propublica/guides",
    "last_updated": "2025-08-18T06:02:36+00:00"
  },
  {
    "full_name": "Roblox/cube",
    "name": "cube",
    "description": "Roblox Foundation Model for 3D Intelligence",
    "language": "Jupyter Notebook",
    "topics": [
      "3d",
      "shape-generation",
      "text-to-3d",
      "3d-generation",
      "3d-aigc"
    ],
    "readme": "# Cube: Generative AI System for 3D\n\n<p align=\"center\">\n  <img src=\"./resources/teaser.png\" width=\"800\" style=\"margin: 5px;\">\n</p>\n\n<div align=\"center\">\n  <a href=https://corp.roblox.com/newsroom/2025/03/introducing-roblox-cube target=\"_blank\"><img src=https://img.shields.io/badge/Roblox-Blog-000000.svg?logo=Roblox height=22px></a>\n  <a href=https://arxiv.org/abs/2503.15475 target=\"_blank\"><img src=https://img.shields.io/badge/ArXiv-Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://huggingface.co/Roblox/cube3d-v0.5 target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Models-d96902.svg height=22px></a>\n  <a href=https://huggingface.co/spaces/Roblox/cube3d-interactive target=\"_blank\"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Demo-blue.svg height=22px></a>\n  <a href=https://colab.research.google.com/drive/1ZvTj49pjDCD_crX5WPZNTAoTTzL6-E5t target=\"_blank\"><img src=https://img.shields.io/badge/Colab-Demo-blue.svg?logo=googlecolab height=22px></a>\n</div>\n\n\nFoundation models trained on vast amounts of data have demonstrated remarkable reasoning and\ngeneration capabilities in the domains of text, images, audio and video. Our goal is to build\nsuch a foundation model for 3D intelligence, a model that can support developers in producing all aspects\nof a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to\nproducing programmatic scripts describing object behaviors. As we start open-sourcing a family of models \ntowards this vision, we hope to engage others in the research community to address these goals with us.\n\n## July 2025 Update: Cube 3D v0.5 ✨\nWith the v0.5 model, we introduce two upgrades to the auto-regressive base model for 3D geometry generation from text: *higher fidelity 3D compositions* and *bounding box conditioning*.\nThe example gif below shows the model's capacity to generate 3D shapes capturing mixtures of concepts expressed in text, for example *mechanica",
    "url": "https://github.com/Roblox/cube",
    "last_updated": "2025-09-01T06:04:53+00:00"
  },
  {
    "full_name": "BuzzFeedNews/everything",
    "name": "everything",
    "description": "An index of all our open-source data, analysis, libraries, tools, and guides.",
    "language": "",
    "topics": [],
    "readme": "# BuzzFeedNews/everything\n\nAn index of all our open-source data, analysis, libraries, tools, and guides.\n\n## Table of Contents\n\n- [Data and Analyses](#data-and-analyses)\n- [Standalone Datasets](#standalone-datasets)\n- [Libraries and Tools](#libraries-and-tools)\n- [Guides](#guides)\n\n---\n\n## Data and Analyses\n\nDate|Description|Repo(s)|Article(s)\n----|-----------|:--:|:-----:\n`2022-04-27`|Data and analysis of state child abuse and neglect registries and appeals|[:link:](https://github.com/BuzzFeedNews/2022-04-registries)|[:link:](https://www.buzzfeednews.com/article/scottpham/child-abuse-and-neglect-registries-punish-parents-of-color)\n`2022-04-25`|Data and analysis of intermediate care facilities|[:link:](https://github.com/BuzzFeedNews/2022-04-icf-analysis)|[:link:](https://www.buzzfeednews.com/article/kendalltaggart/kkr-brightspring-disability-private-equity-abuse)\n`2021-09-17`|Data and analysis re. US adult guardianship filing counts|[:link:](https://github.com/BuzzFeedNews/2021-09-guardianship-filings)|[:link:](https://buzzfeednews.com/article/heidiblake/conservatorship-investigation-free-britney-spears)\n`2021-05-26`|Analysis of excess deaths caused by the February 2021 winter storm and power outages in Texas|[:link:](https://buzzfeednews.github.io/2021-05-tx-winter-storm-deaths/)|[:link:](https://www.buzzfeednews.com/article/peteraldhous/texas-winter-storm-power-outage-death-toll)\n`2020-11-11`|Analysis of county-level COVID-19 deaths and presidential voter preference|[:link:](https://buzzfeednews.github.io/2020-11-covid-election/)|[:link:](https://www.buzzfeednews.com/article/peteraldhous/coronavirus-deaths-unemployment-trump-election-results)\n`2020-10-28`|Analysis of 2020's \"Electoral College effect\" by demographic|[:link:](https://github.com/BuzzFeedNews/2020-10-electoral-college-effect-by-demographic)|[:link:](https://www.buzzfeednews.com/article/johntemplon/the-electoral-college-still-favors-white-voters)\n`2020-06-04`|Analysis of \"1033\" program transfers since",
    "url": "https://github.com/BuzzFeedNews/everything",
    "last_updated": "2025-08-29T00:13:29+00:00"
  },
  {
    "full_name": "yaringal/DropoutUncertaintyExps",
    "name": "DropoutUncertaintyExps",
    "description": "Experiments used in \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\"",
    "language": "Python",
    "topics": [],
    "readme": "This is the code used for the uncertainty experiments in the paper [\"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" (2015)](http://www.cs.ox.ac.uk/people/yarin.gal/website/publications.html#Gal2015Dropout), with a few adaptions following recent (2018) feedback from the community (many thanks to @capybaralet for spotting some bugs, and @omegafragger for restructuring the code). This code is based on the code by José Miguel Hernández-Lobato used for his paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\". The datasets supplied here are taken from the UCI machine learning repository. Note the data splits used in these experiments (which are identical to the ones used in Hernández-Lobato's code). Because of the small size of the data, if you split the data yourself you will most likely get different and non-comparable results to the ones here.\n\n**Update (2018)**\nWe replaced the Bayesian optimisation implementation (which was used to find hypers) with a grid-search over the hypers. This is following feedback from @capybaralet who spotted test-set contamination (some train-set points, used to tune hypers which were shared across all splits, were used as test-set points in later splits). The new implementation iterates over the 20 splits, and for each train-test split it creates a _new_ train-val split to tune hypers. These hypers are discarded between different train-test splits. \n\nBelow we report the new results using grid-search (_new_, with code in this updated repo) vs. results obtained from a re-run of the original code used in the paper which used Bayesian optimisation (_paper_, code in [previous commit](https://github.com/yaringal/DropoutUncertaintyExps/tree/a6259f1db8f5d3e2d743f88ecbde425a07b12445)). Note that we report slightly different numbers for _paper_ than in the [previous commit](https://github.com/yaringal/DropoutUncertaintyExps/tree/a6259f1db8f5d3e2d743f88ecbde425a07b12445), due t",
    "url": "https://github.com/yaringal/DropoutUncertaintyExps",
    "last_updated": "2025-08-24T06:48:37+00:00"
  },
  {
    "full_name": "kortleb/chinainafrica",
    "name": "chinainafrica",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# chinainafrica\n\nChina's influence in Africa has grown over the past 20 years. Here, I examine China's aid to Zimbabwe and Equatorial Guinea using AirData's dataset, which takes advantage of media reports to get a more accurate picture of China's aid to Africa. \n",
    "url": "https://github.com/kortleb/chinainafrica",
    "last_updated": "2019-01-05T19:03:15+00:00"
  },
  {
    "full_name": "wikimedia-research/ayatollahsong",
    "name": "ayatollahsong",
    "description": "Analysis of the impact of the HTTPS switchover in Iran",
    "language": "R",
    "topics": [],
    "readme": "# ayatollahsong\n\n    there's a ban\n        in Iran\n    that they can't resist\n    disappeared,\n        and we feared\n    got this site in a twist.\n\n    And even in these crypto-hate times\n    We can't help wishing it was fine…\n\n([context](https://www.youtube.com/watch?v=aGBfYoldZQ4))\n\nWikimedia sites switched over to HTTPS for readers by default in June 2015. Subsequently we saw a massive drop in pageviews from Iran. This is a simple project to try and understand if the two things are concretely linked, and whether the same event hit search requests.\n",
    "url": "https://github.com/wikimedia-research/ayatollahsong",
    "last_updated": "2015-07-14T17:14:38+00:00"
  },
  {
    "full_name": "rdrr1990/kerasformula",
    "name": "kerasformula",
    "description": "A high-level interface to keras for R that takes advantage of formulas",
    "language": "R",
    "topics": [],
    "readme": "kerasformula\n================\nPete Mohanty\nAugust 17, 2018\n\n[![](https://cranlogs.r-pkg.org/badges/kerasformula)](https://cran.r-project.org/package=kerasformula) [![cran checks](https://cranchecks.info/badges/summary/kerasformula)](https://cranchecks.info/pkgs/kerasformula) [![cran version](http://www.r-pkg.org/badges/version/kerasformula)](https://cran.r-project.org/package=kerasformula)\n\nkerasformula\n============\n\nNow on CRAN, `kerasformula` offers a high-level interface to [keras](https://keras.rstudio.com/) neural nets. `kerasformula` streamlines everything from data manipulation to model design to cross-validation and hyperparameter selection.\n\n`kms`, as in `keras_model_sequential()`, is a regression-style function that lets you build `keras` neural nets with `R` `formula` objects. `kms()` accepts a number of parameters, allowing users to customize the number of units, layers, activation function, loss function, optimizer, and so on. `kms()` accepts a number of parameters (like loss and optimizer) and splits the data into (optionally sparse) test and training matrices. `kms()` facilitates setting advanced hyperparameters (e.g., dropout rate and regularization) to prevent overfitting. `kms()` optionally accept a compiled `keras_sequential_model()`. `kms()` returns a single object with predictions, a confusion matrix, and function call details.\n\n`kms` accepts the major parameters found in `library(keras)` as inputs (loss function, batch size, number of epochs, etc.) and allows users to customize basic neural nets which, by default, now include regularizers. `kms` also accepts a compiled `keras_model_sequential` to `kms` as an argument (preferable for more complex models). The examples here (and the in the examples folder) don't provide particularly predictive models so much as show how using `formula` objects can smooth data cleaning and hyperparameter selection.\n\nA worked example can be found on the RStudio Tensorflow website here: [Twitter data](https://tensor",
    "url": "https://github.com/rdrr1990/kerasformula",
    "last_updated": "2025-03-22T11:11:01+00:00"
  },
  {
    "full_name": "run-llama/llama_index",
    "name": "llama_index",
    "description": "LlamaIndex is the leading framework for building LLM-powered agents over your data.",
    "language": "Python",
    "topics": [
      "agents",
      "application",
      "data",
      "fine-tuning",
      "framework",
      "llamaindex",
      "llm",
      "rag",
      "vector-database",
      "multi-agents"
    ],
    "readme": "# 🗂️ LlamaIndex 🦙\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n[![Build](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml/badge.svg)](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml)\n[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)\n[![Twitter](https://img.shields.io/twitter/follow/llama_index)](https://x.com/llama_index)\n[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/LlamaIndex?style=plastic&logo=reddit&label=r%2FLlamaIndex&labelColor=white)](https://www.reddit.com/r/LlamaIndex/)\n[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNSIgaGVpZ2h0PSI0IiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxwYXRoIGQ9Ik00LjQzIDEuODgyYTEuNDQgMS40NCAwIDAgMS0uMDk4LjQyNmMtLjA1LjEyMy0uMTE1LjIzLS4xOTIuMzIyLS4wNzUuMDktLjE2LjE2NS0uMjU1LjIyNmExLjM1MyAxLjM1MyAwIDAgMS0uNTk1LjIxMmMtLjA5OS4wMTItLjE5Mi4wMTQtLjI3OS4wMDZsLTEuNTkzLS4xNHYtLjQwNmgxLjY1OGMuMDkuMDAxLjE3LS4xNjkuMjQ2LS4xOTFhLjYwMy42MDMgMCAwIDAgLjItLjEwNi41MjkuNTI5IDAgMCAwIC4xMzgtLjE3LjY1NC42NTQgMCAwIDAgLjA2NS0uMjRsLjAyOC0uMzJhLjkzLjkzIDAgMCAwLS4wMzYtLjI0OS41NjcuNTY3IDAgMCAwLS4xMDMtLjIuNTAyLjUwMiAwIDAgMC0uMTY4LS4xMzguNjA4LjYwOCAwIDAgMC0uMjQtLjA2N0wyLjQzNy43MjkgMS42MjUuNjcxYS4zMjIuMzIyIDAgMCAwLS4yMzIuMDU4LjM3NS4zNzUgMCAwIDAtLjExNi4yMzJsLS4xMTYgMS40NS0uMDU4LjY5Ny0uMDU4Ljc1NEwuNzA1IDRsLS4zNTctLjA3OUwuNjAyLjkwNkMuNjE3LjcyNi42NjMuNTc0LjczOS40NTRhLjk1OC45NTggMCAwIDEgLjI3NC0uMjg1Ljk3MS45NzEgMCAwIDEgLjMzNy0uMTRjLjExOS0uMDI2LjIyNy0uMDM0LjMyNS0uMDI2TDMuMjMyLjE2Yy4xNTkuMDE0LjMzNi4wMy40NTkuMDgyYTEuMTczIDEuMTczIDAgMCAxIC41NDUuNDQ3Yy4wNi4wOTQuMTA5LjE5Mi4xNDQuMjkzYTEuMzkyIDEuMzkyIDAgMCAxIC4wNzguNThsLS4wMjkuMzJaIiBmaWxsPSIjRjI3NzdBIi8+CiAgPH",
    "url": "https://github.com/run-llama/llama_index",
    "last_updated": "2025-09-02T10:05:34+00:00"
  },
  {
    "full_name": "sckott/pdfimager",
    "name": "pdfimager",
    "description": "Extract images from pdfs using the pdfimages tool from poppler https://poppler.freedesktop.org/",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "poppler",
      "pdfimages",
      "pdf"
    ],
    "readme": "pdfimager\n=========\n\n\n\n[![R-check](https://github.com/sckott/pdfimager/workflows/R-check/badge.svg)](https://github.com/sckott/pdfimager/actions/)\n\n\n`pdfimager` - Extract images from pdfs\n\nDocs: <https://sckott.github.io/pdfimager/>\n\nThis packages uses `sys` R package to \"shell out\" to pdfimages. Apparently pdfimages is not in poppler cpp, so is not in pdftools R pkg\n\n## Install pdfimages\n\npdfimages is installed when you install poppler\n\nInstallation instructions can be found at <https://poppler.freedesktop.org/>\n\n## Install pdfimager\n\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"sckott/pdfimager\")\n```\n\n\n``` r\nlibrary(\"pdfimager\")\n```\n\n## Set the path\n\nSome users may need to manually set the path to `pdfimages`. \n\nYou can do so with a function in this package like\n\n```r\npdimg_set_path()\n``` \n\nor  set the path for pdfimages before starting R with an env var like:\n\n```\nPDFIMAGER_PATH=C:/some/path/to/poppler/24/bin/pdfimages.exe R\n```\n \nOr set within R like:\n\n```r\nSys.setenv(PDFIMAGER_PATH=\"C:/some/path/to/poppler/24/bin/pdfimages.exe\")\n```\n\n## help info\n\n\n``` r\npdimg_help()\n#> pdfimages version 24.04.0\n#> Copyright 2005-2024 The Poppler Developers - http://poppler.freedesktop.org\n#> Copyright 1996-2011, 2022 Glyph & Cog, LLC\n#> Usage: pdfimages [options] <PDF-file> <image-root>\n#>   -f <int>                 : first page to convert\n#>   -l <int>                 : last page to convert\n#>   -png                     : change the default output format to PNG\n#>   -tiff                    : change the default output format to TIFF\n#>   -j                       : write JPEG images as JPEG files\n#>   -jp2                     : write JPEG2000 images as JP2 files\n#>   -jbig2                   : write JBIG2 images as JBIG2 files\n#>   -ccitt                   : write CCITT images as CCITT files\n#>   -all                     : equivalent to -png -tiff -j -jp2 -jbig2 -ccitt\n#>   -list                    : print list of images instead of saving\n#>   -opw <string>            : owner pa",
    "url": "https://github.com/sckott/pdfimager",
    "last_updated": "2025-08-31T09:17:06+00:00"
  },
  {
    "full_name": "yamadashy/repomix",
    "name": "repomix",
    "description": "📦 Repomix is a powerful tool that packs your entire repository into a single, AI-friendly file. Perfect for when you need to feed your codebase to Large Language Models (LLMs) or other AI tools like Claude, ChatGPT, DeepSeek, Perplexity, Gemini, Gemma, Llama, Grok, and more.",
    "language": "TypeScript",
    "topics": [
      "ai",
      "llm",
      "generative-ai",
      "chatgpt",
      "claude",
      "developer-tools",
      "chatbot",
      "gpt",
      "openai",
      "javascript",
      "nodejs",
      "typescript",
      "anthropic",
      "gemini",
      "language-model",
      "artificial-intelligence",
      "genai",
      "deepseek",
      "llama",
      "mcp"
    ],
    "readme": "<div align=\"center\" markdown=\"1\">\n   <sup>Special thanks to:</sup>\n   <br>\n   <br>\n   <a href=\"https://go.warp.dev/repomix\">\n      <img alt=\"Warp sponsorship\" width=\"400\" src=\"https://raw.githubusercontent.com/warpdotdev/brand-assets/main/Github/Sponsor/Warp-Github-LG-01.png\">\n   </a>\n\n### [Warp, built for coding with multiple AI agents](https://go.warp.dev/repomix)\n[Available for MacOS, Linux, & Windows](https://go.warp.dev/repomix)<br>\n\n   <br>\n\n   <a href=\"https://tuple.app/repomix\">\n      <img alt=\"Tuple sponsorship\" width=\"400\" src=\"website/client/src/public/images/sponsors/tuple/github_repo_sponsorship.png\">\n   </a>\n\n### [Tuple, the premier screen sharing app for developers on macOS and Windows.](https://tuple.app/repomix)\n\n</div>\n\n\n<hr />\n\n<div align=\"center\">\n  <a href=\"https://repomix.com\">\n    <img src=\"website/client/src/public/images/repomix-title.png\" alt=\"Repomix\" width=\"500\" height=\"auto\" />\n  </a>\n  <p align=\"center\">\n    <b>Pack your codebase into AI-friendly formats</b>\n  </p>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://repomix.com\"><b>Use Repomix online! 👉 repomix.com</b></a><br>\n</p>\n\n<p align=\"center\">\n  Need discussion? Join us on <a href=\"https://discord.gg/wNYzTwZFku\">Discord</a>!<br>\n  <i>Share your experience and tips</i><br>\n  <i>Stay updated on new features</i><br>\n  <i>Get help with configuration and usage</i><br>\n</p>\n\n<hr />\n\n[![npm](https://img.shields.io/npm/v/repomix.svg?maxAge=1000)](https://www.npmjs.com/package/repomix)\n[![npm](https://img.shields.io/npm/d18m/repomix)](https://www.npmjs.com/package/repomix)\n[![Actions Status](https://github.com/yamadashy/repomix/actions/workflows/ci.yml/badge.svg)](https://github.com/yamadashy/repomix/actions?query=workflow%3A\"ci\")\n[![codecov](https://codecov.io/github/yamadashy/repomix/graph/badge.svg)](https://codecov.io/github/yamadashy/repomix)\n[![Sponsors](https://img.shields.io/github/sponsors/yamadashy?logo=github)](https://github.com/sponsors/yamadashy)\n[![Discord](https://badgen.net/di",
    "url": "https://github.com/yamadashy/repomix",
    "last_updated": "2025-09-02T09:37:33+00:00"
  },
  {
    "full_name": "ThinkR-open/stopwords",
    "name": "stopwords",
    "description": "stop words in several languages",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n``` r\nlibrary(\"stopwords\")\nsample( stopwords_iso$fr, 20)\n#>  [1] \"en\"            \"aura\"          \"troisièmement\" \"leur\"         \n#>  [5] \"pourquoi\"      \"si\"            \"prealable\"     \"suis\"         \n#>  [9] \"doit\"          \"pfut\"          \"soi-même\"      \"quanta\"       \n#> [13] \"exactement\"    \"premièrement\"  \"cinquantaine\"  \"proche\"       \n#> [17] \"semble\"        \"tel\"           \"u\"             \"f\"\n```\n",
    "url": "https://github.com/ThinkR-open/stopwords",
    "last_updated": "2023-09-11T14:05:38+00:00"
  },
  {
    "full_name": "NARKOZ/hacker-scripts",
    "name": "hacker-scripts",
    "description": "Based on a true story",
    "language": "JavaScript",
    "topics": [],
    "readme": "\nEnglish | [简体中文](./README.zh-CN.md)\n\n# Hacker Scripts\n\nBased on a _[true\nstory](https://www.jitbit.com/alexblog/249-now-thats-what-i-call-a-hacker/)_:\n\n> xxx: OK, so, our build engineer has left for another company. The dude was literally living inside the terminal. You know, that type of a guy who loves Vim, creates diagrams in Dot and writes wiki-posts in Markdown... If something - anything - requires more than 90 seconds of his time, he writes a script to automate that.\n\n> xxx: So we're sitting here, looking through his, uhm, \"legacy\"\n\n> xxx: You're gonna love this\n\n> xxx: [`smack-my-bitch-up.sh`](https://github.com/NARKOZ/hacker-scripts/blob/master/smack-my-bitch-up.sh) - sends a text message \"late at work\" to his wife (apparently). Automatically picks reasons from an array of strings, randomly. Runs inside a cron-job. The job fires if there are active SSH-sessions on the server after 9pm with his login.\n\n> xxx: [`kumar-asshole.sh`](https://github.com/NARKOZ/hacker-scripts/blob/master/kumar-asshole.sh) - scans the inbox for emails from \"Kumar\" (a DBA at our clients). Looks for keywords like \"help\", \"trouble\", \"sorry\" etc. If keywords are found - the script SSHes into the clients server and rolls back the staging database to the latest backup. Then sends a reply \"no worries mate, be careful next time\".\n\n> xxx: [`hangover.sh`](https://github.com/NARKOZ/hacker-scripts/blob/master/hangover.sh) - another cron-job that is set to specific dates. Sends automated emails like \"not feeling well/gonna work from home\" etc. Adds a random \"reason\" from another predefined array of strings. Fires if there are no interactive sessions on the server at 8:45am.\n\n> xxx: (and the oscar goes to) [`fucking-coffee.sh`](https://github.com/NARKOZ/hacker-scripts/blob/master/fucking-coffee.sh) - this one waits exactly 17 seconds (!), then opens a telnet session to our coffee-machine (we had no frikin idea the coffee machine is on the network, runs linux and has a TCP socket up and running) ",
    "url": "https://github.com/NARKOZ/hacker-scripts",
    "last_updated": "2025-09-02T09:27:34+00:00"
  },
  {
    "full_name": "yihui/printr",
    "name": "printr",
    "description": "Some (magical) printing methods for knitr",
    "language": "R",
    "topics": [
      "knitr",
      "printr",
      "r"
    ],
    "readme": "# printr\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/yihui/printr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/yihui/printr/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\nThis is a companion package to [**knitr**](https://yihui.org/knitr). Its main\npurpose is to extend the S3 generic function `knit_print()` in **knitr**, which\nis the default value of the chunk option `render`, as explained in the vignette\n[knit_print.html](https://cran.r-project.org/package=knitr/vignettes/knit_print.html).\n\nYou can install the stable version from CRAN or the development version from my personal repo:\n\n```r\n# CRAN version\ninstall.packages('printr')\n\n# or the dev version\ninstall.packages(\n  'printr',\n  repos = c('https://yihui.r-universe.dev', 'https://cran.rstudio.com')\n)\n```\n\nTo enable the printing methods defined in this package, just `library(printr)`\nin a code chunk (in the beginning) of your **knitr** document. Then some objects\nwill be printed differently with what you would have seen in a normal R console.\nFor example:\n\n- matrices, data frames, and contingency tables are printed as tables (LaTeX,\n  HTML, or Markdown, depending on your output format)\n- the help page (from `?foo` or `help(foo)`) can be rendered as HTML, LaTeX, or\n  plain text, and you can also specify which section(s) of the help page to\n  include in the output\n- the results from `browseVignettes()`, `help.search()`, `data()`, and\n  `vignette()` are rendered as tables\n- the package information from `library(help = 'foo')` is rendered as plain text\n\nFor more information, please check out the [package\nvignette](https://yihui.org/printr/):\n\n```r\nvignette('printr', package = 'printr')\n```\n\nYou are welcome to contribute more S3 methods to this package, and you may want\nto read the existing methods in this package before you get started. In most\ncases, I guess you will end up rendering either plain text (see\n`knit_print.packageInfo` for example) or tables (see `knit_prin",
    "url": "https://github.com/yihui/printr",
    "last_updated": "2025-04-17T01:27:17+00:00"
  },
  {
    "full_name": "wikimedia-research/Talkcicity",
    "name": "Talkcicity",
    "description": "An analysis of the toxicity of en.wikipedia talkpages.",
    "language": "R",
    "topics": [],
    "readme": "WikiTalkParser\n==============\n\nWikiTalkParser is a library for extracting and parsing Wikipedia talk pages, \nidentifying comments with their signature, date and indentation in the thread \nstructure.\nIn the current version, talk pages are extracted from the WIkipedia API, given \nin input a list of articles. Only the English language version is supported.\n\n### Language\nTested with Python 2.7\n\n### Authors\nDavid Laniado and Riccardo Tasso\n\n### Limitations/TODO\n* The parser works only for the English Wikipedia. We are currently working to make it multilingual\n* This version was only tested with article talk pages. Support for user talk pages will be added\n* Users are identified via user name, and user id generated by the software (official Wikipedia user ids are not supported)\n* \"Outdent\" command is currently not managed\n\n### References\nFor further information, see research paper: \n[When the Wikipedians talk: network and tree structure of Wikipedia discussion \npages](http://airwiki.ws.dei.polimi.it/index.php/When_the_Wikipedians_Talk)\n",
    "url": "https://github.com/wikimedia-research/Talkcicity",
    "last_updated": "2017-10-23T23:16:37+00:00"
  },
  {
    "full_name": "pytorch/captum",
    "name": "captum",
    "description": "Model interpretability and understanding for PyTorch",
    "language": "Python",
    "topics": [
      "interpretability",
      "interpretable-ai",
      "interpretable-ml",
      "feature-importance",
      "feature-attribution"
    ],
    "readme": "![Captum Logo](./website/static/img/captum_logo.png)\n\n<hr/>\n\n<!--- BADGES: START --->\n[![GitHub - License](https://img.shields.io/github/license/pytorch/captum?logo=github&style=flat&color=green)][#github-license]\n[![Conda](https://img.shields.io/conda/vn/pytorch/captum?logo=anaconda&style=flat&color=orange)](https://anaconda.org/pytorch/captum)\n[![PyPI](https://img.shields.io/pypi/v/captum.svg)][#pypi-package]\n[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/captum?logo=anaconda&style=flat)][#conda-forge-package]\n[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/captum?logo=anaconda&style=flat&color=orange)][#conda-forge-package]\n[![Conda Recipe](https://img.shields.io/static/v1?logo=conda-forge&style=flat&color=green&label=recipe&message=captum)][#conda-forge-feedstock]\n[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=captum&style=flat&color=pink&label=docs&message=captum)][#docs-package]\n\n[#github-license]: https://github.com/pytorch/captum/blob/master/LICENSE\n[#pypi-package]: https://pypi.org/project/captum/\n[#conda-forge-package]: https://anaconda.org/conda-forge/captum\n[#conda-forge-feedstock]: https://github.com/conda-forge/captum-feedstock\n[#docs-package]: https://captum.ai/\n<!--- BADGES: END --->\n\n\nCaptum is a model interpretability and understanding library for PyTorch.\nCaptum means comprehension in Latin and contains general purpose implementations\nof integrated gradients, saliency maps, smoothgrad, vargrad and others for\nPyTorch models. It has quick integration for models built with domain-specific\nlibraries such as torchvision, torchtext, and others.\n\n\n#### About Captum\n\nWith the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important. Model understanding is both an active area of research as well as an area of focus for practical applications across industries using machine learning. Captum provides state-of-the-art algorithms suc",
    "url": "https://github.com/pytorch/captum",
    "last_updated": "2025-09-01T15:04:26+00:00"
  },
  {
    "full_name": "hrbrmstr/sparrow",
    "name": "sparrow",
    "description": "Temporary Shorcut For Reading Arrow/Parquet Bits Into R via 'reticulate'",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "parquet",
      "pandas-dataframe",
      "arrow"
    ],
    "readme": "\n# sparrow\n\nTemporary Shorcut For Reading Arrow/Parquet Bit Into R via ‘reticulate’\n\n## Description\n\nWork is being done to make Parquet/Arrow a first-class R citizen but –\nuntil then – I don’t always want a Drill server round trip just to read\nin some data and same goes for firing up a Spark instance (srsly). So,\nthis is a quick hack until the R packages are done.\n\n## NOTE\n\n**Requires** Python 3.5+, `pyarrow` and `pandas`.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `read_parquet`: Read in data from Parquet into an R data frame via\n    ‘reticulate’\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/sparrow\")\n```\n\n## Usage\n\n``` r\nlibrary(sparrow)\n\n# current verison\npackageVersion(\"sparrow\")\n```\n\n    ## [1] '0.1.0'\n\n``` r\nread_parquet(\"/tmp/honeypot.parquet\")\n```\n\n    ## # A tibble: 1,678,931 x 7\n    ##       duration service proto ts                  src             src_country_code dport\n    ##          <dbl> <chr>   <chr> <chr>               <chr>           <list>           <int>\n    ##  1 121.        -       tcp   2018-01-23 19:55:09 154.70.142.5    <chr [1]>          445\n    ##  2 108.        -       tcp   2018-01-23 20:02:54 46.29.195.27    <chr [1]>          445\n    ##  3   0.0000380 -       tcp   2018-01-23 19:29:50 121.52.205.133  <chr [1]>          443\n    ##  4  13.8       -       tcp   2018-01-23 20:07:03 199.227.118.42  <chr [1]>          445\n    ##  5  17.4       -       tcp   2018-01-23 20:10:18 115.110.123.190 <chr [1]>          445\n    ##  6  17.9       -       tcp   2018-01-23 20:14:02 221.132.113.69  <chr [1]>          445\n    ##  7 117.        -       tcp   2018-01-23 20:14:47 201.238.247.234 <chr [1]>          445\n    ##  8   2.22      ssh     tcp   2018-01-23 19:34:34 123.59.134.76   <chr [1]>           22\n    ##  9  19.3       -       tcp   2018-01-23 20:19:05 185.120.222.35  <chr [1]>          445\n    ## 10  27.7       -       tcp   2018-01-23 19:46:49 118.166.51.193  <chr [1]>          445\n    ## # ... with",
    "url": "https://github.com/hrbrmstr/sparrow",
    "last_updated": "2025-03-22T11:07:28+00:00"
  },
  {
    "full_name": "pommedeterresautee/fastrtext",
    "name": "fastrtext",
    "description": "R wrapper for fastText",
    "language": "C++",
    "topics": [
      "fasttext",
      "rstats",
      "machine-learning",
      "nlp",
      "classification",
      "word-embeddings",
      "text-classification",
      "neural-network",
      "embeddings"
    ],
    "readme": "![fastrtext](https://github.com/pommedeterresautee/fastrtext/raw/master/tools/logo.png) \n=========\n\n[![Travis-CI Build Status](https://travis-ci.org/pommedeterresautee/fastrtext.svg?branch=master)](https://travis-ci.org/pommedeterresautee/fastrtext)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/pommedeterresautee/fastrtext?branch=master&svg=true)](https://ci.appveyor.com/project/pommedeterresautee/fastrtext)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/fastrtext)](https://cran.r-project.org/package=fastrtext)\n[![CRAN_time_from_release](https://www.r-pkg.org/badges/ago/fastrtext)](https://cran.r-project.org/package=fastrtext)\n[![CRAN_Download](http://cranlogs.r-pkg.org/badges/fastrtext)](http://cran.rstudio.com/web/packages/fastrtext/index.html) \n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![codecov](https://codecov.io/gh/pommedeterresautee/fastrtext/branch/master/graph/badge.svg)](https://codecov.io/gh/pommedeterresautee/fastrtext)\n[![Follow](https://img.shields.io/twitter/follow/pommedeterre33.svg?style=social)](https://twitter.com/intent/follow?screen_name=pommedeterre33)\n\n[R Documentation](https://pommedeterresautee.github.io/fastrtext/) | [Release Notes](https://github.com/pommedeterresautee/fastrtext/blob/master/NEWS.md) | [FAQ](https://fasttext.cc/docs/en/faqs.html) | [Multilingual pretrained models](https://fasttext.cc/docs/en/crawl-vectors.html)\n\nR wrapper for [fastText](https://github.com/facebookresearch/fastText) C++ code from Facebook.\n\nFastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices.\n\n\n## License\n\n© Contributors, 2019. Licensed under a MIT license.\n",
    "url": "https://github.com/pommedeterresautee/fastrtext",
    "last_updated": "2024-07-27T15:10:20+00:00"
  },
  {
    "full_name": "xdevplatform/Gnip-Trend-Detection",
    "name": "Gnip-Trend-Detection",
    "description": "Trend detection algorithms for Twitter time series data",
    "language": "Python",
    "topics": [],
    "readme": "# Introduction\n\nThis repository contains the \"Trend Detection in Social Data\" whitepaper,\nalong with software that implements a variety of models for trend detection.\n\nWe focus on trend detection in social data times series. A time series is\ndefined by the presence of a word, a phrase, a hashtags, a mention, or any\nother characteristic of a social media event that can be counted in a\nseries of time intervals. To do trend detection, we quantify \nthe degree to which each count in the time series is atypical. We refer to\nthis figure of merit with the Greek letter *eta*, and we say that a \ntime series and its associated topic are \"trending\" if the figure of merit\nexceeds a pre-defined threshold denoted by the Greek letter *theta*. \n\n# Whitepaper\n\nThe trends whitepaper source can be found in the `paper` directory, which\nalso includes a subdirectory for figures, `figs`. A PDF version of the \npaper is included but it is not gaurenteed to be up-to-date. A new version can\nbe generated from the source by running:\n\n`pdflatex paper/trends.tex`\n\nInstallation of `pdflatex` and/or additional .sty files may be required.\n\n# Gnip-Trend-Detection Software\n\n## Input Data\n\nThe input data consists of CSV records, and is expected to contain data for one\nquantity (\"counter\") and one time interval on each line, in the following format:\n\n| interval start time | interval duration in sec. | count | counter name |\n| ------------------- | --------- | ---------- | ------------------- |\n| 2015-01-01 00:03:25.0  | 195 | 201 | TweetCounter |\n| 2015-01-01 00:03:25.0  | 195 | 13 | ReTweetCounter |\n|2015-01-01 00:06:40.0| 195 | 191 | TweetCounter |\n|2015-01-01 00:06:40.0| 195 | 10 | ReTweetCounter |\n\nThe format of the interval start time can be any of the large number of standard\nformats recognized by Python's [dateutil](https://dateutil.readthedocs.io/en/stable/) package. \n\nThe recommended way to produce time series data in the correct format is to use\nthe [Gnip-Analysis-Pipeline](https://github.com/j",
    "url": "https://github.com/xdevplatform/Gnip-Trend-Detection",
    "last_updated": "2024-08-07T02:58:06+00:00"
  },
  {
    "full_name": "NguyenThaoVi0702/vtb_biname",
    "name": "vtb_biname",
    "description": "Binary classification of names ",
    "language": "",
    "topics": [],
    "readme": "# vtb_biname\nA binary organization/ individual name classification library for PySpark, in which: \n1 stands for organization\n0 stands for individual\n",
    "url": "https://github.com/NguyenThaoVi0702/vtb_biname",
    "last_updated": "2025-01-15T15:29:06+00:00"
  },
  {
    "full_name": "the-markup/blacklight-collector",
    "name": "blacklight-collector",
    "description": "",
    "language": "TypeScript",
    "topics": [],
    "readme": "# Blacklight-Collector\n\n**NOTE: This repo contains some, but not all, of the code backing [Blacklight](https://themarkup.org/blacklight). It may not be very useful on its own. We're thinking about ways to move more of the functionality into this package in order to make it more generally useful.**\n\nFor more information about the `blacklight-collector` please read our [methodology](https://themarkup.org/blacklight/2020/09/22/how-we-built-a-real-time-privacy-inspector).\n\n`blacklight-collector` is available on npm. You can add it to your own project with the following command.\n\n```\nnpm i @themarkup/blacklight-collector\n```\n\nIf you are interested in running it locally you can clone this repository and follow the instructions below.\n\n## Build\n\n`nvm use`\n\n`npm install`\n\n`npm run build`\n\n## Usage\n\n`npm run example`.\n\nResults are stored in `demo-dir` by default\n\n## Collector configuration\n\n`collect` takes the following arguments:\n\n- `inUrl` **required**\n  - The URL you want to scrape\n- `outDir`\n  - default: saves to tmp directory and deletes after completion\n  - To save the full report provide a directory path\n- `blTests`\n  - Array of tests to run\n  - default: All\n    - \"behaviour_event_listeners\"\n    - \"canvas_fingerprinters\"\n    - \"canvas_font_fingerprinters\"\n    - \"cookies\"\n    - \"fb_pixel_events\"\n    - \"google_analytics_events\"\n    - \"key_logging\"\n    - \"session_recorders\"\n    - \"third_party_trackers\"\n- `numPages`\n  - default: 3\n  - crawl depth\n- `headless`\n  - Boolean flag, useful for debugging.\n- `emulateDevice`\n  - Puppeteer makes device emulation pretty easy. Choose from [this list](https://pptr.dev/#?product=Puppeteer&version=v5.2.1&show=api-puppeteerdevices)\n- `captureHar`\n  - default: true\n  - Boolean flag to save the HTTP requests to a file in the HAR(Http Archive Format).\n  - Note: You will need to provide a path to `outDir` if you want to see the captured file\n- `captureLinks`\n  - default: false\n  - Save first and third party links from the pages\n- `enableAdBl",
    "url": "https://github.com/the-markup/blacklight-collector",
    "last_updated": "2025-08-20T10:07:33+00:00"
  },
  {
    "full_name": "VowpalWabbit/vowpal_wabbit",
    "name": "vowpal_wabbit",
    "description": "Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning.  ",
    "language": "C++",
    "topics": [
      "c-plus-plus",
      "machine-learning",
      "online-learning",
      "contextual-bandits",
      "reinforcement-learning",
      "active-learning",
      "learning-to-search",
      "cpp"
    ],
    "readme": "<img src=\"/logo_assets/vowpal-wabbits-github-logo@3x.png\" height=\"auto\" width=\"100%\" alt=\"Vowpal Wabbit\">\n\n[![Linux build status](https://img.shields.io/azure-devops/build/vowpalwabbit/3934113c-9e2b-4dbc-8972-72ab9b9b4342/23?label=Linux%20build&logo=Azure%20Devops)](https://dev.azure.com/vowpalwabbit/Vowpal%20Wabbit/_build/latest?definitionId=23&branchName=master)\n[![Windows build status](https://img.shields.io/azure-devops/build/vowpalwabbit/3934113c-9e2b-4dbc-8972-72ab9b9b4342/14?label=Windows%20build&logo=Azure%20Devops)](https://dev.azure.com/vowpalwabbit/Vowpal%20Wabbit/_build/latest?definitionId=14&branchName=master)\n\n[![codecov](https://codecov.io/gh/VowpalWabbit/vowpal_wabbit/branch/master/graph/badge.svg)](https://codecov.io/gh/VowpalWabbit/vowpal_wabbit)\n[![Total Alerts](https://img.shields.io/lgtm/alerts/g/JohnLangford/vowpal_wabbit.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/JohnLangford/vowpal_wabbit/alerts/)\n\nThis is the *Vowpal Wabbit* fast online learning code.\n\n## Why Vowpal Wabbit?\nVowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning. There is a specific focus on reinforcement learning with several contextual bandit algorithms implemented and the online nature lending to the problem well. Vowpal Wabbit is a destination for implementing and maturing state of the art algorithms with performance in mind.\n\n- **Input Format.** The input format for the learning algorithm is substantially more flexible than might be expected. Examples can have features consisting of free form text, which is interpreted in a bag-of-words way. There can even be multiple sets of free form text in different namespaces.\n- **Speed.** The learning algorithm is fast -- similar to the few other online algorithm implementations out there. There are several optimization algorithms available with the baseline being sparse g",
    "url": "https://github.com/VowpalWabbit/vowpal_wabbit",
    "last_updated": "2025-08-26T03:29:36+00:00"
  },
  {
    "full_name": "njtierney/naniar",
    "name": "naniar",
    "description": "Tidy data structures, summaries, and visualisations for missing data",
    "language": "R",
    "topics": [
      "missing-data",
      "data-visualisation",
      "ggplot2",
      "missingness",
      "tidy-data",
      "r-package"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# naniar <a href=\"https://naniar.njtierney.com/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/njtierney/naniar/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/njtierney/naniar/actions/workflows/R-CMD-check.yaml)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/njtierney/naniar/master.svg)](https://app.codecov.io/github/njtierney/naniar?branch=master)\n[![CRAN Status\nBadge](https://www.r-pkg.org/badges/version/naniar)](https://cran.r-project.org/package=naniar)\n[![CRAN Downloads Each\nMonth](https://cranlogs.r-pkg.org/badges/naniar)](https://CRAN.R-project.org/package=naniar)\n[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n<!-- badges: end -->\n\n`naniar` provides principled, tidy ways to summarise, visualise, and\nmanipulate missing data with minimal deviations from the workflows in\nggplot2 and tidy data. It does this by providing:\n\n- Shadow matrices, a tidy data structure for missing data:\n  - `bind_shadow()` and `nabular()`\n- Shorthand summaries for missing data:\n  - `n_miss()` and `n_complete()`\n  - `pct_miss()`and `pct_complete()`\n- Numerical summaries of missing data in variables and cases:\n  - `miss_var_summary()` and `miss_var_table()`\n  - `miss_case_summary()`, `miss_case_table()`\n- Statistical tests of missingness:\n  - `mcar_test()` for [Little’s\n    (1988)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478722)\n    missing completely at random (MCAR) test\n- Visualisation for missing data:\n  - `geom_miss_point()`\n  - `gg_miss_var()`\n  - `gg_miss_case()`\n  - `gg_miss_fct()`\n\nFor more details on the workflow and theory underpinning naniar, read\nthe vignette [Getting started with\nnaniar](https://naniar.njtierney.com/articles/naniar.html).\n\nFor a short primer on the data visualisation available in na",
    "url": "https://github.com/njtierney/naniar",
    "last_updated": "2025-06-21T14:51:28+00:00"
  },
  {
    "full_name": "kartikgupta-at-anu/spline-calibration",
    "name": "spline-calibration",
    "description": "Code implementation of our ICLR'21 paper \"Calibration of Neural Networks using Splines\"",
    "language": "Python",
    "topics": [],
    "readme": "# Calibration of Neural Networks using Splines\n\nThis repository is the official implementation of ICLR 2021 paper: [Calibration of Neural Networks using Splines](https://openreview.net/forum?id=eQe8DEWNN2W).\n\nThis code is for research purposes only.\n\nAny questions or discussions are welcomed!\n\n\n## Installation\n\nSetup python virtual environment.\n\n```\nvirtualenv -p python3 venv\nsource venv/bin/activate                                 \npip3 install -r requirements.txt\nmkdir saved_logits\n```\n\n\n## Setup\n\nDownload the logits for different data and network combinations from [here](https://drive.google.com/drive/folders/1e1ai-bKb7LukKShqpn3S_gYXJGzhUYgm) and put them under `saved_logits` folder. \n\n\n## Recalibration\n\nTo find a recalibration function and evaluate the calibration:\n\n```bash\npython recalibrate.py\n```\n\nThe results for pre-calibration and post-calibration with various metrics will be saved in csv format under `out/{dataset}/{network}/beforeCALIB_results.csv` and `out/{dataset}/{network}/afterCALIBsplinenatual6_results.csv`. Calibration graphs such as Figure 1 in the main paper will be generated under `out/{dataset}/{network}` folder. \n\n## Cite\n\nIf you make use of this code in your own work, please cite our paper:\n\n```\n@inproceedings{\ngupta2021calibration,\ntitle={Calibration of Neural Networks using Splines},\nauthor={Kartik Gupta and Amir Rahimi and Thalaiyasingam Ajanthan and Thomas Mensink and Cristian Sminchisescu and Richard Hartley},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=eQe8DEWNN2W}\n}\n```\n\n#### Contact\nKartik Gupta (kartik.gupta@anu.edu.au).\n",
    "url": "https://github.com/kartikgupta-at-anu/spline-calibration",
    "last_updated": "2025-04-14T23:16:30+00:00"
  },
  {
    "full_name": "robertzk/testthatsomemore",
    "name": "testthatsomemore",
    "description": "R package for extending testthat to more testing helpers",
    "language": "R",
    "topics": [],
    "readme": "Extended Testing Framework for R [![Build Status](https://travis-ci.org/robertzk/testthatsomemore.svg?branch=master)](https://travis-ci.org/robertzk/testthatsomemore) [![Coverage Status](https://coveralls.io/repos/robertzk/testthatsomemore/badge.png?branch=master)](https://coveralls.io/r/robertzk/testthatsomemore?branch=master)\n=========\n\nHadley's [testthat](https://github.com/hadley/testthat) revolutionized package development\nin the R-sphere, but there are still some features lacking. `testthatsomemore` provides:\n\n  * The ability to mock and stub functions and closures, including those in packages.\n  * Creation of hierarchical file structures for testing of IO-related functions.\n  * Pretending now is some other time, like Ruby's [Timecop gem](https://github.com/travisjeffery/timecop).\n  * Indicating that some tests are pending.\n\nTo use, simply run `install.packages(\"testthatsomemore\")` (for the latest\nstable version 0.2.4, uploaded to CRAN on October 10, 2015), or the below\ncode to install the latest development version:\n\n```r\nif (!require(testthatsomemore)) {\n  if (!require(devtools)) install.packages('devtools'); require(devtools)\n  install_github('robertzk/testthatsomemore') \n}\n```\n\nExplicitly load `library(testthatsomemore)` before running unit tests with\n`devtools::test`.\n\nExamples\n========\n\nStubbing\n-------\n\nTo test certain functions in your package it may be required to \"stub\" away\nsome of the functionality to avoid unnecessary action. For example, when\ntesting a function that trains an algorithm over an extended period of time,\nit would be helpful to skip the computationally expensive step and ensure\nthe rest of the code is working as expected. We can use testthatsomemore\nin our tests as follows. Imagine we have a function as below.\n\n```R\ncomplicated_function <- function() {\n  input <- something_simple()\n  output <- something_long_and_complicated(input)\n  something_else_simple(output)\n}\n```\n\nWe can then \"stub\" away the long and complicated sub-function so ",
    "url": "https://github.com/robertzk/testthatsomemore",
    "last_updated": "2024-10-08T08:12:46+00:00"
  },
  {
    "full_name": "dssg/data-science-101",
    "name": "data-science-101",
    "description": "Methods, tools, tips, and tricks for anyone interested in getting started doing data science for the social good.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "Data Science (for Social Good) 101\n================\n\nResources and learning modules covering methods, tools, tips, and tricks for anyone interested in getting started doing data science for the social good.\n\n###What's Here\n\n- Introductions to key concepts (markdown files)\n- Resource lists (markdown files)\n- Learning Modules (folders)\n\n\n###Directories\n\n- Methods\n- Tools\n- Projects\n  \n###How to Contribute\n\n- If you're a DSSG member, commit away\n- If you would like to contribute materials, ask to be added to the organization or submit a pull request\n",
    "url": "https://github.com/dssg/data-science-101",
    "last_updated": "2024-11-23T17:40:57+00:00"
  },
  {
    "full_name": "r-lib/log4r",
    "name": "log4r",
    "description": "A fast & lightweight approach to logging in R based on the widely-emulated Apache Log4j project.",
    "language": "R",
    "topics": [
      "logging",
      "r"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file. -->\n\n# log4r\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/log4r)](https://cran.r-project.org/package=log4r)\n[![R-CMD-check](https://github.com/johnmyleswhite/log4r/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/johnmyleswhite/log4r/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n**log4r** is a fast, lightweight, object-oriented approach to logging in\nR based on the widely-emulated [Apache\nLog4j](https://logging.apache.org/log4j/) project.\n\n**log4r** differs from other R logging packages in its focus on\nperformance and simplicity. As such, it has fewer features – although it\nis still quite extensible, as seen below – but is much faster. See\n`vignette(\"performance\", package = \"log4r\")` for details.\n\nUnlike other R logging packages, **log4r** also has first-class support\nfor structured logging. See\n`vignette(\"structured-logging\", package = \"log4r\")` for details.\n\n## Installation\n\nThe package is available from CRAN:\n\n``` r\ninstall.packages(\"log4r\")\n```\n\nIf you want to use the development version, you can install the package\nfrom GitHub as follows:\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"johnmyleswhite/log4r\")\n```\n\n## Usage\n\nLogging is configured by passing around `logger` objects created by\n`logger()`. By default, this will log to the console and suppress\nmessages below the `\"INFO\"` level:\n\n``` r\nlogger <- logger()\n\nlog_info(logger, \"Located nearest gas station.\")\n#> INFO  [2019-09-04 16:31:04] Located nearest gas station.\nlog_warn(logger, \"Ez-Gas sensor network is not available.\")\n#> WARN  [2019-09-04 16:31:04] Ez-Gas sensor network is not available.\nlog_debug(logger, \"Debug messages are suppressed by default.\")\n```\n\nLogging destinations are controlled by **Appenders**, a few of which are\nprovided by the package. For instance, if we want to debug-level\nmessages to a file:\n\n``` r\nlog_file <- tempfile()\nlogger <- logger(",
    "url": "https://github.com/r-lib/log4r",
    "last_updated": "2025-08-11T03:59:46+00:00"
  },
  {
    "full_name": "snowplow/snowplow",
    "name": "snowplow",
    "description": "The leader in Customer Data Infrastructure",
    "language": "Scala",
    "topics": [
      "snowplow",
      "analytics",
      "data",
      "data-pipeline",
      "data-collection",
      "product-analytics",
      "marketing-analytics",
      "snowplow-pipeline",
      "snowplow-events"
    ],
    "readme": "[![Snowplow logo][logo-image]][website]\n\n---\n\nAs of January 8, 2024, Snowplow introduced the [Snowplow Limited Use License Agreement](https://docs.snowplow.io/limited-use-license-1.1/), and we are releasing new versions of our core pipeline technology under this license. There will be no security patches made to versions of our software that pre-date January 2024.\n\nIf you are currently using the pipeline in production, or in a way that is competitive with Snowplow, these changes affect you. If you wish to use the current version of Snowplow software, [please contact us](https://snowplow.io/snowplow-oss-license-change) to discuss a plan that works for you.\n\nWe value all of our users and remain dedicated to helping our community use Snowplow in the optimal capacity that fits their business goals and needs.\n\nRead more about this change [here](https://docs.snowplow.io/docs/resources/limited-use-license-faq/).\n\n---\n\n### Introduction\n\nWelcome to Snowplow, the leader in customer data infrastructure (CDI) for AI, enabling every organization to transform raw behavioral data into governed, high-fidelity fuel for AI-powered applications—including advanced analytics, real-time personalization engines, and AI agents.\n\nDigital-first companies like Strava, HelloFresh, Auto Trader, Burberry, and DPG Media rely on Snowplow to collect and process event-level data in real time—delivering it securely to their warehouse, lake, or stream—and to integrate deep customer context into their applications.\n\n---\n\n### Why Customer Data Infrastructure (CDI)?\n\nSnowplow lays the foundation for an organization’s advanced analytics, operational, and ML/AI use cases—including customer insights, predicting customer behaviors, hyper-personalizing customer experiences, and detecting fraud in real time.\n\n### Key benefits of Snowplow’s CDI:\n\n* Data depth and quality  \n* Centralized data governance  \n* Real-time operationalization  \n* Privacy and compliance  \n* AI- and BI-ready behavioral data\n\n---\n\n### Why",
    "url": "https://github.com/snowplow/snowplow",
    "last_updated": "2025-09-01T15:10:17+00:00"
  },
  {
    "full_name": "rweekly/rweekly.org",
    "name": "rweekly.org",
    "description": "R Weekly",
    "language": "Rez",
    "topics": [
      "weekly",
      "r",
      "blog",
      "community",
      "data-science",
      "statistics",
      "visualization",
      "data-visualization",
      "rweekly"
    ],
    "readme": "# R Weekly\n\n<img align=\"right\" src=\"https://rweekly.org/images/icons/icon-256x256.png\">\n\nR weekly provides weekly updates from the R community. You are welcome to contribute as long as you follow [our code of conduct](CODE_OF_CONDUCT.md) and [our contributing guide](CONTRIBUTING.md).\n\n## How to have (my) content shared by R Weekly?\n\nWe're all about spreading content about R, be it blog posts, tutorials, screencasts, slidedecks, etc.\nPlease help us!\n\n**In general for feeds and one time sharing we prefer https over http.**\n\n### Regular R posts: submit your RSS feed\n\nSubmit your blog RSS feed by submitting a PR adding a line to [rss_feeds.csv](https://github.com/rweekly/rweekly.org/blob/gh-pages/rss_feeds.csv)\nIf your blog doesn't have an RSS feed yet, look up for resources / help for doing it, it's worth it!\n\nWhat rules are there?\n\n* The RSS feed has to be mainly related to R, so if your content mixes R and baking posts please create a specific R feed. Now, if there's one off topic post once in a while, it's fine, we'll remove it from the release.\n\n*  Please💡 Use [W3C Feed Validation](https://validator.w3.org/feed/) Service to checks the syntax of Atom or RSS feeds.\n\n* We don't need a full content RSS feed since we'll only use links and titles.\n\n### One off sharing\n\n**What?**\n\nAn URL to a **free** resource about R, or using R.\nIf it's a book, we won't advertise it unless there's a free online version.\nIn posts/tutorials, the R code has to be visible or easy to find.\nWe check for duplicates over the last issues.\n\n**How?**\n\n* Via this very GitHub repo. Update the [draft](https://github.com/rweekly/rweekly.org/blob/gh-pages/draft.md) post, and create a pull request. Please respect the categories indicated in the [contributing guide](CONTRIBUTING.md) but don't worry, we can reshuffle content as needed. You can suggest an image in the comment of the PR but don't add images to your PR edits, the editor in charge of the release will pick images.\n\nWe prefer PRs because they'r",
    "url": "https://github.com/rweekly/rweekly.org",
    "last_updated": "2025-09-02T09:50:12+00:00"
  },
  {
    "full_name": "oxford-cs-deepnlp-2017/lectures",
    "name": "lectures",
    "description": " Oxford Deep NLP 2017 course",
    "language": "",
    "topics": [
      "deep-learning",
      "machine-learning",
      "natural-language-processing",
      "nlp",
      "oxford"
    ],
    "readme": "# Preamble\nThis repository contains the lecture slides and course description for the [Deep Natural Language Processing](http://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/) course offered in Hilary Term 2017 at the University of Oxford. \n\nThis is an advanced course on natural language processing. Automatically processing natural language inputs and producing language outputs is a key component of Artificial General Intelligence. The ambiguities and noise inherent in human communication render traditional symbolic AI techniques ineffective for representing and analysing language data. Recently statistical techniques based on neural networks have achieved a number of remarkable successes in natural language processing leading to a great deal of commercial and academic interest in the field\n\nThis is an applied course focussing on recent advances in analysing and generating speech and text using recurrent neural networks. We introduce the mathematical definitions of the relevant machine learning models and derive their associated optimisation algorithms. The course covers a range of applications of neural networks in NLP including analysing latent dimensions in text, transcribing speech to text, translating between languages, and answering questions. These topics are organised into three high level themes forming a progression from understanding the use of neural networks for sequential language modelling, to understanding their use as conditional language models for transduction tasks, and finally to approaches employing these techniques in combination with other mechanisms for advanced applications. Throughout the course the practical implementation of such models on CPU and GPU hardware is also discussed.\n\nThis course is organised by Phil Blunsom and delivered in partnership with the DeepMind Natural Language Research Group.\n\n# Lecturers\n* Phil Blunsom (Oxford University and DeepMind)\n* Chris Dyer (Carnegie Mellon University and DeepMind)\n* Edward Grefenstette (De",
    "url": "https://github.com/oxford-cs-deepnlp-2017/lectures",
    "last_updated": "2025-09-01T16:56:02+00:00"
  },
  {
    "full_name": "metacran/mason",
    "name": "mason",
    "description": "A friendly craftsman that builds you great R packages",
    "language": "R",
    "topics": [],
    "readme": "\n# mason\n\n> Friendly Craftsman Who Builds Slick R Packages\n\n[![Linux Build Status](https://travis-ci.org/metacran/mason.svg?branch=master)](https://travis-ci.org/gaborcsardi/mason)\n[![Windows Build status](https://ci.appveyor.com/api/projects/status/github/gaborcsardi/mason?svg=true)](https://ci.appveyor.com/project/gaborcsardi/mason)\n[![](http://www.r-pkg.org/badges/version/mason)](http://www.r-pkg.org/pkg/mason)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/mason)](http://www.r-pkg.org/pkg/mason)\n\n![](/inst/mason.png)\n\nMason builds R packages. It asks you some simple questions, fills in a\ntemplate based on your answers, and creates proper metadata files, READMEs\nwith badges, git repositories, everything you need to just start writing\nand committing your code.\n\nMason is extensible: each template is a separate R package named\n`mason.<template>`, where `<template>` is the name of the template. To use\na new template, you need to install it first. Mason will be then able to\nuse it immediately, without any configuration.  See\ne.g. [mason.rpkg](https://github.com/gaborcsardi/mason.rpkg) for a generic\nR package template. \n\nMason makes sure that you get your package as quick as possible, with the\nleast number of keystrokes: it tries to find out your username, name email\naddress, GitHub login name, etc. from your settings, to give you reasonable\ndefaults, so that all you need to do is press `ENTER` a couple of times.\n\nMason's role model is [Yeoman](http://yeoman.io).\n\n## Installation\n\nYou can install Mason and its dependencies from GitHub, using the\n`devtools` package:\n\n```r\ndevtools::install_github(\"metacran/mason\")\n```\n\nYou need to install at least one template as well:\n\n```r\ndevtools::install_github(\"metacran/mason.rpkg\")\ndevtools::install_github(\"metacran/mason.github\")\n```\n\n## Usage\n\nCreate an empty directory and make that your current directory.\nThe new package will be created within that:\n\n```r\ndir.create(\"mypackage\")\nsetwd(\"mypackage\")\n```\n\nThen",
    "url": "https://github.com/metacran/mason",
    "last_updated": "2024-11-04T13:37:14+00:00"
  },
  {
    "full_name": "jayphelps/git-blame-someone-else",
    "name": "git-blame-someone-else",
    "description": "Blame someone else for your bad code.",
    "language": "Shell",
    "topics": [],
    "readme": "# git-blame-someone-else\n\n> \"I love git-blame-someone-else!!\" -[Linus Torvalds says](https://github.com/jayphelps/git-blame-someone-else/commit/e5cfe4bb2190a2ae406d5f0b8f49c32ac0f01cd7)*\n\n## Install\n\n```bash\n$ git clone https://github.com/jayphelps/git-blame-someone-else.git\n$ cd git-blame-someone-else\n$ sudo make install\n```\n\n## Usage\n\n```bash\n$ git blame-someone-else <author> <commit>\n```\n\n![ezgif-1396449034](https://cloud.githubusercontent.com/assets/762949/12863650/068e2820-cc2e-11e5-80c5-6ebdb71f51ea.gif)\n\n## Disclaimer:\n\nThis changes not only who authored the commit but the listed commiter as well. It also is something I wrote as a joke, so please don't run this against your production repo and complain if this script deletes everything.\n\n*Linus Torvalds didn't really approve of this. It's a joke to prove it works. [See his fake commit here](https://github.com/jayphelps/git-blame-someone-else/commit/e5cfe4bb2190a2ae406d5f0b8f49c32ac0f01cd7)\n\n:shipit:\n",
    "url": "https://github.com/jayphelps/git-blame-someone-else",
    "last_updated": "2025-09-02T08:45:53+00:00"
  },
  {
    "full_name": "RooCodeInc/Roo-Code",
    "name": "Roo-Code",
    "description": "Roo Code gives you a whole dev team of AI agents in your code editor.",
    "language": "TypeScript",
    "topics": [],
    "readme": "<div align=\"center\">\n<sub>\n\n<b>English</b> • [Català](locales/ca/README.md) • [Deutsch](locales/de/README.md) • [Español](locales/es/README.md) • [Français](locales/fr/README.md) • [हिंदी](locales/hi/README.md) • [Bahasa Indonesia](locales/id/README.md) • [Italiano](locales/it/README.md) • [日本語](locales/ja/README.md)\n\n</sub>\n<sub>\n\n[한국어](locales/ko/README.md) • [Nederlands](locales/nl/README.md) • [Polski](locales/pl/README.md) • [Português (BR)](locales/pt-BR/README.md) • [Русский](locales/ru/README.md) • [Türkçe](locales/tr/README.md) • [Tiếng Việt](locales/vi/README.md) • [简体中文](locales/zh-CN/README.md) • [繁體中文](locales/zh-TW/README.md)\n\n</sub>\n</div>\n<br>\n<div align=\"center\">\n  <h1>Roo Code</h1>\n  <p align=\"center\">\n  <img src=\"https://media.githubusercontent.com/media/RooCodeInc/Roo-Code/main/src/assets/docs/demo.gif\" width=\"100%\" />\n  </p>\n  <p>Connect with developers, contribute ideas, and stay ahead with the latest AI-powered coding tools.</p>\n  \n  <a href=\"https://discord.gg/roocode\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Join%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Join Discord\"></a>\n  <a href=\"https://www.reddit.com/r/RooCode/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Join%20Reddit-FF4500?style=for-the-badge&logo=reddit&logoColor=white\" alt=\"Join Reddit\"></a>\n  \n</div>\n<br>\n<br>\n\n<div align=\"center\">\n\n<a href=\"https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Download%20on%20VS%20Marketplace-blue?style=for-the-badge&logo=visualstudiocode&logoColor=white\" alt=\"Download on VS Marketplace\"></a>\n<a href=\"https://github.com/RooCodeInc/Roo-Code/discussions/categories/feature-requests?discussions_q=is%3Aopen+category%3A%22Feature+Requests%22+sort%3Atop\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Feature%20Requests-yellow?style=for-the-badge\" alt=\"Feature Requests\"></a>\n<a href=\"https://marketplace.visualst",
    "url": "https://github.com/RooCodeInc/Roo-Code",
    "last_updated": "2025-09-02T09:55:31+00:00"
  },
  {
    "full_name": "github-education-resources/classroom",
    "name": "classroom",
    "description": "GitHub Classroom automates repository creation and access control, making it easy for teachers to distribute starter code and collect assignments on GitHub.",
    "language": "Ruby",
    "topics": [
      "ruby",
      "github",
      "education",
      "classroom",
      "rails",
      "oauth"
    ],
    "readme": "# GitHub Classroom\n[![Build Status](https://travis-ci.org/education/classroom.svg?branch=master)](https://travis-ci.org/education/classroom) [![Code Climate](https://codeclimate.com/github/education/classroom/badges/gpa.svg)](https://codeclimate.com/github/education/classroom)\n\n---\n\nMoving forwards, GitHub Classroom will be developed internally alongside GitHub.com. As a result, we’re archiving the open source repository. This represents a significant “growing up” moment for GitHub Classroom as we strive to offer you the best education tools possible.\n\nWe’re making this decision for a few important reasons:\n\n1. Quality. To provide the best services to our varied and global community, we need GitHub Classroom to meet the operational excellence of GitHub.com. Continuing development internally allows us to make better use of GitHub’s internal resources, tooling, and infrastructure. \n\n2. Better support. Using GitHub Support as our primary feedback mechanism, instead of issues, allows us to assist our users in a more productive way. We can leverage the support infrastructure already in place for GitHub.com, and all user feedback can be kept in a single place. The Education team will continue to respond to all GitHub Classroom requests.\n\nGitHub Classroom will continue to evolve and improve based on your feedback. We’re growing faster than ever, and can’t wait to launch some very exciting new features in the coming months.\n\nIf you have questions, concerns, or feedback, don’t hesitate to contact us at https://support.github.com/contact.\n\nThank you!\n\nThe GitHub Classroom Team\n\n---\n\n## Table of Contents\n\n- [The workflow you use as a developer, scaled for the needs of students.](#the-workflow-you-use-as-a-developer-scaled-for-the-needs-of-students)\n- [Why try GitHub Classroom?](#why-try-classroom)\n- [Design principles](#design-principles)\n- [GitHub Classroom and the edtech ecosystem](#github-classroom-and-the-edtech-ecosystem)\n- [The technical details](#the-technical-details)\n",
    "url": "https://github.com/github-education-resources/classroom",
    "last_updated": "2025-08-26T15:22:42+00:00"
  },
  {
    "full_name": "salimk/Rcrawler",
    "name": "Rcrawler",
    "description": "An R web crawler and scraper",
    "language": "R",
    "topics": [
      "r",
      "rpackage",
      "crawler",
      "scraper",
      "webcrawler",
      "webscraping",
      "webscraper",
      "webscrapping",
      "crawlers"
    ],
    "readme": "# An R web crawler and scraper \n[![CRAN version](http://www.r-pkg.org/badges/version/Rcrawler)](https://CRAN.R-project.org/package=Rcrawler)\n[![Build Test](https://api.travis-ci.org/salimk/Rcrawler.svg?branch=master)](https://travis-ci.org/salimk/Rcrawler)\n![Downloads Stats](http://cranlogs.r-pkg.org/badges/Rcrawler)\n[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)\n[![DOI Paper](https://user-images.githubusercontent.com/17308124/31905494-44039a56-b826-11e7-9ace-01db4904176d.png)](https://doi.org/10.1016/j.softx.2017.04.004)\n\nRcrawler is an R package for web crawling websites and extracting structured data which can be used for a wide range of useful applications, like web mining, text mining, web content mining, and web structure mining. So what is the difference between Rcrawler and rvest : rvest extracts data from one specific page by navigating through selectors. However, Rcrawler automatically traverses and parse all web pages of a website, and extract all data you need from them at once with a single command. \n\nFor example collect all published posts on a blog, or extract all products data on a shopping website, or gathering all websites comments or reviews for your opinion mining studies.\n\nMore than that,  Rcrawler can help you studies web site structure by building a network representation of a website internal and external hyperlinks (nodes & edges). \n\n>  **Announcements** : Actually I'm looking for a one or two more people to maintain the package: implementing new features, writing documentations. Especially those interested in publishing scientific papers. Skills needed : R programming, c++ and good writing. \n\n> Help us improve Rcrawler by **asking questions**, **revealing issues**, **suggesting new features**.\n> If you have a blog **write** about it, or just **share** it with your colleg,ues. \n\n> :thumbsup: Thank you  :relaxed:\n\n#### Subscribe to Rcrawler Mailing List \n",
    "url": "https://github.com/salimk/Rcrawler",
    "last_updated": "2025-07-30T18:52:38+00:00"
  },
  {
    "full_name": "danielfrg/word2vec",
    "name": "word2vec",
    "description": "Python interface to Google word2vec",
    "language": "C",
    "topics": [
      "python",
      "word2vec",
      "doc2vec"
    ],
    "readme": "# NOT MAINTAINED\n\n- I have not used this code in a long time\n- No issues or PRs can be created\n- Latest release doesn't work with newer versions of numpy\n- I recommened moving to a native alternative in Tensorflow or PyTorch\n\n# word2vec\n\n[![pypi](https://badge.fury.io/py/word2vec.svg)](https://pypi.org/project/word2vec/)\n[![build](https://github.com/danielfrg/word2vec/workflows/test/badge.svg)](http://github.com/danielfrg/word2vec/actions/workflows/test.yml)\n[![coverage](https://codecov.io/gh/danielfrg/word2vec/branch/master/graph/badge.svg)](https://codecov.io/gh/danielfrg/word2vec?branch=master)\n[![license](https://img.shields.io/:license-Apache%202-blue.svg)](http://github.com/danielfrg/word2vec/blob/master/LICENSE.txt)\n\nPython interface to Google word2vec.\n\nTraining is done using the original C code, other functionality is pure Python with numpy.\n\n## Installation\n\n```\npip install word2vec\n```\n\n### Compilation\n\nThe installation requires to compile the original C code using `gcc`.\n\nYou can override the compilation flags if needed:\n\n```\nWORD2VEC_CFLAGS='-march=corei7' pip install word2vec\n```\n\n**Windows:** There is basic some support for this support based on this [win32 port](https://github.com/zhangyafeikimi/word2vec-win32).\n\n## Usage\n\nExample notebook: [word2vec](https://nbviewer.org/github/danielfrg/word2vec/blob/main/examples/word2vec.ipynb)\n\nThe default functionality from word2vec is available with the following commands:\n- `word2vec`\n- `word2phrase`\n- `word2vec-distance`\n- `word2vec-word-analogy`\n- `word2vec-compute-accuracy`\n\nExperimental functionality on doc2vec can be found in this example:\n[doc2vec](https://nbviewer.org/github/danielfrg/word2vec/blob/main/examples/doc2vec.ipynb)\n",
    "url": "https://github.com/danielfrg/word2vec",
    "last_updated": "2025-08-07T12:20:35+00:00"
  },
  {
    "full_name": "cdeterman/RViennaCL",
    "name": "RViennaCL",
    "description": "R package providing ViennaCL header files",
    "language": "C++",
    "topics": [],
    "readme": "## RViennaCL: ViennaCL Headers for R  \n[![Build Status](https://travis-ci.org/cdeterman/RViennaCL.svg)](https://travis-ci.org/cdeterman/RViennaCL) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/cdeterman/RViennaCL?branch=master)](https://ci.appveyor.com/project/cdeterman/RViennaCL) [![Downloads](http://cranlogs.r-pkg.org/badges/RViennaCL?color=brightgreen)](http://www.r-pkg.org/pkg/RViennaCL)\n\nThis package provides [R](CRAN.R-project.org) with access to\n[ViennaCL](http://viennacl.sourceforge.net/) header files.  \n[ViennaCL](http://viennacl.sourceforge.net/) provides a free C++ source \nlibrary for simple computing on multi-core architectures (GPU, MIC) and\nmulti-core CPUs.  The library supports CUDA, OpenCL, and OpenMP.  It also\nincludes BLAS level 1-3 support and linear algebra solvers.\n\nThis package aims to provide a simple means of linking to the \n[ViennaCL](http://viennacl.sourceforge.net/) header files for use among CRAN\npackages. Similar to the popular R package [BH](https://CRAN.R-project.org/package=BH), \nby placing these libraries in this package, we offer a more\nefficient distribution system for CRAN as replication of this code in the\nsources of other packages is avoided.\n\nIt can be used via the `LinkingTo:` field in the `DESCRIPTION` field of an R\npackage --- and the R package infrastructure tools will then know how to set\ninclude flags correctly on all architectures supported by R.\n\nNote that this is only for the header only aspects of this package. If you wish\nto link against the libviennacl shared object, you must create this file\nseparately and link against it. \n\n### Authors \n\nCharles Determan Jr.\n\n",
    "url": "https://github.com/cdeterman/RViennaCL",
    "last_updated": "2024-10-24T11:49:55+00:00"
  },
  {
    "full_name": "shosaco/vistime",
    "name": "vistime",
    "description": "Pretty timelines in R.",
    "language": "R",
    "topics": [
      "timelines",
      "charts"
    ],
    "readme": "# vistime - Pretty Timelines in R <img src=\"man/figures/logo.png\" align=\"right\" width=\"120\" />\n\n[![Buy Me A Coffee](https://i.imgur.com/xI5UtRm.png)](https://www.buymeacoffee.com/shosaco)\n[![CRAN](https://www.r-pkg.org/badges/version/vistime)](https://cran.r-project.org/package=vistime)\n[![R build status](https://github.com/shosaco/vistime/workflows/R-CMD-check/badge.svg)](https://github.com/shosaco/vistime/actions)\n[![Downloads](https://cranlogs.r-pkg.org/badges/grand-total/vistime)](https://www.r-pkg.org/pkg/vistime)\n[![codecov](https://codecov.io/github/shosaco/vistime/branch/master/graphs/badge.svg)](https://app.codecov.io/github/shosaco/vistime)\n[![Github Stars](https://img.shields.io/github/stars/shosaco/vistime.svg)](https://github.com/shosaco/vistime)\n\nA library for creating time-based charts, like Gantt or timelines. Possible outputs include `ggplot`s, `plotly` graphs, `Highcharts` or data.frames. Results can be used in the RStudio viewer pane, in R Markdown documents or in Shiny apps. In the interactive outputs created by `vistime()` and `hc_vistime()` you can interact with the plot using mouse hover or zoom. Timelines and their components can afterwards be manipulated using `ggplot::theme()`, `plotly_build()` or `hc_*`functions (for `gg_vistime()`, `vistime()` or `hc_vistime()`, respectively). When choosing the `data.frame` output, you can use your own plotting package for visualizing the graph.\n\nI'm glad if this can help people save time and effort, like this feedback suggests:\n\n![grafik](https://github.com/shosaco/vistime/assets/20717764/2af48992-6285-493a-8f4d-d5bb6e991b67)\n\nIf you find vistime useful, please consider supporting its development: <a href=\"https://www.buymeacoffee.com/shosaco\" target=\"_blank\"><img src=\"https://i.imgur.com/kN1GxnC.png\" alt=\"Buy Me A Coffee\"></a>\n\n\n**Feedback welcome:** [sa.ra.online@posteo.de](mailto:sa.ra.online@posteo.de)\n\n## Table of Contents\n\n1. [Installation](#1-installation)\n2. [Main functionality](#2-main-functiona",
    "url": "https://github.com/shosaco/vistime",
    "last_updated": "2025-03-22T11:09:33+00:00"
  },
  {
    "full_name": "khakieconomics/state_space_polls",
    "name": "state_space_polls",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "## Important\n\nTo compile this simple rmd model, you will need to have the dev version \nof ggplot2 installed. If you don't have it installed, the rmd file \nwon't knit. To install: \n\n```\nlibrary(devtools)\ninstall_github(\"hadley/ggplot2\")\n```\n\nThis file should be self-contained. To download the latest polls from \nReal Clear Politics, run the model and generate the plot, simply\nopen the .Rmd file in RStudio, make sure you have all the libraries at \nthe top of the code chunk installed, and click \"Knit HTML\". \n\nIf you want to just produce the chart, you can copy out the code chunk\ninto an R script and run it interactively. \n\nEnjoy!\n",
    "url": "https://github.com/khakieconomics/state_space_polls",
    "last_updated": "2025-01-13T18:01:52+00:00"
  },
  {
    "full_name": "jpiironen/dimreduce",
    "name": "dimreduce",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "# dimreduce\n\n\nAn R package to perform (supervised) dimensionality reduction. The package contains essentially the techniques from the paper of Piironen and Vehtari (2018). The functions are very easy to use and require minimal input from the user.\n\nBelow are some simple examples about how to use the package. You can get more help by installing the package and then typing `?dimreduce` in R, and then exploring the function documentations.\n\n\nInstallation\n------------\n\n```R  \nif (!require(devtools)) {\n  install.packages(\"devtools\")\n  library(devtools)\n}\ndevtools::install_github('jpiironen/dimreduce')\n```\n\nExample\n-------\n\n```R\nlibrary(dimreduce)\nlibrary(ggplot2)\n\n# load the features x and target values y for the prostate cancer data\ndata('prostate', package = 'dimreduce')\nx <- prostate$x\ny <- prostate$y\n\n# try the difference dimension reductions \ndr0 <- spca(x) # pca\ndr1 <- spca(x,y, nctot=2) # spca\ndr2 <- ispca(x,y, nctot=2) # ispca\n\n# choose one of the methods and visualize the data using the first two\n# latent features\nz <- predict(dr2,x)\nggplot() + geom_point(aes(x=z[,1], y=z[,2]), color=y+2)\n\n# compute the p-values for the univariate relevances of each original feature\npval <- featscore.test(x,y)\nqplot(pval) # should have uniform distribution if no relevant variables\nsum(pval < 0.001) # number of variables with p-value below some threshold\n```\n\n\nReferences\n------------\n\nBair, E., Hastie, T., Paul, D., and Tibshirani, R. (2006). Prediction by supervised principal components. *Journal of the American Statistical Association*, 101(473):119-137.\n\nNeal, R. and Zhang, J. (2006). High dimensional classification with Bayesian neural networks and Dirichlet diffusion trees. In Guyon, I., Gunn, S., Nikravesh, M., and Zadeh, L. A., editors, *Feature Extraction, Foundations and Applications*, pages 265-296. Springer.\n\nPiironen, J. and Vehtari, A. (2018). Iterative supervised principal components. In *Proceedings of the 21st International Conference on Artificial Intelligence an",
    "url": "https://github.com/jpiironen/dimreduce",
    "last_updated": "2024-09-07T18:56:27+00:00"
  },
  {
    "full_name": "r-lib/rlang",
    "name": "rlang",
    "description": "Low-level API for programming with R",
    "language": "R",
    "topics": [
      "r"
    ],
    "readme": "rlang <img src=\"man/figures/logo.png\" align=\"right\" />\n=======================================================\n\n<!-- badges: start -->\n[![Codecov test coverage](https://codecov.io/gh/r-lib/rlang/branch/main/graph/badge.svg)](https://app.codecov.io/gh/r-lib/rlang?branch=main)\n[![Lifecycle Status](https://img.shields.io/badge/lifecycle-stable-green.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![R-CMD-check](https://github.com/r-lib/rlang/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/rlang/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n\nrlang is a collection of frameworks and APIs for programming with R.\n\n\n## Frameworks\n\nTwo comprehensive frameworks are implemented in rlang.\n\n*   __tidy eval__, a programmable [data-masking](https://rlang.r-lib.org/reference/topic-data-mask.html) framework used in tidyverse packages like dplyr and ggplot2. As a user, you will encounter the embracing operator [`{{`](https://rlang.r-lib.org/reference/embrace-operator.html) and name injection with the [glue](https://glue.tidyverse.org/) operators [`\"{\"`](https://rlang.r-lib.org/reference/glue-operators.html) and [`\"{{\"`](https://rlang.r-lib.org/reference/glue-operators.html).\n\n*   __rlang errors__, a set of tools to signal and display errors. This includes backtrace capture with `global_entrace()` and backtrace display with `last_error()` and `last_warnings()`. Use `abort()` to create errors with bullet lists, structured metadata, and error chaining support.\n\n    The display of error messages is optimised for bullet lists and chained errors and optionally integrates with the cli package (see `local_use_cli()`).\n\n\n## Argument intake\n\nA set of tools help you check, validate, and preprocess arguments.\n\n*   Checking function arguments, e.g. `arg_match()`, `check_required()`, and `check_exclusive()`.\n\n*   Checking dots, e.g. `check_dots_used()` and `check_dots_empty()`.\n\n*   Collecting [dynamic dots](https://rlang.r-lib.org/reference/dyn-dots.htm",
    "url": "https://github.com/r-lib/rlang",
    "last_updated": "2025-08-21T23:09:40+00:00"
  },
  {
    "full_name": "ibis-project/ibis",
    "name": "ibis",
    "description": "the portable Python dataframe library",
    "language": "Python",
    "topics": [
      "python",
      "impala",
      "pandas",
      "database",
      "clickhouse",
      "postgresql",
      "sqlite",
      "mysql",
      "datafusion",
      "sql",
      "pyspark",
      "duckdb",
      "bigquery",
      "pyarrow",
      "mssql",
      "polars",
      "snowflake",
      "trino"
    ],
    "readme": "# Ibis\n\n[![Documentation status](https://img.shields.io/badge/docs-docs.ibis--project.org-blue.svg)](http://ibis-project.org)\n[![Project chat](https://img.shields.io/badge/zulip-join_chat-purple.svg?logo=zulip)](https://ibis-project.zulipchat.com)\n[![Anaconda badge](https://anaconda.org/conda-forge/ibis-framework/badges/version.svg)](https://anaconda.org/conda-forge/ibis-framework)\n[![PyPI](https://img.shields.io/pypi/v/ibis-framework.svg)](https://pypi.org/project/ibis-framework)\n[![Build status](https://github.com/ibis-project/ibis/actions/workflows/ibis-main.yml/badge.svg)](https://github.com/ibis-project/ibis/actions/workflows/ibis-main.yml?query=branch%3Amain)\n[![Build status](https://github.com/ibis-project/ibis/actions/workflows/ibis-backends.yml/badge.svg)](https://github.com/ibis-project/ibis/actions/workflows/ibis-backends.yml?query=branch%3Amain)\n[![Codecov branch](https://img.shields.io/codecov/c/github/ibis-project/ibis/main.svg)](https://codecov.io/gh/ibis-project/ibis)\n\n## What is Ibis?\n\nIbis is the portable Python dataframe library:\n\n- Fast local dataframes (via DuckDB by default)\n- Lazy dataframe expressions\n- Interactive mode for iterative data exploration\n- [Compose Python dataframe and SQL code](#python--sql-better-together)\n- Use the same dataframe API for [nearly 20 backends](#backends)\n- Iterate locally and deploy remotely by [changing a single line of code](#portability)\n\nSee the documentation on [\"Why Ibis?\"](https://ibis-project.org/why) to learn more.\n\n## Getting started\n\nYou can `pip install` Ibis with a backend and example data:\n\n```bash\npip install 'ibis-framework[duckdb,examples]'\n```\n\n> 💡 **Tip**\n>\n> See the [installation guide](https://ibis-project.org/install) for more installation options.\n\nThen use Ibis:\n\n```python\n>>> import ibis\n>>> ibis.options.interactive = True\n>>> t = ibis.examples.penguins.fetch()\n>>> t\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ is",
    "url": "https://github.com/ibis-project/ibis",
    "last_updated": "2025-09-02T02:38:19+00:00"
  },
  {
    "full_name": "BVLC/caffe",
    "name": "caffe",
    "description": "Caffe: a fast open framework for deep learning.",
    "language": "C++",
    "topics": [
      "deep-learning",
      "machine-learning",
      "vision"
    ],
    "readme": "# Caffe\n\n[![Build Status](https://travis-ci.org/BVLC/caffe.svg?branch=master)](https://travis-ci.org/BVLC/caffe)\n[![License](https://img.shields.io/badge/license-BSD-blue.svg)](LICENSE)\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by Berkeley AI Research ([BAIR](http://bair.berkeley.edu))/The Berkeley Vision and Learning Center (BVLC) and community contributors.\n\nCheck out the [project site](http://caffe.berkeleyvision.org) for all the details like\n\n- [DIY Deep Learning for Vision with Caffe](https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p)\n- [Tutorial Documentation](http://caffe.berkeleyvision.org/tutorial/)\n- [BAIR reference models](http://caffe.berkeleyvision.org/model_zoo.html) and the [community model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)\n- [Installation instructions](http://caffe.berkeleyvision.org/installation.html)\n\nand step-by-step examples.\n\n## Custom distributions\n\n - [Intel Caffe](https://github.com/BVLC/caffe/tree/intel) (Optimized for CPU and support for multi-node), in particular Intel® Xeon processors.\n- [OpenCL Caffe](https://github.com/BVLC/caffe/tree/opencl) e.g. for AMD or Intel devices.\n- [Windows Caffe](https://github.com/BVLC/caffe/tree/windows)\n\n## Community\n\n[![Join the chat at https://gitter.im/BVLC/caffe](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/BVLC/caffe?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nPlease join the [caffe-users group](https://groups.google.com/forum/#!forum/caffe-users) or [gitter chat](https://gitter.im/BVLC/caffe) to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on [Issues](https://github.com/BVLC/caffe/issues).\n\nHappy brewing!\n\n## License and Citation\n\nCaffe is released under the [BSD 2-Clause license](https://github.com/BVLC/caffe/blob/master/LICENSE).\nThe BAIR/BVLC refe",
    "url": "https://github.com/BVLC/caffe",
    "last_updated": "2025-09-02T06:35:50+00:00"
  },
  {
    "full_name": "facebook/zstd",
    "name": "zstd",
    "description": "Zstandard - Fast real-time compression algorithm",
    "language": "C",
    "topics": [],
    "readme": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/facebook/zstd/dev/doc/images/zstd_logo86.png\" alt=\"Zstandard\"></p>\n\n__Zstandard__, or `zstd` as short version, is a fast lossless compression algorithm,\ntargeting real-time compression scenarios at zlib-level and better compression ratios.\nIt's backed by a very fast entropy stage, provided by [Huff0 and FSE library](https://github.com/Cyan4973/FiniteStateEntropy).\n\nZstandard's format is stable and documented in [RFC8878](https://datatracker.ietf.org/doc/html/rfc8878). Multiple independent implementations are already available.\nThis repository represents the reference implementation, provided as an open-source dual [BSD](LICENSE) OR [GPLv2](COPYING) licensed **C** library,\nand a command line utility producing and decoding `.zst`, `.gz`, `.xz` and `.lz4` files.\nShould your project require another programming language,\na list of known ports and bindings is provided on [Zstandard homepage](https://facebook.github.io/zstd/#other-languages).\n\n**Development branch status:**\n\n[![Build Status][travisDevBadge]][travisLink]\n[![Build status][CircleDevBadge]][CircleLink]\n[![Build status][CirrusDevBadge]][CirrusLink]\n[![Fuzzing Status][OSSFuzzBadge]][OSSFuzzLink]\n\n[travisDevBadge]: https://api.travis-ci.com/facebook/zstd.svg?branch=dev \"Continuous Integration test suite\"\n[travisLink]: https://travis-ci.com/facebook/zstd\n[CircleDevBadge]: https://circleci.com/gh/facebook/zstd/tree/dev.svg?style=shield \"Short test suite\"\n[CircleLink]: https://circleci.com/gh/facebook/zstd\n[CirrusDevBadge]: https://api.cirrus-ci.com/github/facebook/zstd.svg?branch=dev\n[CirrusLink]: https://cirrus-ci.com/github/facebook/zstd\n[OSSFuzzBadge]: https://oss-fuzz-build-logs.storage.googleapis.com/badges/zstd.svg\n[OSSFuzzLink]: https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:zstd\n\n## Benchmarks\n\nFor reference, several fast compression algorithms were tested and compared\non a desktop featuring a Core i7-9700K CPU @ 4.9GH",
    "url": "https://github.com/facebook/zstd",
    "last_updated": "2025-09-02T10:17:34+00:00"
  },
  {
    "full_name": "marcotcr/checklist",
    "name": "checklist",
    "description": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# CheckList\r\nThis repository contains code for testing NLP Models as described in the following paper:  \r\n>[Beyond Accuracy: Behavioral Testing of NLP models with CheckList](http://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf)  \r\n> Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh\r\n> Association for Computational Linguistics (ACL), 2020\r\n\r\nBibtex for citations:\r\n```bibtex\r\n @inproceedings{checklist:acl20,  \r\n author = {Marco Tulio Ribeiro and Tongshuang Wu and Carlos Guestrin and Sameer Singh},  \r\n title = {Beyond Accuracy: Behavioral Testing of NLP models with CheckList},  \r\n booktitle = {Association for Computational Linguistics (ACL)},  \r\n year = {2020}\r\n }\r\n```\r\n\r\n\r\nTable of Contents\r\n=================\r\n\r\n   * [CheckList](#checklist)\r\n      * [Table of Contents](#table-of-contents)\r\n      * [Installation](#installation)\r\n      * [Tutorials](#tutorials)\r\n      * [Paper tests](#paper-tests)\r\n         * [Notebooks: how we created the tests in the paper](#notebooks-how-we-created-the-tests-in-the-paper)\r\n         * [Replicating paper tests, or running them with new models](#replicating-paper-tests-or-running-them-with-new-models)\r\n            * [Sentiment Analysis](#sentiment-analysis)\r\n            * [QQP](#qqp)\r\n            * [SQuAD](#squad)\r\n            * [Testing huggingface transformer pipelines](#testing-huggingface-transformer-pipelines)\r\n      * [Code snippets](#code-snippets)\r\n         * [Templates](#templates)\r\n         * [RoBERTa suggestions](#roberta-suggestions)\r\n            * [Multilingual suggestions](#multilingual-suggestions)\r\n         * [Lexicons (somewhat multilingual)](#lexicons-somewhat-multilingual)\r\n         * [Perturbing data for INVs and DIRs](#perturbing-data-for-invs-and-dirs)\r\n         * [Creating and running tests](#creating-and-running-tests)\r\n         * [Custom expectation functions](#custom-expectation-functions)\r\n         * [Test Suites](#test-suites)\r\n      * [API reference](#api-reference)\r\n      * [Code of ",
    "url": "https://github.com/marcotcr/checklist",
    "last_updated": "2025-08-26T04:32:35+00:00"
  },
  {
    "full_name": "feast-dev/feast",
    "name": "feast",
    "description": "The Open Source Feature Store for AI/ML",
    "language": "Python",
    "topics": [
      "machine-learning",
      "features",
      "ml",
      "big-data",
      "feature-store",
      "python",
      "mlops",
      "data-engineering",
      "data-science",
      "data-quality"
    ],
    "readme": "<!--Do not modify this file. It is auto-generated from a template (infra/templates/README.md.jinja2)-->\n\n<p align=\"center\">\n    <a href=\"https://feast.dev/\">\n      <img src=\"https://raw.githubusercontent.com/feast-dev/feast/master/docs/assets/feast_logo.png\" width=\"550\">\n    </a>\n</p>\n<br />\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/feast)](https://pypi.org/project/feast/)\n[![GitHub contributors](https://img.shields.io/github/contributors/feast-dev/feast)](https://github.com/feast-dev/feast/graphs/contributors)\n[![unit-tests](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml/badge.svg?branch=master&event=pull_request)](https://github.com/feast-dev/feast/actions/workflows/unit_tests.yml)\n[![integration-tests-and-build](https://github.com/feast-dev/feast/actions/workflows/master_only.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/master_only.yml)\n[![linter](https://github.com/feast-dev/feast/actions/workflows/linter.yml/badge.svg?branch=master&event=push)](https://github.com/feast-dev/feast/actions/workflows/linter.yml)\n[![Docs Latest](https://img.shields.io/badge/docs-latest-blue.svg)](https://docs.feast.dev/)\n[![Python API](https://img.shields.io/badge/docs-latest-brightgreen.svg)](http://rtd.feast.dev/)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue)](https://github.com/feast-dev/feast/blob/master/LICENSE)\n[![GitHub Release](https://img.shields.io/github/v/release/feast-dev/feast.svg?style=flat&sort=semver&color=blue)](https://github.com/feast-dev/feast/releases)\n\n\n## Join us on Slack!\n👋👋👋 [Come say hi on Slack!](https://communityinviter.com/apps/feastopensource/feast-the-open-source-feature-store)\n\n[Check out our DeepWiki!](https://deepwiki.com/feast-dev/feast)\n\n## Overview\n<a href=\"https://trendshift.io/repositories/8046\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/8046\" alt=\"feast-dev%2Ffeast | Trendshift\" style=\"width: 250px; height: 55px;\"",
    "url": "https://github.com/feast-dev/feast",
    "last_updated": "2025-09-02T10:03:57+00:00"
  },
  {
    "full_name": "avikhagol/scoobi",
    "name": "scoobi",
    "description": "Solar Conventionality-based Organizing Observation data ( SCOOBI ) for ARIES Nainital",
    "language": "Python",
    "topics": [],
    "readme": "# scoobi\nSolar Conventionality-based Organizing Observation data ( SCOOBI )\n\nNew**:\n```\nscoobi -cc #this creates new configuration file in local directory called .config\nscoobi -rc #this reads the config file .config and return a dictionary for you to see,\nscoobi -cc -c=scoobi.config #this can be used to change the name of the config file\nscoobi -rc -c=scoobi.config #if config file is scoobi.config\n```\nif you need to hardcode for ex. \nrootfolder you can now manually open the config file and replace the values,\nto use the values run the code normally combining with `-rc`\n\n\nfor example:\nchanged the \n`fits_folder=/home/avi/test-fits/` in `.config`\n\nand then to generate thumbnails from the new configuration \n```bash\nscoobi -f /path/to/fits/ -th -rc\n```\n\n\n\n```bash\nusage: scoobi [-h] [-f FITS_FOLDER] [-t TIFF_FOLDER] [-r RAW_FOLDER] [-o OUTPUT_FILE] [-c CONFIG_FILE] [-hdr HEADER] [-hf HEADER_FILE]\n              [-hfo HEADER_FILE_OUTPUT] [-mm MONTH] [-dd DAYS] [--no-datedfolder] [-rt] [-ct] [-rc] [-cc] [-do] [-th]\n\n                        | |   (_)   \n     ___  ___ ___   ___ | |__  _ \n    / __|/ __/ _ \\ / _ \\| '_ \\| |  \n    \\__ \\ (_| (_) | (_) | |_) | |   \n    |___/\\___\\___/ \\___/|_.__/|_|   \n\n    Solar Conventionality-based Organizing Observation data ( SCOOBI )\n    \n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f FITS_FOLDER, --fits_folder FITS_FOLDER\n                        source\n  -t TIFF_FOLDER, --tiff_folder TIFF_FOLDER\n                        RAW TIFF folder path\n  -r RAW_FOLDER, --raw_folder RAW_FOLDER\n                        RAW FITS folder path\n  -o OUTPUT_FILE, --output_file OUTPUT_FILE\n                        complete path for output csv or log file.\n  -c CONFIG_FILE, --config_file CONFIG_FILE\n                        complete path for config file with inputs\n  -mm MONTH, --month MONTH\n                        comma separated month names\n  -dd DAYS, --days DAYS\n                        comma separated month names for range(a,b) E",
    "url": "https://github.com/avikhagol/scoobi",
    "last_updated": "2025-01-15T15:29:46+00:00"
  },
  {
    "full_name": "notnews/issue_ideology",
    "name": "issue_ideology",
    "description": "Measuring Agendas, and Positions on Agendas",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "### Measuring Agendas, and Positions on Agendas\n\nVariation in agendas and positions on agendas are independently important. The former explains the kinds of issues people think are important, and the latter potentially explains \npeople's positions on the issues. In this paper, we estimate variation on both these dimensions. To do this, we first estimate a supervised topic model using the bill labels from the \nPolicy Agendas Project. Then within each of the topics, we estimate a model of slant using congressional speech as training data. Using this method, we scale 255 news programs across \n50 television channels using original, and closed captions transcripts from these shows. We find large systematic variation in broad agendas of news media, and positions on those agendas.  \n",
    "url": "https://github.com/notnews/issue_ideology",
    "last_updated": "2025-03-16T20:47:58+00:00"
  },
  {
    "full_name": "keplergl/kepler.gl",
    "name": "kepler.gl",
    "description": "Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets.",
    "language": "TypeScript",
    "topics": [
      "data-visualization",
      "geospatial",
      "visualization",
      "mapbox",
      "kepler"
    ],
    "readme": "<p align=\"right\">\n  <a href=\"https://npmjs.org/package/kepler.gl\">\n    <img src=\"https://img.shields.io/npm/v/kepler.gl.svg?style=flat\" alt=\"version\" />\n  </a>\n  <a href=\"https://travis-ci.com/keplergl/kepler.gl\">\n    <img src=\"https://api.travis-ci.com/keplergl/kepler.gl.svg?branch=master\" alt=\"build\" />\n  </a>\n  <a href=\"https://github.com/keplergl/kepler.gl\">\n    <img src=\"https://img.shields.io/github/stars/keplergl/kepler.gl.svg?style=flat\" alt=\"stars\" />\n  </a>\n  <a href='https://opensource.org/licenses/MIT'>\n    <img src='https://img.shields.io/badge/License-MIT-blue.svg' alt='MIT License' />\n  </a>\n  <a href='https://app.fossa.com/projects/custom%2B4458%2Fgithub.com%2Fkeplergl%2Fkepler.gl?ref=badge_shield'>\n    <img src='https://app.fossa.com/api/projects/custom%2B4458%2Fgithub.com%2Fkeplergl%2Fkepler.gl.svg?type=shield' alt='Fossa' />\n  </a>\n  <a href=\"https://app.netlify.com/sites/keplergl/deploys\">\n    <img src=\"https://api.netlify.com/api/v1/badges/0c9b895c-acd0-43fd-8af7-fe960181b686/deploy-status\" alt=\"Netlify Status\"/>\n  </a>\n  <a href='https://coveralls.io/github/keplergl/kepler.gl?branch=master'>\n    <img src='https://coveralls.io/repos/github/keplergl/kepler.gl/badge.svg?branch=master' alt='Coverage Status' />\n  </a>\n</p>\n\n<h1 align=\"center\">\n  kepler.gl | <a href=\"https://kepler.gl\">Website</a> | <a href=\"https://kepler.gl/#/demo\">Demo App</a> | <a href=\"https://docs.kepler.gl/\">Docs</a>\n</h1>\n<h3></h3>\n\n[<img width=\"120\" alt=\"Kepler.gl\" src=\"https://d1a3f4spazzrp4.cloudfront.net/kepler.gl/website/icons/kepler.gl-logo.png\">](http://kepler.gl)\n\n[<img width=\"600\" alt=\"Kepler.gl Demo\" src=\"./screenshots/screenshot.png\">](https://kepler.gl/demo)\n\n[Kepler.gl][web] is a data-agnostic, high-performance web-based application for visual exploration of large-scale geolocation data sets. Built on top of [MapLibre GL](https://maplibre.org/) and [deck.gl](https://deck.gl/), kepler.gl can render millions of points representing thousands of trips and perform spa",
    "url": "https://github.com/keplergl/kepler.gl",
    "last_updated": "2025-09-02T07:38:47+00:00"
  },
  {
    "full_name": "yahoo/CaffeOnSpark",
    "name": "CaffeOnSpark",
    "description": "Distributed deep learning on Hadoop and Spark clusters.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "<!--\nCopyright 2016 Yahoo Inc.\nLicensed under the terms of the Apache 2.0 license.\nPlease see LICENSE file in the project root for terms.\n-->\n### Note: we're lovingly marking this project as Archived since we're no longer supporting it. You are welcome to read the code and fork your own version of it and continue to use this code under the terms of the project license.\n\n# CaffeOnSpark\n\n## What's CaffeOnSpark?\n\nCaffeOnSpark brings deep learning to Hadoop and Spark clusters.  By\ncombining salient features from deep learning framework\n[Caffe](https://github.com/BVLC/caffe) and big-data frameworks [Apache\nSpark](http://spark.apache.org/) and [Apache Hadoop](http://hadoop.apache.org/), CaffeOnSpark enables distributed\ndeep learning on a cluster of GPU and CPU servers.\n\nAs a distributed extension of Caffe, CaffeOnSpark supports neural\nnetwork model training, testing, and feature extraction.  Caffe users\ncan now perform distributed learning using their existing LMDB data\nfiles and minorly adjusted network configuration (as\n[illustrated](../master/data/lenet_memory_train_test.prototxt#L10-L12)).\n\nCaffeOnSpark is a Spark package for deep learning. It is complementary\nto non-deep learning libraries MLlib and Spark SQL.\nCaffeOnSpark's Scala API provides Spark applications with an easy\nmechanism to invoke deep learning (see\n[sample](../master/caffe-grid/src/main/scala/com/yahoo/ml/caffe/examples/MyMLPipeline.scala))\nover distributed datasets.\n\nCaffeOnSpark was developed by Yahoo for [large-scale distributed deep\nlearning on our Hadoop\nclusters](http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop)\nin Yahoo's private cloud.  It's been in use by Yahoo for image search,\ncontent classification and several other use cases.\n\n## Why CaffeOnSpark?\n\nCaffeOnSpark provides some important benefits (see [our blog](http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep)) over alternative deep learning solution",
    "url": "https://github.com/yahoo/CaffeOnSpark",
    "last_updated": "2025-08-27T19:25:40+00:00"
  },
  {
    "full_name": "Layout-Parser/layout-parser",
    "name": "layout-parser",
    "description": "A Unified Toolkit for Deep Learning Based Document Image Analysis",
    "language": "Python",
    "topics": [
      "layout-analysis",
      "deep-learning",
      "object-detection",
      "ocr",
      "layout-parser",
      "detectron2",
      "document-layout-analysis",
      "computer-vision",
      "document-image-processing",
      "layout-detection"
    ],
    "readme": "<p align=\"center\">\n  <img src=\"https://github.com/Layout-Parser/layout-parser/raw/main/.github/layout-parser.png\" alt=\"Layout Parser Logo\" width=\"35%\">\n  <h3 align=\"center\">\n  A unified toolkit for Deep Learning Based Document Image Analysis\n  </h3>\n</p>\n\n<p align=center>\n<a href=\"https://pypi.org/project/layoutparser/\"><img src=\"https://img.shields.io/pypi/v/layoutparser?color=%23099cec&label=PyPI%20package&logo=pypi&logoColor=white\" title=\"The current version of Layout Parser\"></a>\n<a href=\"https://github.com/Layout-Parser/layout-parser/blob/main/LICENSE\"><img src=\"https://img.shields.io/pypi/l/layoutparser\" title=\"Layout Parser uses Apache 2 License\"></a>\n<img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/layoutparser\">\n</p>\n\n<p align=center>\n<a href=\"https://arxiv.org/abs/2103.15348\"><img src=\"https://img.shields.io/badge/paper-2103.15348-b31b1b.svg\" title=\"Layout Parser Paper\"></a>\n<a href=\"https://layout-parser.github.io\"><img src=\"https://img.shields.io/badge/website-layout--parser.github.io-informational.svg\" title=\"Layout Parser Paper\"></a>\n<a href=\"https://layout-parser.readthedocs.io/en/latest/\"><img src=\"https://img.shields.io/badge/doc-layout--parser.readthedocs.io-light.svg\" title=\"Layout Parser Documentation\"></a>\n</p>\n\n---\n\n## What is LayoutParser\n\n![Example Usage](https://github.com/Layout-Parser/layout-parser/raw/main/.github/example.png)\n\nLayoutParser aims to provide a wide range of tools that aims to streamline Document Image Analysis (DIA) tasks. Please check the LayoutParser [demo video](https://youtu.be/8yA5xB4Dg8c) (1 min) or [full talk](https://www.youtube.com/watch?v=YG0qepPgyGY) (15 min) for details. And here are some key features:\n\n- LayoutParser provides a rich repository of deep learning models for layout detection as well as a set of unified APIs for using them. For example, \n  \n  <details>\n  <summary>Perform DL layout detection in 4 lines of code</summary>\n  \n  ```python\n  import layoutparser as lp\n  model = lp.AutoLayout",
    "url": "https://github.com/Layout-Parser/layout-parser",
    "last_updated": "2025-09-02T07:39:44+00:00"
  },
  {
    "full_name": "GoncaloMark/CobWeb-lnx",
    "name": "CobWeb-lnx",
    "description": "CobWeb is a Python library for web scraping. The library consists of two classes: Spider and Scraper.",
    "language": "Python",
    "topics": [
      "cli-tool",
      "python",
      "web-crawler",
      "web-crawler-python",
      "web-scraper",
      "web-scraping-python",
      "crawler",
      "scraper",
      "scraping"
    ],
    "readme": "# CobWeb\n\nCobWeb is a Python library for web scraping. The library consists of two classes: Spider and Scraper.\n\n## Spider\n\nThe Spider class is used to crawl a website and identify internal and external links. It has the following methods:\n\n    __init__(self, url, max_hops = 0): Initializes a Spider object with the given URL and maximum number of links to follow from the initial URL.\n    _getLinks(self): Crawls the website and identifies internal and external links.\n    _showLinks(self): Returns the set of internal and external URLs found during crawling.\n    __str__(self): Returns a string representation of the Spider object.\n    __repr__(self): Returns a string representation of the Spider object.\n\n## Scraper\n\nThe Scraper class extends the functionality of the Spider class by scraping HTML content from web pages based on user-defined parameters. It has the following methods:\n\n    __init__(self, config): Initializes a Scraper object with the given configuration parameters.\n    run(self): A public method to scrape HTML content from web pages based on user-defined parameters.\n    __str__(self): Returns a string representation of the Scraper object.\n    __repr__(self): Returns a string representation of the Scraper object.\n\n## Installation\n\nYou can install CobWeb using pip:\n\n```bash\n\n    pip install CobWeb\n\n```\n\n## Config\n\nConfig is either an object in memory or a YAML file you can build by installing YAMLbuilder or by using the provided template!\nExample of a complete object:\n\n```python\nconfig = {\n            \"url\": \n            \"hops\": \n            \"tags\":\n            \"classes\":\n            \"attrs\":\n            \"attrV\":\n            \"IDv\":\n            \"selectors\":\n        }\n```\n\nExample of YAML file (If you choose this path call the config_parser function and give it a valid path!):\n\n```yaml\nIDv:\nattrV: []\nattrs: []\nclasses: []\nhops: \nselectors: []\ntags:\n    - \n    - \nurl: \n```\n\n## Example Usage\n\n```python\n\nfrom CobWeb import Spider, Scraper\n\n# Create a Spider object",
    "url": "https://github.com/GoncaloMark/CobWeb-lnx",
    "last_updated": "2025-01-23T08:57:12+00:00"
  },
  {
    "full_name": "notnews/get-journalist-data",
    "name": "get-journalist-data",
    "description": "Get data on journalists from http://muckrack.com and http://presspass.me",
    "language": "Python",
    "topics": [],
    "readme": "### Get Data on Journalists\n\nScrape data on journalists from [Presspass](http://www.presspass.me/) and [MuckRack](http://muckrack.com/). \n\nBoth scripts take a csv with column carrying twitter usernames labeled 'twitter.username.' (See [sample data](sample_input.csv).)\n\n#### Running the Scripts\n\n##### Prerequisites\n\t- Python 3.x\n\t- BeautifulSoup 4\n\t\n#### Muckrack\n\n1. Scrape data to file:\n\n    <pre> python scrape_muckrack.py input_file </pre>\n    \n    HTML files will be saved to `./muckrack`\n\n2. Parse and extract information from file:\n\n    <pre> python extract_muckrack.py input_file</pre>\n\n    Output CSV file will be saved as `muckrack-out.csv`\n\n\n#### PressPass\n\n1. Scrape data to file:\n\n   <pre> python scrape_presspass.py input_file </pre>\n\n    HTML files will be saved to  `./presspass`\n\n2. Parse and extract information from file:\n\n    <pre>python extract_presspass.py input_file </pre>\n\n    Output CSV file will be saved as `presspass-out.csv`\n\n#### License\n\nScripts are released under the [MIT License](https://opensource.org/licenses/MIT).\n",
    "url": "https://github.com/notnews/get-journalist-data",
    "last_updated": "2025-08-27T12:58:55+00:00"
  },
  {
    "full_name": "cancan101/graphql-db-api",
    "name": "graphql-db-api",
    "description": "A Python DB API 2.0 for GraphQL APIs",
    "language": "Python",
    "topics": [],
    "readme": "# graphql-db-api [![PyPI version](https://badge.fury.io/py/sqlalchemy-graphqlapi.svg)](https://badge.fury.io/py/sqlalchemy-graphqlapi) ![main workflow](https://github.com/cancan101/graphql-db-api/actions/workflows/main.yml/badge.svg) [![codecov](https://codecov.io/gh/cancan101/graphql-db-api/branch/main/graph/badge.svg?token=TOI17GOA2O)](https://codecov.io/gh/cancan101/graphql-db-api)\n\nA Python DB API 2.0 for GraphQL APIs\n\nThis module allows you to query GraphQL APIs using SQL.\n\n## SQLAlchemy support\n\nThis module provides a SQLAlchemy dialect.\n\n```python\nfrom sqlalchemy.engine import create_engine\n\n# Over HTTPS (default):\nengine_https = create_engine('graphql://host:port/path')\n\n# Over HTTP:\nengine_http = create_engine('graphql://host:port/path?is_https=0')\n\n# With a `Bearer` token in the `Authorization` header:\nengine_http = create_engine('graphql://:token@host:port/path')\n```\n\n### Example Usage\n\n#### Querying Connections\n\n```python\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\n\n# We use GraphQL SWAPI (The Star Wars API) c/o Netlify:\nengine = create_engine('graphql://swapi-graphql.netlify.app/.netlify/functions/index')\n\n# Demonstration of requesting nested resource of homeworld\n# and then selecting fields from it\nquery = \"select name, homeworld__name from 'allPeople?include=homeworld'\"\n\nwith engine.connect() as connection:\n    for row in connection.execute(text(query)):\n        print(row)\n```\n\n#### Querying Lists\n\nWe can mark a given GQL query as being a List when we query that \"Table\" using a query parameter:\n\n```python\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\n\nengine = create_engine('graphql://pet-library.moonhighway.com/')\n\n# The default assumes top level is a Connection.\n# For Lists, we must disable this:\nquery = \"select id, name from 'allPets?is_connection=0'\"\n\nwith engine.connect() as connection:\n    for row in connection.execute(text(query)):\n        print(row)\n```\n\nalternatively, we can set that at the `Engine` lev",
    "url": "https://github.com/cancan101/graphql-db-api",
    "last_updated": "2025-07-23T09:26:46+00:00"
  },
  {
    "full_name": "opencivicdata/python-legistar-scraper",
    "name": "python-legistar-scraper",
    "description": "Scrapes municipal data from Legistar websites",
    "language": "Python",
    "topics": [],
    "readme": "Python Legistar Scraper\n=======================\n\n[![tests](https://github.com/opencivicdata/python-legistar-scraper/actions/workflows/pythonapp.yml/badge.svg)](https://github.com/opencivicdata/python-legistar-scraper/actions/workflows/pythonapp.yml)\n\nScrapes municipal data from Legistar sites.\n\n## To install\n```bash\npip install scraper-legistar\n```\n\n## Contributing\n\n### Local installation\n\nInstall this package and its dependencies. (We recommend working in a virtual\nenvironment, e.g., [`venv`](https://docs.python.org/3/library/venv.html) or\n[`virtualenvwrapper`](https://virtualenvwrapper.readthedocs.io/en/latest/)).\n\n```bash\npip install -e .\n```\n\nInstall the development requirements.\n\n```bash\npip install -r requirements.txt\n```\n\n### Testing\n\nTo run the tests:\n\n```bash\npytest -sv\n```\n\n#### A note on test fixtures\n\nThe tests rely on HTML from assorted Legistar properties. As it would be\nundesirable to hit the live server each time the tests are run, particularly\nduring local development, fixtures are provided in the `tests/fixtures/`\ndirectory.\n\nA convenience script is provided for refreshing the fixtures, i.e., re-scraping\nLegistar and storing the resultant HTML.\n\nThe Legistar site is fairly stable, however if your patch addresses a change\nin page structure, update the fixtures as follows:\n\n```bash\npython tests/refresh_fixtures.py\n```\n\nBe sure to commit the refreshed fixture and submit it with your patch.\n",
    "url": "https://github.com/opencivicdata/python-legistar-scraper",
    "last_updated": "2025-08-28T04:36:41+00:00"
  },
  {
    "full_name": "SWE-bench/SWE-bench",
    "name": "SWE-bench",
    "description": "SWE-bench [Multimodal]: Can Language Models Resolve Real-world Github Issues?",
    "language": "Python",
    "topics": [
      "benchmark",
      "language-model",
      "software-engineering"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"http://swe-bench.github.io\">\n    <img src=\"docs/assets/figures/swellama_banner.svg\" style=\"height: 10em\" alt=\"Kawi the SWE-Llama\" />\n  </a>\n</p>\n\n<p align=\"center\"><strong>[&nbsp;<a href=\"https://swebench.com/SWE-bench/\">Read the Docs</a>&nbsp;]</strong></p>\n\n<p align=\"center\">\n  <a href=\"docs/other_languages/README_JP.md\">日本語</a> |\n  <a href=\"docs/other_languages/README_CN.md\">中文简体</a> |\n  <a href=\"docs/other_languages/README_TW.md\">中文繁體</a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://www.python.org/\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/Python-3.8+-1f425f.svg?color=purple\">\n    </a>\n    <a href=\"https://copyright.princeton.edu/policy\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-blue\">\n    </a>\n    <a href=\"https://badge.fury.io/py/swebench\">\n        <img src=\"https://badge.fury.io/py/swebench.svg\">\n    </a>\n</p>\n\n---\n\nCode and data for the following works:\n* [ICLR 2025] <a href=\"https://arxiv.org/abs/2410.03859\">SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?</a>\n* [ICLR 2024 Oral] <a href=\"https://arxiv.org/abs/2310.06770\">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a>\n\n## 📰 News\n* **[Jan. 13, 2025]**: We've integrated [SWE-bench Multimodal](https://swebench.github.io/multimodal) ([paper](https://arxiv.org/abs/2410.03859), [dataset](https://huggingface.co/datasets/SWE-bench/SWE-bench_Multimodal)) into this repository! Unlike SWE-bench, we've kept evaluation for the test split *private*. Submit to the leaderboard using [sb-cli](https://github.com/swe-bench/sb-cli/tree/main), our new cloud-based evaluation tool.\n* **[Jan. 11, 2025]**: Thanks to [Modal](https://modal.com/), you can now run evaluations entirely on the cloud! See [here](https://github.com/swe-bench/SWE-bench/blob/main/docs/assets/evaluation.md#%EF%B8%8F-evaluation-with-modal) for more details.\n* **[Aug. 13, 2024]**: Introducing *SWE-bench Verified*! Part 2 of our collab",
    "url": "https://github.com/SWE-bench/SWE-bench",
    "last_updated": "2025-09-02T09:03:17+00:00"
  },
  {
    "full_name": "dwillis/post_haste",
    "name": "post_haste",
    "description": "Ruby wrapper to washingtonpost.com articles and blog posts",
    "language": "Ruby",
    "topics": [],
    "readme": "# PostHaste\n\nA Ruby library that wraps the JSON endpoints provided for Washington Post articles and blog posts. Potentially suitable for building custom feeds of Washington Post content, in the event that you don't want to actually visit washingtonpost.com. It handles articles and blog posts from the Post's CMS (along with the most recent 15 comments), as well as The Post's WordPress-powered blogs. Post Haste also provides a wrapper to \"Most Viewed\" articles and blog posts.\n\nTested under Ruby 1.9.3, 2.0.0, 2.1.0 and 2.2.x.\n\n## Installation\n\nAdd this line to your application's Gemfile:\n\n    gem 'post_haste'\n\nAnd then execute:\n\n    $ bundle\n\nOr install it yourself as:\n\n    $ gem install post_haste\n\n## Usage\n\nPost Haste currently can accept a URL of a Washington Post article or blog post, and converts that URL into a Ruby object with a number of methods that describe it, including its title, byline, published and updated datetimes, and more:\n\n```ruby\n  require 'post_haste'\n  include PostHaste\n\n  url = \"http://www.washingtonpost.com/blogs/the-fix/post/republicans-on-the-2012-gop-field-blah/2012/03/15/gIQAT7CSFS_blog.html\"\n\n  @article = Article.create_from_url(url, 10) # 10 represents number of comments to grab, default is 25.\n\n  @article.title\n\n  => \"Republicans on the 2012 GOP field: Blah.\"\n\n  @article.display_datetime.to_s\n\n  => \"2012-03-16T06:30:00-04:00\"\n\n  @article.comments.first.author\n\n  => \"iseasygoing\"\n\n  @article.comments.first.permalink(@article)\n\n  => \"http://www.washingtonpost.com/blogs/the-fix/post/republicans-on-the-2012-gop-field-blah/2012/03/15/gIQAT7CSFS_comment.html?commentID=washingtonpost.com/ECHO/item/1332046095-915-174\"\n```\n\nSee the full list of `Article` instance methods in article.rb.\n\nFor a listing of Most Viewed articles and blog posts at the time of the request:\n\n```ruby\n  require 'post_haste'\n  include PostHaste\n\n  mv = MostViewed.all\n  mv.first\n  => <PostHaste::MostViewed:0x007fb4e5066138 @platform=\"web\", @type=\"all\", @datetime=#<DateTime: ",
    "url": "https://github.com/dwillis/post_haste",
    "last_updated": "2024-10-13T20:59:45+00:00"
  },
  {
    "full_name": "tidyverse/dbplyr",
    "name": "dbplyr",
    "description": "Database (DBI) backend for dplyr",
    "language": "R",
    "topics": [
      "r",
      "database"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# dbplyr <a href='https://dbplyr.tidyverse.org'><img src='man/figures/logo.png' align=\"right\" height=\"139\" /></a>\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/dbplyr)](https://cran.r-project.org/package=dbplyr)\n[![R-CMD-check](https://github.com/tidyverse/dbplyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/dbplyr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/dbplyr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/dbplyr?branch=main)\n<!-- badges: end -->\n\n## Overview\n\ndbplyr is the database backend for [dplyr](https://dplyr.tidyverse.org).\nIt allows you to use remote database tables as if they are in-memory\ndata frames by automatically converting dplyr code into SQL.\n\nTo learn more about why you might use dbplyr instead of writing SQL, see\n`vignette(\"sql\")`. To learn more about the details of the SQL\ntranslation, see `vignette(\"translation-verb\")` and\n`vignette(\"translation-function\")`.\n\n## Installation\n\n``` r\n# The easiest way to get dbplyr is to install the whole tidyverse:\ninstall.packages(\"tidyverse\")\n\n# Alternatively, install just dbplyr:\ninstall.packages(\"dbplyr\")\n\n# Or the development version from GitHub:\n# install.packages(\"pak\")\npak::pak(\"tidyverse/dbplyr\")\n```\n\n## Usage\n\ndbplyr is designed to work with database tables as if they were local\ndata frames. To demonstrate this I’ll first create an in-memory SQLite\ndatabase and copy over a dataset:\n\n``` r\nlibrary(dplyr, warn.conflicts = FALSE)\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, mtcars)\n```\n\nNote that you don’t actually need to load dbplyr with `library(dbplyr)`;\ndplyr automatically loads it for you when it sees you working with a\ndatabase. Database connections are coordinated by the DBI package. Learn\nmore at <https://dbi.r-dbi.org/>\n\nNow you can retrieve a table using `tbl()` (see `",
    "url": "https://github.com/tidyverse/dbplyr",
    "last_updated": "2025-08-28T04:09:23+00:00"
  },
  {
    "full_name": "tlouarn/pyopenfigi",
    "name": "pyopenfigi",
    "description": "Python wrapper for the OpenFIGI API",
    "language": "Python",
    "topics": [
      "bloomberg",
      "figi",
      "openfigi",
      "python"
    ],
    "readme": "## pyopenfigi\n\n![Python 3.10](https://img.shields.io/badge/python-3.10-blue)\n![Black](https://img.shields.io/badge/code%20style-black-black)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nPython wrapper for the [OpenFIGI API](https://www.openfigi.com/api) v3.\n\n## Table of contents\n\n- [About OpenFIGI](#about-openfigi)\n- [Installation](#installation)\n- [API key](#api-key)\n- [Mapping](#mapping)\n- [Filtering](#filtering)\n- [Troubleshooting](#troubleshooting)\n\n## About OpenFIGI\n\n- The **F**inancial **I**nstrument **G**lobal **I**dentifier (FIGI) is a universal system for identifying instruments\n  globally and across all asset classes\n- OpenFIGI is an application programming interface that provides automated access\n  to mapping various symbols with their corresponding FIGI. It is available at https://www.openfigi.com/\n- [pyopenfigi](https://github.com/tlouarn/pyopenfigi) is a\n  thin Python wrapper to access OpenFIGI\n\nThe API contains 3 endpoints:\n\n| endpoint | description                                          |\n|----------|------------------------------------------------------|\n| /mapping | Map third-party identifiers to FIGIs                 |\n| /filter  | Filter for FIGIs using keywords and optional filters |\n| /search  | Search for FIGIs using keywords and optional filters |\n\n_Note: given that the */search* endpoint is strictly superseded by the */filter* endpoint, we\nchoose not to include it in the wrapper._\n\n## Installation\n\n**pyopenfigi** is published on [PyPI](https://pypi.org/project/pyopenfigi/). To install it, simply run:\n\n```commandline\npip install pyopenfigi\n```\n\n## API key\n\nThe API can be used with or without API key.\nGetting an API key is free and loosens the [rate limits](https://www.openfigi.com/api#rate-limit).\n\nWhen instantiating the wrapper, the API key is optional:\n\n```python\nfrom pyopenfigi import OpenFigi\n\nwrapper = OpenFigi()\nwrapper = OpenFigi(api_key=\"XXXXXXXXXX\")\n```\n\n## Mapping\n\nTh",
    "url": "https://github.com/tlouarn/pyopenfigi",
    "last_updated": "2025-08-27T12:34:07+00:00"
  },
  {
    "full_name": "taolei87/text_convnet",
    "name": "text_convnet",
    "description": "Molding CNNs for text (http://arxiv.org/abs/1508.04112)",
    "language": "Python",
    "topics": [],
    "readme": "\n#### News\nThe code has been re-factored and integrated into the new repo: https://github.com/taolei87/rcnn/tree/master/code/sentiment\n\nThe new repo is recommended because it is more modular and supports more running options, type of models etc. \n\n## CNNs with non-linear and non-consecutive feature maps\n\n\nThis repo contains an implementation of CNNs described in the paper [Molding CNNs for text: non-linear, non-consecutive convolutions](http://arxiv.org/abs/1508.04112) by Tao Lei, Regina Barzilay and Tommi Jaakkola.\n\n\n\n#### Dependencies\n\n * [Theano](http://deeplearning.net/software/theano/) >= 0.7\n * Python >= 2.7\n * Numpy\n\n\n\n#### Data\n \n * [Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/index.html): <br>\n  The annotated constituency trees from the treebank are converted into plain text sequences. `data/stsa.fine.*` are annotated with 5-class fine-grained labels; `data/stsa.binary.*` are annotated with binary labels. <br>\n  In the training files `data/stsa.binary.phrases.train` and `data/stsa.fine.phrases.train` we also add phrase annotations. Each phrase (and its sentiment label) is a separate training instance.\n\n * [Glove](http://nlp.stanford.edu/projects/glove/) word embeddings: <br>\n  We use the 840B Common Crawl version. Note the original compressed file is 2GB. In the directory `word_vectors/` we provide a smaller version (~37MB) by pruning words not in the sentiment treebank.\n\n * [Sogou Chinese news corpora](http://www.sogou.com/labs/dl/c.html): <br>\n  Data redistribution is not allowed. Please contact Sogou Lab to obtain the news corpora.\n\n\n#### Results\n\nDirectory `trained_models` contains saved models of the sentiment classification task. We reproduced the results mentioned in our paper by setting the random seed explicitly. The performance of each trained models are listed below:  \n\nFine-grained models  |  Dev acc.  |  Test acc. \n:--- | --- | ---\nstsa.root.fine.pkl.gz  |  49.5  | 50.6 \nstsa.phrases.fine.pkl.gz  |  53.4  | 51.2  \nstsa.phrase",
    "url": "https://github.com/taolei87/text_convnet",
    "last_updated": "2025-01-21T03:56:04+00:00"
  },
  {
    "full_name": "bearloga/learning-rcpp",
    "name": "learning-rcpp",
    "description": "My notes as I learn C++ and Rcpp for fast machine learning in R",
    "language": "C++",
    "topics": [
      "c-plus-plus",
      "rcpp",
      "r",
      "notes",
      "learning-by-doing"
    ],
    "readme": "My main goal in this educational endeavor is to be able to use the [MLPACK](http://www.mlpack.org/), [Shark](http://image.diku.dk/shark/), and [dlib](http://dlib.net/) C++ machine learning libraries in R for computationally intensive problems. Now, there is a [RcppMLPACK](https://cran.r-project.org/package=RcppMLPACK), but that one apparently uses version 1 of MLPACK (which is now in version 2) and doesn't include any supervised learning methods, just unsupervised learning methods.\n\n-   [Setup](#setup)\n    -   [Software Libraries](#software-libraries)\n    -   [Mac OS X](#mac-os-x)\n    -   [Ubuntu/Debian](#ubuntudebian)\n        -   [Building Shark library](#building-shark-library)\n    -   [R Packages](#r-packages)\n-   [Rcpp](#rcpp)\n    -   [Basics](#basics)\n    -   [Using Libraries](#using-libraries)\n        -   [Armadillo vs RcppArmadillo](#armadillo-vs-rcpparmadillo)\n        -   [Fast K-Means](#fast-k-means)\n    -   [Fast Classification](#fast-classification)\n        -   [External Pointers](#external-pointers)\n    -   [Other Libraries](#other-libraries)\n        -   [Shark](#shark)\n            -   [Classification](#classification)\n        -   [DLib](#dlib)\n    -   [Random Number Generation](#random-number-generation)\n    -   [Serialization](#serialization)\n        -   [Armadillo Serialization (Not Working)](#armadillo-serialization-not-working)\n-   [References](#references)\n\nSetup\n=====\n\nSoftware Libraries\n------------------\n\nMac OS X\n--------\n\n``` bash\n## To install Homebrew:\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n## Then:\nbrew tap homebrew/versions && brew tap homebrew/science && brew update\n# brew install gcc --enable-cxx && brew link --overwrite gcc && brew link cmake\nbrew install boost --c++11\n# Installing cmake may require: sudo chown -R `whoami` /usr/local/share/man/man7\nbrew install mlpack shark dlib\n```\n\nUbuntu/Debian\n-------------\n\n``` bash\nsudo apt-get install libmlpack-dev libdlib-dev\n```\n\n### B",
    "url": "https://github.com/bearloga/learning-rcpp",
    "last_updated": "2025-03-22T11:18:31+00:00"
  },
  {
    "full_name": "balajiln/mondrianforest",
    "name": "mondrianforest",
    "description": "Code for Mondrian Forests (for classification and regression)",
    "language": "Python",
    "topics": [],
    "readme": "This folder contains the scripts used in the following papers:\n\n**Mondrian Forests: Efficient Online Random Forests**\n\nBalaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh\n\n*Advances in Neural Information Processing Systems (NIPS), 2014.*\n\n[Link to PDF](http://www.gatsby.ucl.ac.uk/~balaji/mondrian_forests_nips14.pdf)\n\n**Mondrian Forests for Large-Scale Regression when Uncertainty Matters**\n\nBalaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh\n\n*Proceedings of AISTATS, 2016.*\n\n[Link to PDF](http://www.gatsby.ucl.ac.uk/~balaji/mfregression_aistats16.pdf)\n\nPlease cite the appropriate paper if you use this code.\n\n\nI ran my experiments using Enthought python (which includes all the necessary python packages).\nIf you are running a different version of python (e.g. anaconda), you will need the following python packages \n(and possibly other packages) to run the scripts:\n\n* numpy\n* scipy\n* matplotlib (for plotting Mondrian partitions)\n* pydot and graphviz (for printing Mondrian trees)\n* sklearn (for reading libsvm format files)\n\nSome of the packages (e.g. pydot, matplotlib) are necessary only for '--draw_mondrian 1' option. If you just want to run experiments\nwithout plotting the Mondrians, these packages may not be necessary.\n\nPaul Heideman has created requirements.txt, which makes it easy to install the packages using 'pip install -r requirements.txt'.\nDan Stowell pointed out that dvipng package is required in ubuntu to draw the Mondrians.\n\n\nThe datasets are not included here; you need to download them from the UCI repository. You can run \nexperiments using toy data though. Run **commands.sh** in **process_data** folder for automatically \ndownloading and processing the datasets. I have tested these scripts only on Ubuntu, but it should be straightforward to process datasets in other platforms.\n\nIf you have any questions/comments/suggestions, please contact me at \n[balaji@gatsby.ucl.ac.uk](mailto:balaji@gatsby.ucl.ac.uk).\n\nCode released under MIT license (see COPYING for mo",
    "url": "https://github.com/balajiln/mondrianforest",
    "last_updated": "2025-08-27T16:11:03+00:00"
  },
  {
    "full_name": "nlegrand/yze",
    "name": "yze",
    "description": "Something to emulate YZE dice throwing",
    "language": "Python",
    "topics": [],
    "readme": "The main goal of this Python library is to propose an object to\nemulate Year Zero Engine dice throwing. It also feature probability\nsimulations to give you an idea of the chances you get to succeed or\nfail.\n\n# What is it about?\n\nThe Year Zero Engine is a very nice Roleplaying Game system created by\nthe terrific Tomas Härenstam at [Free League\nPublishing](https://freeleaguepublishing.com/). You can check the\n[SRD](https://freeleaguepublishing.com/en/free-tabletop-licenses/) to\nsee how it is done or [the\ngames](https://freeleaguepublishing.com/en/store/) directly.\n\nThis Python module is unofficial and not affiliated with Free\nLeague. I've made it just for fun.\n\n# System supported:\n- [X] Mutant: Year Zero\n- [X] Forbidden Lands\n- [X] Twilight 2000\n- [X] Alien\n- [X] Blade Runner\n\n# TODO\n\n- [ ] negative dice for MYZ and FBL\n\n# Example\n```\n. my_py_venv/bin/activate\ngit clone https://github.com/nlegrand/yze.git\ncd yze\npython -m pip install -e .\npython\nPython 3.11.3 (main, Apr  8 2023, 02:16:51) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from yze.dice import MutantDicePool\n>>> d = MutantDicePool(attr=3, skill=2, gear=2)\n>>> d.throw()\n{'attr': [5, 6, 6], 'skill': [1, 4], 'gear': [6, 2]}\n>>> d.push()\n{'attr': [2, 6, 6], 'skill': [5, 2], 'gear': [6, 1]}\n>>> from yze.dice import FBLDicePool\n>>> fbl = FBLDicePool(attr=2, skill=1, artefact=12)\n>>> fbl.throw()\n{'attr': [2, 2], 'skill': [2], 'gear': [], 'artefact': (8, 2)}\n>>> fbl.push()\n{'attr': [2, 6], 'skill': [3], 'gear': [], 'artefact': (8, 2)}\n>>> from yze.dice import AlienDicePool\n>>> alien = AlienDicePool(pool=4, stress=1)\n>>> alien.throw()\n{'pool': [4, 1, 6, 1], 'stress': [2]}\n>>> alien.push()\n{'pool': [1, 3, 6, 2], 'stress': [1, 1]}\n```\n\n# Probability simulation: Odds of pushing\n\nFree League gives very general chances to get a succes when rolling\nand pushing a dice pool according to it’s size. But how can we get\nodds after the first roll is made",
    "url": "https://github.com/nlegrand/yze",
    "last_updated": "2025-01-15T15:29:19+00:00"
  },
  {
    "full_name": "hrbrmstr/greynoise",
    "name": "greynoise",
    "description": "Query 'GreyNoise Intelligence 'API' in R",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "r-cyber",
      "greynoise-intelligence"
    ],
    "readme": "\n# greynoise\n\nQuery ‘GreyNoise Intelligence ’API’\n\n## Description\n\nTools are provided to query the ‘GreyNoise Intelligence ’API’.\n\nGreyNoise has a web site but there’s nothing there at the moment:\n<http://greynoise.io/>\n\n## What’s Inside The Tin\n\n  - `gn_list_tags`: List GreyNoise Intelligence Tags\n  - `gn_query_by_ip`: Query all tags associated with a given IP address\n  - `gn_query_by_tag`: Query all IPs that have a given tag\n\nThe following functions are implemented:\n\n## Installation\n\n``` r\ndevtools::install_github(\"hrbrmstr/greynoise\")\n```\n\n## Usage\n\n``` r\nlibrary(greynoise)\nlibrary(tidyverse)\n\n# current verison\npackageVersion(\"greynoise\")\n```\n\n    ## [1] '0.1.0'\n\n### List tags\n\n``` r\ngn_list_tags()\n```\n\n    ##   [1] \"VNC_SCANNER_HIGH\"                  \"PING_SCANNER_LOW\"                  \"COBALT_STRIKE_SCANNER_HIGH\"       \n    ##   [4] \"JBOSS_WORM\"                        \"NETIS_ROUTER_ADMIN_SCANNER_LOW\"    \"GOOGLEBOT\"                        \n    ##   [7] \"SNMP_SCANNER_LOW\"                  \"PYCURL_HTTP_CLIENT\"                \"ROUTER_RPC_SCANNER_HIGH\"          \n    ##  [10] \"TELNET_SCANNER_HIGH\"               \"MONGODB_SCANNER_HIGH\"              \"WEB_SCANNER_HIGH\"                 \n    ##  [13] \"NTP_SCANNER_LOW\"                   \"ZMAP_CLIENT\"                       \"DNS_SCANNER_HIGH\"                 \n    ##  [16] \"HTTP_ALT_SCANNER_HIGH\"             \"PINGDOM\"                           \"COUNTERSTRIKE_SERVER_SCANNER_LOW\" \n    ##  [19] \"IPIP\"                              \"MSSQL_SCANNER_LOW\"                 \"TELNET_WORM_LOW\"                  \n    ##  [22] \"FTP_SCANNER_HIGH\"                  \"SSH_WORM_HIGH\"                     \"CASSANDRA_SCANNER_LOW\"            \n    ##  [25] \"SMB_SCANNER_HIGH\"                  \"RABBITMQ_SCANNER_LOW\"              \"COUNTERSTRIKE_SERVER_SCANNER_HIGH\"\n    ##  [28] \"SIEMENS_PLC_SCANNER_HIGH\"          \"PROJECT_SONAR\"                     \"YANDEX_SEARCH_ENGINE\"             \n    ##  [31] \"ROUTER_RPC_SCANNER_LOW\"            \"SSDP_UPNP_SCANNER_LOW\"  ",
    "url": "https://github.com/hrbrmstr/greynoise",
    "last_updated": "2025-03-22T11:13:11+00:00"
  },
  {
    "full_name": "mietzen/porkbun-ddns",
    "name": "porkbun-ddns",
    "description": "porkbun-ddns is an unoffical DDNS-Client for Porkbun Domains.",
    "language": "Python",
    "topics": [
      "ddns",
      "docker",
      "pip",
      "porkbun",
      "python",
      "dns",
      "dynamic-dns",
      "porkbun-ddns",
      "porkbun-api",
      "dev"
    ],
    "readme": "# Disclaimer\n\n**This package is not related to or developed by Porkbun. No relationship between the developer of this package and Porkbun exists.**\n\n**All trademarks, logos and brand names are the property of their respective owners. All company, product and service names used in this package are for identification purposes only. Use of these names,trademarks and brands does not imply endorsement.**\n\n# Porkbun DDNS\n\n`porkbun-ddns` is a unofficial DDNS-Client for Porkbun Domains.\nThis library will only update the records if the IP(s) have changed or the dns entry didn't exist before, it will also set/update A (IPv4) and AAAA (IPv6) records.\n\n\nSince [porkbun-dynamic-dns-python](https://github.com/porkbundomains/porkbun-dynamic-dns-python) is deprecated I took it into my own hands to code a decent DDNS Client for Porkbun.\nInspired by [con-f-use](https://github.com/con-f-use) [pull request](https://github.com/porkbundomains/porkbun-dynamic-dns-python/pull/6), I built a pip Package and a docker container.\n\nAs alternative to cert-bun use my [lego-certbot](https://github.com/mietzen/lego-certbot) image.\n\n## Setup on Porkbun\n\nMake sure that any domain you use this client with has API access enabled. See the below picture for reference.\n\n![API Access Enabled](API_Access_Enabled.png)\n\nIf this is not enabled, you'll see an error about your API keys being invalid, despite them being correct.\n\n# CLI\n\n**Minimum required python version: 3.10**\n\n## Install via pip\n\n```shell\npip install porkbun-ddns\n```\n\n## Usage\n\n```Shell\nusage: porkbun-ddns [-h] [-c CONFIG] [-e ENDPOINT] [-pk APIKEY] [-sk SECRETAPIKEY] [-i [PUBLIC_IPS ...]] [-f FRITZBOX] [-4 | -6] [-v] [--env_only] domain [subdomains ...]\n\npositional arguments:\n  domain                Domain to be updated\n  subdomains            Subdomain(s)\n\noptions:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        Path to config file (default: ~/.config/porkbun-ddns-config.json)\n  -e EN",
    "url": "https://github.com/mietzen/porkbun-ddns",
    "last_updated": "2025-08-29T05:21:06+00:00"
  },
  {
    "full_name": "ageitgey/face_recognition",
    "name": "face_recognition",
    "description": "The world's simplest facial recognition api for Python and the command line",
    "language": "Python",
    "topics": [
      "machine-learning",
      "face-detection",
      "face-recognition",
      "python"
    ],
    "readme": "# Face Recognition\n\n_You can also read a translated version of this file [in Chinese 简体中文版](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md) or [in Korean 한국어](https://github.com/ageitgey/face_recognition/blob/master/README_Korean.md) or [in Japanese 日本語](https://github.com/m-i-k-i/face_recognition/blob/master/README_Japanese.md)._\n\nRecognize and manipulate faces from Python or from the command line with\nthe world's simplest face recognition library.\n\nBuilt using [dlib](http://dlib.net/)'s state-of-the-art face recognition\nbuilt with deep learning. The model has an accuracy of 99.38% on the\n[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.\n\nThis also provides a simple `face_recognition` command line tool that lets\nyou do face recognition on a folder of images from the command line!\n\n\n[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)\n[![Build Status](https://github.com/ageitgey/face_recognition/workflows/CI/badge.svg?branch=master&event=push)](https://github.com/ageitgey/face_recognition/actions?query=workflow%3ACI)\n[![Documentation Status](https://readthedocs.org/projects/face-recognition/badge/?version=latest)](http://face-recognition.readthedocs.io/en/latest/?badge=latest)\n\n## Features\n\n#### Find faces in pictures\n\nFind all the faces that appear in a picture:\n\n![](https://cloud.githubusercontent.com/assets/896692/23625227/42c65360-025d-11e7-94ea-b12f28cb34b4.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_locations = face_recognition.face_locations(image)\n```\n\n#### Find and manipulate facial features in pictures\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625282/7f2d79dc-025d-11e7-8728-d8924596f8fa.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_land",
    "url": "https://github.com/ageitgey/face_recognition",
    "last_updated": "2025-09-02T09:14:29+00:00"
  },
  {
    "full_name": "fivethirtyeight/guns-data",
    "name": "guns-data",
    "description": "",
    "language": "R",
    "topics": [
      "data"
    ],
    "readme": "Code and data behind FiveThirtyEight's [Gun Deaths in America](http://fivethirtyeight.com/gun-deaths/) project.\n\nThere are five files in this repository:\n- `CDC_parser.R` - code to download, clean and parse data from the CDC's [Multiple Cause of Death datafile](http://www.cdc.gov/nchs/data_access/VitalStatsOnline.htm#Mortality_Multiple).\n- `interactive_prep.R` - code to generate the data behind our interactive data visualization.\n- `interactive_data.csv` - clean data behind our interactive data visualization\n- `full_data.csv` - cleaned gun-death data from the CDC for 2012-2014.\n- `SHR_parser.R` - code to clean and parse data from the FBI's [Supplementary Homicide Reports](https://www.icpsr.umich.edu/icpsrweb/content/NACJD/guides/ucr.html).\n\nQuestions/comments/corrections: [email Ben Casselman](mailto:ben.casselman@fivethirtyeight.com)\n",
    "url": "https://github.com/fivethirtyeight/guns-data",
    "last_updated": "2025-04-23T12:02:03+00:00"
  },
  {
    "full_name": "johnmchambers/XR",
    "name": "XR",
    "description": "Structure for Interfaces (from \"Extending R\")",
    "language": "R",
    "topics": [],
    "readme": "# XR - Structure for Interfaces from R\n\nThis package provides a central structure for interfaces from R to\nother languages that share the concept of evaluating calls to\nfunctions or methods and a system of classes for objects.\n\nThe functions and other software in this package will not generally be\nused directly by applications. Instead, other packages will provide\nspecialized interfaces to particular languages built on the common\nstructure but with a programming interface and implementation\ncustomized to that language.\nExamples included in this repository are `XRPython` and `XRJulia`.\n\nThe structure consists of a class of evaluator objects, generic\nfunctions to customize communication with the server language and a\nset of tools to manage evaluators in the session.\n\nThe interface structure is described in the book\n*Extending R* (John M. Chambers, 2016, Chapman & Hall).\nA pdf version of the XR chapter from the book is included with the\ndocumentation of this package.  To open the pdf file from R:\n  `RShowDoc(\"Chapter_XR\", package = \"XR\")`\n\n\n\n",
    "url": "https://github.com/johnmchambers/XR",
    "last_updated": "2024-11-06T19:59:34+00:00"
  },
  {
    "full_name": "andrie/RHadoop-tutorial",
    "name": "RHadoop-tutorial",
    "description": "A tutorial on R and Hadoop, using the RHadoop project",
    "language": "HTML",
    "topics": [],
    "readme": "# RHadoop-tutorial\nA tutorial on R and Hadoop, using the RHadoop project\n\n##Slides\n\n1. [Using R with Hadoop](http://htmlpreview.github.io/?https://github.com/andrie/RHadoop-tutorial/blob/master/1-Using-R-with-Hadoop.html#/)\n2. [Taxi analysis with RHadoop](http://htmlpreview.github.io/?https://github.com/andrie/RHadoop-tutorial/blob/master/2-Taxi-analysis-with-RHadoop.html)\n3. [Computing on distributed matrices](http://htmlpreview.github.io/?https://github.com/andrie/RHadoop-tutorial/blob/master/4-Computing-on-distributed-matrices.html)\n4. [Using hive](http://htmlpreview.github.io/?https://github.com/andrie/RHadoop-tutorial/blob/master/5-hive.html)\n\n",
    "url": "https://github.com/andrie/RHadoop-tutorial",
    "last_updated": "2021-11-05T12:31:09+00:00"
  },
  {
    "full_name": "gojiplus/likes-followers-views",
    "name": "likes-followers-views",
    "description": "Track Facebook Likes, Twitter Followers, YouTube Views",
    "language": "R",
    "topics": [],
    "readme": "### Likes Followers Views\n\nFetch FB likes, Twitter followers, and Youtube channel subscribers and individual video views for a given set of handles. \n\nRun a cronjob to ping regularly.\n\n#### Scripts\n\n**Note** The scripts expect twitter handles, facebook usernames, and youtube ids to be in column names that are in [data/congress.csv](data/congress.csv)\n\n* [Fetch FB Likes](fb_likes.R)\n* [Fetch Twitter Followers](twtr_followers.R)\n* [Fetch YouTube Views](yt_views.R)\n\n#### Setting up a cronjob on Windows\n\n1. Create a .bat file: `\"path\\to\\RScript\" path\\to\\r\\script.R`\n2. Schedule task: `schtasks /create /tn \"name\" /tr path\\to\\batfile.bat /sc daily /st hh:mm:ss`\n\n\n### License\nScripts are released under the [MIT License](License.md).\n",
    "url": "https://github.com/gojiplus/likes-followers-views",
    "last_updated": "2025-04-16T22:23:34+00:00"
  },
  {
    "full_name": "gojiplus/java_ocr_parser_for_factbook",
    "name": "java_ocr_parser_for_factbook",
    "description": "Java OCR and Parser for Warren's TV and Cable Factbook (From 2013)",
    "language": "Java",
    "topics": [],
    "readme": "## OCR and Parse Warren's TV and Cable Factbook\n\nThe [java scripts](src/) are from **2013.**\n\n[Sample input](factbook_sample.pdf) and [output](factbook_out.csv).\n\nDifferent scripts were used to produce the dataset that is published [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GTSTEY).\n\n",
    "url": "https://github.com/gojiplus/java_ocr_parser_for_factbook",
    "last_updated": "2025-04-16T22:18:10+00:00"
  },
  {
    "full_name": "huggingface/transformers",
    "name": "transformers",
    "description": "🤗 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
    "language": "Python",
    "topics": [
      "nlp",
      "natural-language-processing",
      "pytorch",
      "pytorch-transformers",
      "transformer",
      "model-hub",
      "pretrained-models",
      "speech-recognition",
      "hacktoberfest",
      "python",
      "machine-learning",
      "deep-learning",
      "audio",
      "deepseek",
      "gemma",
      "glm",
      "llm",
      "qwen",
      "vlm"
    ],
    "readme": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com",
    "url": "https://github.com/huggingface/transformers",
    "last_updated": "2025-09-02T10:16:33+00:00"
  },
  {
    "full_name": "fghjorth/parlbias",
    "name": "parlbias",
    "description": "Reproduction materials for \"Intergroup Bias in Parliamentary Rule Enforcement\", forthcoming in Political Research Quarterly",
    "language": "HTML",
    "topics": [],
    "readme": "This repo contains reproduction materials for \"Intergroup Bias in Parliamentary Rule Enforcement\", forthcoming in *Political Research Quarterly*. The materials are organized in the following folders:\n\n`analysis` - R scripts for analyzing the data\n\n`data` - Data prepared for analysis\n\n`dataprep` - Scripts for extracting data from html debate transcripts\n\n`manuscript` - Manuscript in TeX format and typeset PDF version\n\n`rawdata` - Original debate transcripts in html format\n\nThe code for the analyses in the paper can be found in the R script file `parlbias_analysis.R` in the `analysis` folder. The analyses use the R data file `ft.rds`, stored in the `data` folder. For those using other statistical software packages, the `data` folder also holds an identical file in csv format, `ft.csv`.\n\n",
    "url": "https://github.com/fghjorth/parlbias",
    "last_updated": "2017-03-10T17:09:29+00:00"
  },
  {
    "full_name": "writefreely/writefreely",
    "name": "writefreely",
    "description": "A clean, Markdown-based publishing platform made for writers. Write together and build a community.",
    "language": "Go",
    "topics": [
      "writing",
      "publishing",
      "blogging",
      "web-application",
      "go",
      "activitypub",
      "federation",
      "beta",
      "writefreely",
      "markdown",
      "writers"
    ],
    "readme": "&nbsp;\n<p align=\"center\">\n\t<a href=\"https://writefreely.org\"><img src=\"https://writefreely.org/img/writefreely.svg\" width=\"350px\" alt=\"WriteFreely\" /></a>\n</p>\n<hr />\n<p align=\"center\">\n\t<a href=\"https://github.com/writefreely/writefreely/releases/\">\n\t\t<img src=\"https://img.shields.io/github/release/writefreely/writefreely.svg\" alt=\"Latest release\" />\n\t</a>\n\t<a href=\"https://github.com/writefreely/writefreely/releases/latest\">\n\t\t<img src=\"https://img.shields.io/github/downloads/writefreely/writefreely/total.svg\" />\n\t</a>\n\t<a href=\"https://goreportcard.com/report/github.com/writefreely/writefreely\">\n\t\t<img src=\"https://goreportcard.com/badge/github.com/writefreely/writefreely\" alt=\"Go Report Card\" />\n\t</a>\n\t<a href=\"https://ghcr.io/writefreely/writefreely\">\n\t\t<img src=\"https://img.shields.io/badge/docker-%230db7ed.svg?logo=docker&logoColor=white\" />\n\t</a>\n\t<a href=\"https://github.com/writefreely/writefreely/actions/workflows/docker-publish.yml\">\n\t\t<img src=\"https://github.com/writefreely/writefreely/actions/workflows/docker-publish.yml/badge.svg\" alt=\"Build container image, publish as GitHub-package\" />\n\t</a>\n</p>\n&nbsp;\n\nWriteFreely is a clean, minimalist publishing platform made for writers. Start a blog, share knowledge within your organization, or build a community around the shared act of writing.\n\n![Screenshot of the Reader view of a WriteFreely instance, pen.writefree.ly.](https://files.writefreely.org/img/screens/pen-reader.png)\n\n[Try the writing experience](https://write.as/new)\n\n[Find an instance](https://writefreely.org/instances)\n\n## Features\n\n### Made for writing\n\nBuilt on a plain, auto-saving editor, WriteFreely gives you a distraction-free writing environment. Once published, your words are front and center, and easy to read.\n\n### A connected community\n\nStart writing together, publicly or privately. Connect with other communities, whether running WriteFreely, [Plume](https://joinplu.me/), or other ActivityPub-powered software. And bring members on boar",
    "url": "https://github.com/writefreely/writefreely",
    "last_updated": "2025-09-01T23:23:36+00:00"
  },
  {
    "full_name": "intel/hyperscan",
    "name": "hyperscan",
    "description": "High-performance regular expression matching library",
    "language": "C++",
    "topics": [
      "regex"
    ],
    "readme": "# Hyperscan\n\nHyperscan is a high-performance multiple regex matching library. It follows the\nregular expression syntax of the commonly-used libpcre library, but is a\nstandalone library with its own C API.\n\nHyperscan uses hybrid automata techniques to allow simultaneous matching of\nlarge numbers (up to tens of thousands) of regular expressions and for the\nmatching of regular expressions across streams of data.\n\nHyperscan is typically used in a DPI library stack.\n\n# Documentation\n\nInformation on building the Hyperscan library and using its API is available in\nthe [Developer Reference Guide](http://intel.github.io/hyperscan/dev-reference/).\n\n# License\n\nHyperscan is licensed under the BSD License. See the LICENSE file in the\nproject repository.\n\n# Versioning\n\nThe `master` branch on Github will always contain the most recent release of\nHyperscan. Each version released to `master` goes through QA and testing before\nit is released; if you're a user, rather than a developer, this is the version\nyou should be using.\n\nFurther development towards the next release takes place on the `develop`\nbranch.\n\n# Get Involved\n\nThe official homepage for Hyperscan is at [www.hyperscan.io](https://www.hyperscan.io).\n\nIf you have questions or comments, we encourage you to [join the mailing\nlist](https://lists.01.org/mailman/listinfo/hyperscan). Bugs can be filed by\nsending email to the list, or by creating an issue on Github.\n\nIf you wish to contact the Hyperscan team at Intel directly, without posting\npublicly to the mailing list, send email to\n[hyperscan@intel.com](mailto:hyperscan@intel.com).\n",
    "url": "https://github.com/intel/hyperscan",
    "last_updated": "2025-09-02T08:55:37+00:00"
  },
  {
    "full_name": "mosaicml/composer",
    "name": "composer",
    "description": "Supercharge Your Model Training",
    "language": "Python",
    "topics": [
      "deep-learning",
      "pytorch",
      "neural-networks",
      "ml-systems",
      "ml-efficiency",
      "ml-training",
      "machine-learning",
      "neural-network"
    ],
    "readme": "<br />\n<p align=\"center\">\n    <a href=\"https://github.com/mosaicml/composer#gh-light-mode-only\" class=\"only-light\">\n      <img src=\"./docs/source/_static/logo-light-mode.png\" width=\"50%\"/>\n    </a>\n    <!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN -->\n    <a href=\"https://github.com/mosaicml/composer#gh-dark-mode-only\" class=\"only-dark\">\n      <img src=\"./docs/source/_static/logo-dark-mode.png\" width=\"50%\"/>\n    </a>\n    <!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END -->\n</p>\n\n<h2><p align=\"center\">Supercharge your Model Training</p></h2>\n<h3><p align=\"center\">Deep Learning Framework for Training at Scale</p></h3>\n\n<h4><p align='center'>\n<a href=\"https://www.mosaicml.com\">[Website]</a>\n- <a href=\"https://docs.mosaicml.com/projects/composer/en/stable/getting_started/installation.html\">[Getting Started]</a>\n- <a href=\"https://docs.mosaicml.com/projects/composer/\">[Docs]</a>\n- <a href=\"https://www.databricks.com/company/careers/open-positions?department=Mosaic%20AI&location=all\">[We're Hiring!]</a>\n</p></h4>\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/mosaicml/\">\n        <img alt=\"PyPi Version\" src=\"https://img.shields.io/pypi/pyversions/mosaicml\">\n    </a>\n    <a href=\"https://pypi.org/project/mosaicml/\">\n        <img alt=\"PyPi Package Version\" src=\"https://img.shields.io/pypi/v/mosaicml\">\n    </a>\n    <a href=\"https://pepy.tech/project/mosaicml/\">\n        <img alt=\"PyPi Downloads\" src=\"https://static.pepy.tech/personalized-badge/mosaicml?period=month&units=international_system&left_color=grey&right_color=blue&left_text=Downloads/month\">\n    </a>\n    <a href=\"https://docs.mosaicml.com/projects/composer/en/stable/\">\n        <img alt=\"Documentation\" src=\"https://readthedocs.org/projects/composer/badge/?version=stable\">\n    </a>\n    <a href=\"https://dub.sh/mcomm\">\n        <img alt=\"Chat @ Slack\" src=\"https://img.shields.io/badge/slack-chat-2eb67d.svg?logo=slack\">\n    </a>\n    <a href=\"https://github.com/mosaicml/composer/blob/dev/LICENSE\">\n        <img alt=\"Licens",
    "url": "https://github.com/mosaicml/composer",
    "last_updated": "2025-09-02T05:46:21+00:00"
  },
  {
    "full_name": "realpython/python-guide",
    "name": "python-guide",
    "description": "Python best practices guidebook, written for humans. ",
    "language": "Batchfile",
    "topics": [
      "python",
      "guide",
      "book"
    ],
    "readme": "Hitchhiker's Guide to Python\n============================\n\n**Python Best Practices Guidebook**\n\n→ Read the free guide at: `docs.python-guide.org <https://docs.python-guide.org>`_\n\n.. image:: https://farm1.staticflickr.com/628/33173824932_58add34581_k_d.jpg\n\n-----------\n\n**Work in progress. If you'd like to help, please do. There's a lot of work to\nbe done.**\n\nThis guide is currently under heavy development. This opinionated guide\nexists to provide both novice and expert Python developers a best practice\nhandbook to the installation, configuration, and usage of Python on a daily\nbasis.\n\n\nTopics include:\n\n- Platform and version-specific installations\n- Py2app, Py2exe, bbfreeze, pyInstaller\n- Pip\n- Numpy, scipy, statpy, pyplot, matplotlib\n- Virtualenv\n- Fabric\n- Exhaustive module recommendations, grouped by topic/purpose\n- Which libraries to use for what\n- Server configurations & tools for various web frameworks\n- Documentation: writing it\n- Testing: Jenkins & tox guides\n- How to easily interface ``hg`` from ``git``\n\nIf you aren't fond of reading reStructuredText, there is an\nalmost up-to-date `HTML version at docs.python-guide.org\n<https://docs.python-guide.org>`_.\n",
    "url": "https://github.com/realpython/python-guide",
    "last_updated": "2025-09-02T09:09:09+00:00"
  },
  {
    "full_name": "markwest1972/smart-security-camera",
    "name": "smart-security-camera",
    "description": "A Pi Zero and Motion based webcamera that forwards images to Amazon Web Services for Image Processing",
    "language": "Java",
    "topics": [
      "web-camera",
      "image-analysis",
      "motion",
      "orchestration",
      "javascript",
      "nodejs",
      "aws",
      "aws-s3",
      "aws-lambda",
      "aws-ses",
      "aws-step-function",
      "aws-rekognition",
      "aws-iam",
      "aws-sdk-js",
      "nodemailer",
      "java-8",
      "java",
      "aws-sdk",
      "bucket-policy",
      "arn"
    ],
    "readme": "# smart-security-camera\n\nThis project elevates a [Pi Zero simple webcamera with Motion Detection](https://www.bouvet.no/bouvet-deler/utbrudd/building-a-motion-activated-security-camera-with-the-raspberry-pi-zero) into a smart security camera by adding Cloud based image analysis via [AWS Rekognition](https://aws.amazon.com/rekognition/).\n\nYou can read more about this solution in the following blog posts:\n* [Smarten up Your Pi Zero Web Camera with Image Analysis and Amazon Web Services Part 1](https://www.bouvet.no/bouvet-deler/utbrudd/smarten-up-your-pi-zero-web-camera-with-image-analysis-and-amazon-web-services-part-1).\n* [Smarten up Your Pi Zero Web Camera with Image Analysis and Amazon Web Services Part 2](https://www.bouvet.no/bouvet-deler/utbrudd/smarten-up-your-pi-zero-web-camera-with-image-analysis-and-amazon-web-services-part-2).\n\nYou can also check out [this presentation](https://vimeo.com/233849443) from JavaZone 2017 where I describe the solution.  The [slides from this talk are also available](https://www.slideshare.net/markawest/javazone-2017-building-a-smart-security-camera-with-raspberry-pi-zero-java-and-aws).\n\n## Java or Node.js?\n\nBoth __Java__ and __Node.js__ versions of the AWS Lambda Functions are provided.  Due to naming differences I have also provided seperate Step Function definitions for both the Java and Node.js versions.\n\nHeres another [blogpost that describes the differences between the two versions](https://www.bouvet.no/bouvet-deler/comparing-java-and-node.js-on-aws-lambda).\n\n## Contents\n\n1. **[s3-upload](https://github.com/markwest1972/smart-security-camera/tree/master/s3-upload)**: Handles upload of image files from Pi Zero to Amazon s3.\n2. **[motion-config](https://github.com/markwest1972/smart-security-camera/tree/master/motion-config)**: Configuration files for Motion (running on a Pi Zero).\n3. **[aws-lambda-functions](https://github.com/markwest1972/smart-security-camera/tree/master/aws-lambda-functions)**: Choose between Node.js or",
    "url": "https://github.com/markwest1972/smart-security-camera",
    "last_updated": "2025-08-13T18:09:37+00:00"
  },
  {
    "full_name": "mkearney/textfeatures",
    "name": "textfeatures",
    "description": "👷‍♂️ A simple package for extracting useful features from character objects 👷‍♀️",
    "language": "R",
    "topics": [
      "rstats",
      "machine-learning",
      "feature-extraction",
      "text-mining",
      "r",
      "mkearney-r-package",
      "neural-networks",
      "word2vec",
      "neural-network"
    ],
    "readme": "\n# 👷 textfeatures 👷<img src=\"man/figures/logo.png\" width=\"160px\" align=\"right\" />\n\n[![Build\nstatus](https://travis-ci.org/mkearney/textfeatures.svg?branch=master)](https://travis-ci.org/mkearney/textfeatures)\n[![AppVeyor build\nstatus](https://ci.appveyor.com/api/projects/status/github/mkearney/textfeatures?branch=master&svg=true)](https://ci.appveyor.com/project/mkearney/textfeatures)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/textfeatures)](https://cran.r-project.org/package=textfeatures)\n[![Coverage\nStatus](https://codecov.io/gh/mkearney/textfeatures/branch/master/graph/badge.svg)](https://codecov.io/gh/mkearney/textfeatures?branch=master)\n[![DOI](https://zenodo.org/badge/123046986.svg)](https://zenodo.org/badge/latestdoi/123046986)\n\n![Downloads](https://cranlogs.r-pkg.org/badges/textfeatures)\n![Downloads](https://cranlogs.r-pkg.org/badges/grand-total/textfeatures)\n[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)\n\n> Easily extract useful features from character objects.\n\n## Install\n\nInstall from CRAN.\n\n``` r\n## download from CRAN\ninstall.packages(\"textfeatures\")\n```\n\nOr install the development version from Github.\n\n``` r\n## install from github\ndevtools::install_github(\"mkearney/textfeatures\")\n```\n\n## Usage\n\n### `textfeatures()`\n\nInput a character vector.\n\n``` r\n## vector of some text\nx <- c(\n  \"this is A!\\t sEntence https://github.com about #rstats @github\",\n  \"and another sentence here\", \"THe following list:\\n- one\\n- two\\n- three\\nOkay!?!\"\n)\n\n## get text features\ntextfeatures(x, verbose = FALSE)\n#> # A tibble: 3 x 36\n#>   n_urls n_uq_urls n_hashtags n_uq_hashtags n_mentions n_uq_mentions n_chars n_uq_chars n_commas\n#>    <dbl>     <dbl>      <dbl>         <dbl>      <dbl>         <dbl>   <dbl>      <dbl>    <dbl>\n#> 1  1.15      1.15       1.15          1.15       1.15          1.15    0.243      0.330        0\n#> 2 -0.577    -0.577     -0.577        -0.577     -0.577     ",
    "url": "https://github.com/mkearney/textfeatures",
    "last_updated": "2025-08-26T01:35:03+00:00"
  },
  {
    "full_name": "kosukeimai/fastLink",
    "name": "fastLink",
    "description": "R package fastLink: Fast Probabilistic Record Linkage",
    "language": "R",
    "topics": [],
    "readme": "fastLink: Fast Probabilistic Record Linkage \n===========================================================================================================================================================================================================================================================================================================================================================\n[![CRAN Version](https://www.r-pkg.org/badges/version-last-release/fastLink)](https://CRAN.R-project.org/package=fastLink) [![Build Status](https://travis-ci.org/kosukeimai/fastLink.svg?branch=master)](https://travis-ci.org/kosukeimai/fastLink) ![CRAN downloads](http://cranlogs.r-pkg.org/badges/grand-total/fastLink)\n\nAuthors:\n\n-   [Ted Enamorado](https://www.tedenamorado.com/)\n-   [Ben Fifield](https://www.benfifield.com/)\n-   [Kosuke Imai](https://imai.fas.harvard.edu/)\n\nSuggested citation: \n\nEnamorado, Ted, Benjamin Fifield, and Kosuke Imai. 2017. fastLink: Fast Probabilistic Record Linkage with Missing Data. Version 0.6.\n\nFor a detailed description of the method see:\n\n-   [Using a Probabilistic Model to Assist Merging of Large-scale Administrative Records](https://imai.fas.harvard.edu/research/files/linkage.pdf) *American Political Science Review*\n\nApplications of the method:\n\n-   [Validating Self-reported Turnout by Linking Public Opinion Surveys with Administrative Records](https://imai.fas.harvard.edu/research/files/turnout.pdf) *Public Opinion Quarterly*\n\nTechnical reports:\n\n-   [User’s Guide and Codebook for the ANES 2016 Time Series Voter Validation Supplemental Data](https://www.electionstudies.org/wp-content/uploads/2018/03/anes_timeseries_2016voteval_userguidecodebook.pdf)\n\n-   [User’s Guide and Codebook for the CCES 2016 Voter Validation Supplemental Data](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/2NNA4L)\n\nData:\n\n-   [ANES 2016 Time Series Voter Validation Supplemental Data](https://www.electionstudies.org/studypages/download/datacenter_",
    "url": "https://github.com/kosukeimai/fastLink",
    "last_updated": "2025-08-28T17:27:50+00:00"
  },
  {
    "full_name": "heike/rotations",
    "name": "rotations",
    "description": "R package for 3 dimensional rotations",
    "language": "R",
    "topics": [],
    "readme": "# rotations\n=======================================================\n\nThis package implements tools for working with rotational data: it allows simulation from the most commonly used distributions on the sphere, it estimates different mean and median type estimators for the main direction of a number of rotations, it provides (bootstrap) confidence regions for the estimates, and it allows to visualize rotational data.\n\n## Installation\n\nTo install in your R session use:\n```\nlibrary(devtools)\ninstall_github(\"rotations\",\"heike\") #Do this once\nlibrary(rotations)\n```\n## Data generation\n\nThree-dimensional rotations are determined uniquely by three numbers: two to define an axis and one to specify the rotation of all three dimensions about that axis.  This package follows the uniform-axis random-spin (*UARS*) framework of Bingham et al (2009a) in simulating the axis and angle of rotation independently.  As the UARS name suggests, the axis, sometimes called the *misorientation axis* is chosen uniformly on the unit sphere and the angle of rotation, sometimes called the *misorientation angle*, is choose according to some circular distribution symmetric about zero and in the interval $[-\\pi,\\pi)$.  The distribution chosen is usually referred to as the *angular distribution* as it describes the distributions of the misorientation angles.\n\nTo date there are four angular distributions from which to choose and two parameterizations of rotations.  The former are the uniform, von Mises circular, Cayley and the von Mises-Fisher matrix distributions.  One can describe a rotation in a 3-by-3 *rotation matrix* or a four dimensional vector with unit length called a *quaternion*.  Each will be described below. \n\n### Misorientation angle simulation\nThere are four angular distributions to simulate from.  They are all symmetric about zero and in the interval $[-\\pi,\\pi)$.\n\n* `rhaar(n)`  simulates data from the uniform distribution on the sphere, also called the Haar measure\n* `rvmises(n,kappa",
    "url": "https://github.com/heike/rotations",
    "last_updated": "2022-06-24T20:35:22+00:00"
  },
  {
    "full_name": "kimmobrunfeldt/squint",
    "name": "squint",
    "description": "Makes visual reviews of web app releases easy.",
    "language": "TypeScript",
    "topics": [
      "puppeteer",
      "node",
      "visual",
      "testing",
      "diff",
      "visual-diff",
      "screenshot",
      "crawl"
    ],
    "readme": "[![Status badge](https://github.com/kimmobrunfeldt/squint/actions/workflows/test.yml/badge.svg?branch=main)](https://github.com/kimmobrunfeldt/squint/actions?query=branch%3Amain)\n\n[![NPM](https://nodei.co/npm/squint-cli.png?compact=true)](https://npmjs.org/package/squint-cli)\n\n# Squint\n\n> Makes visual reviews of web app releases easy.\n\n![Example terminal usage](https://raw.githubusercontent.com/kimmobrunfeldt/squint/main/docs/terminal.gif)\n\n`squint compare https://prod.myapp.com https://beta.myapp.com` uses Puppeteer to:\n\n* automatically crawl all url paths from the beta version *(see [this issue](https://github.com/kimmobrunfeldt/squint/issues/2))*\n* take screenshots of each page from prod and beta versions of the app\n* output all diff images for pages that had visual differences\n\nThat's the main intended use case. The diffs will likely\nhave false positives due to async loading, animations, and different data.\nThat's ok, the main intention is not to be a full solution, but rather a light-weight\nalternative.\n\nFor most production setups, I'd recommend using [Percy](https://percy.io/) instead.\nThat said, CLI flags have been designed customization in mind: you can for example run\ncustom JS code before Puppeteer takes a screenshot.\n\n**Goodies:**\n\n* [Puppeteer](https://github.com/puppeteer/puppeteer) under the hood\n* Smart defaults but [highly configurable](#usage).\n\n    The goal is to provide most convenience flags via CLI, but allow flexible JS options for advanced tricks.\n\n* Supports [connecting to your local Chrome](#connect-to-a-chrome-session), with its existing logins and sessions. No need to deal with cookies in code.\n* Tested on all platforms: macOS, Windows, and Linux\n\n![Example image of diff](https://raw.githubusercontent.com/kimmobrunfeldt/squint/main/docs/diff.png)\n\n## Known limitations\n\nThe path structure needs to be 1-to-1 match between the two sites. If you have preview builds for example under\n`https://prod.myapp.com/previews/123/...`, this won't work au",
    "url": "https://github.com/kimmobrunfeldt/squint",
    "last_updated": "2025-08-17T06:33:24+00:00"
  },
  {
    "full_name": "delta-io/delta",
    "name": "delta",
    "description": "An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs",
    "language": "Scala",
    "topics": [
      "spark",
      "acid",
      "big-data",
      "analytics",
      "delta-lake"
    ],
    "readme": "<img src=\"https://docs.delta.io/latest/_static/delta-lake-white.png\" width=\"200\" alt=\"Delta Lake Logo\"></img>\n\n[![Test](https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg)](https://github.com/delta-io/delta/actions/workflows/test.yaml)\n[![License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg)](https://github.com/delta-io/delta/blob/master/LICENSE.txt)\n[![PyPI](https://img.shields.io/pypi/v/delta-spark.svg)](https://pypi.org/project/delta-spark/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/delta-spark)](https://pypistats.org/packages/delta-spark)\n\nDelta Lake is an open-source storage framework that enables building a [Lakehouse architecture](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python. \n* See the [Delta Lake Documentation](https://docs.delta.io) for details.\n* See the [Quick Start Guide](https://docs.delta.io/latest/quick-start.html) to get started with Scala, Java and Python.\n* Note, this repo is one of many Delta Lake repositories in the [delta.io](https://github.com/delta-io) organizations including\n[delta](https://github.com/delta-io/delta), \n[delta-rs](https://github.com/delta-io/delta-rs),\n[delta-sharing](https://github.com/delta-io/delta-sharing),\n[kafka-delta-ingest](https://github.com/delta-io/kafka-delta-ingest), and\n[website](https://github.com/delta-io/website).\n\nThe following are some of the more popular Delta Lake integrations, refer to [delta.io/integrations](https://delta.io/integrations/) for the complete list:\n\n* [Apache Spark™](https://docs.delta.io/): This connector allows Apache Spark™ to read from and write to Delta Lake.\n* [Apache Flink (Preview)](https://github.com/delta-io/delta/tree/master/connectors/flink): This connector allows Apache Flink to write to Delta Lake.\n* [PrestoDB](https://prestodb.io/docs/current/connector/deltalake.html): This connector allows Prest",
    "url": "https://github.com/delta-io/delta",
    "last_updated": "2025-09-02T04:39:31+00:00"
  },
  {
    "full_name": "piskvorky/gensim",
    "name": "gensim",
    "description": "Topic Modelling for Humans",
    "language": "Python",
    "topics": [
      "gensim",
      "topic-modeling",
      "information-retrieval",
      "machine-learning",
      "natural-language-processing",
      "nlp",
      "data-science",
      "python",
      "data-mining",
      "word2vec",
      "word-embeddings",
      "neural-network",
      "document-similarity",
      "word-similarity",
      "fasttext"
    ],
    "readme": "gensim – Topic Modelling in Python\n==================================\n\n<!--\nThe following image URLs are obfuscated = proxied and cached through\nGoogle because of Github's proxying issues. See:\nhttps://github.com/RaRe-Technologies/gensim/issues/2805\n-->\n\n[![Build Status](https://github.com/RaRe-Technologies/gensim/actions/workflows/tests.yml/badge.svg?branch=develop)](https://github.com/RaRe-Technologies/gensim/actions)\n[![GitHub release](https://img.shields.io/github/release/rare-technologies/gensim.svg?maxAge=3600)](https://github.com/RaRe-Technologies/gensim/releases)\n[![Downloads](https://img.shields.io/pypi/dm/gensim?color=blue)](https://pepy.tech/project/gensim/)\n[![DOI](https://zenodo.org/badge/DOI/10.13140/2.1.2393.1847.svg)](https://doi.org/10.13140/2.1.2393.1847)\n[![Mailing List](https://img.shields.io/badge/-Mailing%20List-blue.svg)](https://groups.google.com/g/gensim)\n[![Follow](https://img.shields.io/twitter/follow/gensim_py.svg?style=social&style=flat&logo=twitter&label=Follow&color=blue)](https://twitter.com/gensim_py)\n\nGensim is a Python library for *topic modelling*, *document indexing*\nand *similarity retrieval* with large corpora. Target audience is the\n*natural language processing* (NLP) and *information retrieval* (IR)\ncommunity.\n\n## ⚠️ Want to help out? [Sponsor Gensim](https://github.com/sponsors/piskvorky) ❤️\n\n## ⚠️ Gensim is in stable maintenance mode: we are not accepting new features, but bug and documentation fixes are still welcome! ⚠️\n\nFeatures\n--------\n\n-   All algorithms are **memory-independent** w.r.t. the corpus size\n    (can process input larger than RAM, streamed, out-of-core),\n-   **Intuitive interfaces**\n    -   easy to plug in your own input corpus/datastream (trivial\n        streaming API)\n    -   easy to extend with other Vector Space algorithms (trivial\n        transformation API)\n-   Efficient multicore implementations of popular algorithms, such as\n    online **Latent Semantic Analysis (LSA/LSI/SVD)**, **Latent\n    Dirich",
    "url": "https://github.com/piskvorky/gensim",
    "last_updated": "2025-09-01T12:53:45+00:00"
  },
  {
    "full_name": "paultopia/mergepdfs",
    "name": "mergepdfs",
    "description": "just merge all the pdfs in a directory in abc order",
    "language": "Clojure",
    "topics": [],
    "readme": "# mergepdf\n\n**DEPRECATED** --- it seems to get the alphabetical order wrong. For Mac users, you might try a rewrite in Swift [in this repo](https://github.com/paultopia/pdfmerge).\n\nSimple personal commandline utility to merge all pdfs in a directory. Only tested on OSX, built using lein-bin. Built because I often find myself with a directory full of PDFs (chapters of bigger writing project, anonymized student work that needs to be compiled and shared, etc.) that need to be merged and it's an annoying hassle to do it with big icky GUI apps or web services.\n\n## Installation\n\nDownload the binary `merge-pdfs` either in the `binary` directory here or from the releases page.  Stick it on your path. Make it executable. \n\n(or use one of the JAR files, or build it yourself, or whev.)\n\n## Usage\n\n    $ merge-pdfs outfile.pdf\n\nwill take every PDF in the directory you run it from and merge them into a file called output.pdf\n\nit's basically alphabetical order, I think it goes in the order that directory listings show up in.\n\nAlso, it will produce a bunch of PDFBox warnings about not closing PDF files. This comes from one of the libraries I use, and I'm not worrying about it.  I like to think that the OS or the JVM's garbage collector will manage to free up the resources left by leaving a few files open. But if you use this to merge LOTS of files, it might be an issue. Sorry about that. :-(\n\n## Options\n\nYou get to specify the filename of the generated file. That's about it.\n\n## License\n\nAll the code here is trivial, and some of it is straight cookbook code, so I'm not claiming any kind of copyright at all. Any bits that I could claim copyright in are hereby committed to the public domain. Ignore any cruft floating around in the files leiningen created to the contrary.\n",
    "url": "https://github.com/paultopia/mergepdfs",
    "last_updated": "2023-09-28T04:11:37+00:00"
  },
  {
    "full_name": "bart6114/scheduleR",
    "name": "scheduleR",
    "description": "An interface to schedule R scripts",
    "language": "JavaScript",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/Bart6114/scheduleR.svg?branch=master)](https://travis-ci.org/Bart6114/scheduleR)\n\n# scheduleR\n\n__scheduleR__ is a framework that can be used to deploy R tasks, reports and apps.\n\n* __Tasks__ are 'regular' R scripts that you want to schedule to be  executed on a regular basis (often ETL related scripts).\n* __Reports__ are Rmarkdown (.Rmd) reports that can be converted to a PDF or HTML. See [rmarkdown](https://github.com/rstudio/rmarkdown) for more info.\n* __Apps__ are [Shiny](http://shiny.rstudio.com/) apps, support for these in scheduleR is experimental.\n\nAn easy web interface for scheduling is provided for adding tasks, maintenance and viewing logs. __scheduleR__ provides extensive logging support and  error/success notifications.\n\n__scheduleR__ is built to be used on a server. It can be used locally but that mean that you have to keep a mongodb server and scheduleR running at all times.\n\n![](http://i.imgur.com/mh1Yeyw.png)\n\n## Requirements\n\nMinimal dependencies:\n\n- [Node.js](http://nodejs.org/) (with npm)\n- [R](http://www.r-project.org/)\n- [mongodb](http://www.mongodb.org/)\n\nOptional dependencies (necessary for generating Rmarkdown reports):\n\n- [Pandoc](http://johnmacfarlane.net/pandoc/) (> v1.13.0)\n- [rmarkdown](https://github.com/rstudio/rmarkdown) (most recent version installed using devtools)\n- [knitr](http://yihui.name/knitr/)\n\n__scheduleR__'s web interface is built using Node.js and tested under GNU/Linux and Windows. Feedback on Mac compatibility is appreciated.\n\n\n## Installation\n\nMake sure you have access to a running [MongoDB](http://www.mongodb.org/) server. You can set one up locally or use a service such as [mongolab](https://mongolab.com/) for testing.\n\nFirst download the repository using the latest [tarball](https://api.github.com/repos/Bart6114/scheduleR/tarball/) / [zip file](https://github.com/Bart6114/scheduleR/archive/master.zip) and extract it or simply clone the repository. Cloning the repository is advis",
    "url": "https://github.com/bart6114/scheduleR",
    "last_updated": "2023-03-18T06:16:55+00:00"
  },
  {
    "full_name": "hrbrmstr/urlscan",
    "name": "urlscan",
    "description": "👀 Analyze Websites and Resources They Request",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "urlscan",
      "analyze-websites",
      "scanning",
      "urlscan-io",
      "r-cyber"
    ],
    "readme": "\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/urlscan.svg?branch=master)](https://travis-ci.org/hrbrmstr/urlscan)\n[![Coverage\nStatus](https://codecov.io/gh/hrbrmstr/urlscan/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/urlscan)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/urlscan)](https://cran.r-project.org/package=urlscan)\n\n# urlscan\n\nAnalyze Websites and Resources They Request\n\n## Description\n\nThe \\<urlscan.io\\> service provides an ‘API’ enabling analysis of\nwebsites and the resources they request. Much like the ‘Inspector’ of\nyour browser, \\<urlscan.io\\> will let you take a look at the individual\nresources that are requested when a site is loaded. Tools are provided\nto search public \\<urlscans.io\\> scan submissions/results and submit\nURLs for scanning.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `urlscan_search`: Perform a urlscan.io query\n  - `urlscan_result`: Retrieve detailed results for a given scan ID\n  - `urlscan_submit`: Submit a URL for scanning\n\n## Installation\n\n``` r\ndevtools::install_git(\"https://git.sr.ht/~hrbrmstr/urlscan\")\n# or\ndevtools::install_gitlab(\"hrbrmstr/urlscan\")\n# or\ndevtools::install_github(\"hrbrmstr/urlscan\")\n```\n\n## Usage\n\n``` r\nlibrary(urlscan)\nlibrary(tidyverse) # for demos\n\n# current verison\npackageVersion(\"urlscan\")\n```\n\n    ## [1] '0.2.0'\n\n``` r\nx <- urlscan_search(\"domain:r-project.org\")\n\nas_tibble(x$results$task) %>% \n  bind_cols(as_tibble(x$results$page)) %>% \n  mutate(\n    time = anytime::anytime(time),\n    id = x$results$`_id`\n  ) %>%\n  arrange(desc(time)) %>% \n  select(url, country, server, ip, id) -> xdf\n\nures <- urlscan_result(xdf$id[2], include_dom = TRUE, include_shot = TRUE)\n\nures\n```\n\n    ##             URL: https://cran.r-project.org/\n    ##         Scan ID: cdc2b957-548c-447a-a1b2-bebd6a734aec\n    ##       Malicious: FALSE\n    ##      Ad Blocked: FALSE\n    ##     Total Links: 0\n    ## Secure Requests: 9\n    ##    Secure Req %: 100%\n\n``` r\nmagic",
    "url": "https://github.com/hrbrmstr/urlscan",
    "last_updated": "2025-07-28T19:21:43+00:00"
  },
  {
    "full_name": "Deltares/dfm_tools",
    "name": "dfm_tools",
    "description": "A Python package for pre- and postprocessing D-Flow FM model input and output files",
    "language": "Python",
    "topics": [],
    "readme": "[![pytest](https://github.com/Deltares/dfm_tools/actions/workflows/pytest.yml/badge.svg?branch=main)](https://github.com/Deltares/dfm_tools/actions/workflows/pytest.yml)\n[![codecov](https://img.shields.io/codecov/c/github/deltares/dfm_tools.svg?style=flat-square)](https://app.codecov.io/gh/deltares/dfm_tools?displayType=list)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=Deltares_dfm_tools&metric=alert_status)](https://sonarcloud.io/summary/overall?id=Deltares_dfm_tools)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Deltares/dfm_tools/HEAD?urlpath=/tree/docs/notebooks)\n[![Available on pypi](https://img.shields.io/pypi/v/dfm_tools.svg)](https://pypi.python.org/pypi/dfm_tools)\n[![Supported versions](https://img.shields.io/pypi/pyversions/dfm_tools.svg)](https://pypi.org/project/dfm_tools)\n[![Downloads](https://img.shields.io/pypi/dm/dfm_tools.svg)](https://pypistats.org/packages/dfm_tools)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7857393.svg)](https://doi.org/10.5281/zenodo.7857393)\n\n# dfm_tools\n\nA Python package for pre- and postprocessing D-FlowFM model input and output files. Contains convenience functions built on top of other packages like [xarray](https://github.com/pydata/xarray), [xugrid](https://github.com/Deltares/xugrid), [hydrolib-core](https://github.com/Deltares/HYDROLIB-core) and many more.\n\n## Information\n\n- install with `pip install dfm_tools -U` (or [installation guide](https://deltares.github.io/dfm_tools/installation))\n- [online documentation](https://deltares.github.io/dfm_tools) with installation guide, tutorials/examples, API reference and contributing guide.\n- Bug or feature request? Create a [GitHub issue](https://github.com/Deltares/dfm_tools/issues)\n- dfm_tools is updated regularly to keep up with all developments in packages it depends on, so make sure to update regularly. You can subscribe to dfm_tools updates via the \"Watch\" button of this repository (tip: only ti",
    "url": "https://github.com/Deltares/dfm_tools",
    "last_updated": "2025-09-01T14:27:55+00:00"
  },
  {
    "full_name": "nalepae/pandarallel",
    "name": "pandarallel",
    "description": "A simple and efficient tool to parallelize Pandas operations on all available CPUs",
    "language": "Python",
    "topics": [
      "pandas",
      "python",
      "parallel"
    ],
    "readme": "# Pandaral·lel\n\n[![PyPI version fury.io](https://badge.fury.io/py/pandarallel.svg)](https://pypi.python.org/pypi/pandarallel/)\n[![PyPI license](https://img.shields.io/pypi/l/pandarallel.svg)](https://pypi.python.org/pypi/pandarallel/)\n[![PyPI download month](https://img.shields.io/pypi/dm/pandarallel.svg)](https://pypi.python.org/pypi/pandarallel/)\n\n| Without parallelization  | ![Without Pandarallel](https://github.com/nalepae/pandarallel/blob/master/docs/progress_apply.gif?raw=true)       |\n| :----------------------: | ----------------------------------------------------------------------------------------------------------------- |\n| **With parallelization** | ![With Pandarallel](https://github.com/nalepae/pandarallel/blob/master/docs/progress_parallel_apply.gif?raw=true) |\n\n**Pandaral.lel** provides a simple way to parallelize your pandas operations on all your\nCPUs by changing only one line of code. It also displays progress bars.\n\n## **⚠️ Pandaral·lel is looking for a maintainer! ⚠️**\n\nIf you are interested, please contact me or open a GitHub issue.\n\n## Maintainers\n- [Manu NALEPA](https://github.com/nalepae)\n\n## Former maintainers (thanks a lot for your work!)\n- [till-m](https://github.com/till-m)\n\n## Installation\n\n```bash\npip install pandarallel [--upgrade] [--user]\n```\n\n## Quickstart\n\n```python\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True)\n\n# df.apply(func)\ndf.parallel_apply(func)\n```\n\n## Usage\n\nBe sure to check out the [documentation](https://nalepae.github.io/pandarallel).\n\n## Examples\n\nAn example of each available `pandas` API is available:\n\n- For [Mac & Linux](https://github.com/nalepae/pandarallel/blob/master/docs/examples_mac_linux.ipynb)\n- For [Windows](https://github.com/nalepae/pandarallel/blob/master/docs/examples_windows.ipynb)\n",
    "url": "https://github.com/nalepae/pandarallel",
    "last_updated": "2025-08-29T18:06:29+00:00"
  },
  {
    "full_name": "davharris/mistnet",
    "name": "mistnet",
    "description": "stochastic neural networks in R",
    "language": "R",
    "topics": [],
    "readme": "mistnet: Structured prediction with neural networks in R\n=========\n\n[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.12423.png)](http://dx.doi.org/10.5281/zenodo.12423) \n[![Travis-CI Build Status](https://travis-ci.org/davharris/mistnet.svg?branch=master)](https://travis-ci.org/davharris/mistnet)\n\nMistnet is an R package that produces probability densities over multivariate outcomes.  Ecologists can use it to define probability densities over possible species assemblages, as described in [this paper](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12332/full) I wrote for *Methods and Ecology and Evolution*.\n\nMistnet models are *stochastic* neural networks, meaning that they include stochastic latent variables (like random effects) that account for correlations among the outcome variables that cannot be explained by the inputs.\n\nThe model uses a Generalized Expectation Maximization approach to model fitting (maximized penalized likelihood), as described in [this paper](http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.pdf) from Tang and Salakhutdinov at NIPS 2013 and in the *Methods* paper referred to above.\n",
    "url": "https://github.com/davharris/mistnet",
    "last_updated": "2025-06-24T08:11:53+00:00"
  },
  {
    "full_name": "brianlovin/security-checklist",
    "name": "security-checklist",
    "description": "A checklist for staying safe on the internet",
    "language": "JavaScript",
    "topics": [
      "javascript",
      "react",
      "nextjs",
      "styled-components",
      "security-tools",
      "privacy-protection"
    ],
    "readme": "# Security checklist\nA checklist for staying safe on the internet.\n\n### This project has moved\nIn the interest of maintaining fewer services, codebases, and domains, I've integrated this project into my [personal website](https://github.com/brianlovin/brian-lovin-next). View the live project at [brianlovin.com/security](https://brianlovin.com/security).\n\n### Motivation\nThis project is the result of a conversation started during a [recent episode](https://spec.fm/podcasts/design-details/249464) of the [Design Details Podcast](https://spec.fm/podcasts/design-details/) and a subsequent tweet by [Michael Knepprath](https://twitter.com/mknepprath/status/1083966912420372481).\n\n### Contributing\nThis project should be considered a living document of resources and applications that improve people's digital security and privacy. Contributions, edits, and extensions are welcome!\n\nIf you have resources to add to existing sections, please open a pull request.\n\n- Aim for reputable sources for guides and news coverage.\n- If adding an app, please include links to as many platforms as possible. See `config/*.js` for examples of how data is formatted.\n- Try to use approachable human-readable language. Remember that even non-technical folks need to stay safe online.\n\nIf you would like to create a new category of security and privacy resources, please open an issue first with your proposed category. Please explain why this additional category should stand alone from other existing sections.\n\n### Run this locally\n1. `$ git clone git@github.com:brianlovin/security-checklist.git`\n2. `$ cd security-checklist`\n3. `$ npm install`\n4. `$ npm run dev`\n5. View the running app in your browser at `http://localhost:3000`\n\n### Deploying\nYou can deploy this project yourself with ZEIT + Now by configuring `now.json` and running `$ now`.\n\n### Feedback\nPlease open issues at any time for general feedback, or you can reach me directly at hi@brianlovin.com.\n",
    "url": "https://github.com/brianlovin/security-checklist",
    "last_updated": "2025-07-16T23:17:32+00:00"
  },
  {
    "full_name": "tensorflow/models",
    "name": "models",
    "description": "Models and examples built with TensorFlow",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n  <img src=\"https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)\n[![tf-models-official PyPI](https://badge.fury.io/py/tf-models-official.svg)](https://badge.fury.io/py/tf-models-official)\n\n\n# Welcome to the Model Garden for TensorFlow\n\nThe TensorFlow Model Garden is a repository with a number of different\nimplementations of state-of-the-art (SOTA) models and modeling solutions for\nTensorFlow users. We aim to demonstrate the best practices for modeling so that\nTensorFlow users can take full advantage of TensorFlow for their research and\nproduct development.\n\nTo improve the transparency and reproducibility of our models, training logs on\n[TensorBoard.dev](https://tensorboard.dev) are also provided for models to the\nextent possible though not all models are suitable.\n\n| Directory | Description |\n|-----------|-------------|\n| [official](official) | • A collection of example implementations for SOTA models using the latest TensorFlow 2's high-level APIs<br />• Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow<br />• Reasonably optimized for fast performance while still being easy to read<br /> For more details on the capabilities, check the guide on the [Model-garden](https://www.tensorflow.org/tfmodels)|\n| [research](research) | • A collection of research model implementations in TensorFlow 1 or 2 by researchers<br />• Maintained and supported by researchers |\n| [community](community) | • A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2 |\n| [orbit](orbit) | • A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with `tf.distribute` and supports running on different device types (CPU, GPU,",
    "url": "https://github.com/tensorflow/models",
    "last_updated": "2025-09-01T19:10:26+00:00"
  },
  {
    "full_name": "kern/filepizza",
    "name": "filepizza",
    "description": ":pizza: Peer-to-peer file transfers in your browser",
    "language": "TypeScript",
    "topics": [
      "webrtc",
      "react"
    ],
    "readme": "<a href=\"https://xkcd.com/949/\"><img src=\"http://imgs.xkcd.com/comics/file_transfer.png\" alt=\"XKCD 949\" width=\"30%\" align=\"right\" /></a> <img src=\"public/images/wordmark.png\" alt=\"FilePizza wordmark\" width=\"50%\" /> <h3>Peer-to-peer file transfers in your browser</h3>\n\n*Cooked up by [Alex Kern](https://kern.io) & [Neeraj Baid](https://github.com/neerajbaid) while eating Sliver @ UC Berkeley.*\n\nUsing [WebRTC](http://www.webrtc.org), FilePizza eliminates the initial upload step required by other web-based file sharing services. Because data is never stored in an intermediary server, the transfer is fast, private, and secure.\n\nA hosted instance of FilePizza is available at [file.pizza](https://file.pizza).\n\n## What's new with FilePizza v2\n\n* A new UI with dark mode support, now built on modern browser technologies.\n* Works on most mobile browsers, including Mobile Safari.\n* Transfers are now directly from the uploader to the downloader's browser (WebRTC without WebTorrent) with faster handshakes.\n* Uploaders can monitor the progress of the transfer and stop it if they want.\n* Better security and safety measures with password protection and reporting.\n* Support for uploading multiple files at once, which downloaders receive as a zip file.\n* Streaming downloads with a Service Worker.\n* Out-of-process storage of server state using Redis.\n\n## Development\n\n```\n$ git clone https://github.com/kern/filepizza.git\n$ pnpm install\n$ pnpm dev\n$ pnpm build\n$ pnpm start\n```\n\n## Running with Docker\n\n```\n$ pnpm docker:build\n$ pnpm docker:up\n$ pnpm docker:down\n```\n\n## Stack\n\n* Next.js\n* Tailwind\n* TypeScript\n* React\n* PeerJS for WebRTC\n* View Transitions\n* Redis (optional)\n\n## Configuration\n\nThe server can be customized with the following environment variables:\n\n- `REDIS_URL` – Connection string for a Redis instance used to store channel metadata. If not set, FilePizza falls back to in-memory storage.\n- `COTURN_ENABLED` – When set to `true`, enables TURN support for connecting peers behi",
    "url": "https://github.com/kern/filepizza",
    "last_updated": "2025-09-02T06:01:24+00:00"
  },
  {
    "full_name": "dedupeio/dedupe",
    "name": "dedupe",
    "description": ":id: A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.",
    "language": "Python",
    "topics": [
      "dedupe",
      "record-linkage",
      "python",
      "python-library",
      "entity-resolution",
      "dedupe-library",
      "de-duplicating",
      "datamade",
      "clustering"
    ],
    "readme": "# Dedupe Python Library\n\n[![Tests Passing](https://github.com/dedupeio/dedupe/workflows/tests/badge.svg)](https://github.com/dedupeio/dedupe/actions?query=workflow%3Atests)[![codecov](https://codecov.io/gh/dedupeio/dedupe/branch/main/graph/badge.svg?token=aauKUrTEgh)](https://codecov.io/gh/dedupeio/dedupe)\n\n_dedupe is a python library that uses machine learning to perform fuzzy matching, deduplication and entity resolution quickly on structured data._\n\n__dedupe__ will help you: \n\n* __remove duplicate entries__ from a spreadsheet of names and addresses\n* __link a list__ with customer information to another with order history, even without unique customer IDs\n* take a database of campaign contributions and __figure out which ones were made by the same person__, even if the names were entered slightly differently for each record\n\ndedupe takes in human training data and comes up with the best rules for your dataset to quickly and automatically find similar records, even with very large databases.\n\n## Important links\n* Documentation: https://docs.dedupe.io/\n* Repository: https://github.com/dedupeio/dedupe\n* Issues: https://github.com/dedupeio/dedupe/issues\n* Mailing list: https://groups.google.com/forum/#!forum/open-source-deduplication\n* Examples: https://github.com/dedupeio/dedupe-examples\n\n## dedupe library consulting\n\nIf you or your organization would like professional assistance in working with the dedupe library, Dedupe.io LLC offers consulting services. [Read more about pricing and available services here](https://dedupe.io/pricing/#consulting).\n\n## Tools built with dedupe\n\n### [Dedupe.io](https://dedupe.io/)\nA cloud service powered by the dedupe library for de-duplicating and finding matches in your data. It provides a step-by-step wizard for uploading your data, setting up a model, training, clustering and reviewing the results.\n\n[Dedupe.io](https://dedupe.io/) also supports record linkage across data sources and continuous matching and training through an [API]",
    "url": "https://github.com/dedupeio/dedupe",
    "last_updated": "2025-08-26T13:25:23+00:00"
  },
  {
    "full_name": "terryum/awesome-deep-learning-papers",
    "name": "awesome-deep-learning-papers",
    "description": "The most cited deep learning papers",
    "language": "TeX",
    "topics": [
      "deep-learning",
      "deep-neural-networks",
      "machine-learning"
    ],
    "readme": "# Awesome - Most Cited Deep Learning Papers\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.\n\nA curated list of the most cited deep learning papers (2012-2016)\n\nWe believe that there exist *classic* deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a *curated list* of the awesome deep learning papers which are considered as *must-reads* in certain research domains.\n\n## Background\n\nBefore this list, there exist other *awesome deep learning lists*, for example, [Deep Vision](https://github.com/kjw0612/awesome-deep-vision) and [Awesome Recurrent Neural Networks](https://github.com/kjw0612/awesome-rnn). Also, after this list comes out, another awesome list for deep learning beginners, called [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap), has been created and loved by many deep learning researchers.\n\nAlthough the *Roadmap List* includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce **top 100 deep learning papers** here as a good starting point of overviewing deep learning researches.\n\nTo get the news for newly released papers everyday, follow my [twitter](https://twitter.com/TerryUm_ML) or [facebook page](https://www.facebook.com/terryum.io/)! \n\n## Awesome list criteria\n\n1. A list of **top 100 deep learning papers** published from 2012 to 2016 is suggested.\n2. If a paper is added to the list, another paper (usually from *More Papers from 2016\" section) should b",
    "url": "https://github.com/terryum/awesome-deep-learning-papers",
    "last_updated": "2025-09-02T08:09:59+00:00"
  },
  {
    "full_name": "OCA/stock-logistics-warehouse",
    "name": "stock-logistics-warehouse",
    "description": "Odoo Warehouse Management Addons",
    "language": "HTML",
    "topics": [
      "hacktoberfest",
      "erp",
      "odoo",
      "python"
    ],
    "readme": "\n[![Runboat](https://img.shields.io/badge/runboat-Try%20me-875A7B.png)](https://runboat.odoo-community.org/builds?repo=OCA/stock-logistics-warehouse&target_branch=18.0)\n[![Pre-commit Status](https://github.com/OCA/stock-logistics-warehouse/actions/workflows/pre-commit.yml/badge.svg?branch=18.0)](https://github.com/OCA/stock-logistics-warehouse/actions/workflows/pre-commit.yml?query=branch%3A18.0)\n[![Build Status](https://github.com/OCA/stock-logistics-warehouse/actions/workflows/test.yml/badge.svg?branch=18.0)](https://github.com/OCA/stock-logistics-warehouse/actions/workflows/test.yml?query=branch%3A18.0)\n[![codecov](https://codecov.io/gh/OCA/stock-logistics-warehouse/branch/18.0/graph/badge.svg)](https://codecov.io/gh/OCA/stock-logistics-warehouse)\n[![Translation Status](https://translation.odoo-community.org/widgets/stock-logistics-warehouse-18-0/-/svg-badge.svg)](https://translation.odoo-community.org/engage/stock-logistics-warehouse-18-0/?utm_source=widget)\n\n<!-- /!\\ do not modify above this line -->\n\n# Stock Warehouse\n\nExtend the stock related models (warehouse, location, picking, move...) but without impact flows and processes. It's mainly adding fields or buttons.\n\nAre you looking for modules related to logistics? Or would like to contribute\nto? There are many repositories with specific purposes. Have a look at this\n[README](https://github.com/OCA/wms/blob/18.0/README.md).\n\n<!-- /!\\ do not modify below this line -->\n\n<!-- prettier-ignore-start -->\n\n[//]: # (addons)\n\nAvailable addons\n----------------\naddon | version | maintainers | summary\n--- | --- | --- | ---\n[account_move_line_stock_info](account_move_line_stock_info/) | 18.0.1.0.0 |  | Account Move Line Stock Info\n[procurement_auto_create_group](procurement_auto_create_group/) | 18.0.1.0.1 |  | Allows to configure the system to propose automatically new procurement groups during the procurement run.\n[stock_archive_constraint](stock_archive_constraint/) | 18.0.1.0.0 | <a href='https://github.com/victoralma",
    "url": "https://github.com/OCA/stock-logistics-warehouse",
    "last_updated": "2025-09-01T07:24:25+00:00"
  },
  {
    "full_name": "rmcelreath/stat_rethinking_2022",
    "name": "stat_rethinking_2022",
    "description": "Statistical Rethinking course winter 2022",
    "language": "R",
    "topics": [],
    "readme": "<img src=\"title.gif\" width=\"500\">\n\nStatistical Rethinking (2022 Edition)\n===============\n\nInstructor: Richard McElreath\n\nLectures: Uploaded <[Playlist](https://www.youtube.com/playlist?list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN)> and pre-recorded, two per week\n\nDiscussion: Online, Fridays 3pm-4pm Central European Time\n\n# Purpose\n\nThis course teaches data analysis, but it focuses on scientific models first. The unfortunate truth about data is that nothing much can be done with it, until we say what caused it. We will prioritize conceptual, causal models and precise questions about those models. We will use Bayesian data analysis to connect scientific models to evidence. And we will learn powerful computational tools for coping with high-dimension, imperfect data of the kind that biologists and social scientists face.\n\n# Format\n\nOnline, flipped instruction. The lectures are pre-recorded. We'll meet online once a week for an hour to work through the solutions to the assigned problems.\n\nWe'll use the 2nd edition of my book, <[Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)>. I'll provide a PDF of the book to enrolled students.\n\nRegistration: Please sign up via <[COURSE IS FULL SORRY]>. I've also set aside 100 audit tickets at the same link, for people who want to participate, but who don't need graded work and course credit.\n\n# Calendar & Topical Outline\n\nThere are 10 weeks of instruction. Links to lecture recordings will appear in this table. Weekly problem sets are assigned on Fridays and due the next Friday, when we discuss the solutions in the weekly online meeting.\n\nLecture playlist on Youtube: <[Statistical Rethinking 2022](https://www.youtube.com/playlist?list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN)>\n\n[//]: # (11 Feb SPP conflict , 25 Feb Winter Break conflict )\n\n| Week ## | Meeting date | Reading | Lectures |\n| ------- | -------------- | ------------- | ---------------------- |\n| Week 01 | 07 January  | Chapters 1, 2 and 3 | [1] <[The Golem of Pr",
    "url": "https://github.com/rmcelreath/stat_rethinking_2022",
    "last_updated": "2025-08-14T14:50:03+00:00"
  },
  {
    "full_name": "Hootrix/text-box-wrapper",
    "name": "text-box-wrapper",
    "description": "给指定文本添加ASCII码字符边框",
    "language": "Python",
    "topics": [
      "ascii",
      "ascii-art",
      "python3"
    ],
    "readme": "# Text Box Wrapper\n\nA simple Python package to wrap text with ASCII art or other characters. It also supports custom alignment (left, center, or right) and can be used as a decorator.\n\n## Installation\n\nInstall the package using pip:\n\n```bash\npip install text-box-wrapper\n```\n\n# Usage\n## Basic usage\nImport the `wrap_with_ascii_art` function and use it to wrap your text:\n```python\nfrom text_box_wrapper import wrap_with_ascii_art\n\ntext = \"Hello, World!\"\nwrapped_text = wrap_with_ascii_art(text)\nprint(wrapped_text)\n\n```\n## Example output:\n\n```\n###########################################\n###########################################\n#####                                 #####\n#####                                 #####\n#####          Hello, World!          #####\n#####                                 #####\n#####                                 #####\n###########################################\n###########################################\n```\n\n# Customizing the wrapper\nYou can customize the padding, border string, \nand alignment:\n```\nfrom text_box_wrapper import wrap_with_ascii_art\n\ntext = \"你好，成都\"\nborder_string = \"#\"\nwrapped_text = wrap_with_ascii_art(text, min_padding=5, vertical_padding=2, border_string=border_string, alignment=\"center\")\nprint(wrapped_text)\n\n```\n## Example output:\n\n```\n######################\n#                    #\n#                    #\n#     你好，成都       #\n#                    #\n#                    #\n######################\n```\n\n# Using the decorator\nYou can also use the `wrap` decorator to automatically wrap the output of a function:\n\n```python3\nfrom text_box_wrapper import wrap\n\n@wrap(min_padding=7, vertical_padding=1, border_string=\"*\", alignment=\"center\")\ndef greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"John\"))\n\n```\n\n## Example output:\n\n```\n****************************\n*                          *\n*       Hello, John!       *\n*                          *\n****************************\n```\n\n# Contributing\nFeel free to submit issues or pull requests",
    "url": "https://github.com/Hootrix/text-box-wrapper",
    "last_updated": "2025-01-15T15:28:42+00:00"
  },
  {
    "full_name": "juliasilge/widyr",
    "name": "widyr",
    "description": "Widen, process, and re-tidy a dataset",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# widyr: Widen, process, and re-tidy a dataset\n\n**Authors:** [Julia Silge](https://juliasilge.com/), [David\nRobinson](http://varianceexplained.org/)<br/> **License:**\n[MIT](https://opensource.org/licenses/MIT)\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/juliasilge/widyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/juliasilge/widyr/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/widyr)](https://cran.r-project.org/package=widyr)\n[![Codecov test\ncoverage](https://codecov.io/gh/juliasilge/widyr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/juliasilge/widyr?branch=main)\n<!-- badges: end -->\n\nThis package wraps the pattern of un-tidying data into a wide matrix,\nperforming some processing, then turning it back into a tidy form. This\nis useful for several mathematical operations such as co-occurrence\ncounts, correlations, or clustering that are best done on a wide matrix.\n\n## Installation\n\nYou can install the released version of widyr from\n[CRAN](https://CRAN.R-project.org) with:\n\n``` r\ninstall.packages(\"widyr\")\n```\n\nAnd the development version from [GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"juliasilge/widyr\")\n```\n\n## Towards a precise definition of “wide” data\n\nThe term “wide data” has gone out of fashion as being “imprecise”\n[(Wickham 2014)](http://vita.had.co.nz/papers/tidy-data.pdf), but I\nthink with a proper definition the term could be entirely meaningful and\nuseful.\n\nA **wide** dataset is one or more matrices where:\n\n-   Each row is one **item**\n-   Each column is one **feature**\n-   Each value is one **observation**\n-   Each matrix is one **variable**\n\nWhen would you want data to be wide rather than tidy? Notable examples\ninclude classification, clustering, correlation, factorization, or other\noperations that can take advantage of a matrix structu",
    "url": "https://github.com/juliasilge/widyr",
    "last_updated": "2025-08-28T20:31:40+00:00"
  },
  {
    "full_name": "jlevy/og-equity-compensation",
    "name": "og-equity-compensation",
    "description": "Stock options, RSUs, taxes — read the latest edition: www.holloway.com/ec",
    "language": "",
    "topics": [],
    "readme": "# The Open Guide to Equity Compensation\n\n❇️ *This guide is now [published on Holloway](https://www.holloway.com/g/equity-compensation/about).\nRead it there for search, booksmarks/highlights, expert commentary, and PDF/EPUB download.*\n\n## Introduction\n\n**Equity compensation** is the practice of granting partial ownership in a company in\nexchange for work.\nIn its ideal form, equity compensation aligns the interests of individual employees with\nthe goals of the company they work for, which can yield dramatic results in team building,\ninnovation, and longevity of employment.\nEach of these [contributes](#history-and-significance) to the creation of value—for a\ncompany, for its users and customers, and for the individuals who work to make it a\nsuccess.\n\nThe [ways equity can be granted](#how-equity-is-granted) as compensation—including restricted\nstock, stock options, and restricted stock units—are **notoriously complex**. Equity\ncompensation involves confounding terminology, legal obscurities, and many high-stakes\ndecisions for those who give and receive it.\n\nIf you talk to enough employees and hiring managers, you’ll hear stories of how they or\ntheir colleagues met with the painful consequences of not learning enough up front.\nThough many people learn the basic ideas from personal experience or from colleagues or\nhelpful friends who have been through it before, the intricacies of equity compensation\nare best understood by tax attorneys, corporate lawyers, and other professionals.\n\nDecisions related to [negotiating an offer](#offers-and-negotiations) and\n[exercising stock options](#stock-option-scenarios), in particular, can have **major financial\nconsequences**. Because the value of employee equity is determined by the fate of the\ncompany, an employee’s equity may be [illiquid](#what-is-private-stock-worth) for a long time\nor ultimately [worth nothing](#growth-and-risk), while taxes and the costs of exercise, if\nthey apply, may not be recouped.\nEven when a company is doi",
    "url": "https://github.com/jlevy/og-equity-compensation",
    "last_updated": "2025-08-31T06:58:00+00:00"
  },
  {
    "full_name": "abess-team/abess",
    "name": "abess",
    "description": "Fast Best-Subset Selection Library",
    "language": "C++",
    "topics": [
      "polynomial-algorithm",
      "high-dimensional-data",
      "best-subset-selection",
      "machine-learning",
      "python",
      "r",
      "scikit-learn",
      "principal-component-analysis",
      "linear-regression",
      "classification-algorithm",
      "cox-regression",
      "logistic-regression",
      "multitask-learning",
      "ordinal-regression",
      "poisson-regression",
      "robust-principal-component-analysis",
      "sparse-principal-component-analysis",
      "feature-selection",
      "sure-independence-screening"
    ],
    "readme": "<img src='https://raw.githubusercontent.com/abess-team/abess/master/docs/image/icon_long.png' align=\"center\"/></a>\n\n# abess: Fast Best-Subset Selection in Python and R\n\n[![Python Build](https://github.com/abess-team/abess/actions/workflows/python_test.yml/badge.svg)](https://github.com/abess-team/abess/actions/workflows/python_test.yml)\n[![R Build](https://github.com/abess-team/abess/actions/workflows/r_test.yml/badge.svg)](https://github.com/abess-team/abess/actions/workflows/r_test.yml)\n[![codecov](https://codecov.io/gh/abess-team/abess/branch/master/graph/badge.svg?token=LK56LHXV00)](https://codecov.io/gh/abess-team/abess)\n[![docs](https://readthedocs.org/projects/abess/badge/?version=latest)](https://abess.readthedocs.io/en/latest/?badge=latest)\n[![R docs](https://github.com/abess-team/abess/actions/workflows/r_website.yml/badge.svg)](https://abess-team.github.io/abess/)\n[![cran](https://img.shields.io/cran/v/abess?logo=R)](https://cran.r-project.org/package=abess)\n[![pypi](https://img.shields.io/pypi/v/abess?logo=Pypi)](https://pypi.org/project/abess)\n[![Conda version](https://img.shields.io/conda/vn/conda-forge/abess.svg?logo=condaforge)](https://anaconda.org/conda-forge/abess)\n[![pyversions](https://img.shields.io/pypi/pyversions/abess)](https://img.shields.io/pypi/pyversions/abess)\n[![License](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](http://www.gnu.org/licenses/gpl-3.0)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/3f6e60a3a3e44699a033159633981b76)](https://www.codacy.com/gh/abess-team/abess/dashboard?utm_source=github.com&utm_medium=referral&utm_content=abess-team/abess&utm_campaign=Badge_Grade)\n[![CodeFactor](https://www.codefactor.io/repository/github/abess-team/abess/badge)](https://www.codefactor.io/repository/github/abess-team/abess)\n[![Platform](https://anaconda.org/conda-forge/abess/badges/platforms.svg)](https://anaconda.org/conda-forge/abess)\n[![Downloads](https://pepy.tech/badge/abess)](https://pepy.tech/project/abess",
    "url": "https://github.com/abess-team/abess",
    "last_updated": "2025-08-28T13:30:09+00:00"
  },
  {
    "full_name": "tidyverse/modelr",
    "name": "modelr",
    "description": "Helper functions for modelling",
    "language": "R",
    "topics": [
      "r",
      "modelling"
    ],
    "readme": "\n# modelr <img src=\"man/figures/logo.png\" align=\"right\" />\n\n<!-- badges: start -->\n\n[![Lifecycle:\nsuperseded](https://img.shields.io/badge/lifecycle-superseded-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#superseded)\n[![R-CMD-check](https://github.com/tidyverse/modelr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/modelr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/modelr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/modelr?branch=main)\n<!-- badges: end -->\n\n## Overview\n\nThe modelr package provides functions that help you create elegant\npipelines when modelling. It was designed primarily to support teaching\nthe basics of modelling for the 1st edition of [R for Data\nScience](https://r4ds.had.co.nz/model-basics.html).\n\nWe no longer recommend it and instead suggest\n<https://www.tidymodels.org/> for a more comprehensive framework for\nmodelling within the tidyverse.\n\n## Installation\n\n``` r\n# The easiest way to get modelr is to install the whole tidyverse:\ninstall.packages(\"tidyverse\")\n\n# Alternatively, install just modelr:\ninstall.packages(\"modelr\")\n```\n\n## Getting started\n\n``` r\nlibrary(modelr)\n```\n\n### Partitioning and sampling\n\nThe `resample` class stores a “reference” to the original dataset and a\nvector of row indices. A resample can be turned into a dataframe by\ncalling `as.data.frame()`. The indices can be extracted using\n`as.integer()`:\n\n``` r\n# a subsample of the first ten rows in the data frame\nrs <- resample(mtcars, 1:10)\nas.data.frame(rs)\n#>                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#> Mazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n#> Mazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n#> Datsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n#> Hornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n#> Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 1",
    "url": "https://github.com/tidyverse/modelr",
    "last_updated": "2025-08-26T01:35:39+00:00"
  },
  {
    "full_name": "tidymodels/tidymodels",
    "name": "tidymodels",
    "description": "Easily install and load the tidymodels packages",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tidymodels <a href='https://tidymodels.tidymodels.org'><img src='man/figures/logo.png' align=\"right\" height=\"139\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/tidymodels/tidymodels/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidymodels/tidymodels/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidymodels/tidymodels/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/tidymodels?branch=main)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/tidymodels)](https://CRAN.r-project.org/package=tidymodels)\n[![Downloads](https://cranlogs.r-pkg.org/badges/tidymodels)](https://CRAN.r-project.org/package=tidymodels)\n[![lifecycle](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n<!-- badges: end -->\n\n## Overview\n\n[tidymodels](https://www.tidymodels.org/) is a “meta-package” for\nmodeling and statistical analysis that shares the underlying design\nphilosophy, grammar, and data structures of the\n[tidyverse](https://www.tidyverse.org/).\n\nIt includes a core set of packages that are loaded on startup:\n\n- [`broom`](https://broom.tidymodels.org/) takes the messy output of\n  built-in functions in R, such as `lm`, `nls`, or `t.test`, and turns\n  them into tidy data frames.\n\n- [`dials`](https://dials.tidymodels.org) has tools to create and manage\n  values of tuning parameters.\n\n- [`dplyr`](https://dplyr.tidyverse.org) contains a grammar for data\n  manipulation.\n\n- [`ggplot2`](https://ggplot2.tidyverse.org) implements a grammar of\n  graphics.\n\n- [`infer`](https://infer.tidymodels.org) is a modern approach to\n  statistical inference.\n\n- [`parsnip`](https://parsnip.tidymodels.org) is a tidy, unified\n  interface to creating models.\n\n- [`purrr`](https://purrr.tidyverse.org) is a functional programming\n  toolkit.\n\n- [`recipes`](https://recipes.tidymodels.org",
    "url": "https://github.com/tidymodels/tidymodels",
    "last_updated": "2025-09-02T07:50:00+00:00"
  },
  {
    "full_name": "icaronunes/branch-time",
    "name": "branch-time",
    "description": "Count your time used in each branch",
    "language": "Python",
    "topics": [
      "branch",
      "git",
      "python",
      "cli"
    ],
    "readme": "# Time Branch\n\nTime Branch is a command-line interface tool that allows you to track the time you spend working on different branches in your repository. With Time Branch, you can easily monitor how much time you spend in each branch, helping you stay on top of your activities and improve productivity. Additionally, Time Branch offers features to export collected data to an output file, making it easy to analyze and share information with your team. With the help of Time Branch, you can optimize your software development activities and improve your efficiency\n\n## Install\n\n```bash\npip install branch-time\n```\n\n## How to use\n\n- #### time values ​​in minutes\n\n```bash\nbranchtime 'your\\repository.git' --time 20 --output 'output\\directory'\nbranchtime 'your\\repository.git' -t 20 -o 'output\\directory'\nbranchtime 'your\\repository.git' --time 20 --output 'output\\directory'\nor\nbranch-time 'your\\repository.git' --time 20 --output 'output\\directory'\n```\n\n### default values:\n\n- repository = your current directory\n- --time = 5 minutes\n- --output = your current directory\n\n## Help\n\n<img src=\"https://firebasestorage.googleapis.com/v0/b/livro-android-1327.appspot.com/o/help%20branch.PNG?alt=media&token=0739a15b-91b7-41f5-801f-1b7674f492f6\">\n\n## Output\n\n2023-04-24.txt\n\n```txt\nBranch: master - Time: 20:57:17\nFINISHED... 21:44:11\n\nBranch: main - Time: 21:58:19\n\nBranch: AUTO-29975 - Time: 22:10:29\n\nBranch: AUTO-32659 - Time: 22:34:10\n\nBranch: AUTO-31839 - Time: 23:18:46\n\nBranch: AUTO-29975 - Time: 23:40:17\n\nBranch: main - Time: 23:55:32\nFINISHED... 23:055:33\n```\n",
    "url": "https://github.com/icaronunes/branch-time",
    "last_updated": "2025-08-18T18:33:22+00:00"
  },
  {
    "full_name": "mlflow/mlflow",
    "name": "mlflow",
    "description": "The open source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.",
    "language": "Python",
    "topics": [
      "machine-learning",
      "ai",
      "ml",
      "mlflow",
      "apache-spark",
      "model-management",
      "agentops",
      "agents",
      "evaluation",
      "langchain",
      "llm-evaluation",
      "llmops",
      "observability",
      "open-source",
      "openai",
      "prompt-engineering",
      "ai-governance",
      "mlops"
    ],
    "readme": "<h1 align=\"center\" style=\"border-bottom: none\">\n    <a href=\"https://mlflow.org/\">\n        <img alt=\"MLflow logo\" src=\"https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg\" width=\"200\" />\n    </a>\n</h1>\n<h2 align=\"center\" style=\"border-bottom: none\">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align=\"center\">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href=\"https://twitter.com/intent/follow?screen_name=mlflow\" target=\"_blank\">\n<img src=\"https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5\"\n      alt=\"follow on X(Twitter)\"></a>\n<a href=\"https://www.linkedin.com/company/mlflow-org/\" target=\"_blank\">\n<img src=\"https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff\"\n      alt=\"follow on LinkedIn\"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align=\"center\">\n   <div>\n      <a href=\"https://mlflow.org/\"><strong>Website</strong></a> ·\n      <a href=\"https://mlflow.org/docs/latest/index.html\"><strong>Docs</strong></a> ·\n      <a href=\"https://github.com/mlflow/mlflow/issues/new/choose\"><strong>Feature Request</strong></a> ·\n      <a href=\"https://mlflow.org/blog\"><strong>News</strong></a> ·\n      <a href=\"https://www.youtube.com/@mlflowoss\"><strong>YouTube</strong></a> ·\n      <a href=\"https://lu.ma/mlflow?k=c\"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## 🚀 Installation\n\nTo install the MLflow Python package, run the",
    "url": "https://github.com/mlflow/mlflow",
    "last_updated": "2025-09-02T08:37:57+00:00"
  },
  {
    "full_name": "rochelleterman/PS239T",
    "name": "PS239T",
    "description": "Introduction to Computational Tools and Techniques for Social Research",
    "language": "HTML",
    "topics": [],
    "readme": "# PS239T: Introduction To Computational Tools And Techniques For Social Research\n\nThis course will provide graduate students the critical technical skills necessary to conduct research in computational social science and digital humanities, introducing them to the basic computer literacy, programming skills, and application knowledge that students need to be successful in further methods work.\n\nThe course is divided into three main sections: skills, applications, and community engagement. The “skills” portion will introduce students to basic computer literacy, terminologies, and programming languages - i.e. Bash, R, Python, and Git. The second part of the course provides students the opportunity to use the skills they learned in part 1 towards practical applications such as automated text analysis, geospatial analysis, data collection via APIs, webscraping, etc. The third section on community engagement will introduce topics such as ethics and privacy, best practices of reproducible research, scholarly communication and collaboration, and how to further one’s research using UC Berkeley campus resources.\n\n### Contact\n\nRochelle Terman: rterman@gmail.com\n\n![Creative Commons License](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)\n\nThis work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\n\n",
    "url": "https://github.com/rochelleterman/PS239T",
    "last_updated": "2025-08-25T16:31:07+00:00"
  },
  {
    "full_name": "ogham/exa",
    "name": "exa",
    "description": "A modern replacement for ‘ls’.",
    "language": "Rust",
    "topics": [
      "rust",
      "ls",
      "command-line",
      "files"
    ],
    "readme": "# exa is unmaintained, use the [fork eza](https://github.com/eza-community/eza) instead.\n\n(This repository isn’t archived because the only person with the rights to do so is unreachable).\n\n---\n\n<div align=\"center\">\n\n# exa\n\n[exa](https://the.exa.website/) is a modern replacement for _ls_.\n\n**README Sections:** [Options](#options) — [Installation](#installation) — [Development](#development)\n\n[![Unit tests](https://github.com/ogham/exa/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/ogham/exa/actions/workflows/unit-tests.yml)\n</div>\n\n![Screenshots of exa](screenshots.png)\n\n---\n\n**exa** is a modern replacement for the venerable file-listing command-line program `ls` that ships with Unix and Linux operating systems, giving it more features and better defaults.\nIt uses colours to distinguish file types and metadata.\nIt knows about symlinks, extended attributes, and Git.\nAnd it’s **small**, **fast**, and just **one single binary**.\n\nBy deliberately making some decisions differently, exa attempts to be a more featureful, more user-friendly version of `ls`.\nFor more information, see [exa’s website](https://the.exa.website/).\n\n\n---\n\n<a id=\"options\">\n<h1>Command-line options</h1>\n</a>\n\nexa’s options are almost, but not quite, entirely unlike `ls`’s.\n\n### Display options\n\n- **-1**, **--oneline**: display one entry per line\n- **-G**, **--grid**: display entries as a grid (default)\n- **-l**, **--long**: display extended details and attributes\n- **-R**, **--recurse**: recurse into directories\n- **-T**, **--tree**: recurse into directories as a tree\n- **-x**, **--across**: sort the grid across, rather than downwards\n- **-F**, **--classify**: display type indicator by file names\n- **--colo[u]r**: when to use terminal colours\n- **--colo[u]r-scale**: highlight levels of file sizes distinctly\n- **--icons**: display icons\n- **--no-icons**: don't display icons (always overrides --icons)\n\n### Filtering options\n\n- **-a**, **--all**: show hidden and 'dot' files\n- **-d**, **",
    "url": "https://github.com/ogham/exa",
    "last_updated": "2025-09-01T23:44:09+00:00"
  },
  {
    "full_name": "weaviate/weaviate",
    "name": "weaviate",
    "description": "Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native database​.",
    "language": "Go",
    "topics": [
      "search-engine",
      "semantic-search",
      "semantic-search-engine",
      "vector-search",
      "vector-search-engine",
      "vector-database",
      "approximate-nearest-neighbor-search",
      "image-search",
      "hnsw",
      "information-retrieval",
      "mlops",
      "nearest-neighbor-search",
      "neural-search",
      "recommender-system",
      "similarity-search",
      "vectors",
      "generative-search",
      "hybrid-search",
      "weaviate",
      "grpc"
    ],
    "readme": "# Weaviate <img alt='Weaviate logo' src='https://weaviate.io/img/site/weaviate-logo-light.png' width='148' align='right' />\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/weaviate/weaviate?style=social)](https://github.com/weaviate/weaviate)\n[![Go Reference](https://pkg.go.dev/badge/github.com/weaviate/weaviate.svg)](https://pkg.go.dev/github.com/weaviate/weaviate)\n[![Build Status](https://github.com/weaviate/weaviate/actions/workflows/.github/workflows/pull_requests.yaml/badge.svg?branch=main)](https://github.com/weaviate/weaviate/actions/workflows/.github/workflows/pull_requests.yaml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/weaviate/weaviate)](https://goreportcard.com/report/github.com/weaviate/weaviate)\n[![Coverage Status](https://codecov.io/gh/weaviate/weaviate/branch/main/graph/badge.svg)](https://codecov.io/gh/weaviate/weaviate)\n[![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://weaviate.io/slack)\n\n**Weaviate** is an open-source, cloud-native vector database that stores both objects and vectors, enabling semantic search at scale. It combines vector similarity search with keyword filtering, retrieval-augmented generation (RAG), and reranking in a single query interface. Common use cases include RAG systems, semantic and image search, recommendation engines, chatbots, and content classification.\n\nWeaviate supports two approaches to store vectors: automatic vectorization at import using [integrated models](https://docs.weaviate.io/weaviate/model-providers) (OpenAI, Cohere, HuggingFace, and others) or direct import of [pre-computed vector embeddings](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors). Production deployments benefit from built-in multi-tenancy, replication, RBAC authorization, and [many other features](#weaviate-features).\n\nTo get started quickly, have a look at one of these tutorials:\n\n- [Quickstart - Weaviate Cloud](https://docs.weaviate.io/weaviate/quickstart)\n- [Quickst",
    "url": "https://github.com/weaviate/weaviate",
    "last_updated": "2025-09-02T09:58:49+00:00"
  },
  {
    "full_name": "microsoft/AzureSMR",
    "name": "AzureSMR",
    "description": "AzureSMR is no longer being actively developed. For ongoing support of Azure in R, see: https://github.com/Azure/AzureR",
    "language": "R",
    "topics": [
      "r",
      "azure",
      "hdinsight",
      "azure-resources"
    ],
    "readme": "# AzureSMR: Manage and Interact with Azure Resources.\n\n[![Project Status: Inactive – The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](https://www.repostatus.org/badges/latest/inactive.svg)](https://www.repostatus.org/#inactive)\n \n---\n \n `AzureSMR` is an R Package for managing a selection of Azure resources, using the Azure Service Manager API. The package exposes function to manage resources, resource groups, storage (blobs and containers), ARM templates, virtual machines, HDInsight (nodes, Hive and Spark) and Azure Data Lake Store. To use the package, you must configure an Azure Active Directory application and service principal in the Azure portal.\n\n**AzureSMR is no longer being actively developed. For ongoing support of Azure in R, please see the [AzureR](https://github.com/Azure/AzureR) family of packages.**\n\n## Code of conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).  \nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n",
    "url": "https://github.com/microsoft/AzureSMR",
    "last_updated": "2025-07-19T00:31:33+00:00"
  },
  {
    "full_name": "Planeshifter/node-Rstats",
    "name": "node-Rstats",
    "description": "[UNMAINTAINED] An interface for node.js to statistical programming language R based on the fabulous Rcpp package",
    "language": "C++",
    "topics": [],
    "readme": "[![NPM version][npm-image]][npm-url]\n[![Build Status][travis-image]][travis-url]\n[![Coverage Status][coveralls-image]][coveralls-url]\n[![Dependencies][dependencies-image]][dependencies-url]\n\nnode-Rstats\n===========\n\n> An interface for node.js to statistical programming language R based on the fabulous Rcpp package\n\n## Installation\n\nCurrently, `rstats` is ONLY supported for Unix operating systems.\n\nAlso, it is required that the R packages `RInside`, `Rcpp` and `RJSONIO` are installed inside R. Additionally, building the package using `node-gyp` requires\n\n  * `python` (`v2.7`, `v3.x.x` is __*not*__ supported)\n  * `make`\n  * A C/C++ compiler toolchain, such as GCC\n\nWith these prerequisites satisfied, one can simply install `rstats` using npm\n\n```bash\nnpm install rstats\n```\n\n## Getting Started\n\nAfter installation, the package can be loaded as follows:\n\n```javascript\nvar rstats  = require('rstats');\n```\n\nOnce the package is loaded, we can create an R session by the command\n\n```javascript\nvar R  = new rstats.session();\n```\n## Important Functions\n\n### parseEvalQ\n\nEvaluating R expressions is easy. We can use the *parseEvalQ* function as follows:\n\n```javascript\nR.parseEvalQ(\"cat('\\n Hello World \\n')\");\n```\n\n### parseEval\n\nTo evaluate an R expression and directly capture its return value, one can use the *parseEval* function.\n\n```javascript\nvar x = R.parseEval(\"c(1,2,3)\");\n```\n\nThe variable `x` is now equal to the array `[1,2,3]`.\n\n### assign\n\nNumeric values can be easily assigned to variables in the current R session:\n\n```javascript\nR.assign('x', 17);\nR.assign('y', 3);\n\n// calculate the sum of x+y and print the result\nR.parseEvalQ(\"res = x + y; print(res);\");\n```\n\n### get\n\nTo retrieve an object from the R session, we use the *get* command. For example, let us create a 2x2 matrix in R and retrieve it in JavaScript as a nested array:\n\n```javascript\nR.parseEvalQ(\"mat = matrix(1:4,ncol=2,nrow=2)\");\nvar mat = R.get('mat');\n```\n\nInternally, the *get* function uses JSON in order to",
    "url": "https://github.com/Planeshifter/node-Rstats",
    "last_updated": "2025-02-14T22:19:02+00:00"
  },
  {
    "full_name": "hansalemaos/locate_pixelcolor_cythonsingle",
    "name": "locate_pixelcolor_cythonsingle",
    "description": "Compiled Cython Code - Detects colors in images 2-3 x faster than Numpy",
    "language": "Python",
    "topics": [
      "color",
      "cython",
      "rgb",
      "search"
    ],
    "readme": "# Compiled Cython Code - Detects colors in images 2-3 x faster than Numpy \r\n\r\n### pip install locate-pixelcolor-cythonsingle\r\n\r\n#### Tested+compiled against Windows 10 / Python 3.10 / Anaconda\r\n\r\n#### If you can't import it, compile it on your system (code at the end of this page)\r\n\r\n\r\n\r\n### How to use it in Python \r\n\r\n```python\r\nimport numpy as np\r\nimport cv2\r\nfrom locate_pixelcolor_cythonsingle import search_colors\r\n# 4525 x 6623 x 3 picture https://www.pexels.com/pt-br/foto/foto-da-raposa-sentada-no-chao-2295744/\r\npicx = r\"C:\\Users\\hansc\\Downloads\\pexels-alex-andrews-2295744.jpg\"\r\npic = cv2.imread(picx)\r\ncolors0 = np.array([[255, 255, 255]],dtype=np.uint8)\r\nresus0 = search_colors(pic=pic, colors=colors0)\r\ncolors1=np.array([(66,  71,  69),(62,  67,  65),(144, 155, 153),(52,  57,  55),(127, 138, 136),(53,  58,  56),(51,  56,  54),(32,  27,  18),(24,  17,   8),],dtype=np.uint8)\r\nresus1 =  search_colors(pic=pic, colors=colors1)\r\n####################################################################\r\n%timeit resus0 = search_colors(pic=pic, colors=colors0)\r\n51 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n\r\nb,g,r = pic[...,0],pic[...,1],pic[...,2]\r\n%timeit np.where(((b==255)&(g==255)&(r==255)))\r\n150 ms ± 209 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n####################################################################\r\n%timeit resus1 =  search_colors(pic=pic, colors=colors1)\r\n443 ms ± 1.19 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit np.where(((b==66)&(g==71)&(r==69))|((b==62)&(g==67)&(r==65))|((b==144)&(g==155)&(r==153))|((b==52)&(g==57)&(r==55))|((b==127)&(g==138)&(r==136))|((b==53)&(g==58)&(r==56))|((b==51)&(g==56)&(r==54))|((b==32)&(g==27)&(r==18))|((b==24)&(g==17)&(r==8)))\r\n1 s ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n####################################################################\r\n```\r\n\r\n\r\n### The Cython Code \r\n\r\n```python\r\n# cython: language_level=3\r\n\r\nimport numpy as np\r\ncimport numpy",
    "url": "https://github.com/hansalemaos/locate_pixelcolor_cythonsingle",
    "last_updated": "2025-06-27T02:39:19+00:00"
  },
  {
    "full_name": "JacobChrist/YouTube-Caption",
    "name": "YouTube-Caption",
    "description": "YoutTube Caption API Python example converted to Python 3",
    "language": "Python",
    "topics": [],
    "readme": "# YouTube-Caption\nYouTube Caption API Python example converted to Python 3\n\nSuccessful attempted at extracting closed caption information from Siraj Raval's ML videos.\nThe ultimate goal of this extraction is to make a Sirajbot so that we can all have our own personal Siraj.\n\nThe original code is found in the download call for this API:\n\nhttps://developers.google.com/youtube/v3/docs/captions\n\nTo run this code you need to get OAuth2 credentials from google:\n\nhttps://console.developers.google.com/apis/credentials\n\nUsage examples:\n\nThese examples extract the closed caption information from this video on creating a chatbot:\n\nhttps://www.youtube.com/watch?v=t5qgjJIBy9g\n\npython youtube-caption.py --action=list --videoid=t5qgjJIBy9g\nCaption track '(CTcNv09WGJMRU2JMppJBW2SFXPfsrJtllhu4z_DJ_fQ=)' in 'en' language.\nCreated and managed caption tracks.\n\npython youtube-caption.py --action=download --videoid=t5qgjJIBy9g --captionid=CTcNv09WGJMRU2JMppJBW2SFXPfsrJtllhu4z_DJ_fQ=\nFirst line of caption track: b\"1\\n00:00:00,000 --> 00:00:04,529\\nhello world its Suraj and let's build a\\n\\n2\\n00:00:02,429 --> 00:00:07,140\\nchat bot that can answer questions about\\n\\n3\\n00:00:04,529 --> 00:00:09,540\\nany text you give it it an article or\\n\\n4\\n00:00:07,140 --> 00:00:11,790\\neven a book using care off just imagine\\n\\n5\\n00:00:09,540 --> 00:00:13,920\\nthe boost in productivity all of us will\\n\\n6\\n00:00:11,790 --> 00:00:16,350\\nhave once we have access to expert\\n\\n7\\n00:00:13,920 --> 00:00:18,029\\nsystems for any given topic instead of\\n\\n8\\n00:00:16,350 --> 00:00:20,250\\nsifting through all the jargon in a\\n\\n9\\n00:00:18,029 --> 00:00:22,230\\nscientific paper you just give it the\\n\\n10\\n00:00:20,250 --> 00:00:25,230\\npaper then ask it the relevant questions\\n\\n11\\n00:00:22,230 --> 00:00:27,779\\nentire textbooks libraries videos images\\n\\n12\\n00:00:25,230 --> 00:00:30,240\\nwhatever you just feed it some data and\\n\\n13\\n00:00:27,779 --> 00:00:32,279\\nit would become an expert at it all 7\\n\\n",
    "url": "https://github.com/JacobChrist/YouTube-Caption",
    "last_updated": "2025-01-23T05:48:00+00:00"
  },
  {
    "full_name": "pablobarbera/twitter_ideology",
    "name": "twitter_ideology",
    "description": "Estimating Ideological Positions with Twitter Data",
    "language": "R",
    "topics": [],
    "readme": "Estimating Ideological Positions with Twitter Data\n----------------\n\nThis GitHub repository contains code and materials related to the article \"[Birds of a Feather Tweet Together. Bayesian Ideal Point Estimation Using Twitter Data](http://pan.oxfordjournals.org/content/23/1/76.full),\" published in Political Analysis in 2015. \n\nThe original replication code can be found in the `replication` folder. See also [Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/26589) for the full replication materials, including data and output.\n\nFor updated versions of the code, that implement a more computationally efficient approach introduced in [Barberá et al (2015, Psychological Science)](https://journals.sagepub.com/doi/abs/10.1177/0956797615594620), see the folders [2016-update/](https://github.com/pablobarbera/twitter_ideology/tree/master/2016-update), [2018-update/](https://github.com/pablobarbera/twitter_ideology/tree/master/2018-update), and [2020-update/](https://github.com/pablobarbera/twitter_ideology/tree/master/2020-update).\n\nAs an application of the method, in June 2015 I wrote a blog post on The Monkey Cage / Washington Post entitled [\"Who is the most conservative Republican candidate for president?.\"](http://www.washingtonpost.com/blogs/monkey-cage/wp/2015/06/16/who-is-the-most-conservative-republican-candidate-for-president/) The replication code for the figure in the post is available in the `primary` folder.\n\nFinally, this repository also contains an R package (`tweetscores`) with several functions to facilitate the application of this method in future research. The rest of this README file provides a tutorial with instructions showing how to use it.\n\n**NOTE**: the package currently only support v1.1 of Twitter's API, which at some point in the near future will be deprecated. This package is not currently maintained, so use at your own risk.\n\n### Authentication\n\nIn order to download data from Twitter’s API, you will first need to ",
    "url": "https://github.com/pablobarbera/twitter_ideology",
    "last_updated": "2025-06-26T06:58:36+00:00"
  },
  {
    "full_name": "KKBOX/spark-deployer",
    "name": "spark-deployer",
    "description": "Deploy Spark cluster in an easy way.",
    "language": "Scala",
    "topics": [],
    "readme": "# spark-deployer\n\n[![Join the chat at https://gitter.im/KKBOX/spark-deployer](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/KKBOX/spark-deployer?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n* A Scala tool which helps deploying [Apache Spark](http://spark.apache.org/) stand-alone cluster on [EC2](http://aws.amazon.com/ec2/) and submitting job.\n* Currently supports Spark 2.0.0+.\n* There are two modes when using spark-deployer: SBT plugin mode and embedded mode.\n\n## SBT plugin mode\n\nHere are the basic steps to run a Spark job (all the sbt commands support TAB-completion):\n\n1. Set the environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.\n2. Prepare a project with structure like below:\n\n  ```\n  project-root\n  ├── build.sbt\n  ├── project\n  │   └── plugins.sbt\n  └── src\n      └── main\n          └── scala\n              └── mypackage\n                  └── Main.scala\n  ```\n\n3. Add one line in `project/plugins.sbt`:\n\n  ```\n  addSbtPlugin(\"net.pishen\" % \"spark-deployer-sbt\" % \"3.0.2\")\n  ```\n\n4. Write your Spark project's `build.sbt` (Here we give a simple example):\n\n  ```\n  name := \"my-project-name\"\n   \n  scalaVersion := \"2.11.8\"\n   \n  libraryDependencies ++= Seq(\n    \"org.apache.spark\" %% \"spark-core\" % \"2.0.0\" % \"provided\"\n  )\n  ```\n\n5. Write your job's algorithm in `src/main/scala/mypackage/Main.scala`:\n\n  ```scala\n  package mypackage\n   \n  import org.apache.spark._\n   \n  object Main {\n    def main(args: Array[String]) {\n      //setup spark\n      val sc = new SparkContext(new SparkConf())\n      //your algorithm\n      val n = 10000000\n      val count = sc.parallelize(1 to n).map { i =>\n        val x = scala.math.random\n        val y = scala.math.random\n        if (x * x + y * y < 1) 1 else 0\n      }.reduce(_ + _)\n      println(\"Pi is roughly \" + 4.0 * count / n)\n    }\n  }\n  ```\n\n6. Enter `sbt`, and build a config by:\n\n  ```\n  > sparkBuildConfig\n  ```\n  \n  (Most settings have default values, just hit Enter to go thr",
    "url": "https://github.com/KKBOX/spark-deployer",
    "last_updated": "2025-08-25T16:30:56+00:00"
  },
  {
    "full_name": "everypolitician/everypolitician",
    "name": "everypolitician",
    "description": "Data about every national legislature in the world, freely available for you to use",
    "language": "",
    "topics": [],
    "readme": "# EveryPolitician\n\n> Data about every national legislature in the world, freely available for you to use\n\n- **[everypolitician.org](http://everypolitician.org)** —  [data](http://everypolitician.org/countries.html) | [about](http://docs.everypolitician.org/)\n- [Report an issue](https://github.com/everypolitician/everypolitician/issues)\n\n## Repo summary\n\nThese are some of the key repos in the EveryPolitician family. There are others.\n\n- **everypolitician** (this repo): contains no code, but is where [issues/tickets for the whole project](https://github.com/everypolitician/everypolitician/issues) live\n\n- **[everypolitician-data](https://github.com/everypolitician/everypolitician-data)**: where the data is stored -- but if you want to download it, get it from:\n  - human? go via the [EveryPolitician website](http://everypolitician.org)\n  - program? use the RawGit CDN, via links in `countries.json`, which we [explain here](http://docs.everypolitician.org/repo_structure.html)\n\n- **[viewer-static](https://github.com/everypolitician/viewer-static)**: the live website http://everypolitician.org (`gh-pages`)\n\n- **[viewer-sinatra](https://github.com/everypolitician/viewer-sinatra)**: Sinatra app for generating a dynamic version EveryPolitician website\n\n- **[webhook-manager](https://github.com/everypolitician/webhook-manager)**: sends out EveryPolitician WebHooks: [register your URL here!](https://everypolitician-app-manager.herokuapp.com/)\n\n- **[everypolitician-docs](https://github.com/everypolitician/everypolitician-docs)**: documentation at http://docs.everypolitician.org/ (`gh-pages`)\n\n* **[rebuilder](https://github.com/everypolitician/rebuilder)** rebuilds data from source\n\n- libraries for easily manipulating EveryPolitician data (useful for all devs, but we use the Ruby ones ourselves, of course!):\n  - Ruby:\n    **[everypolitician-ruby](https://github.com/everypolitician/everypolitician-ruby)**\n    and **[everypolitician-popolo](https://github.com/everypolitician/everypol",
    "url": "https://github.com/everypolitician/everypolitician",
    "last_updated": "2021-07-16T12:44:36+00:00"
  },
  {
    "full_name": "aadityadar/crosswalks-bihar",
    "name": "crosswalks-bihar",
    "description": "",
    "language": "Stata",
    "topics": [],
    "readme": "# crosswalks-bihar\n\nLinking of records across datasets is difficult because geographic identifiers are not standardized. In this repo, I provide data files that track common spelling variations for various geographies in Bihar.  \n\nTo merge district codes:  \n\n```\n* prep to merge district codes\nrename districtNameInYourData dname \nreplace dname = strlower(stritrim(strtrim(dname)))\n\n* link dnames to dcodes\nmerge m:1 dname using \"output/id_dcode_2011_br.dta\", gen(m_dcode) keepusing(dcode_2011)\n\n* sanity check \nassert m_dcode != 1\ndrop if m_dcode != 3\n```\n\nTo merge block codes, first standardize district codes and then merge on standardized district code and block name:  \n\n```\n* prep to merge block codes\nrename blockNameInYourData bname\nreplace bname = strlower(stritrim(strtrim(bname)))\n\n* link dcode-bnames to dcode-bcode\nmerge m:1 dcode_2011 bname using \"output/id_bcode_2011_br.dta\", gen(m_bcode) keepusing(bcode_2011)\n\n* sanity check \nassert m_bcode != 1\ndrop if m_bcode != 3\n```\n\nIf you find these files helpful and use them in your work, please cite them as:\nDar, A. (2023). India Bridge: Crosswalks for Bihar (Version 1.0.0) [Computer software]. https://github.com/aadityadar/crosswalks-bihar",
    "url": "https://github.com/aadityadar/crosswalks-bihar",
    "last_updated": "2023-09-18T20:44:15+00:00"
  },
  {
    "full_name": "dwyl/english-words",
    "name": "english-words",
    "description": ":memo: A text file containing 479k English words for all your dictionary/word-based projects e.g: auto-completion / autosuggestion",
    "language": "Python",
    "topics": [],
    "readme": "List Of English Words\n=============\n\nA text file containing over 466k English words.\n\nWhile searching for a list of english words (for an auto-complete tutorial)\nI found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to [https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable](https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable) (archived).\n\nNo idea why infochimps put the word list inside an excel (.xls) file.\n\nI pulled out the words into a simple new-line-delimited text file.\nWhich is more useful when building apps or importing into databases etc.\n\nCopyright still belongs to them.\n\nFiles you may be interested in:\n\n-  [words.txt](words.txt) contains all words.\n-  [words_alpha.txt](words_alpha.txt) contains only [[:alpha:]] words (words that only have letters, no numbers or symbols). If you want a quick solution choose this.\n-  [words_dictionary.json](words_dictionary.json) contains all the words from words_alpha.txt as json format. \nIf you are using Python, you can easily load this file and use it as a dictionary for faster performance. All the words are assigned with 1 in the dictionary.\n\nSee [read_english_dictionary.py](read_english_dictionary.py) for example usage.\n",
    "url": "https://github.com/dwyl/english-words",
    "last_updated": "2025-09-02T09:55:32+00:00"
  },
  {
    "full_name": "acoppock/Green-Lab-SOP",
    "name": "Green-Lab-SOP",
    "description": "Standard Operating Procedures for Don Green's Lab at Columbia",
    "language": "HTML",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\nThis standard operating procedure (SOP) document describes the default practices of the experimental research group led by Donald P. Green at Columbia University. These defaults apply to analytic decisions that have not been made explicit in pre-analysis plans (PAPs). They are not meant to override decisions that are laid out in PAPs. The content of our lab's SOP is available for public use. We welcome others to copy or adapt it to suit their research purposes.\n\n[HTML version of SOP](http://acoppock.github.io/Green-Lab-SOP/Green_Lab_SOP.html)\n\n[PDF version of SOP](http://acoppock.github.io/Green-Lab-SOP/Green_Lab_SOP.pdf)\n\n[Article on motivations for SOP](https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf)\n\nThis is a living document. To suggest changes or additions, please feel free to e-mail us or submit an [issue on GitHub](https://github.com/acoppock/Green-Lab-SOP/issues). Also, when referencing our SOP, please be sure to note the version and date.\n\nAlex Coppock (<alex.coppock@yale.edu>)\n\nDon Green (<dpg2110@columbia.edu>)\n\nWinston Lin (<Linston@gmail.com>)\n",
    "url": "https://github.com/acoppock/Green-Lab-SOP",
    "last_updated": "2025-02-19T14:52:45+00:00"
  },
  {
    "full_name": "openai/openai-cookbook",
    "name": "openai-cookbook",
    "description": "Examples and guides for using the OpenAI API",
    "language": "Jupyter Notebook",
    "topics": [
      "openai",
      "chatgpt",
      "gpt-4",
      "openai-api"
    ],
    "readme": "<a href=\"https://cookbook.openai.com\" target=\"_blank\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"/images/openai-cookbook-white.png\" style=\"max-width: 100%; width: 400px; margin-bottom: 20px\">\n    <img alt=\"OpenAI Cookbook Logo\" src=\"/images/openai-cookbook.png\" width=\"400px\">\n  </picture>\n</a>\n\n<h3></h3>\n \n> ✨ Navigate at [cookbook.openai.com](https://cookbook.openai.com)\n\nExample code and guides for accomplishing common tasks with the [OpenAI API](https://platform.openai.com/docs/introduction). To run these examples, you'll need an OpenAI account and associated API key ([create a free account here](https://platform.openai.com/signup)). Set an environment variable called `OPENAI_API_KEY` with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an `.env` file at the root of your repo containing `OPENAI_API_KEY=<your API key>`, which will be picked up by the notebooks.\n\nMost code examples are written in Python, though the concepts can be applied in any language.\n\nFor other useful tools, guides and courses, check out these [related resources from around the web](https://cookbook.openai.com/related_resources).\n\n## License\n\nMIT License\n",
    "url": "https://github.com/openai/openai-cookbook",
    "last_updated": "2025-09-02T09:49:32+00:00"
  },
  {
    "full_name": "jtleek/slipper",
    "name": "slipper",
    "description": "Tidy and easy bootstrapping",
    "language": "R",
    "topics": [],
    "readme": "Bootstrapping made easy and tidy with slipper\n=================\n\n![](slipper.jpg)\n\nYou've heard of [broom](https://cran.r-project.org/web/packages/broom/index.html) for tidying up your R functions. slipper is an R package for tidy/easy bootstrapping. There are already a bunch of good bootstrapping packages out there including [bootstrap](https://cran.r-project.org/web/packages/bootstrap/) and [boot](https://cran.r-project.org/web/packages/boot/). You can also bootstrap with [dplyr and broom](https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html) or with [purrr and modelr](https://cran.r-project.org/web/packages/modelr).\n\nBut I'm too dumb for any of those. So slipper includes some simple,pipeable bootstrapping functions for me \n\n### install\nwith `devtools`:\n\n```r\ndevtools::install_github('jtleek/slipper')\n```\n\n### use\n\nThere are only two functions in this package. \n\nCall `slipper` to bootstrap any function that returns\na single value. \n\n```r\nslipper(mtcars,mean(mpg),B=100)\n```\n\nslipper is built to work with pipes and the tidyverse too. \n\n```r\nmtcars %>% slipper(mean(mpg),B=100)\n```\n\nThe output is a data frame with the values of the function on the original data set and the bootstrapped replicates. You can calculate confidence intervals using summarize\n\n```r\nmtcars %>% slipper(mean(mpg),B=100) %>%\n  filter(type==\"bootstrap\") %>% \n  summarize(ci_low = quantile(value,0.025),\n            ci_high = quantile(value,0.975))\n```\n\nYou can also bootstrap linear models using `slipper_lm` just pass the data frame and the formula you want to fit on the original data and on the bootstrap samples. \n\n```r\n slipper_lm(mtcars,mpg ~ cyl,B=100)\n```\n\nThis is also pipeable\n\n```r\nmtcars %>% slipper_lm(mpg ~ cyl,B=100)\n```\n\nThe default behavior is to bootstrap complete cases, but if you want to bootstrap residuals set `boot_resid=TRUE`\n\n```S\nmtcars %>% slipper_lm(mpg ~ cyl,B=100,boot_resid=TRUE)\n```\n\nYou can calculate bootstrap confidence intervals in the same way as you",
    "url": "https://github.com/jtleek/slipper",
    "last_updated": "2025-03-22T11:19:56+00:00"
  },
  {
    "full_name": "benmarwick/UW-eScience-reproducibility-social-sciences",
    "name": "UW-eScience-reproducibility-social-sciences",
    "description": "This repository contains my slides and references for a presentation to the UW eScience Institute on reproducible research in the social sciences (9 April 2014). To view the slides, go to http://benmarwick.github.io/UW-eScience-reproducibility-social-sciences",
    "language": "",
    "topics": [],
    "readme": "",
    "url": "https://github.com/benmarwick/UW-eScience-reproducibility-social-sciences",
    "last_updated": "2019-08-20T22:27:20+00:00"
  },
  {
    "full_name": "travisbrady/word2phrase",
    "name": "word2phrase",
    "description": "Python port of Mikolov's word2phrase.c from the word2vec toolkit",
    "language": "Python",
    "topics": [],
    "readme": "word2phrase\n===========\n\nPython port of Mikolov's word2phrase.c from the word2vec toolkit\n\nGiven a document or documents this program attempts to learn phrases.\nIt does so by progressively joining adjacent pairs of words with an '_' character.\nYou can then run the code multiple times to create multiword phrases.\n\nTake a look at [example.py](example.py) for an example of using this code from Python. The example requires the textblob module (available via pip) to tokenize the input.\n\nExample using the text8 corpus used in Mikolov's experiments:\n```\n$ time python word2phrase.py --train=text8 --output=text8-phrase-py --min-count=5 --threshold=500.0\n# Now count instances of phrases.  We separate phrases with an underscore\n$ cat text8-phrase-py| tr ' ' '\\n' | grep '_' | python wordcount.py 20\n9913 united_states\n7100 th_century\n6761 external_links\n4942 new_york\n3147 rather_than\n2371 united_kingdom\n2184 prime_minister\n1602 soviet_union\n1507 civil_war\n1464 main_article\n1266 no_longer\n1247 science_fiction\n1100 don_t\n1095 new_zealand\n1069 hong_kong\n1067 http_www\n1019 north_america\n998 los_angeles\n959 roman_catholic\n940 air_force\n```\n\n### More Information\nFor more detail on the (very simple) approach here check out:\n- https://github.com/tmikolov/word2vec\n- Mikolov's paper: http://arxiv.org/abs/1310.4546\n",
    "url": "https://github.com/travisbrady/word2phrase",
    "last_updated": "2025-02-16T12:54:29+00:00"
  },
  {
    "full_name": "dsidavis/RScopusAPI",
    "name": "RScopusAPI",
    "description": "An interface to Elsevier's Scopus API",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/dsidavis/RScopusAPI",
    "last_updated": "2023-11-18T22:08:48+00:00"
  },
  {
    "full_name": "jayjacobs/dga",
    "name": "dga",
    "description": "Classifier to separate legitimate domains from those generated by a domain generating algorithm (DGA).",
    "language": "R",
    "topics": [],
    "readme": "This is an implement of a classification algorithm trained on legitamate domains (taken from the Alexa list of popular web sites and the Open DNS popular domains list), as well as algorithmically generated domains from the Cryptolocker and GOZ botnet.\n\nGiven a domain name the function will classify it as either \"dga\" or \"legit\" and include the probability of the classification.\n\nBegin by loading up the DGA library (note: you may get an error on install\\_github if you had never ‘git clone’d before, or added the host as a known SSH host).\n\n``` {.r}\ndevtools::install_github(\"jayjacobs/dga\")\n```\n\n``` {.r}\nlibrary(dga)\n```\n\nLet's test with the easy most popular websites, and classify them as either \"legit\" or \"dga\".\n\n``` {.r}\ngood20 <- c(\"facebook.com\", \"google.com\", \"youtube.com\",\n           \"yahoo.com\", \"baidu.com\", \"wikipedia.org\",\n           \"amazon.com\", \"live.com\", \"quicken.com\",\n           \"taobao.com\", \"blogspot.com\", \"google.co.in\",\n           \"twitter.com\", \"linkedin.com\", \"yahoo.co.jp\",\n           \"bing.com\", \"sina.com.cn\", \"yandex.ru\",\n           \"msn.com\", \"vikings.com\")\n\ndgaPredict(good20)\n```\n\n    ## Loading required package: randomForest\n    ## randomForest 4.6-10\n    ## Type rfNews() to see new features/changes/bug fixes.\n\n    ##         name class  prob\n    ## 1   facebook legit 1.000\n    ## 2     google legit 1.000\n    ## 3    youtube legit 1.000\n    ## 4      yahoo legit 1.000\n    ## 5      baidu legit 1.000\n    ## 6  wikipedia legit 0.998\n    ## 7     amazon legit 1.000\n    ## 8       live legit 1.000\n    ## 9    quicken legit 1.000\n    ## 10    taobao legit 1.000\n    ## 11  blogspot legit 1.000\n    ## 12    google legit 1.000\n    ## 13   twitter legit 1.000\n    ## 14  linkedin legit 1.000\n    ## 15     yahoo legit 1.000\n    ## 16      bing legit 1.000\n    ## 17      sina legit 1.000\n    ## 18    yandex legit 1.000\n    ## 19       msn legit 1.000\n    ## 20   vikings legit 1.000\n\nNow some domain generated algorithms from the cryptolocker botnet:\n\n``` ",
    "url": "https://github.com/jayjacobs/dga",
    "last_updated": "2024-12-06T06:18:12+00:00"
  },
  {
    "full_name": "hrbrmstr/wand",
    "name": "wand",
    "description": "Use 'magic' to guess file types",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "magic-bytes",
      "file",
      "r-cyber"
    ],
    "readme": "\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-100%25-lightgrey.svg)\n[![Linux build\nStatus](https://travis-ci.org/hrbrmstr/wand.svg?branch=master)](https://travis-ci.org/hrbrmstr/wand)\n[![builds.sr.ht\nstatus](https://builds.sr.ht/~hrbrmstr/wand.svg)](https://builds.sr.ht/~hrbrmstr/wand?)\n[![Windows build\nstatus](https://ci.appveyor.com/api/projects/status/github/hrbrmstr/wand?svg=true)](https://ci.appveyor.com/project/hrbrmstr/wand)\n[![Coverage\nStatus](https://codecov.io/gh/hrbrmstr/wand/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/wand)\n[![cran\nchecks](https://cranchecks.info/badges/worst/wand)](https://cranchecks.info/pkgs/wand)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/wand)](https://www.r-pkg.org/pkg/wand)\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-3.2.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# wand\n\nRetrieve Magic Attributes from Files and Directories\n\n## Description\n\nMIME types are shorthand descriptors for file contents and can be\ndetermined from “magic” bytes in file headers, file contents or intuited\nfrom file extensions. Tools are provided to perform curated “magic”\ntests as well as mapping MIME types from a database of over 1,800\nextension mappings.\n\n## SOME IMPORTANT DETAILS\n\nThe header checking is minimal (i.e. nowhere near as comprehensive as\n`libmagic`) but covers quite a bit of ground. If there are content-check\ntypes from [`magic\nsources`](https://github.com/threatstack/libmagic/tree/master/magic/)\nthat you would like coded into the package, please file an issue and\n*include the full line(s)* from that linked `magic.tab` that you would\nlike mapped.\n\n## Wha",
    "url": "https://github.com/hrbrmstr/wand",
    "last_updated": "2025-03-22T10:58:55+00:00"
  },
  {
    "full_name": "gojiplus/bloomjoin",
    "name": "bloomjoin",
    "description": "bloomjoin: An R package implementing Bloom filter-based joins for improved performance with large datasets.",
    "language": "R",
    "topics": [],
    "readme": "# BloomJoin: Bloom Filter Based Joins\n\nAn R package implementing Bloom filter-based joins for improved performance with large datasets.\n\n## Overview\n\nBloomJoin provides an alternative join implementation for R that uses a hash-based approach inspired by Bloom filters to optimize the performance of joins between data frames. Traditional joins in R can be inefficient when dealing with large datasets, especially when one table is significantly larger than the other and the join key selectivity is low.\n\n## Installation\n\n```r\n# Install from GitHub\ndevtools::install_github(\"gojiplus/bloomjoin\")\n```\n\n## Usage\n\n```r\nlibrary(bloomjoin)\n\n# Basic usage\nresult <- bloom_join(df1, df2, by = \"id\", type = \"inner\")\n\n# With multiple join columns\nresult <- bloom_join(df1, df2, by = c(\"id\", \"date\"), type = \"left\")\n\n# With performance tuning parameters\nresult <- bloom_join(df1, df2, \n                    by = \"id\", \n                    type = \"inner\",\n                    bloom_size = 1000000, \n                    false_positive_rate = 0.001,\n                    verbose = TRUE)\n```\n\n## How It Works\n\nBloomJoin uses a hash-based approach to optimize joins:\n\n1. Create a hash set of all keys from the lookup table (y)\n2. Filter the primary table (x) to only include rows with keys that exist in the hash set\n3. Perform a standard join on the filtered dataset\n\nThis pre-filtering step can significantly reduce the size of the join operation when many keys in the primary table don't exist in the lookup table.\n\n## Performance Benchmarks\n\nSee [here](https://htmlpreview.github.io/?https://github.com/gojiplus/bloomjoin/blob/main/doc/benchmarking-bloomjoin.html)\n\n## Future Work\n\n1. Implement true Bloom filters for potentially better memory efficiency\n2. Optimize for composite keys and other join types\n3. Parallel processing for hash creation and filtering\n4. Automatic parameter tuning based on input data characteristics\n\n## License\n\nMIT\n\n## Contributing\n\nContributions welcome! Please feel free to submit ",
    "url": "https://github.com/gojiplus/bloomjoin",
    "last_updated": "2025-04-01T20:30:57+00:00"
  },
  {
    "full_name": "neuralmagic/deepsparse",
    "name": "deepsparse",
    "description": "Sparsity-aware deep learning inference runtime for CPUs",
    "language": "Python",
    "topics": [
      "machinelearning",
      "onnx",
      "inference",
      "computer-vision",
      "object-detection",
      "pruning",
      "quantization",
      "pretrained-models",
      "nlp",
      "cpus",
      "sparsification",
      "llm-inference",
      "performance",
      "deepsparse"
    ],
    "readme": "<!--\nCopyright (c) 2021 - 2025 / Neuralmagic, Inc. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <h1 style=\"display: flex; align-items: center;\" >\n     <img width=\"60\" height=\"60\" alt=\"tool icon\" src=\"https://neuralmagic.com/wp-content/uploads/2024/03/icon_DeepSparse-005.png\" />\n      <span>&nbsp;&nbsp;DeepSparse</span>\n  </h1>\n  <h4>Sparsity-aware deep learning inference runtime for CPUs</h4>\n\n## 🚨 2025 End of Life Announcement: DeepSparse, SparseML, SparseZoo, and Sparsify\n\nDear Community,\n\nWe’re reaching out with heartfelt thanks and important news. Following [Neural Magic’s acquisition by Red Hat in January 2025](https://www.redhat.com/en/about/press-releases/red-hat-completes-acquisition-neural-magic-fuel-optimized-generative-ai-innovation-across-hybrid-cloud), we’ve shifted our focus to commercial and open-source offerings built around [vLLM (virtual large language models)](https://www.redhat.com/en/topics/ai/what-is-vllm).\n\nAs part of this transition, we ceased development and deprecated the community versions of **DeepSparse (including DeepSparse Enterprise), SparseML, SparseZoo, and Sparsify on June 2, 2025**. These tools no longer will receive updates or support.\n\nFrom day one, our mission was to democratize AI through efficient, accessible tools. We’ve learned so much from your feedback, creativity, and collaboration—watching these tools become vital parts of your ML journeys has meant",
    "url": "https://github.com/neuralmagic/deepsparse",
    "last_updated": "2025-08-29T23:43:07+00:00"
  },
  {
    "full_name": "Librum-Reader/Librum",
    "name": "Librum",
    "description": "The Librum client application",
    "language": "C++",
    "topics": [
      "cmake",
      "cpp",
      "qml",
      "qt",
      "ebook-reader",
      "ebooks",
      "library-management",
      "reader",
      "linux",
      "qt6"
    ],
    "readme": "# Librum\n\nLibrum is an application designed to make reading <b>enjoyable</b> and <b>straightforward</b> for everyone.\n\nIt's not **just** an e-book reader. With Librum, you can manage your own online library and access it from any device anytime, anywhere. It has features like note-taking, AI tooling, and highlighting, while offering customization to make it as personal as you want!\n \nLibrum also provides free access to over 70,000 books and personal reading statistics while being free and completely open source.\n\n<br>\n\nDownload Librum from [our website](https://librumreader.com)!\n\n<br>\n\n# Table of Contents\n- [Preview](#Preview)\n- [Contributing](#Contributing)\n- [Contact](#Contact)\n- [Donations](#Donations)\n- [Translations](#Translations)\n- [Documentation](#Documentation)\n- [Self-hosting](#Self-hosting)\n- [Details](#Details)\n- [Build Guide](#Build-Guide)\n\n<br>\n\n# Preview\n\nSetup and manage your own online library\n\n![HomeScreenDark](https://github.com/Librum-Reader/Librum/assets/69865187/ea94fc68-1bf0-4933-8d80-43a57c6590c5)\n\n<br>\n\nA simple and modern interface\n\n![image](https://github.com/Librum-Reader/Librum/assets/69865187/bf1d0401-62bd-4f4e-b008-523fb2efd275)\n\n\n<br>\n\nAdd your books to collections, tag them, and sort them in any way you want\n\n![folders_dark](https://github.com/Librum-Reader/Librum/assets/69865187/00fec031-a835-4cae-89f1-79dbce24b356)\n\n\n<br>\n\nCustomize Librum to make it personal to you\n\n![image](https://github.com/Librum-Reader/Librum/assets/69865187/b8995cf1-a0e6-4993-8c8b-92f7f8e79ebd)\n\n\n<br>\n\n# Contributing\n\nIf you'd to support Librum's development, check out: https://librumreader.com/contribute\n<br>\n<br>\nAll of the current feature requests, bugs and tasks are listed in the [issues](https://github.com/Librum-Reader/Librum/issues). Easy tasks are labeled \"good first issue\", so that is a good starting point.\n<br>\n<br>\nPS: Feel free to tag me (@DavidLazarescu) in the comments of any issue if you have questions. \n\n<br>\n\n# Contact\n\nFor questions, you c",
    "url": "https://github.com/Librum-Reader/Librum",
    "last_updated": "2025-09-02T00:19:48+00:00"
  },
  {
    "full_name": "wsjdata/clinton-email-cruncher",
    "name": "clinton-email-cruncher",
    "description": "Download Hillary Clinton's emails and query them with sqlite",
    "language": "Python",
    "topics": [],
    "readme": "# Get and analyze Hillary Clinton's email\n\nIn response to a public records request, the U.S. State Department is releasing Hillary Clinton's email messages from her time as secretary of state. Every month, newly released messages are posted to [foia.state.gov](https://foia.state.gov/) as PDFs, with some metadata.\n\nThis collection of tools automates downloading and helps analyze the messages. The Wall Steet Journal's interactive graphics team uses some of this code to power our [Clinton inbox search](http://graphics.wsj.com/hillary-clinton-email-documents/) interactive.\n\nWe welcome your pull requests and issue reports.\n\n## What's in the toolkit\n* **run.sh** runs all of the Python scripts in the toolkit automatically, allowing easy updates when messages are released.\n\n* **downloadMetadata.py** scrapes sender, recipient, message date and subject from [the message list](https://foia.state.gov/Search/Results.aspx?collection=Clinton_Email) and writes this metadata to a sqlite database, `hrcemail.sqlite`.\n* **generatePDFList.py** writes `pdflist.txt`, a newline-delimited list of HTTPS URLs of the message PDFs.\n* **zipPDFs.py** makes a zip file of PDFs for each release of messages.\n* **pdfTextToDatabase.py** extracts text from the PDF files (which are OCR'd by State) and writes the text to a sqlite database, `hrcemail.sqlite`.\n\n* **HRCEMAIL_names.csv** is a list that pairs sender and recipient names provided by the State Department website with that person's commonly-used name. For example, `HRC` becomes `Hillary Clinton`.\n\n## How to get started\n\nClone the repo.\n```\ngit clone https://github.com/wsjdata/clinton-email-cruncher.git\ncd clinton-email-cruncher\n```\nInstall [virtualenv](http://docs.python-guide.org/en/latest/dev/virtualenvs/) if necessary.\n```\npip install virtualenv\n```\n\nCreate a virtual environment. **Python 2.7.9** is required, specifically for SSL (HTTPS) support. State Department's website requires HTTPS.\n```\nvirtualenv -p /usr/bin/python2.7 virt-hrcemail\nsourc",
    "url": "https://github.com/wsjdata/clinton-email-cruncher",
    "last_updated": "2024-08-12T19:19:08+00:00"
  },
  {
    "full_name": "stas00/ml-engineering",
    "name": "ml-engineering",
    "description": "Machine Learning Engineering Open Book",
    "language": "Python",
    "topics": [
      "pytorch",
      "slurm",
      "large-language-models",
      "llm",
      "machine-learning",
      "scalability",
      "transformers",
      "machine-learning-engineering",
      "mlops",
      "ai",
      "inference",
      "training"
    ],
    "readme": "# Machine Learning Engineering Open Book\n\nThis is an open collection of methodologies, tools and step by step instructions to help with successful training and fine-tuning of large language models and multi-modal models and their inference.\n\nThis is a technical material suitable for LLM/VLM training engineers and operators. That is the content here contains lots of scripts and copy-n-paste commands to enable you to quickly address your needs.\n\nThis repo is an ongoing brain dump of my experiences training Large Language Models (LLM) (and VLMs); a lot of the know-how I acquired while training the open-source [BLOOM-176B](https://huggingface.co/bigscience/bloom) model in 2022 and [IDEFICS-80B](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) multi-modal model in 2023, and RAG models at [Contextual.AI](https://contextual.ai/) in 2024.\n\nI've been compiling this information mostly for myself so that I could quickly find solutions I have already researched in the past and which have worked, but as usual I'm happy to share these notes with the wider ML community.\n\n\n## Table of Contents\n\n\n**Part 1. Insights**\n\n1. **[The AI Battlefield Engineering](./insights/ai-battlefield.md)** - what you need to know in order to succeed.\n\n1. **[How to Choose a Cloud Provider](./insights/how-to-choose-cloud-provider.md)** - these questions will empower you to have a successful compute cloud experience.\n\n**Part 2. Hardware**\n\n1. **[Compute](compute)** - accelerators, CPUs, CPU memory.\n\n1. **[Storage](storage)** - local, distributed and shared file systems.\n\n1. **[Network](network)** - intra- and inter-node networking.\n\n\n**Part 3. Orchestration**\n\n1. **[Orchestration Systems](orchestration)** - managing containers and resources\n1. **[SLURM](orchestration/slurm)** - Simple Linux Utility for Resource Management\n\n\n**Part 4. Training**\n\n1. **[Training](training)** - model training-related guides\n\n\n**Part 5. Inference**\n\n1. **[Inference](inference)** - model inference insights\n\n\n**Part 6",
    "url": "https://github.com/stas00/ml-engineering",
    "last_updated": "2025-09-02T06:24:57+00:00"
  },
  {
    "full_name": "connorferster/handcalcs",
    "name": "handcalcs",
    "description": "Python library for converting Python calculations into rendered latex.",
    "language": "CSS",
    "topics": [],
    "readme": "<p>\r\n<a href='https://coveralls.io/github/connorferster/handcalcs?branch=master'><img src='https://coveralls.io/repos/github/connorferster/handcalcs/badge.svg?branch=master' alt='Coverage Status' /></a>\r\n  <img src=\"https://img.shields.io/badge/code%20style-black-000000.svg\">\r\n  <img src=\"https://img.shields.io/pypi/v/handcalcs\">\r\n  <img src=\"https://img.shields.io/pypi/pyversions/handcalcs\">\r\n  <img src=\"https://img.shields.io/github/license/connorferster/handcalcs\">\r\n  <img src=\"https://static.pepy.tech/badge/handcalcs\">\r\n</p>\r\n<p align=\"center\">\r\n  <img src=\"docs/images/handcalcs.jpg\"><br>\r\n  Covert art by <a href = \"https://www.copperkettlegameworks.ca/\">Joshua Hoiberg</a>\r\n</p>\r\n\r\n<h1 align = \"center\">handcalcs:<br>Python calculations in Jupyter,<br>as though you wrote them by hand.</h1>\r\n\r\n`handcalcs` is a library to render Python calculation code automatically in Latex, but in a manner that mimics how one might format their calculation if it were written with a pencil:  write the symbolic formula, **followed by numeric substitutions**, and then the result.\r\n\r\nBecause `handcalcs` shows the numeric substitution, the calculations become significantly easier to check and verify by hand.\r\n\r\n> ### Engineers who use handcalcs\r\n> Did you know that you can _link_ your handcalc Jupyter notebooks together so that the result of one notebook can be available as an input for the next?\r\n> \r\n> [Opt-in here](https://www.structuralpython.com/handcalcs-the-chaining-technique) to see how you can use the [Chaining Technique](https://www.structuralpython.com/handcalcs-the-chaining-technique) to create entire engineering automations using your handcalcs notebooks.\r\n\r\n\r\n## Contents\r\n\r\n* [Basic Demo](https://github.com/connorferster/handcalcs#basic-demo)\r\n* [Installation](https://github.com/connorferster/handcalcs#installing)\r\n* [Basic Usage](https://github.com/connorferster/handcalcs#basic-usage-1-as-a-jupyter-cell-magic-render)\r\n* [Enhanced Usage](https://github.com/connorferster/h",
    "url": "https://github.com/connorferster/handcalcs",
    "last_updated": "2025-09-02T09:38:26+00:00"
  },
  {
    "full_name": "derekgreene/dynamic-nmf",
    "name": "dynamic-nmf",
    "description": "Dynamic Topic Modeling via Non-negative Matrix Factorization",
    "language": "Python",
    "topics": [],
    "readme": "# dynamic-nmf: Dynamic Topic Modeling\n\n### Summary\n\nStandard topic modeling approaches assume the order of documents does not matter, making them unsuitable for time-stamped corpora. In contrast, *dynamic topic modeling* approaches track how language changes and topics evolve over time. We have developed a two-level approach for dynamic topic modeling via Non-negative Matrix Factorization (NMF), which links together topics identified in snapshots of text sources appearing over time.\n\nIf you make use of this implementation, please consider citing [the associated paper](https://doi.org/10.1017/pan.2016.7):\n\n* Greene, Derek, and James P. Cross. \"Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach.\" Political Analysis 25.1 (2017): 77-94. [[PDF]](http://derekgreene.com/papers/greene17europarl.pdf) [[BibTeX]](http://derekgreene.com/bib/greene17europarl.bib) [[Preprint]](http://arxiv.org/abs/1607.03055)\n\t\nThis repository contains a Python reference implementation for the approach described in the paper.\n\n### Dependencies\nTested with Python 3.5+, and requiring the following packages which are available via PIP:\n\n* Required: [numpy >= 1.8.0](http://www.numpy.org/)\n* Required: [scikit-learn >= 0.14](http://scikit-learn.org/stable/)\n* Required for utility tools: [prettytable >= 0.7.2](https://code.google.com/p/prettytable/)\n* Required for automatic model selection: [gensim >= 0.10.3](https://radimrehurek.com/gensim/)\n\n### Basic Usage\n\nTo perform dynamic topic modeling, the input corpus of documents should consist of plain text files (one document per file), organised into two or more sub-directories. Each of these sub-directories should correspond to a unique *time window*, representing a different time interval. The names of these sub-directories is arbitrary, once their alphabetic ordering corresponds to their order in time (e.g 2000, 2001, 2002; month1, month2, month3; 2010-q1, 2010-q2, 2010-q3). \n\nThe dynamic topic modeling pro",
    "url": "https://github.com/derekgreene/dynamic-nmf",
    "last_updated": "2025-07-30T14:07:57+00:00"
  },
  {
    "full_name": "Textualize/rich",
    "name": "rich",
    "description": "Rich is a Python library for rich text and beautiful formatting in the terminal.",
    "language": "Python",
    "topics": [
      "python",
      "python3",
      "python-library",
      "terminal",
      "terminal-color",
      "markdown",
      "tables",
      "syntax-highlighting",
      "ansi-colors",
      "progress-bar-python",
      "progress-bar",
      "traceback",
      "rich",
      "tracebacks-rich",
      "emoji",
      "tui"
    ],
    "readme": "[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rich)](https://pypi.org/project/rich/) [![PyPI version](https://badge.fury.io/py/rich.svg)](https://badge.fury.io/py/rich)\n\n[![Downloads](https://pepy.tech/badge/rich/month)](https://pepy.tech/project/rich)\n[![codecov](https://img.shields.io/codecov/c/github/Textualize/rich?label=codecov&logo=codecov)](https://codecov.io/gh/Textualize/rich)\n[![Rich blog](https://img.shields.io/badge/blog-rich%20news-yellowgreen)](https://www.willmcgugan.com/tag/rich/)\n[![Twitter Follow](https://img.shields.io/twitter/follow/willmcgugan.svg?style=social)](https://twitter.com/willmcgugan)\n\n![Logo](https://github.com/textualize/rich/raw/master/imgs/logo.svg)\n\n[English readme](https://github.com/textualize/rich/blob/master/README.md)\n • [简体中文 readme](https://github.com/textualize/rich/blob/master/README.cn.md)\n • [正體中文 readme](https://github.com/textualize/rich/blob/master/README.zh-tw.md)\n • [Lengua española readme](https://github.com/textualize/rich/blob/master/README.es.md)\n • [Deutsche readme](https://github.com/textualize/rich/blob/master/README.de.md)\n • [Läs på svenska](https://github.com/textualize/rich/blob/master/README.sv.md)\n • [日本語 readme](https://github.com/textualize/rich/blob/master/README.ja.md)\n • [한국어 readme](https://github.com/textualize/rich/blob/master/README.kr.md)\n • [Français readme](https://github.com/textualize/rich/blob/master/README.fr.md)\n • [Schwizerdütsch readme](https://github.com/textualize/rich/blob/master/README.de-ch.md)\n • [हिन्दी readme](https://github.com/textualize/rich/blob/master/README.hi.md)\n • [Português brasileiro readme](https://github.com/textualize/rich/blob/master/README.pt-br.md)\n • [Italian readme](https://github.com/textualize/rich/blob/master/README.it.md)\n • [Русский readme](https://github.com/textualize/rich/blob/master/README.ru.md)\n • [Indonesian readme](https://github.com/textualize/rich/blob/master/README.id.md)\n • [فارسی readme](https://github.com/textualize",
    "url": "https://github.com/Textualize/rich",
    "last_updated": "2025-09-02T07:11:11+00:00"
  },
  {
    "full_name": "jkeirstead/scholar",
    "name": "scholar",
    "description": "Analyse citation data from Google Scholar",
    "language": "R",
    "topics": [],
    "readme": "# scholar\n<!-- badges: start -->\n[![CRAN status](https://www.r-pkg.org/badges/version/scholar)](https://CRAN.R-project.org/package=scholar)\n[![R-CMD-check](https://github.com/jkeirstead/scholar/workflows/R-CMD-check/badge.svg)](https://github.com/jkeirstead/scholar/actions)\n<!-- badges: end -->\n\nThe scholar R package provides functions to extract citation data from [Google Scholar](http://scholar.google.com).  In addition to retrieving basic information about a single scholar, the package also allows you to compare multiple scholars and predict future h-index values.\n\n*Development of the scholar package is ongoing with [GuangchuangYu](https://github.com/GuangchuangYu) acting as maintainer. \nPlease continue to file issues and make pull requests against https://github.com/YuLab-SMU/scholar going forwards.*\n\n## Installation\n\n```r\n# from CRAN\ninstall.packages(\"scholar\")\n\n# from GitHub\nif(!requireNamespace('remotes')) install.packages(\"remotes\")\nremotes::install_github('jkeirstead/scholar')\n```\n\n## Basic features\n\nIndividual scholars are referenced by a unique character string, which can be found by searching for an author and inspecting the resulting scholar homepage.  For example, the profile of physicist Richard Feynman is located at http://scholar.google.com/citations?user=B7vSqZsAAAAJ and so his unique id is `B7vSqZsAAAAJ`.\n\nBasic information on a scholar can be retrieved as follows:\n\n```\n# Define the id for Richard Feynman\nid <- 'B7vSqZsAAAAJ'\n\n# Get his profile and print his name\nl <- get_profile(id)\nl$name \n\n# Get his citation history, i.e. citations to his work in a given year \nget_citation_history(id)\n\n# Get his publications (a large data frame)\nget_publications(id)\n```\n\nAdditional functions allow the user to query the publications list, e.g. `get_num_articles`, `get_num_distinct_journals`, `get_oldest_article`, `get_num_top_journals`.  Note that Google doesn't explicit categorize publications as journal articles, book chapters, etc, and so *journal* or *articl",
    "url": "https://github.com/jkeirstead/scholar",
    "last_updated": "2025-07-28T10:13:22+00:00"
  },
  {
    "full_name": "paultopia/juriscraper",
    "name": "juriscraper",
    "description": "An API to scrape American court websites for metadata.",
    "language": "HTML",
    "topics": [],
    "readme": "[![Build Status](https://travis-ci.org/freelawproject/juriscraper.svg?branch=master)][12]\n\nWhat is This?\n=============\nJuriscraper is a scraper library started several years ago that gathers \njudicial opinions and oral arguments in the American court system. It is \ncurrently able to scrape:\n\n  - opinions from all major appellate Federal courts\n  - opinions from all state courts of last resort (typically their \"Supreme \n    Court\")\n  - oral arguments from all appellate federal courts that offer them\n\nJuriscraper is part of a two-part system. The second part is your code, which\ncalls Juriscraper. Your code is responsible for calling a scraper, downloading \nand saving its results. A reference implementation of the caller has been \ndeveloped and is in use at [CourtListener.com][2]. The code for that caller \ncan be [found here][1]. There is also a basic sample caller [included in \nJuriscraper][5] that can be used for testing or as a starting point when \ndeveloping your own.\n\nSome of the design goals for this project are:\n\n - extensibility to support video, oral argument audio, etc.\n - extensibility to support geographies (US, Cuba, Mexico, California)\n - Mime type identification through magic numbers\n - Generalized architecture with minimal code repetition\n - XPath-based scraping powered by lxml's html parser\n - return all meta data available on court websites (caller can pick what it needs)\n - no need for a database\n - clear log levels (DEBUG, INFO, WARN, CRITICAL)\n - friendly as possible to court websites\n\n\nInstallation & Dependencies\n===========================\nFirst step: Install Python 2.7.x, then:\n    \n    # install the dependencies\n    sudo apt-get install libxml2-dev libxslt-dev  # In Ubuntu prior to 14.04 this is libxslt-devel\n    \n    # Install PhantomJS\n    sudo pip install selenium\n    wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.7-linux-x86_64.tar.bz2\n    tar -x -f phantomjs-1.9.7-linux-x86_64.tar.bz2\n    sudo mkdir -p /usr/local/phanto",
    "url": "https://github.com/paultopia/juriscraper",
    "last_updated": "2015-06-23T17:48:47+00:00"
  },
  {
    "full_name": "nielathome/Phoenix",
    "name": "Phoenix",
    "description": "wxPython's Project Phoenix.  A new implementation of wxPython, better, stronger, faster than he was before.",
    "language": "Python",
    "topics": [],
    "readme": "=========================\nwxPython Project Phoenix\n=========================\n\n.. image:: demo/bitmaps/splash.png\n   :align: center\n\n\nIntroduction\n------------\n\nWelcome to wxPython's Project Phoenix! Phoenix is the improved next-generation\nwxPython, \"better, stronger, faster than he was before.\" This new\nimplementation is focused on improving speed, maintainability and\nextensibility. Just like \"Classic\" wxPython, Phoenix wraps the wxWidgets C++\ntoolkit and provides access to the user interface portions of the wxWidgets\nAPI, enabling Python applications to have a native GUI on Windows, Macs or\nUnix systems, with a native look and feel and requiring very little (if any)\nplatform specific code.\n\n.. note:: \n    This document is primarily intended for those who will be working on\n    wxPython, or at least building with the source code fetched directly from\n    GitHub. If that's not you then please refer to the instructions at the\n    `wxPython website <https://wxpython.org/pages/downloads/>`_ about how to get\n    the current release of wxPython for your platform and chosen Python\n    environment.\n\n.. contents:: **Contents**\n\n\nHow to build wxPython Phoenix\n-----------------------------\n\nFirst of all, this README is intended primarily for those who want to build\nwxPython from a workspace checked out from the wxPython Phoenix repository. If\nyou are not making changes to wxPython, or needing to build it for some\nunsupported compiler or some other hardware architecture, then you probably do\nnot need to put yourself through the pain for building in this way. It's a\ncomplicated build, and can sometimes be confusing even for the experts.\nInstead, if the binaries available at PyPI are not what you need then you can\nuse pip to build from the released source archives, or from the source archives\ncreated in the pre-release snapshot builds. See the notes about it at: \n\n* https://wxpython.org/pages/downloads/\n* https://wxpython.org/blog/2017-08-17-builds-for-linux-with-pip\n\n\nNext, revi",
    "url": "https://github.com/nielathome/Phoenix",
    "last_updated": "2025-01-15T15:30:14+00:00"
  },
  {
    "full_name": "CitationGecko/citation-network-explorer",
    "name": "citation-network-explorer",
    "description": "Tool for exploring local citation networks.",
    "language": "JavaScript",
    "topics": [],
    "readme": "**_ WARNING: This repo has been deprecated in favour of https://github.com/CitationGecko/gecko-react _**\n\n# Welcome to Citation Gecko!\n\n[![Open Source Love](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badges/)\n[![MIT Licence](https://badges.frapsoft.com/os/mit/mit.svg?v=103)](https://opensource.org/licenses/mit-license.php)\n\nThis is a tool that uses the citation relations between scientific papers to help researchers find interesting and relevant papers.\n\nThe user specifies several 'seed' papers which define the specific area of the scientific landscape they are interested in.\n\nThe tool then searches several databases to find the papers that cite or are cited-by the seed papers.\n\nPapers that are cited by a lot of the seed papers are likely to be important foundational papers in the field (or certainly worth being aware of at least).\n\nPapers that cite a lot of the seed papers are likely to be more recent papers in the same area that might be worth reading.\n\nThe tool allows the user to view these highly connected papers either in a table or in the context of the network.\n\n## Live demo\n\n[citationgecko.com](http://citationgecko.com)\n\n## Running Citation Gecko locally\n\n1. Clone the git repo:\n   `git clone https://github.com/CitationGecko/citation-network-explorer.git`\n2. If you don't have it already install Node.js from https://nodejs.org/en/.\n3. Open a terminal and navigate to the repository folder.\n4. Run `npm install` from the command line to install all the package dependencies.\n5. Run `npm run build` from the command line to build the app.\n6. Run `node server.js` to launch the server.\n7. The application will start at http://localhost:3000\n\n## Instructions for use\n\n1. Go to [citationgecko.com](http://citationgecko.com) or [localhost:3000](http://localhost:3000) if you're running application locally\n2. Add some seed papers by clicking 'Add more seed papers' button in the left-hand panel.\n3. There are several ways",
    "url": "https://github.com/CitationGecko/citation-network-explorer",
    "last_updated": "2025-03-05T09:30:31+00:00"
  },
  {
    "full_name": "automeris-io/WebPlotDigitizer",
    "name": "WebPlotDigitizer",
    "description": "Computer vision assisted tool to extract numerical data from plot images.",
    "language": "JavaScript",
    "topics": [
      "javascript",
      "webplotdigitizer",
      "html",
      "data-mining",
      "visualization",
      "reverse-engineering",
      "charts",
      "computer-vision"
    ],
    "readme": "# WebPlotDigitizer\n\nA large quantity of useful data is locked away in images of data visualizations. WebPlotDigitizer is a computer vision assisted software that helps extract numerical data from images of a variety of data visualizations.\n\nWPD has been used by thousands in academia and industry since its creation in 2010 (Google Scholar Citations)\n\nTo use WPD, sign-up on https://automeris.io\n\n![WPD Screenshot](images/wpd5.png \"WebPlotDigitizer UI\")\n\n## Donate\n\nDonatations help keeping WPD free for thousands of scientists and researchers across the world.\n\n<a href='https://ko-fi.com/L4L010CWIY' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi6.png?v=6' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>\n\n## Documentation\n\nVisit: https://automeris.io/docs/\n\n## License\n\nWPD frontend is distributed under GNU AGPL v3 license (this repository). \n\nAutomeris \"AI Assist\" and other related cloud based systems are closed source and owned by Automeris LLC (owned by Ankit Rohatgi).\n\n## Contact\n\nPrimary Author and Maintainer: Ankit Rohatgi\n\nEmail: plots@automeris.io\n\n## Contributions\n\nWPD does not have an official roadmap. Please consult before submitting contributions.\n\n\n## Local build (for development)\n\nWith Docker:\n```\ndocker compose up --build               # install depedencies, build and host\ndocker compose run wpd npm run build    # rebuild\ndocker compose run wpd npm run format   # autoformat code\nhttp://localhost:8080/tests             # run tests\n```\n\nWithout Docker:\n```\nnpm install     # install dependencies\nnpm run build   # build artifacts\nnpm start       # host locally\nnpm run format  # autoformat code\nnpm run test    # run tests\n```\n",
    "url": "https://github.com/automeris-io/WebPlotDigitizer",
    "last_updated": "2025-08-31T15:27:15+00:00"
  },
  {
    "full_name": "paulhendricks/scorer",
    "name": "scorer",
    "description": "Metrics for scoring machine learning models in R",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\nscorer\n======\n\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/scorer)](http://cran.r-project.org/package=scorer) [![Downloads from the RStudio CRAN mirror](http://cranlogs.r-pkg.org/badges/scorer)](https://cran.rstudio.com/web/packages/scorer/index.html) [![Build Status](https://travis-ci.org/paulhendricks/scorer.png?branch=master)](https://travis-ci.org/paulhendricks/scorer) [![Build status](https://ci.appveyor.com/api/projects/status/vuumrc0607xa44q9/branch/master?svg=true)](https://ci.appveyor.com/project/paulhendricks/scorer/branch/master) [![codecov.io](http://codecov.io/github/paulhendricks/scorer/coverage.svg?branch=master)](http://codecov.io/github/paulhendricks/scorer?branch=master) [![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)\n\n`scorer` is a set of tools for quickly scoring models in data science and machine learning. This toolset is written in C++, where possible, for blazing fast performance. This toolset's API follows that of Python's [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) as closely as possible so one can easily switch back and forth between R and Python without too much cognitive dissonance. The following types of metrics are currently implemented in `scorer`:\n\n-   Regression metrics (implemented in 0.2.0)\n-   Classification metrics (implemented in 0.3.0)\n\nThe following types of metrics are soon to be implemented in `scorer`:\n\n-   Multilabel ranking metrics (to be implemented in 0.4.0)\n-   Clustering metrics (to be implemented in 0.4.0)\n-   Biclustering metrics (to be implemented in 0.4.0)\n-   Pairwise metrics (to be implemented in 0.4.0)\n\nInstallation\n------------\n\nYou can install the latest development version from CRAN:\n\n``` r\ninstall.packages(\"scorer\")\n```\n\nOr f",
    "url": "https://github.com/paulhendricks/scorer",
    "last_updated": "2024-04-18T03:58:02+00:00"
  },
  {
    "full_name": "BuzzFeedNews/2016-01-tennis-betting-analysis",
    "name": "2016-01-tennis-betting-analysis",
    "description": "Methodology and code supporting the BuzzFeed News/BBC article, \"The Tennis Racket,\" published Jan. 17, 2016.",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Methodology and Code: Detecting Match-Fixing Patterns In Tennis\n\nA closer look at the data analysis behind BuzzFeed News’ investigation into corruption in tennis. \n\n## General Notes\n\n**In “[The Tennis Racket](http://www.buzzfeed.com/heidiblake/the-tennis-racket),” a yearlong investigation into match-fixing in professional tennis, BuzzFeed News published findings from an original data analysis we performed. That analysis revealed many examples of one particularly suspicious pattern: heavy betting against a player, followed by that player’s loss.**\n\nBetting patterns alone **aren’t proof of fixing**. Players can underperform for all sorts of reasons — injury, fatigue, bad luck — and sometimes that underperformance will just happen to coincide with heavy betting against them. But it's extremely unlikely for a player to underperform repeatedly in matches on which people just happen to be betting massive sums against him.\n\nIn developing this analysis, BuzzFeed News consulted with Abraham Wyner, a professor of statistics at the University of Pennsylvania, and Thomas Severini, a professor of statistics at Northwestern University.\n\nTo see the code that we used for the analysis, [go here](./notebooks/tennis-analysis.ipynb).\n\n**An important note:** The analysis was undertaken with only the betting information that is publicly available. Tennis authorities and betting houses have access to much finer-grained data, such as the accounts placing bets, as well as forensic evidence such as phone data and bank records. Without access to such information, it is impossible to know with a sufficient degree of certainty whether these suspicious patterns are indeed the result of match fixing. For this reason, BuzzFeed News has decided not to name the players.\n\n## Methodology\n\n1. **Data Acquisition.** The analysis began by collecting the opening and closing odds of more than 26,000 tennis matches that occurred between 2009 and mid-September 2015. We downloaded the odds for Association of",
    "url": "https://github.com/BuzzFeedNews/2016-01-tennis-betting-analysis",
    "last_updated": "2025-01-21T20:47:49+00:00"
  },
  {
    "full_name": "HughParsonage/grattan",
    "name": "grattan",
    "description": "Common quantitative tasks for Australian policy analysts",
    "language": "R",
    "topics": [
      "r",
      "australian-analysts",
      "tax"
    ],
    "readme": "[![Coverage\nstatus](https://codecov.io/gh/HughParsonage/grattan/branch/master/graph/badge.svg)](https://app.codecov.io/github/HughParsonage/grattan?branch=master)\n\ngrattan\n=======\n\nUtilities for costing and evaluating Australian tax policy, including\nhigh-performance tax and transfer calculators, a fast method of\nprojecting tax collections from ATO sample files, and an interface to\ncommon indices from the Australian Bureau of Statistics. Written to\nsupport Grattan Institute’s Australian Perspectives program.\n\nOverview\n========\n\n``` r\ninstall.packages(\"grattan\")\n```\n\n``` r\nlibrary(grattan)\n```\n\n`income_tax`\n------------\n\nCalculates the income tax for a given taxable income and financial year:\n\n``` r\nincome_tax(50e3, \"2015-16\")\n```\n\n    ## [1] 8547\n\n### With sample files\n\n`income_tax` is designed to work well with the ATO’s sample files. You\ncan obtain the sample files from my repo:\n\n``` r\n# install.packages(\"taxstats\", repos = \"https://hughparsonage.github.io/tax-drat\")\nlibrary(taxstats)\n\nlibrary(hutils)\nlibrary(data.table) \nlibrary(magrittr)\nlibrary(ggplot2)\n```\n\nSimply pass the sample file to `.dots.ATO` and the complexities of\nthings like Medicare levy and the Seniors and Pensioners Tax Offset are\nhandled for you. For example:\n\n``` r\ns1314 <- as.data.table(sample_file_1314)\ns1314 %>%\n  .[, tax := income_tax(Taxable_Income, \"2013-14\", .dots.ATO = s1314)] %>%\n  .[, .(Taxable_Income, tax)]\n```\n\n    ##         Taxable_Income       tax\n    ##      1:           4800     0.000\n    ##      2:         126122 36503.970\n    ##      3:          39742  4655.410\n    ##      4:         108123 29574.355\n    ##      5:          85957 21040.445\n    ##     ---                         \n    ## 258770:          24462  1111.710\n    ## 258771:          37055  3701.525\n    ## 258772:          45024  6530.520\n    ## 258773:           5134     0.000\n    ## 258774:          46368  7007.640\n\n`model_income_tax`: modelling changes to personal income tax\n-----------------------------------------",
    "url": "https://github.com/HughParsonage/grattan",
    "last_updated": "2025-03-22T08:14:13+00:00"
  },
  {
    "full_name": "TeamHG-Memex/hh-page-classifier",
    "name": "hh-page-classifier",
    "description": "Headless Horseman Page Classifier service",
    "language": "Python",
    "topics": [],
    "readme": "Headless Horseman Page Classifier\n=================================\n\nIt gets pages and their labels from Sitehound\n(previously The Headless Horseman, or THH)\nvia a kafka queue, trains a model, and sends back both model\nand some quality report. The user of THH then might label more pages,\nallowing the classifier to reach higher accuracy.\n\nIncoming message example::\n\n    {\n      \"id\": \"some id that will be returned in the answer message\",\n      \"pages\": [\n        {\n          \"url\": \"http://example.com\",\n          \"html\": \"<h1>hi</h1>\",\n          \"relevant\": true\n        },\n        {\n          \"url\": \"http://example.com/1\",\n          \"html\": \"<h1>hi 1</h1>\",\n          \"relevant\": false\n        },\n        {\n          \"url\": \"http://example.com/2\",\n          \"html\": \"<h1>hi 2</h1>\",\n          \"relevant\": null\n        }\n      ]\n    }\n\nOutgoing message with trained model::\n\n    {\n      \"id\": \"the same id\",\n      \"quality\": \"{ ... }\",\n      \"model\": \"b64-encoded page classifier model\"\n    }\n\n``quality`` field is a JSON-encoded string. Here is an example::\n\n    {\n     \"advice\": [\n      {\n       \"kind\": \"Warning\",\n       \"text\": \"The quality of the classifier is not very good, ROC AUC is just 0.67. Consider labeling more pages, or re-labeling them using different criteria.\"\n      }\n     ],\n     \"description\": [\n      {\"heading\": \"Dataset\", \"text\": \"183 documents, 183 with labels (100%) across 129 domains.\"},\n      {\"heading\": \"Class balance\", \"text\": \"40% relevant, 60% not relevant.\"},\n      {\"heading\": \"Metrics\", \"text\": \"\"},\n      {\"heading\": \"Accuracy\", \"text\": \"0.628 ± 0.087\"},\n      {\"heading\": \"F1\", \"text\": \"0.435 ± 0.140\"},\n      {\"heading\": \"ROC AUC\", \"text\": \"0.666 ± 0.127\"}\n     ],\n     \"tooltips\": {\n      \"Accuracy\": \"Accuracy is the ratio of pages classified correctly as relevant or not relevant. This metric is easy to interpret but not very good for unbalanced datasets.\",\n      \"F1\": \"F1 score is a combination of recall and precision for detecting relevant pages.",
    "url": "https://github.com/TeamHG-Memex/hh-page-classifier",
    "last_updated": "2024-01-23T22:46:56+00:00"
  },
  {
    "full_name": "wolffg/tf-tutorial",
    "name": "tf-tutorial",
    "description": "Short tutorial for TensorFlow, designed to be presented in-person",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# TensorFlow Tutorial\n\nWelcome to the TensorFlow Tutorial.\n\nPlease install TensorFlow on your computer before you arrive.\n\nFor this tutorial, you can do one of the following:\n\n 1. [Install TensorFlow directly on your Mac.](install-mac-native.md)\n*This is recommended*, as it installs the least amount on your computer\nand will run the fastest.\n\n 1. [Use Docker on Mac, running TensorFlow in a local\nVM.](install-mac-docker.md) If for some reason you can't install\nTensorFlow natively or already have Docker installed, you can try this.\n\n 1. [Install TensorFlow directly on your Ubuntu/Linux\nmachine.](install-ubuntu.md)\n\n_Note that TensorFlow is not currently supported natively on Windows._\n\nIf you run into trouble, our full install instrictions are available\non http://tensorflow.org.\n\n\n\n\n\n\n",
    "url": "https://github.com/wolffg/tf-tutorial",
    "last_updated": "2025-01-23T05:56:41+00:00"
  },
  {
    "full_name": "notnews/unreadable_news",
    "name": "unreadable_news",
    "description": "Unreadable News: How Readable is American News?",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Unreadable News: How Readable is American News?\n\n\"[M]ore than half of Americans between the ages of 16 and 74 (54%) read below the equivalent of a sixth-grade level.\"\n\nhttps://apmresearchlab.org/10x-adult-literacy\n\nAre these low levels of reading comprehension a barrier to becoming politically informed? To shed light on the concern, we estimate the readability of news produced by prominent outlets: [The New York Times](https://github.com/notnews/nytimes-corpus-extractor),  [CNN](https://github.com/notnews/cnn_transcripts), [NPR](https://github.com/zcgzcgzcg1/MediaSum/tree/main/data), and [MSNBC](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FUPJDE1).\n\n### NYT is the least readable\n\nThe NYT has the lowest readability score of the four outlets, with readers needing a high school education or more to read the average article. NYT also uses more unique terms. Even so, the readability of NYT articles has increased slightly while the other three news outlets (CNN, NPR, MSNBC) have either remained as readable or become slightly worse over time. The trend in NYT's readability suggests they feel competition from alternative news sources, both traditional and non-traditional. Our findings suggest NYT is striving to increase accessibility while maintaining its key demographic of college-educated Americans.\n\n### Data and Methods\n\nNYT article data is available for 1987&ndash;2007 (21 years), N = 1.8 million:\n\n<p align=\"center\"><img width=\"60%\" src=\"figs/data_coverage/nyt_monthyear.png\"></p>\n\nAt a finer resolution, we can see that the Sunday newspaper has more articles than the weekday editions, and increasingly so. Below is a plot for two full years of NYT data coverage:\n\n<p align=\"center\"><img width=\"60%\" src=\"figs/data_coverage/nyt_dowmonth_1987.png\"></p>\n\n<p align=\"center\"><img width=\"60%\" src=\"figs/data_coverage/nyt_dowmonth_2006.png\"></p>\n\nBefore computing readability and lexical richness scores, we drop articles over the 99th percentile in",
    "url": "https://github.com/notnews/unreadable_news",
    "last_updated": "2023-09-30T19:21:45+00:00"
  },
  {
    "full_name": "mysociety/fixmystreet",
    "name": "fixmystreet",
    "description": "This is mySociety's popular map-based reporting platform: easy to install in new countries and regions",
    "language": "Perl",
    "topics": [
      "international",
      "civic-tech",
      "map",
      "reporting",
      "councils",
      "mysociety",
      "civictech",
      "fixmystreet"
    ],
    "readme": "# Welcome to FixMyStreet Platform\n\nFixMyStreet Platform is an open source project to help people run websites for\nreporting common street problems such as potholes and broken street lights to\nthe appropriate authority.\n\nUsers locate problems using a combination of address and sticking a pin\nin a map without worrying about the correct authority to report it to.\nFixMyStreet then works out the correct authority using the problem location and\ntype and sends a report, by email or using a web service such as Open311.\nReported problems are visible to everyone so they can see if something has\nalready been reported and leave updates. Users can also subscribe to email or\nRSS alerts of problems in their area.\n\nIt was created in 2007 by [mySociety](https://www.mysociety.org/) for reporting\nproblems to UK councils and has been copied around the world. The FixMyStreet\nPlatform is now at version 6.0; see CHANGELOG.md for a version history.\n\n## Installation\n\nWe've been working hard to make the FixMyStreet Platform easy to install and\nre-use in other countries - please see our site at <https://fixmystreet.org/>\nfor help and documentation in installing the FixMyStreet Platform.\n\nFor development, if you have Vagrant installed, you can clone the repo and run\n'vagrant up'. We use [Scripts to Rule Them All](https://githubengineering.com/scripts-to-rule-them-all/)\nso `script/update` will update your checkout, `script/server` will run a dev\nserver, and `script/test` will run the tests.\n\n## Contribution Guidelines\n\nWhilst many contributions come as part of people setting up their own\ninstallation for their area, we of course welcome stand-alone contributions as\nwell. The [*Suitable for\nVolunteers*](https://github.com/mysociety/fixmystreet/labels/Suitable%20for%20Volunteers)\nlabel in our GitHub issues hopefully labels some potential tasks that might be\nsuitable for that situation, though please do search through the other issues\nto see if what you're after has been suggested or discussed - o",
    "url": "https://github.com/mysociety/fixmystreet",
    "last_updated": "2025-09-01T12:16:28+00:00"
  },
  {
    "full_name": "taubergm/news_readability",
    "name": "news_readability",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# news_readability\n",
    "url": "https://github.com/taubergm/news_readability",
    "last_updated": "2021-07-08T20:13:02+00:00"
  },
  {
    "full_name": "r-lib/progress",
    "name": "progress",
    "description": "Progress bar in your R terminal",
    "language": "R",
    "topics": [
      "r"
    ],
    "readme": "\n<h1 align=\"center\">\n    <br>\n    <br>\n    <img width=\"400\" src=\"man/figures/logo.png\" alt=\"progress\">\n    <br>\n    <br>\n    <br>\n</h1>\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/r-lib/progress/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/progress/actions/workflows/R-CMD-check.yaml)\n[![](https://www.r-pkg.org/badges/version/progress)](https://r-pkg.org/pkg/progress)\n[![Codecov test coverage](https://codecov.io/gh/r-lib/progress/graph/badge.svg)](https://app.codecov.io/gh/r-lib/progress)\n<!-- badges: end -->\n\n> Progress bar in your R terminal\n\nAn R package to show ASCII progress bars. Heavily influenced by\nthe https://github.com/tj/node-progress JavaScript project.\n\n## Installation\n\nInstall the package from CRAN:\n\n```r\ninstall.packages(\"progress\")\n```\n\nIf you need the development version, install it from GitHub:\n\n```r\npak::pak(\"r-lib/progress\")\n```\n\n## Usage\n\nUse the `progress_bar` R6 class:\n\n```r\nlibrary(progress)\npb <- progress_bar$new(total = 100)\nfor (i in 1:100) {\n  pb$tick()\n  Sys.sleep(1 / 100)\n}\n```\n\n```\n[==========================================================-------------]  81%\n```\n\nThe progress bar is displayed after the first `tick` command.\nThis might not be desirable for long computations, because\nnothing is shown before the first tick. It is good practice to\ncall `tick(0)` at the beginning of the computation or download,\nwhich shows the progress bar immediately.\n\n```r\npb <- progress_bar$new(total = 100)\nf <- function() {\n  pb$tick(0)\n  Sys.sleep(3)\n  for (i in 1:100) {\n    pb$tick()\n    Sys.sleep(1 / 100)\n  }\n}\nf()\n```\n\nCustom format, with estimated time of completion:\n\n```r\npb <- progress_bar$new(\n  format = \"  downloading [:bar] :percent eta: :eta\",\n  total = 100, clear = FALSE, width= 60)\nfor (i in 1:100) {\n  pb$tick()\n  Sys.sleep(1 / 100)\n}\n```\n\n```\n  downloading [========----------------------]  28% eta:  1s\n```\n\nWith elapsed time:\n\n```r\npb <- progress_bar$new(\n  format = \"  downloading [:bar] :percent ",
    "url": "https://github.com/r-lib/progress",
    "last_updated": "2025-08-21T22:01:34+00:00"
  },
  {
    "full_name": "danilofreire/rmarkdown-templates",
    "name": "rmarkdown-templates",
    "description": "A collection of personal templates for RMarkdown",
    "language": "HTML",
    "topics": [
      "rmarkdown",
      "template",
      "pdf",
      "article",
      "letter",
      "title-page",
      "rmarkdown-templates",
      "syllabus"
    ],
    "readme": "# RMarkdown Templates\n\nThis repository contains a collection of personal [RMarkdown](http://rmarkdown.rstudio.com/) templates. The article, letter, syllabus, and title page have a similar style and use [Linux Libertine](http://www.linuxlibertine.org/) and [Inconsolata](https://fonts.google.com/specimen/Inconsolata) fonts, British English spelling, coloured links, double spacing, back references, and numbered sections. Also, the article template already includes all packages required by [kableExtra](https://github.com/haozhu233/kableExtra); it should work out of the box. The syllabus template is a slightly modified version of [Steven Miller's example](https://github.com/svmiller/svm-r-markdown-templates/tree/master/syllabus-example) and the letter template borrows a lot from the [linl R package](https://github.com/eddelbuettel/linl/). The presentation template was made with [xaringan](https://github.com/yihui/xaringan) and [xaringanthemer](https://github.com/gadenbuie/xaringanthemer). I would like to thank the authors for sharing their code. I plan to include templates for CVs and academic posters soon.\n\nEach folder contains a pdf with the output of the template. Check them out to see how the documents look like. Comments and suggestions are most welcome.\n\nTo compile the pdfs, you need to install [R](https://www.r-project.org/) (and the [rmarkdown package](https://cran.r-project.org/package=rmarkdown)), [pandoc](http://pandoc.org/), and a [TeX system](https://www.latex-project.org/get/).\n",
    "url": "https://github.com/danilofreire/rmarkdown-templates",
    "last_updated": "2025-06-07T14:42:58+00:00"
  },
  {
    "full_name": "thomasahle/arithmetic-transformer",
    "name": "arithmetic-transformer",
    "description": "Teaching Addition to Small Transformers",
    "language": "Python",
    "topics": [],
    "readme": "# Learning Arithmetic with Sequence Models\nThis repository contains code for training neural networks to learn the process of addition.\nWe experiment with different causal neural network architectures like LSTM, Transformer, and various positional encodings. The best one being a transformer *using a one layer LSTM as layer 1*.\nThe primary focus of this project is to compare how quickly different models learn to add numbers with varying digit lengths.\n\n## Results\nHere is a summary of the number of epochs different models took to learn addition with various digit lengths:\n\n### 4 layers, 32 hidden size, 1 head\nThe table shows the number of epochs needed to learn addition of `n` digits.\nE.g. the Hybrid model took just 7 epochs (7*10^6 examples) to learn 8 digit addition to 90% accuracy, after it had already learned 7 digit addition to 90% accuracy.\n\nTo learn 14 digit addition, the hybrid model took a total of 135 epochs ~ 10^8 examples.\nThis is of course much less than the total of different 10^28 possible input pairs.\nThe larger models (below) are able to learn even faster.\n\n|Digits| Transformer Learned | Transformer Sine | Transformer NoPE | Transformer LSTM | LSTM | Hybrid |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n| 2 | 1 | 2 | 3 | 1 | 2 | 2 |\n| 3 | 2 | 3 | 4 | 2 | 7 | 3 |\n| 4 | 21 | 6 | 8 | 3 | 11 | 5 |\n| 5 | 16 | 5 | 15 | 7 | 24 | 8 |\n| 6 | 81 | 9 | 33 | 4 | 59 | 9 |\n| 7 | 270+ | 7 | 82 | 3 | 52 | 8 |\n| 8 | - | 15 | 282 | 9 | - | 7 |\n| 9 | - | 27 | - | 8 | - | 11 |\n| 10 | - | 42 | - | 10 | - | 20 |\n| 11 | - | 44 | - | 12 | - | 33 |\n| 12 | - | - | - | 21 | - | 34 |\n| 13 | - | - | - | 27 | - | 48 |\n| 14 | - | - | - | 27 | - | 103 |\n\nEach model in the table had roughly 32K parameters.\n\n\n### 4 layers, 64 hidden size, 4 heads\n\nIncreasing the model sizes just slightly allowed the model to learn multi-digit addition much faster.\nThe hybrid model got up to 18 digits with just a few epochs per digit, before something finally broke during th",
    "url": "https://github.com/thomasahle/arithmetic-transformer",
    "last_updated": "2025-06-13T16:46:52+00:00"
  },
  {
    "full_name": "langchain-ai/intro-to-langsmith",
    "name": "intro-to-langsmith",
    "description": "Resources for the LangSmith Academy Course",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Intro to LangSmith\n\nWelcome to Intro to LangSmith!\n\n## Introduction\nIn this course we will walk through the fundamentals of LangSmith - exploring observability, prompt engineering, evaluations, feedback mechanisms, and production monitoring. Take a look at the setup instructions below so you can follow along with any of our notebook examples.\n\n---\n\n## Setup\nFollow these instructions to make sure you have all the resources necessary for this course!\n\n### Sign up for LangSmith\n* Sign up [here](https://smith.langchain.com/) \n* Navigate to the Settings page, and generate an API key in LangSmith.\n* Create a .env file that mimics the provided .env.example. Set `LANGCHAIN_API_KEY` in the .env file.\n\n### Set OpenAI API key\n* If you don't have an OpenAI API key, you can sign up [here](https://openai.com/index/openai-api/).\n* Set `OPENAI_API_KEY` in the .env file.\n\n### Create an environment and install dependencies\n```\n$ cd intro-to-langsmith\n$ python3 -m venv intro-to-ls\n$ source intro-to-ls/bin/activate\n$ pip install -r requirements.txt\n```\n\n### Self-Hosted LangSmith\nNote: If you are using a self-hosted version of LangSmith, you'll need to set this environment variable in addition to the others - see this [guide](https://docs.smith.langchain.com/self_hosting/usage) for more info\n```\nLANGSMITH_ENDPOINT = \"<your-self-hosted-url>/api/v1\"\n```\n",
    "url": "https://github.com/langchain-ai/intro-to-langsmith",
    "last_updated": "2025-08-26T01:22:23+00:00"
  },
  {
    "full_name": "williamyang1991/Rerender_A_Video",
    "name": "Rerender_A_Video",
    "description": "[SIGGRAPH Asia 2023] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation",
    "language": "Jupyter Notebook",
    "topics": [
      "controlnet",
      "diffusion",
      "video-processing"
    ],
    "readme": "# Rerender A Video - Official PyTorch Implementation\n\n![teaser](https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/aa7dc164-dab7-43f4-a46b-758b34911f16)\n\n<!--https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/82c35efb-e86b-4376-bfbe-6b69159b8879-->\n\n\n**Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation**<br>\n[Shuai Yang](https://williamyang1991.github.io/), [Yifan Zhou](https://zhouyifan.net/), [Ziwei Liu](https://liuziwei7.github.io/) and [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)<br>\nin SIGGRAPH Asia 2023 Conference Proceedings <br>\n[**Project Page**](https://www.mmlab-ntu.com/project/rerender/) | [**Paper**](https://arxiv.org/abs/2306.07954) | [**Supplementary Video**](https://youtu.be/cxfxdepKVaM) | [**Input Data and Video Results**](https://drive.google.com/file/d/1HkxG5eiLM_TQbbMZYOwjDbd5gWisOy4m/view?usp=sharing) <br>\n\n<a href=\"https://huggingface.co/spaces/Anonymous-sub/Rerender\"><img src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg\" alt=\"Web Demo\"></a> ![visitors](https://visitor-badge.laobi.icu/badge?page_id=williamyang1991/Rerender_A_Video)\n\n> **Abstract:** *Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal con",
    "url": "https://github.com/williamyang1991/Rerender_A_Video",
    "last_updated": "2025-09-02T05:54:09+00:00"
  },
  {
    "full_name": "sagarbhure/pyoqs_sdk",
    "name": "pyoqs_sdk",
    "description": "f5oqs_sdk: Python Module for securing applications against quantum",
    "language": "Python",
    "topics": [],
    "readme": "> :warning: **CAUTION**: This project is for demo purposes only and not intended for production use.\n\n\n\n# Quantum-Proof Security with pyoqs_sdk in Python \n\n[![Build status](https://www.python.org/static/community_logos/python-logo.png)](https://pypi.org/project/pyoqs-sdk/)\n\n[![Build status](https://ci.appveyor.com/api/projects/status/jjo1ti9l5e0grgln?svg=true)](https://github.com/sagarbhure/pyoqs_sdk/releases/tag/v2.0) ![Python](https://img.shields.io/badge/python-v3.6+-blue.svg)  ![Dependencies](https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg)  ![Contributions welcome](https://img.shields.io/badge/contributions-welcome-orange.svg)  ![License](https://img.shields.io/badge/license-MIT-blue.svg)\n\npyoqs_sdk PyPi : https://pypi.org/project/pyoqs-sdk/\n\nThe Open Quantum Safe (OQS) project has the goal of developing and prototyping quantum-resistant cryptography.\nliboqs is an open source C library for quantum-resistant cryptographic algorithms. See more about liboqs at https://github.com/open-quantum-safe/liboqs/, including a list of supported algorithms.\n\n\n\npyoqs_sdk is an open-source Python 3 library that wraps the liboqs C library. It offers a unified API for post-quantum key encapsulation and digital signature schemes, as well as a collection of open-source implementations of post-quantum cryptography algorithms. \n\nThe OQS project also includes prototype integrations into various application-level protocols to test the effectiveness of quantum-resistant cryptography. For more information, visit https://openquantumsafe.org/\n## Pre-requisite\nPython 3.x pyoqs_sdk depends on the liboqs C library; liboqs must first be compiled as a Linux/macOS/Windows library.\n## Contents \n\nThis Project contains following Contents\n\n- `pyoqs_sdk/pyoqs_sdk.py`: a Python 3 module wrapper for the liboqs C library.\n- `pyoqs_sdk/rand.py`: a Python 3 module supporting RNGs from <oqs/rand.h>\n- `test`: unit test to be added\n\n\n\n## Installation\n\nThis project is on [PyPI](https",
    "url": "https://github.com/sagarbhure/pyoqs_sdk",
    "last_updated": "2025-01-15T15:29:44+00:00"
  },
  {
    "full_name": "nytimes/ingredient-phrase-tagger",
    "name": "ingredient-phrase-tagger",
    "description": "Extract structured data from ingredient phrases using conditional random fields",
    "language": "Python",
    "topics": [],
    "readme": "# CRF Ingredient Phrase Tagger\n\nThis repo contains scripts to extract the Quantity, Unit, Name, and Comments\nfrom unstructured ingredient phrases. We use it on [Cooking][nytc] to format\nincoming recipes. Given the following input:\n\n    1 pound carrots, young ones if possible\n    Kosher salt, to taste\n    2 tablespoons sherry vinegar\n    2 tablespoons honey\n    2 tablespoons extra-virgin olive oil\n    1 medium-size shallot, peeled and finely diced\n    1/2 teaspoon fresh thyme leaves, finely chopped\n    Black pepper, to taste\n\nOur tool produces something like:\n\n    {\n        \"qty\":     \"1\",\n        \"unit\":    \"pound\"\n        \"name\":    \"carrots\",\n        \"other\":   \",\",\n        \"comment\": \"young ones if possible\",\n        \"input\":   \"1 pound carrots, young ones if possible\",\n        \"display\": \"<span class='qty'>1</span><span class='unit'>pound</span><span class='name'>carrots</span><span class='other'>,</span><span class='comment'>young ones if possible</span>\",\n    }\n\nWe use a conditional random field model (CRF) to extract tags from labelled\ntraining data, which was tagged by human news assistants. We wrote about our\napproach [on the New York Times Open blog][openblog]. More information about\nCRFs can be found [here][crf_tut].\n\nOn a 2012 Macbook Pro, training the model takes roughly 30 minutes for 130k\nexamples using the [CRF++][crfpp] library.\n\n\n## Development\n\nOn OSX:\n\n    brew install crf++\n    python setup.py install\n\n\n## Quick Start\n\nThe most common usage is to train the model with a subset of our data, test the\nmodel against a different subset, then visualize the results. We provide a shell\nscript to do this, at:\n\n    ./roundtrip.sh\n\nYou can edit this script to specify the size of your training and testing set.\nThe default is 20k training examples and 2k test examples.\n\n\n## Usage\n\n### Training\n\nTo train the model, we must first convert our input data into a format which\n`crf_learn` can accept:\n\n    bin/generate_data --data-path=input.csv --count=1000 --offset",
    "url": "https://github.com/nytimes/ingredient-phrase-tagger",
    "last_updated": "2025-08-28T06:27:19+00:00"
  },
  {
    "full_name": "Nixtla/neuralforecast",
    "name": "neuralforecast",
    "description": "Scalable and user friendly neural :brain: forecasting algorithms.",
    "language": "Python",
    "topics": [
      "deep-learning",
      "forecasting",
      "esrnn",
      "nbeats",
      "nbeatsx",
      "time-series",
      "pytorch",
      "transformer",
      "nhits",
      "neural-network",
      "machine-learning",
      "deep-neural-networks",
      "deepar",
      "tft",
      "robust-regression",
      "hierarchical-forecasting",
      "probabilistic-forecasting",
      "baselines",
      "baselines-zoo",
      "hint"
    ],
    "readme": "# Nixtla &nbsp; [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Statistical%20Forecasting%20Algorithms%20by%20Nixtla%20&url=https://github.com/Nixtla/neuralforecast&via=nixtlainc&hashtags=StatisticalModels,TimeSeries,Forecasting) &nbsp;[![Slack](https://img.shields.io/badge/Slack-4A154B?&logo=slack&logoColor=white)](https://join.slack.com/t/nixtlacommunity/shared_invite/zt-1pmhan9j5-F54XR20edHk0UtYAPcW4KQ)\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/Nixtla/neuralforecast/main/nbs/imgs_indx/logo_new.png\" />\n<h1 align=\"center\">Neural 🧠 Forecast</h1>\n<h3 align=\"center\">User friendly state-of-the-art neural forecasting models</h3>\n\n[![CI](https://github.com/Nixtla/neuralforecast/actions/workflows/ci.yaml/badge.svg?branch=main)](https://github.com/Nixtla/neuralforecast/actions/workflows/ci.yaml)\n[![Python](https://img.shields.io/pypi/pyversions/neuralforecast)](https://pypi.org/project/neuralforecast/)\n[![PyPi](https://img.shields.io/pypi/v/neuralforecast?color=blue)](https://pypi.org/project/neuralforecast/)\n[![conda-nixtla](https://img.shields.io/conda/vn/conda-forge/neuralforecast?color=seagreen&label=conda)](https://anaconda.org/conda-forge/neuralforecast)\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://github.com/Nixtla/neuralforecast/blob/main/LICENSE)\n[![docs](https://img.shields.io/website-up-down-green-red/http/nixtla.github.io/neuralforecast.svg?label=docs)](https://nixtla.github.io/neuralforecast/)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-11-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\n**NeuralForecast** offers a large collection of neural forecasting models focusing on their performance, usability, and robustness. The models range from classic networks like RNNs to the latest transformers: `MLP`, `L",
    "url": "https://github.com/Nixtla/neuralforecast",
    "last_updated": "2025-09-02T09:23:22+00:00"
  },
  {
    "full_name": "ecprice/newsdiffs",
    "name": "newsdiffs",
    "description": "Automatic scraper that tracks changes in news articles over time.",
    "language": "Python",
    "topics": [],
    "readme": "NewsDiffs\n==========\n\nA website and framework that tracks changes in online news articles over time.\n\nOriginal installation at newsdiffs.org.\nA product of the Knight Mozilla MIT news hackathon in June 2012.\nAuthors: Eric Price (ecprice@mit.edu), Greg Price (gnprice@gmail.com),\n and Jennifer 8. Lee (jenny@jennifer8lee.com)\n\nThis is free software under the MIT/Expat license; see LICENSE.\nThe project's source code lives at http://github.com/ecprice/newsdiffs .\n\n\nRequirements\n------------\n\nYou need to have installed on your local machine\n* Git\n* Python 2.6 or later\n* Django and other Python libraries\n\nOn a Debian- or Ubuntu-based system, it may suffice (untested) to run\n  $ sudo apt-get install git-core python-django python-django-south python-simplejson\n\nOn Mac OS, the easiest way may be to install pip:\n  http://www.pip-installer.org/en/latest/installing.html\nand then\n  $ pip install Django\n\n\nInitial setup\n-------------\n\n  $ python website/manage.py syncdb && python website/manage.py migrate\n  $ mkdir articles\n\n\nRunning NewsDiffs Locally\n-------------------------\n\nDo the initial setup above.  Then to start the webserver for testing:\n  $ python website/manage.py runserver\n\nand visit http://localhost:8000/\n\n\nRunning the scraper\n-------------------\n\nDo the initial setup above.  You will also need additional Python\nlibraries; on a Debian- or Ubuntu-based system, it may suffice\n(untested) to run\n  $ sudo apt-get install python-bs4 python-beautifulsoup\n\non a Mac, you will want something like\n\n $ pip install beautifulsoup4\n $ pip install beautifulsoup\n $ pip install html5lib\n\nNote that we need two versions of BeautifulSoup, both 3.2 and 4.0;\nsome websites are parsed correctly in only one version.\n\nThen run\n  $ python website/manage.py scraper\n\nThis will populate the articles repository with a list of current news\narticles.  This is a snapshot at a single time, so the website will\nnot yet have any changes. To get changes, wait some time (say, 3\nhours) and run 'python website/m",
    "url": "https://github.com/ecprice/newsdiffs",
    "last_updated": "2025-04-05T21:49:24+00:00"
  },
  {
    "full_name": "jonclayden/regex-performance",
    "name": "regex-performance",
    "description": "A benchmark for regular expression libraries in R",
    "language": "HTML",
    "topics": [],
    "readme": "# A Regular Expression Benchmark for R\n\nThe main constituent of this repository is an R Markdown document, `regex-performance.Rmd`, which when rendered will run the benchmark and plot the results. The document explains its methodology and describes the output. The rendered document will look like [this example](http://rpubs.com/jonclayden/regex-performance).\n\nTo run the benchmark you can either load `regex-performance.Rmd` into [RStudio](http://www.rstudio.com/products/RStudio/) and click on \"Knit HTML\", or run `make` from a (Unix) command line. You'll need the `rmarkdown` package installed in the latter case. You can also manually render the document using `rmarkdown::render(\"regex-performance.Rmd\")`.\n\nContributions to the document are welcome, as are contributions of output from different systems. Please create a pull request for either of these. Results should be created with a suitably descriptive filename within the `results` subdirectory.\n",
    "url": "https://github.com/jonclayden/regex-performance",
    "last_updated": "2024-07-04T10:54:18+00:00"
  },
  {
    "full_name": "t-davidson/hate-speech-and-offensive-language",
    "name": "hate-speech-and-offensive-language",
    "description": "Repository for the paper \"Automated Hate Speech Detection and the Problem of Offensive Language\", ICWSM 2017",
    "language": "Jupyter Notebook",
    "topics": [
      "hatespeech",
      "offensive",
      "nlp",
      "icwsm",
      "twitter",
      "abuse",
      "offensive-language",
      "hate-speech",
      "natural-language-processing",
      "dataset",
      "labeled-data",
      "classifier",
      "machine-learning",
      "computational-social-science"
    ],
    "readme": "# Automated Hate Speech Detection and the Problem of Offensive Language\nRepository for Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\" ICWSM. You read the paper [here](https://ojs.aaai.org/index.php/ICWSM/article/view/14955).\n\n\n# NOTE: This repository is no longer actively maintained. Please do not post issues regarding the compatibility of the existing code with new versions of Python or the packages used. I will not accept any pull requests. If you plan to use this data or code in your research, please review the [issues](https://github.com/t-davidson/hate-speech-and-offensive-language/issues), as several Github users have suggested changes or improvements to the codebase.\n\n## 2019 NEWS\nWe have a new paper on racial bias in this dataset and others, you can read it [here](https://arxiv.org/abs/1905.12516)\n\n\n***WARNING: The data, lexicons, and notebooks all contain content that is racist, sexist, homophobic, and offensive in many other ways.***\n\nYou can find our labeled data in the `data` directory. We have included them as a pickle file (Python 2.7) and as a CSV. You will also find a notebook in the `src` directory containing Python 2.7 code to replicate our analyses in the paper and a lexicon in the `lexicons` directory that we generated to try to more accurately classify hate speech. The `classifier` directory contains a script, instructions, and the necessary files to run our classifier on new data, a test case is provided.\n\n\n***Please cite our paper in any published work that uses any of these resources.***\n~~~\n@inproceedings{hateoffensive,\n  title = {Automated Hate Speech Detection and the Problem of Offensive Language},\n  author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, \n  booktitle = {Proceedings of the 11th International AAAI Conference on Web and Social Media},\n  series = {ICWSM '17},\n  year = {2017},\n  location = {Montreal, Can",
    "url": "https://github.com/t-davidson/hate-speech-and-offensive-language",
    "last_updated": "2025-08-30T06:07:08+00:00"
  },
  {
    "full_name": "e-hulten/july",
    "name": "july",
    "description": "A small library for creating pretty heatmaps of daily data.",
    "language": "Python",
    "topics": [
      "python",
      "visualisation",
      "heatmap",
      "calendar",
      "daily-data",
      "heatmap-visualization",
      "calendar-plot",
      "github-plot",
      "calendar-heatmap",
      "matplotlib"
    ],
    "readme": "![July](https://github.com/e-hulten/july/blob/master/figs/july.png?raw=true)\n# July\nA small library for creating pretty heatmaps of daily data. \n\n### Features\n- Get rid of the eternal matplotlib tweaking every time you want to plot data in proper calendar format.\n- Generate GitHub activity overview-like heatmaps of your daily data.\n- Automatic handling of missing dates in input date range.\n- `July` does not rely only pandas (though it accepts it). Only numpy arrays and native Python data structures are used internally.\n- Accepted date formats: `datetime.datetime`, `datetime.date`, `str`, `pd.DatetimeIndex`\n\n\n### Install\n```\n$ pip install july\n```\n\n### Usage\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport july\nfrom july.utils import date_range\n\ndates = date_range(\"2020-01-01\", \"2020-12-31\")\ndata = np.random.randint(0, 14, len(dates))\n```\n```\n# GitHub Activity like plot (for someone with consistently random work patterns).\njuly.heatmap(dates, data, title='Github Activity', cmap=\"github\")\n```\n![GitHub heatmap](https://github.com/e-hulten/july/blob/master/examples/heatmap_github.jpg?raw=true)\n```\n# Here, 'osl_df' is a pandas df. \njuly.heatmap(osl_df.date, osl_df.temp, cmap=\"golden\", colorbar=True, title=\"Average temperatures: Oslo , Norway\")\n```\n![Golden heatmap](https://github.com/e-hulten/july/blob/master/examples/pandas_oslo_temperature_plot.jpg?raw=true)\n```\n# More exhaustive example using useless, but pretty colours.\njuly.heatmap(dates=dates, \n             data=data, \n             cmap='Pastel1',\n             month_grid=True, \n             horizontal=True,\n             value_label=False,\n             date_label=False,\n             weekday_label=True,\n             month_label=True, \n             year_label=True,\n             colorbar=False,\n             fontfamily=\"monospace\",\n             fontsize=12,\n             title=None,\n             titlesize='large',\n             dpi=100)\n```\n![Pastel heatmap](https://github.com/e-hulten/july/blob/master/exampl",
    "url": "https://github.com/e-hulten/july",
    "last_updated": "2025-08-19T15:21:40+00:00"
  },
  {
    "full_name": "trifle/twitterresearch",
    "name": "twitterresearch",
    "description": "A starter kit with code for data collection, preparation, and analysis of digital trace data collected on Twitter",
    "language": "Python",
    "topics": [],
    "readme": "# twitterresearch\nA starter kit with code for data collection, preparation, and analysis of digital trace data collected on Twitter.\n\n# ! HEADS-UP !\n*2021-01-27*  \nTwitter today announced a much-welcome change in their API licensing. Researchers can now apply for privileged access via the developer dashboard, and receive many of the previously paid features for free, as long as the research is non-commercial. Access requires a project applicaiton. Find out more in this [blog post](https://blog.twitter.com/developer/en_us/topics/tips/2021/enabling-the-future-of-academic-research-with-the-twitter-api.html).\n\n(The following warning no longer applies, but you should still investigate the use of the V2 API which this library has not yet been adapted to.)\nTwitter has announced changes to its API that will likely severely limit free access to data: [Coverage](https://www.theverge.com/2018/4/6/17206524/twitter-tweetbot-twitterrific-apps-features-api-changes). If you intend to work with moderate to large datasets in the future, do your research on the paid options now.\n\n**This kit complements a full-length tutorial which can be found on [SSRN](http://ssrn.com/abstract=2710146)**\n\n# Getting Started\n\n*There is a set of working usage examples in the file examples.py*\n\n1.  Install requirements: `pip3 install -r requirements.txt`\n** IMPORTANT NOTE: A new version of the peewee library has been released and breaks some APIs; take care to use the requirements file **\n2.  (optional) install ipython `pip3 install -U ipython`\n3.  start python\n4.  Import libraries, such as `import rest, streaming`\n5.  Use functions, for example:\n\n```\narchive = rest.fetch_user_archive(\"lessig\")\nfor page in archive:\n    for tweet in page:\n        print(u\"{0}: {1}\".format(tweet[\"user\"][\"screen_name\"], tweet[\"text\"]))\n\n```\n\n## FAQ\n\n- I'm getting unicode errors on windows, what can I do?\n    Make sure that your windows install of python can print unicode characters to screen without throwing errors. If in do",
    "url": "https://github.com/trifle/twitterresearch",
    "last_updated": "2025-05-18T07:28:15+00:00"
  },
  {
    "full_name": "ahmaurya/topics_over_time",
    "name": "topics_over_time",
    "description": "Topics over Time implementation",
    "language": "Python",
    "topics": [],
    "readme": "[![No Maintenance Intended](http://unmaintained.tech/badge.svg)](http://unmaintained.tech/)\n\nTopics Over Time\n===========================================================\n\nThis is an open-source implementation of *A Non-Markov Continuous-Time Model of Topical Trends* by Xuerui Wang and Andrew McCallum. The paper associated each LDA topic with a beta distribution over timestamps which characterized the evolution of that topic with time.\n\nInstructions\n-----------------------------------------------------------\n\n* Sanitize `main_pnas.py` and `visualize_pnas.py` to ensure all input directories, input files, and output directories are present.\n* Run `python main_pnas.py` to execute Topics over Time algorithm.\n* Run `python visualize_pnas.py` to visualize the topic-word distributions as well as the beta distributions showing evolution of topics with time.\n\nDataset\n-----------------------------------------------------------\nThe code is tested on the PNAS titles dataset. The dataset can be found [here](http://www.cs.nyu.edu/~roweis/data/pnas_all.tar). The resulting model is pickled and stored in [the results folder](results/pnas_tot).\n\nResults\n-----------------------------------------------------------\n\n* Topic Distributions for PNAS Titles Dataset\n\n[![Topic Distributions](results/pnas_tot/topic_distributions.png \"Topic Distributions for PNAS Titles Dataset\")](results/pnas_tot/topic_distributions.png)\n\n* Evolution of Topics for PNAS Titles Dataset\n\n[![Topic Evolution](results/pnas_tot/topic_evolutions.png \"Evolution of Topics for PNAS Titles Dataset\")](results/pnas_tot/topic_evolutions.png)\n\nLicense\n-----------------------------------------------------------\n\n[GNU General Public License](GPL.md)\n\nCopyright © 2015 Abhinav Maurya\n",
    "url": "https://github.com/ahmaurya/topics_over_time",
    "last_updated": "2025-08-07T12:20:55+00:00"
  },
  {
    "full_name": "soodoku/military-experience",
    "name": "military-experience",
    "description": "Military Experience of US Presidents and UK Prime Ministers.",
    "language": "R",
    "topics": [
      "us-presidents",
      "uk-prime-ministers",
      "military-experience",
      "veterans"
    ],
    "readme": "### Military Experience of US Presidents and UK Prime Ministers\n\n* [Military Experience of US Presidents](#military-experience-of-us-presidents)\n* [Military Experience of UK Prime Ministers](#military-experience-of-uk-prime-ministers)\n* [Graph of Military Experience of UK PMs](#graph-of-military-experience-of-uk-pms)\n\n----\n\n#### Military Experience of US Presidents\n\nFor 137 of the 219 years the country since its independence, US has had a military veteran as a president. 29 of its 43 presidents have been veterans. And the longest time America has gone without electing a veteran was the 32 year period starting with Taft in 1913 and ending with Roosevelt’s death in 1945. (Incredibly, during this time, the country took part in the two World Wars.) [Related article](http://gbytes.gsood.com/2008/04/23/military-experience-of-us-presidents-1789-%E2%80%93-2008/)\n\n**Data:** Military Experience of US Presidents: [From George Washington till Barack Obama (csv)](USPresMilExp.csv). \n\nColumn names and their description: \n\n| Column Name   \t  | Description   | \n| --------------------|---------------|\n| ID      \t\t\t  | Unique ID \t|\n| President Number    | Number in sequence of unique presidents since founding     |\n| Year \t\t\t\t  | are neat      |\n| President \t\t  | Name of the president|\n| Party \t\t\t  | Party of the president|\n| Mil. Summary \t\t  | Key positions held in the military|\n| Mil. Exp Dummy\t  | Whether or not the president had any military experience|\n| Start.Month \t\t  | Month the presidency started|\n| Start.Year \t\t  | Year the presidency started|\n| End.Month \t\t  | Month the presidency ended|\n| End.Year \t\t\t  | Year the presidency ended|\n| Years.Of.Rule \t  | Years of rule|\n| Months.Of.Rule   \t  | Months of rule|\n\nThere are three potential concerns about the numbers. Eight years of George W. Bush's 'service' in the National Guard have been excluded. Five years of Lincoln presidency have been included (Lincoln participated very briefly in the Black Hawk War of 1832). And Millard F",
    "url": "https://github.com/soodoku/military-experience",
    "last_updated": "2023-10-10T00:00:01+00:00"
  },
  {
    "full_name": "kgretzky/dcrawl",
    "name": "dcrawl",
    "description": "Simple, but smart, multi-threaded web crawler for randomly gathering huge lists of unique domain names.",
    "language": "Go",
    "topics": [],
    "readme": "# dcrawl\n\ndcrawl is a simple, but smart, multi-threaded web crawler for randomly gathering huge lists of unique domain names.\n\n[![baby-gopher](https://raw.githubusercontent.com/drnic/babygopher-site/gh-pages/images/babygopher-badge.png)](http://www.babygopher.org)\n\n![demo](https://raw.githubusercontent.com/kgretzky/dcrawl/master/img/dcrawl.gif)\n\n## How it works?\n\ndcrawl takes one site URL as input and detects all `<a href=...>` links in the site's body. Each found link is put into the queue. Successively, each queued link is crawled in the same way, branching out to more URLs found in links on each site's body.\n\nHow **smart crawling** works:\n* Branching out only to predefined number of links found per one hostname.\n* Maximum number of allowed different hostnames per one domain *(avoids subdomain crawling hell e.g. blogspot.com)*.\n* Can be restarted with same list of domains - last saved domains are added to the URL queue.\n* Crawls only sites that return *text/html* Content-Type in HEAD response.\n* Retrieves site body of maximum 1MB size.\n* Does not save inaccessible domains.\n\n## How to run?\n\n```\ngo build dcrawl.go\n./dcrawl -url http://wired.com -out ~/domain_lists/domains1.txt -t 8\n```\n\n## Usage\n\n```\n     ___                          __\n  __| _/________________ __  _  _|  |\n / __ |/ ___\\_  __ \\__  \\\\ \\/ \\/ /  |\n/ /_/ \\  \\___|  | \\// __ \\\\     /|  |__\n\\____ |\\___  >__|  (____  /\\/\\_/ |____/\n     \\/    \\/           \\/       v.1.0\n\nusage: dcrawl -url URL -out OUTPUT_FILE -t THREADS\n\n  -ms int\n        maximum different subdomains for one domain (def. 10) (default 10)\n  -mu int\n        maximum number of links to spider per hostname (def. 5) (default 5)\n  -out string\n        output file to save hostnames to\n  -t int\n        number of concurrent threads (def. 8) (default 8)\n  -url string\n        URL to start scraping from\n  -v bool\n        verbose (default false)\n```\n\n## License\n\ndcrawl was made by [Kuba Gretzky](https://twitter.com/mrgretzky) from [breakdev.org](https://b",
    "url": "https://github.com/kgretzky/dcrawl",
    "last_updated": "2025-09-01T00:09:06+00:00"
  },
  {
    "full_name": "yinleon/LocalNewsDataset",
    "name": "LocalNewsDataset",
    "description": "The documentation and scripts for the Local News Dataset",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# Local News Dataset 2018\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1345145.svg)](https://doi.org/10.5281/zenodo.1345145)\n\nBy Leon Yin<br>\nOn 2018-08-14\n\n## Introduction\nThis dataset is a machine-readible directory of state-level newspapers, tv stations and magazines. In addition to basic information such as the name of the outlet and state it is located in, all available information regarding web presence, social media (twitter, youtube, facebook) and their owners is scraped, too.\n\nThe sources of this dataset are [usnpl.com](https://www.usnpl.com)-- newspapers and magazines by state, [stationindex.com](https://www.stationindex.com) -- tv stations by state and by owner, and homepages of the media corporations [Meredith](http://www.meredith.com/local-media/broadcast-and-digital), [Sinclair](http://sbgi.net/tv-channels/), [Nexstar](https://www.nexstar.tv/stations/), [Tribune](http://www.tribunemedia.com/our-brands/) and [Hearst](http://www.hearst.com/broadcasting/our-markets).\n\nThis dataset was inspired by ProPublica's [Congress API](https://projects.propublica.org/api-docs/congress-api/). I hope that this dataset will serve a similar purpose as a starting point for research and applications, as well as a bridge between datasets from social media, news articles and online communities.\n\nWhile you use this dataset, if you see irregularities, questionable entries, or missing outlets please [submit an issue](https://github.com/yinleon/LocalNewsDataset/issues/new) on Github or contact me on [Twitter](https://twitter.com/LeonYin). I'd love to hear how this dataset is put to work \n\nHappy hunting\n\nFor an indepth [introduction](https://nbviewer.jupyter.org/github/yinleon/LocalNewsDataset/blob/master/nbs/local_news_dataset.ipynb?flush_cache=true#intro), [specs](https://nbviewer.jupyter.org/github/yinleon/LocalNewsDataset/blob/master/nbs/local_news_dataset.ipynb?flush_cache=true#specs), [data sheet](https://nbviewer.jupyter.org/github/yinleon/LocalNewsDataset/blob/mast",
    "url": "https://github.com/yinleon/LocalNewsDataset",
    "last_updated": "2025-03-23T20:43:58+00:00"
  },
  {
    "full_name": "durtal/betfaiR",
    "name": "betfaiR",
    "description": "R package for the Betfair API",
    "language": "R",
    "topics": [
      "betfair-api",
      "betfair"
    ],
    "readme": "betfaiR v0.8.0\n=======\n\n`betfaiR` is an R package which provides access to Betfair's API, and allows users to retrieve data (in various amounts of detail) from available markets, to place a bet in those markets, cancel bets, replace bets, etc.  The package possibly needs a little work (it's hard to test a package like this, unless someone has suggestions), so please proceed with caution when placing any bets, and provide feedback with any issues you encounter, or features you want added.\n\nInstallation instructions are below, the usage section walks through the primary function in the package and the various API methods available (which I believe is all of them).\n\n#### Installation\n\n```R\n# install devtools\ndevtools::install_github(\"durtal/betfaiR\")\n```\n\n#### Help\n\nThere are help pages available [here](http://durtal.github.io/betfaiR/), issues can be filed [here](https://github.com/durtal/betfaiR/issues)..\n\nThere are a few vignettes and I'll try to add more:\n\nvignette | title | details\n---------|-------|---------------------------------------------------------------\n[vignette one](http://durtal.github.io/betfaiR/vignette_one.html) | place a bet | walks through login, find a market, place a bet, replace the bet and cancel the bet]\n[vignette two](http://durtal.github.io/betfaiR/vignette_two.html) | cron jobs w/ betfaiR | shows how to use `betfaiR` and scheduled tasks to periodically collect betfair data\n[vignette three](http://durtal.github.io/betfaiR/vignette_three.html) | betfair super sunday - 14/02/16 | some simple analysis of the data returned by the code walked through in vignette two\n\nHelp with the package would be welcome, or suggestions on how best to parse the responses from Betfair, what format would _you_ like data to be in when returned from the Exchange, dataframes, lists of dataframes, environments (maybe?) or the raw unparsed response.\n\n#### Usage\n\nThe primary function in **betfaiR** is `betfair`, which takes three arguments, your username, your password",
    "url": "https://github.com/durtal/betfaiR",
    "last_updated": "2024-05-24T19:02:04+00:00"
  },
  {
    "full_name": "EpistasisLab/tpot2",
    "name": "tpot2",
    "description": "A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. ",
    "language": "Jupyter Notebook",
    "topics": [
      "adsp",
      "aiml",
      "alzheimer",
      "alzheimers",
      "automated-machine-learning",
      "automation",
      "automl",
      "data-science",
      "feature-engineering",
      "gradient-boosting",
      "hyperparameter-optimization",
      "machine-learning",
      "model-selection",
      "nia",
      "parameter-tuning",
      "python",
      "random-forest",
      "scikit-learn",
      "ag066833",
      "lm010098"
    ],
    "readme": "# TPOT\n\n<center>\n<img src=\"https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-logo.jpg\" width=300 />\n</center>\n\n<br>\n\n![Tests](https://github.com/EpistasisLab/tpot/actions/workflows/tests.yml/badge.svg)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/tpot?label=pypi%20downloads)](https://pypi.org/project/TPOT)\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/tpot?label=conda%20downloads)](https://anaconda.org/conda-forge/tpot)\n\nTPOT stands for Tree-based Pipeline Optimization Tool. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Consider TPOT your Data Science Assistant.\n\n## Contributors\n\nTPOT recently went through a major refactoring. The package was rewritten from scratch to improve efficiency and performance, support new features, and fix numerous bugs. New features include genetic feature selection, a significantly expanded and more flexible method of defining search spaces, multi-objective optimization, a more modular framework allowing for easier customization of the evolutionary algorithm, and more. While in development, this new version was referred to as \"TPOT2\" but we have now merged what was once TPOT2 into the main TPOT package. You can learn more about this new version of TPOT in our GPTP paper titled \"TPOT2: A New Graph-Based Implementation of the Tree-Based Pipeline Optimization Tool for Automated Machine Learning.\"\n\n    Ribeiro, P. et al. (2024). TPOT2: A New Graph-Based Implementation of the Tree-Based Pipeline Optimization Tool for Automated Machine Learning. In: Winkler, S., Trujillo, L., Ofria, C., Hu, T. (eds) Genetic Programming Theory and Practice XX. Genetic and Evolutionary Computation. Springer, Singapore. https://doi.org/10.1007/978-981-99-8413-8_1\n\nThe current version of TPOT was developed at Cedars-Sinai by:  \n    - Pedro Henrique Ribeiro (Lead developer - https://github.com/perib, https://www.linkedin.com/in/pedro-ribeiro/)  \n    -",
    "url": "https://github.com/EpistasisLab/tpot2",
    "last_updated": "2025-08-24T12:46:21+00:00"
  },
  {
    "full_name": "p-e-w/maybe",
    "name": "maybe",
    "description": " :open_file_folder: :rabbit2: :tophat: See what a program does before deciding whether you really want it to happen (NO LONGER MAINTAINED)",
    "language": "Python",
    "topics": [],
    "readme": "<table>\n  <tr>\n    <td><strong>Package</strong></td>\n    <td>\n      <a href=\"https://pypi.python.org/pypi/maybe\">\n        <img src=\"https://img.shields.io/pypi/v/maybe.svg\">\n      </a>\n      <img src=\"https://img.shields.io/badge/os-linux-blue.svg\">\n      <img src=\"https://img.shields.io/pypi/pyversions/maybe.svg\">\n    </td>\n  </tr>\n  <tr>\n    <td><strong>Tests</strong></td>\n    <td>\n      <a href=\"https://travis-ci.org/p-e-w/maybe\">\n        <img src=\"https://travis-ci.org/p-e-w/maybe.svg?branch=master\">\n      </a>\n      <a href=\"https://coveralls.io/github/p-e-w/maybe?branch=master\">\n        <img src=\"https://coveralls.io/repos/github/p-e-w/maybe/badge.svg?branch=master\">\n      </a>\n      <a href=\"https://gemnasium.com/github.com/p-e-w/maybe\">\n        <img src=\"https://gemnasium.com/badges/github.com/p-e-w/maybe.svg\">\n      </a>\n    </td>\n  </tr>\n</table>\n\n\n---\n\n\n```\nrm -rf pic*\n```\n\nAre you sure? Are you *one hundred percent* sure?\n\n\n# `maybe`...\n\n... allows you to run a command and see what it does to your files *without actually doing it!* After reviewing the operations listed, you can then decide whether you really want these things to happen or not.\n\n![Screenshot](screenshot.png)\n\n\n## What is this sorcery?!?\n\n`maybe` runs processes under the control of [ptrace](https://en.wikipedia.org/wiki/Ptrace) (with the help of the excellent [python-ptrace](https://github.com/haypo/python-ptrace) library). When it intercepts a system call that is about to make changes to the file system, it logs that call, and then modifies CPU registers to both redirect the call to an invalid syscall ID (effectively turning it into a no-op) and set the return value of that no-op call to one indicating success of the original call.\n\nAs a result, the process believes that everything it is trying to do is actually happening, when in reality nothing is.\n\nThat being said, `maybe` **should :warning: NEVER :warning: be used to run untrusted code** on a system you care about! A process running u",
    "url": "https://github.com/p-e-w/maybe",
    "last_updated": "2025-09-01T06:06:26+00:00"
  },
  {
    "full_name": "svenkreiss/unicodeit",
    "name": "unicodeit",
    "description": "Converts LaTeX tags to unicode: \\mathcal{H} → ℋ. Available on the web or as Automator script for the Mac.",
    "language": "Python",
    "topics": [
      "unicode",
      "converts-latex-tags"
    ],
    "readme": "# UnicodeIt\n\nTested on Linux, Mac and Windows: ![Tests](https://github.com/svenkreiss/unicodeit/workflows/Tests/badge.svg)\n\nConverts LaTeX tags to unicode.\nAvailable online at [unicodeit.net](https://www.unicodeit.net).\n\n\n## Examples\n\n```\n\\alpha α, \\beta β, \\infty ∞       e^+ e⁺, \\mu^- μ⁻               \\exists ∃, \\nexists ∄\n\\int ∫, \\sum ∑, \\partial ∂        \\to →, p\\bar{p} pp̅             \\mathcal{H} ℋ, \\mathbb{R} ℝ\n\\slash{\\partial} ∂̸                \\underline{x} x̲                \\phone ☎, \\checkmark ✓\n\\dot{x} ẋ, \\ddot{x} ẍ             A^6 A⁶, m_0 m₀                 \\Im ℑ, \\Re ℜ, \\hbar ℏ\n\\gamma γ, \\Gamma Γ                \\~{O} Õ                        \\perp ⊥, \\parallel ∥\n\\sfrac{3}{5} ⅗                    \\therefore ∴, \\because ∵       \\subset ⊂, \\supset ⊃\n```\n\n\n## Python\n\nInstall with `pip install unicodeit` and run\n\n```sh\npython -m unicodeit.cli \\\\alpha\n```\n\nor in Python\n\n```py\nimport unicodeit\nprint(unicodeit.replace('\\\\alpha'))\n```\n\n\n## JavaScript / TypeScript\n\nInstall with `npm install unicodeit --save-dev` and use it like this:\n\n```js\nvar unicodeit = require('unicodeit');\nconsole.log(unicodeit.replace('\\\\alpha'));\n```\n\n\n## Mac Automator\n\nCreate your own Automator Workflow:\n\n* Create a new \"Quick Action\" (might also be labeled as \"Service\").\n* At the top, leave the defaults: \"Service receives *selected text* in *any application*\"\n* Select the checkmark \"output replaces selected text\".\n* Add the action \"Run Shell Script\".\n* From dropdown, select to \"pass in: as arguments\".\n* The command is: `/usr/local/bin/python3 -m unicodeit.cli $1`. This Python interpreter must have unicodeit installed; e.g. with `/usr/local/bin/python3 -m pip install unicodeit`.\n* It should look something like this:\n\n![automator script](docs/automator.png)\n\nTo set a keyboard shortcut, go to `System Preferences` →\n`Keyboard` → `Shortcuts` → `Services` → `Text` → `UnicodeItAction`.\nChoose a keyboard shortcut like `Command+Option+Shift U`.\n\nNow you are all set to use your new keyboard shortcu",
    "url": "https://github.com/svenkreiss/unicodeit",
    "last_updated": "2025-08-15T00:03:21+00:00"
  },
  {
    "full_name": "ShiroTakeda/econ-bst",
    "name": "econ-bst",
    "description": "BibTeX style file for economics.",
    "language": "BibTeX Style",
    "topics": [
      "tex",
      "bibtex",
      "bibtex-style"
    ],
    "readme": "<!--\nFilename:       README.md\nAuthor:         Shiro Takeda\ne-mail          <shiro.takeda@gmail.com>\nFirst-written:  <2017-07-30>\n-->\n\necon.bst\n==============================\n\n`econ.bst` is a BibTeX style file for economics. It provids the following\nfeatures:\n\n* The author-year type citation (you need `natbib.sty` as well).\n* Reference style used in economics papers (`econ.bst` is not a BibTeX style for a specific journal).\n* Highly customizable.  You can easily customize reference style as you wish.\n* You can use \"certified random order\" proposed by [Ray ⓡ Robson (2018), AER](http://dx.doi.org/10.1257/aer.20161492).\n\n\n## Explanation of files\n\n| File                                   | Explanation                                |\n|:---------------------------------------|:-------------------------------------------|\n| `econ.bst`                             | This is the main bst file.                 |\n| `econ-example.tex`                     | This file explains how to use econ.bst.    |\n| [`econ-example.pdf`](econ-example.pdf) | A PDF file created fom `econ-example.tex`. |\n| `econ-example.bib`                     | This is a bibliography database file.      |\n| [`customization`](customization)       | This folder contains customized bst files. |\n| [`CHANGES.md`](CHANGES.md)             | Changelog file.                            |\n| `README.md`                            | This file.                                 |\n\n\n## Lisence \n\nThe texts file contained in this package may be distributed and/or modified\nunder the conditions of the LaTeX Project Public License, either version 1.3c of\nthis license or (at your option) any later version.  The latest version of this\nlicense is in https://www.latex-project.org/lppl.txt and version 1.3c or later\nis part of all distributions of LaTeX version 2008 or later.\n\nThis work has the LPPL maintenance status \"maintained\".\n\nThe Current Maintainer of this work is Shiro Takeda.\n\n\n<!--\n--------------------\nLocal Variables:\nmode: ma",
    "url": "https://github.com/ShiroTakeda/econ-bst",
    "last_updated": "2025-07-13T16:52:17+00:00"
  },
  {
    "full_name": "minimaxir/reddit-bigquery",
    "name": "reddit-bigquery",
    "description": "Code + Jupyter notebook for analyzing and visualizing Reddit Data quickly and easily",
    "language": "R",
    "topics": [],
    "readme": "# Reddit Data with BigQuery\nCode + Jupyter notebook for analyzing and visualizing Reddit Data quickly and easily. This notebook is the complement for my blog post [How to Analyze Every Reddit Submission and Comment, in Seconds, for Free](http://minimaxir.com/2015/10/reddit-bigquery/).\n",
    "url": "https://github.com/minimaxir/reddit-bigquery",
    "last_updated": "2025-05-25T11:54:09+00:00"
  },
  {
    "full_name": "pablobarbera/voter-files",
    "name": "voter-files",
    "description": "Python scripts to parse U.S. voter files",
    "language": "Python",
    "topics": [],
    "readme": "# voter-files\n\nThis repository contains a set of Python and R scripts to parse voting registration files in the United States into CSV files with the same variables: \n\n- `voter_id`\n- `first_name`\n- `middle_name`\n- `last_name`\n- `birth_date`\n- `gender`\n- `turnout2008`\n- `turnout2010`\n- `turnout2012`\n- `turnout2014`\n- `party_affiliation_2008`\n- `party_affiliation_2010`\n- `party_affiliation_2012`\n- `party_affiliation_2014`\n- `party_affiliation` (current)\n- `residential_address`\n- `zipcode`\n- `race`, `ethnicity` (in Florida and North Carolina)\n\nWhen any of these variables is not available, that column will be left empty (`NA`). The idea is to be able to easily merge all these files if so desired.\n\nPull requests, comments, and suggestions are very welcome!\n\n## Legal restrictions\n\nMost states have restrictions against commercial or non-political use. For a summary of usage restrictions, see NCSL's [Access To and Use Of Voter Registration Lists](https://www.ncsl.org/research/elections-and-campaigns/access-to-and-use-of-voter-registration-lists.aspx).\n\n\n## Availability of voter files across states\n\nThe following lists contains details about if/where voting registration records for each state are available, how much it would cost to obtain them, and other details. (I will expand this list over the next few months.)\n\nTo be clear: this repository does __NOT__ contain any data.\n\nClassification:\n\n- :smile: Available at low cost ($100 or less) or free\n- :money_with_wings: Available at high cost (more than $100)\n- :disappointed: No information online about how to acquire voter file\n\n\n### :money_with_wings: Alabama \n\nThe [Secretary of State](https://www.sos.alabama.gov/alabama-votes) links to an [interactive site](https://www.alabamainteractive.org/sos/voter/voterWelcome.action) that searches by multiple criteria and provides an instant price. As of March 2021, there were 3.5M active voters for a cost of $35K.\n\n### Alaska \n\nIt appears to be possible to [request it via email](http:/",
    "url": "https://github.com/pablobarbera/voter-files",
    "last_updated": "2025-08-02T06:17:40+00:00"
  },
  {
    "full_name": "ngreifer/WeightIt",
    "name": "WeightIt",
    "description": "WeightIt: an R package for propensity score weighting",
    "language": "R",
    "topics": [
      "causal-inference",
      "propensity-scores",
      "inverse-probability-weights",
      "observational-study",
      "r"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# WeightIt: Weighting for Covariate Balance in Observational Studies <img src=\"man/figures/logo.png\" align=\"right\" width=\"150\"/>\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/WeightIt?color=00622B)](https://CRAN.R-project.org/package=WeightIt)\n[![CRAN_Downloads_Badge](https://cranlogs.r-pkg.org/badges/WeightIt?color=00622B)](https://cran.r-project.org/package=WeightIt)\n\n### Overview\n\n*WeightIt* is a one-stop package to generate balancing weights for point\nand longitudinal treatments in observational studies. Support is\nincluded for binary, multi-category, and continuous treatments, a\nvariety of estimands including the ATE, ATT, ATC, ATO, and others, and\nsupport for a wide variety of weighting methods, including those that\nrely on parametric modeling, machine learning, or optimization.\n*WeightIt* also provides functionality for fitting regression models in\nweighted samples that account for estimation of the weights in\nquantifying uncertainty. *WeightIt* uses a familiar formula interface\nand is meant to complement `MatchIt` as a package that provides a\nunified interface to basic and advanced weighting methods.\n\nFor a complete vignette, see the\n[website](https://ngreifer.github.io/WeightIt/articles/WeightIt.html)\nfor *WeightIt* or `vignette(\"WeightIt\")`.\n\nTo install and load *WeightIt*, use the code below:\n\n``` r\n#CRAN version\npak::pkg_install(\"WeightIt\")\n\n#Development version\npak::pkg_install(\"ngreifer/WeightIt\")\n\nlibrary(\"WeightIt\")\n```\n\nThe workhorse function of *WeightIt* is `weightit()`, which generates\nweights from a given formula and data input according to methods and\nother parameters specified by the user. Below is an example of the use\nof `weightit()` to generate propensity score weights for estimating the\nATT:\n\n``` r\ndata(\"lalonde\", package = \"cobalt\")\n\nW <- weightit(treat ~ age + educ + nodegree + \n                married + race + re74 + re75, \n              data = lalonde, met",
    "url": "https://github.com/ngreifer/WeightIt",
    "last_updated": "2025-09-01T11:12:53+00:00"
  },
  {
    "full_name": "mfrasco/Metrics",
    "name": "Metrics",
    "description": "An R package for common supervised machine learning metrics.",
    "language": "R",
    "topics": [],
    "readme": "# Metrics\n\n![Build Status](https://travis-ci.org/mfrasco/Metrics.png)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/Metrics)](https://cran.r-project.org/package=Metrics)\n[![Downloads](http://cranlogs.r-pkg.org/badges/Metrics)](https://cran.rstudio.com/web/packages/Metrics/)\n\n## How to Install this Package\n\nThis package is distributed from CRAN. From the R prompt, run `install.packages(\"Metrics\")`.\n\n## Metrics Repo\n\nThis repository contains code for the **Metrics** package in R. **Metrics** was created by Ben Hamner and came from [this github repo](https://github.com/benhamner/Metrics/). Hamner's repo contains packages for common machine learning metrics in several programming languages, not just R. On 2017-04-21, CRAN orphaned the R package. To revive the status of the R package, I cloned the original and created this repo. I have added new metrics, improved documentation, and fixed bugs. This repository will be the home of active development on the **Metrics** R package moving forward.\n\n## Community Feedback\n\nIf you notice anything wrong with the **Metrics** package or have any ideas on how to improve it, please create an issue in this github repository that describes your issue. I also welcome improvements to this package via a pull request. This is a simple R package, which makes it perfect for first time open source contributors. [Here is a guide](https://opensource.guide/how-to-contribute/) that walks you through how to make an open source contribution.\n\n## What Metrics are Included in this Package?\n\nAll functions in the **Metrics** package take at least two arguments: `actual` and `predicted`. In the table below, I abbreviate `actual` as x and `predicted` as y for the sake of mathematical brevity.\n\n| Metric Type | Metric Name | Function Name | Formula |\n| ---- | ------------------------ | ---- | ------------------------------- |\n| regression | Squared Error | se | ![equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B150%7D%20%28x_i-y_i%29%5E2)",
    "url": "https://github.com/mfrasco/Metrics",
    "last_updated": "2025-06-03T20:44:56+00:00"
  },
  {
    "full_name": "doc-analysis/TableBank",
    "name": "TableBank",
    "description": "TableBank: A Benchmark Dataset for Table Detection and Recognition",
    "language": "",
    "topics": [],
    "readme": "# TableBank\n\nTableBank is a new image-based table detection and recognition dataset built with novel weak supervision from Word and Latex documents on the internet, contains 417K high-quality labeled tables.\n\n## News\n- **We have uploaded the datasets on [HuggingFace](https://huggingface.co/datasets/liminghao1630/TableBank).**\n- **We update the license to Apache-2.0.**\n- **We release an official split for the train/val/test datasets and re-train both of the Table Detection and Table Structure Recognition models using Detectron2 and OpenNMT tools. The benchmark results, the MODEL ZOO, and the download link of TableBank have been updated.**\n- **A new benchmark dataset DocBank ([Paper](https://arxiv.org/abs/2006.01038), [Repo](https://github.com/doc-analysis/DocBank)) is now available for document layout analysis**\n- **Our data can only be used for research purpose**\n- **Our paper has been accepted in [LREC 2020](https://lrec2020.lrec-conf.org/en/conference-programme/accepted-papers/)**\n\n## Introduction\nTo address the need for a standard open domain table\nbenchmark dataset, we propose a novel weak supervision approach\nto automatically create the TableBank, which is orders\nof magnitude larger than existing human labeled datasets for\ntable analysis. Distinct from traditional weakly supervised\ntraining set, our approach can obtain not only large scale but\nalso high quality training data.\n\nNowadays, there are a great\nnumber of electronic documents on the web such as Microsoft\nWord (.docx) and Latex (.tex) files. These online documents\ncontain mark-up tags for tables in their source code by nature.\nIntuitively, we can manipulate these source code by adding\nbounding box using the mark-up language within each document.\nFor Word documents, the internal Office XML code\ncan be modified where the borderline of each table is identified.\nFor Latex documents, the tex code can be also modified\nwhere bounding boxes of tables are recognized. In this\nway, high-quality labeled data is cre",
    "url": "https://github.com/doc-analysis/TableBank",
    "last_updated": "2025-08-18T06:11:51+00:00"
  },
  {
    "full_name": "r-lib/vctrs",
    "name": "vctrs",
    "description": "Generic programming with typed R vectors",
    "language": "C",
    "topics": [
      "r",
      "s3-vectors"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# vctrs <a href=\"https://vctrs.r-lib.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n<!-- badges: start -->\n\n[![Codecov test\ncoverage](https://codecov.io/gh/r-lib/vctrs/graph/badge.svg)](https://app.codecov.io/gh/r-lib/vctrs)\n![Lifecycle:\nmaturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)\n[![R-CMD-check](https://github.com/r-lib/vctrs/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/vctrs/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\nThere are three main goals to the vctrs package, each described in a\nvignette:\n\n-   To propose `vec_size()` and `vec_ptype()` as alternatives to\n    `length()` and `class()`; `vignette(\"type-size\")`. These definitions\n    are paired with a framework for size-recycling and type-coercion.\n    `ptype` should evoke the notion of a prototype, i.e. the original or\n    typical form of something.\n\n-   To define size- and type-stability as desirable function properties,\n    use them to analyse existing base functions, and to propose better\n    alternatives; `vignette(\"stability\")`. This work has been\n    particularly motivated by thinking about the ideal properties of\n    `c()`, `ifelse()`, and `rbind()`.\n\n-   To provide a new `vctr` base class that makes it easy to create new\n    S3 vectors; `vignette(\"s3-vector\")`. vctrs provides methods for many\n    base generics in terms of a few new vctrs generics, making\n    implementation considerably simpler and more robust.\n\nvctrs is a developer-focussed package. Understanding and extending vctrs\nrequires some effort from developers, but should be invisible to most\nusers. It’s our hope that having an underlying theory will mean that\nusers can build up an accurate mental model without explicitly learning\nthe theory. vctrs will typically be used by other packages, making it\neasy for them to provide new classes of S3 vectors that are supported\nthroughout the t",
    "url": "https://github.com/r-lib/vctrs",
    "last_updated": "2025-08-27T16:49:58+00:00"
  },
  {
    "full_name": "texty/manipulative_news_methodology",
    "name": "manipulative_news_methodology",
    "description": "Classifier, data collection scripts, and annotation tool for project on manipulative news",
    "language": "Julia",
    "topics": [],
    "readme": "### How we used NLP to analyse more then 2 millions of news items from hundreds of \"junk sites\"\n#### This is methodology for [\"We've got a bad news\" project](http://texty.org.ua/d/2018/mnews/eng/) (in English) \n\n### Repository structure\n* [`classifier`](/classifier) - scripts for training and applying language model classifier\n* [`data_collection`](/data_collection) - scripts to load RSS feeds, Facebook feeds of selected sites, and scrapy project to load html for each article\n* [`data_processing`](/data_processing) - scripts to prepare data for classifier\n* [Aggregated ranking](https://docs.google.com/spreadsheets/d/114Anuo8eREUVj3LscPaZcQ7fpvIzxti_virynhUVftI/edit#gid=0) - grouped results for the whole database of news. In final product we do not consider Russian sites, big Ukrainian sites, and sites with less than 25% of manipulative news\n* `.._annotation.csv` - annotated sample of news htmls. `html_id` - key id of article in data file, other columns - annotations\n* [`cls_tool`](/cls_tool) - Django site for annotation\n\n### Table of contents\n 1. [Data](#data)\n 2. [Annotation](#annotation)\n 3. [Classification](#classification)\n 4. [Final ranking](#final-ranking)\n\n### Data\nScripts for data collection and their description are in [`data_collection`](/data_collection) folder.<br>\n\nData can be downloaded [here](http://texty.org.ua/d/2018/share/mnews/data_to_publish.jl.zip)(2Gb). `html_id` - key field, `ra_summary` - readability html of article page, `real_url` - link to article.<br>\n\nTotally we collected 306 500 articles in Ukrainian and 2 301 000 articles in Russian. Next we filtered out articles not about Ukrainian politics and society (excluded celebrities, international news etc.). There were left 1 174 000 relevant articles in Russian and 227 400 articles in Ukrainian. Websites in final ranking totally produced 289 300 relevant articles.\n\nData for the project are news from around 200 websites, collected from December 2017 until Nowember 2018. For each site we colle",
    "url": "https://github.com/texty/manipulative_news_methodology",
    "last_updated": "2024-09-09T11:49:47+00:00"
  },
  {
    "full_name": "internetarchive/heritrix3",
    "name": "heritrix3",
    "description": "Heritrix is the Internet Archive's open-source, extensible, web-scale, archival-quality web crawler project.  ",
    "language": "Java",
    "topics": [
      "java",
      "webcrawling",
      "warc",
      "heritrix"
    ],
    "readme": "# Heritrix\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.archive/heritrix/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.archive/heritrix)\n[![Docker](https://img.shields.io/docker/v/iipc/heritrix/latest?label=docker)](https://hub.docker.com/r/iipc/heritrix)\n[![Javadoc](https://javadoc.io/badge2/org.archive/heritrix/javadoc.svg)](https://www.javadoc.io/doc/org.archive.heritrix/heritrix-engine)\n[![LICENSE](https://img.shields.io/badge/license-Apache-blue.svg?style=flat-square)](./LICENSE)\n\n## Introduction\n\nHeritrix is the Internet Archive's open-source, extensible, web-scale, archival-quality web crawler project. Heritrix (sometimes spelled heretrix, or misspelled or missaid as heratrix/heritix/heretix/heratix) is an archaic word for heiress (woman who inherits). Since our crawler seeks to collect and preserve the digital artifacts of our culture for the benefit of future researchers and generations, this name seemed apt.\n\n## Crawl Operators!\n\nHeritrix is designed to respect the [`robots.txt`](http://www.robotstxt.org/robotstxt.html) exclusion directives and [META nofollow tags](http://www.robotstxt.org/meta.html). Please consider the\nload your crawl will place on seed sites and set politeness policies accordingly. Also, always identify your crawl with contact information in the `User-Agent` so sites that may be adversely affected by your crawl can contact you or adapt their server behavior accordingly.\n\n## Documentation\n\n- [Getting Started](https://heritrix.readthedocs.io/en/latest/getting-started.html)\n- [Operating Heritrix](https://heritrix.readthedocs.io/en/latest/operating.html)\n- [Configuring Crawl Jobs](https://heritrix.readthedocs.io/en/latest/configuring-jobs.html)\n- [Bean Reference](https://heritrix.readthedocs.io/en/latest/bean-reference.html)\n- [Wiki](https://github.com/internetarchive/heritrix3/wiki)\n\n## Developer Documentation\n\n- [Developer Manual](http://crawler.archive.org/articles/developer_manual/index.html)",
    "url": "https://github.com/internetarchive/heritrix3",
    "last_updated": "2025-09-02T05:09:15+00:00"
  },
  {
    "full_name": "quiltdata/quilt",
    "name": "quilt",
    "description": "Quilt is a data mesh for connecting people with actionable data",
    "language": "TypeScript",
    "topics": [
      "data",
      "data-engineering",
      "data-version-control",
      "data-versioning",
      "python",
      "serialization",
      "parquet"
    ],
    "readme": "# Quilt: A Data Lakehouse for Actionable Data\n\nQuilt connects teams to actionable data by simplifying data discovery, sharing,\nand analysis. It’s designed to serve data-driven organizations with powerful\ntools for managing data as code, enabling rapid experimentation, and ensuring\ndata integrity at scale.\n\n---\n\n## How to Get Started\n\nQuilt consists of three main elements:\n\n- [Quilt Platform](#quilt-platform-overview) which is a cloud platform for\n  interacting with, visualizing, searching and querying Quilt Packages, which is\n  hosted in an organization's AWS Account.\n- [Quilt Python SDK](#quilt-python-sdk) which provides the ability to create,\n  push, install and delete Quilt Packages.\n- [Quilt Ecosystem](#quilt-ecosystem-and-integrations) which provide extension\n  of the core Quilt Capabilities to enable typical elements of life sciences\n  workflows, such as incorporating orchestration data, and connecting packages\n  to Electronic Lab Notebooks.\n\nTo dive deeper into the capabilities of Quilt, start with our [Quick Start\nGuide](Quickstart.md) or explore the [Installation\nInstructions](Installation.md) for setting up your environment.\n\nIf you have any questions or need help, join our [Slack\ncommunity](https://slack.quiltdata.com/) or submit a support request to\n<support@quiltdata.io>.\n\n---\n\n## Navigating the Documentation\n\nThe Quilt documentation is structured to guide users through different layers of\nthe platform, from basic concepts to advanced integrations. Whether you're a\nbusiness user, developer, or platform administrator, the docs will help you\nquickly find the information you need.\n\n### Quilt Platform Overview\n\nThe **Quilt Platform** powers the core features of the Quilt data catalog,\nproviding tools for browsing, searching, and visualizing data stored in AWS S3.\nThe platform is ideal for teams needing to collaborate on data, with\ncapabilities like embeddable previews and metadata collection.\n\n**Core Sections:**\n\n- [Architecture](Architecture.md) - Learn ho",
    "url": "https://github.com/quiltdata/quilt",
    "last_updated": "2025-09-01T17:58:56+00:00"
  },
  {
    "full_name": "hrbrmstr/packettotal",
    "name": "packettotal",
    "description": "🔍Lookup and Analyze Packet Capture ('PCAP') Files",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "pcap",
      "packet-capture",
      "packet-total"
    ],
    "readme": "\n[![Travis-CI Build\nStatus](https://travis-ci.org/hrbrmstr/packettotal.svg?branch=master)](https://travis-ci.org/hrbrmstr/packettotal)\n[![Coverage\nStatus](https://codecov.io/gh/hrbrmstr/packettotal/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/packettotal)\n[![CRAN\\_Status\\_Badge](https://www.r-pkg.org/badges/version/packettotal)](https://cran.r-project.org/package=packettotal)\n\n# packettotal\n\nLookup and Analyze Packet Capture (‘PCAP’) Files\n\n## Description\n\n‘PacketTotal’ (<https://packettotal.com/>) is an engine for analyzing,\ncategorizing, and sharing packet capture (‘PCAP’) files. The tool was\nbuilt with the information security community in mind and has\napplications in malware analysis and network forensics. Methods are\nprovided to query search for and analyze packet capture files.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `packettotal_api_key`: Get or set PACKETTOTAL\\_API\\_KEY value\n  - `pt_deep_search`/`pt_get_search_results`: Create a new deep search\n    task. Search for a term or with a Lucene query.\n  - `pt_detail`: Get a detailed report of PCAP traffic, carved files,\n    signatures, and top-talkers.\n  - `pt_download`: Download a PCAP analysis archive. The result is a zip\n    archive containing the PCAP itself, CSVs representing various\n    analysis results, and all carved files.\n  - `pt_info`: Get high-level information about a specific PCAP file.\n  - `pt_random`: Get high-level information about a random PCAP file.\n  - `pt_search`: Search with term or with a valid Lucene query.\n  - `pt_similar`: Get a similarity graph relative to the current PCAP\n    file.\n  - `pt_usage`: Retrive usage and subscription plan information.\n\n## Installation\n\n``` r\ninstall.packages(\"packettotal\", repos = \"https://cinc.rud.is/\")\n```\n\n## Usage\n\n``` r\nlibrary(packettotal)\n\n# current version\npackageVersion(\"packettotal\")\n## [1] '0.1.0'\n```\n\n``` r\nstr(pt_random(), 2)\n## List of 1\n##  $ pcap_metadata:List of 11\n##   ..$ md5             ",
    "url": "https://github.com/hrbrmstr/packettotal",
    "last_updated": "2025-09-01T05:43:29+00:00"
  },
  {
    "full_name": "ndphillips/FFTrees",
    "name": "FFTrees",
    "description": "An R package to create and visualise fast-and-frugal decision trees (FFTs)",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please only edit the .Rmd file! -->\n<!-- Title, version and logo: -->\n\n# FFTrees 2.0.0.9000 <img src = \"man/figures/logo.png\" align = \"right\" alt = \"FFTrees\" width = \"160\" />\n\n<!-- Devel badges start: -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/FFTrees)](https://CRAN.R-project.org/package=FFTrees)\n[![Downloads/month](https://cranlogs.r-pkg.org/badges/FFTrees?color='00a9e0')](https://www.r-pkg.org/pkg/FFTrees)\n[![Total\ndownloads](https://cranlogs.r-pkg.org/badges/grand-total/FFTrees?color='00a9e0')](https://www.r-pkg.org/pkg/FFTrees)\n[![R-CMD-check](https://github.com/ndphillips/FFTrees/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ndphillips/FFTrees/actions/workflows/R-CMD-check.yaml)\n<!-- Devel badges end. -->\n\n<!-- Release badges start: -->\n<!-- [![CRAN status](https://www.r-pkg.org/badges/version/FFTrees)](https://CRAN.R-project.org/package=FFTrees) -->\n<!-- [![Total downloads](https://cranlogs.r-pkg.org/badges/grand-total/FFTrees?color='00a9e0')](https://www.r-pkg.org/pkg/FFTrees) -->\n<!-- Release badges end. -->\n<!-- ALL badges start: -->\n<!-- [![CRAN status](https://www.r-pkg.org/badges/version/FFTrees)](https://CRAN.R-project.org/package=FFTrees) -->\n<!-- [![Build Status](https://travis-ci.org/ndphillips/FFTrees.svg?branch=master)](https://travis-ci.org/ndphillips/FFTrees) -->\n<!-- [![R-CMD-check](https://github.com/ndphillips/FFTrees/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ndphillips/FFTrees/actions/workflows/R-CMD-check.yaml) -->\n<!-- [![Downloads/month](https://cranlogs.r-pkg.org/badges/FFTrees?color=brightgreen)](https://www.r-pkg.org/pkg/FFTrees) -->\n<!-- [![Total downloads](https://cranlogs.r-pkg.org/badges/grand-total/FFTrees?color='b22222')](https://www.r-pkg.org/pkg/FFTrees) -->\n<!-- ALL badges end. -->\n<!-- Pkg goal: -->\n\nThe R package **FFTrees** creates, visualizes and evaluates\n*fast-and-frugal decision trees* (FFTs) for solving binary\nclassi",
    "url": "https://github.com/ndphillips/FFTrees",
    "last_updated": "2025-07-25T09:21:34+00:00"
  },
  {
    "full_name": "talgalili/installr",
    "name": "installr",
    "description": "Functions for installing softwares from within R",
    "language": "R",
    "topics": [],
    "readme": "[![Project Status: Unsupported – The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be desired.](https://www.repostatus.org/badges/latest/unsupported.svg)](https://www.repostatus.org/#unsupported)\n\n:exclamation: **This repository is unsupported** (see https://github.com/talgalili/installr/issues/183), a more broadly applicable alternative is Winget/WingetUI (https://www.marticliment.com/wingetui/) (see the discussion in https://github.com/talgalili/installr/issues/183#issuecomment-1725363286).\n\n---\n\n<!-- badges: start -->\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/installr)](https://cran.r-project.org/package=installr)\n[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/talgalili/installr?branch=master&svg=true)](https://ci.appveyor.com/project/talgalili/installr)\n![](http://cranlogs.r-pkg.org/badges/installr?color=yellow)\n![](http://cranlogs.r-pkg.org/badges/grand-total/installr?color=yellowgreen)\n<!-- badges: end -->\n\n# installr\n\n## Introduction\n\nThe *installr* package offers a set of R functions for the installation and updating of software (currently, only on Windows OS), with a special focus on R itself. This package has two main goals:\n\n1. To make updating R (on windows) as easy as running a function.\n2. To make it as easy as possible to install all of the needed software for R development (such as git, RTools, etc), as well as for reproducible research using R (such as MikTeX, pandoc, etc).\n\n## Motivation\n\n\nWhile for Linux users, the installation process of new software may be just running a short line of code, for the regular Windows user it often includes browsing online, finding the latest version, downloading it, running the installer, and deleting the installation file. All of these steps are automatically done using functions in this package.\n\n## Installation\n\nTo install the stable version on CRAN:\n\n```r\ninstall.packages('installr')\n```\n\nTo install the",
    "url": "https://github.com/talgalili/installr",
    "last_updated": "2025-08-23T08:56:01+00:00"
  },
  {
    "full_name": "pytest-dev/pytest",
    "name": "pytest",
    "description": "The pytest framework makes it easy to write small tests, yet scales to support complex functional testing",
    "language": "Python",
    "topics": [
      "unit-testing",
      "test",
      "testing",
      "python",
      "hacktoberfest"
    ],
    "readme": ".. image:: https://github.com/pytest-dev/pytest/raw/main/doc/en/img/pytest_logo_curves.svg\n   :target: https://docs.pytest.org/en/stable/\n   :align: center\n   :height: 200\n   :alt: pytest\n\n\n------\n\n.. image:: https://img.shields.io/pypi/v/pytest.svg\n    :target: https://pypi.org/project/pytest/\n\n.. image:: https://img.shields.io/conda/vn/conda-forge/pytest.svg\n    :target: https://anaconda.org/conda-forge/pytest\n\n.. image:: https://img.shields.io/pypi/pyversions/pytest.svg\n    :target: https://pypi.org/project/pytest/\n\n.. image:: https://codecov.io/gh/pytest-dev/pytest/branch/main/graph/badge.svg\n    :target: https://codecov.io/gh/pytest-dev/pytest\n    :alt: Code coverage Status\n\n.. image:: https://github.com/pytest-dev/pytest/actions/workflows/test.yml/badge.svg\n    :target: https://github.com/pytest-dev/pytest/actions?query=workflow%3Atest\n\n.. image:: https://results.pre-commit.ci/badge/github/pytest-dev/pytest/main.svg\n   :target: https://results.pre-commit.ci/latest/github/pytest-dev/pytest/main\n   :alt: pre-commit.ci status\n\n.. image:: https://www.codetriage.com/pytest-dev/pytest/badges/users.svg\n    :target: https://www.codetriage.com/pytest-dev/pytest\n\n.. image:: https://readthedocs.org/projects/pytest/badge/?version=latest\n    :target: https://pytest.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\n.. image:: https://img.shields.io/badge/Discord-pytest--dev-blue\n    :target: https://discord.com/invite/pytest-dev\n    :alt: Discord\n\n.. image:: https://img.shields.io/badge/Libera%20chat-%23pytest-orange\n    :target: https://web.libera.chat/#pytest\n    :alt: Libera chat\n\n\nThe ``pytest`` framework makes it easy to write small tests, yet\nscales to support complex functional testing for applications and libraries.\n\nAn example of a simple test:\n\n.. code-block:: python\n\n    # content of test_sample.py\n    def inc(x):\n        return x + 1\n\n\n    def test_answer():\n        assert inc(3) == 5\n\n\nTo execute it::\n\n    $ pytest\n    =====================",
    "url": "https://github.com/pytest-dev/pytest",
    "last_updated": "2025-09-02T09:33:15+00:00"
  },
  {
    "full_name": "facebookresearch/fairseq-lua",
    "name": "fairseq-lua",
    "description": "Facebook AI Research Sequence-to-Sequence Toolkit",
    "language": "Lua",
    "topics": [],
    "readme": "# Introduction\n\n***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here, but is provided without any support.\n\nThis is fairseq, a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT).\nIt implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model.\nIt features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU.\nWe provide pre-trained models for English to French, English to German and English to Romanian translation.\n\n![Model](fairseq.gif)\n\n# Citation\n\nIf you use the code in your paper, then please cite it as:\n\n```\n@article{gehring2017convs2s,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title           = \"{Convolutional Sequence to Sequence Learning}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1705.03122},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = May,\n}\n```\n\nand\n\n```\n@article{gehring2016convenc,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},\n  title           = \"{A Convolutional Encoder Model for Neural Machine Translation}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1611.02344},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2016,\n  month           = Nov,\n}\n```\n\n#",
    "url": "https://github.com/facebookresearch/fairseq-lua",
    "last_updated": "2025-08-27T06:32:58+00:00"
  },
  {
    "full_name": "KaTeX/KaTeX",
    "name": "KaTeX",
    "description": "Fast math typesetting for the web.",
    "language": "JavaScript",
    "topics": [
      "math",
      "math-typesetting",
      "latex",
      "javascript",
      "katex"
    ],
    "readme": "<h1><a href=\"https://katex.org/\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://katex.org/img/katex-logo.svg\">\n    <img alt=\"KaTeX\" width=130 src=\"https://katex.org/img/katex-logo-black.svg\">\n  </picture>\n</a></h1>\n\n[![npm](https://img.shields.io/npm/v/katex.svg)](https://www.npmjs.com/package/katex)\n[![semantic-release](https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg)](https://github.com/semantic-release/semantic-release)\n[![CI](https://github.com/KaTeX/KaTeX/workflows/CI/badge.svg?branch=main&event=push)](https://github.com/KaTeX/KaTeX/actions?query=workflow%3ACI)\n[![codecov](https://codecov.io/gh/KaTeX/KaTeX/branch/main/graph/badge.svg)](https://codecov.io/gh/KaTeX/KaTeX)\n[![Discussions](https://img.shields.io/badge/Discussions-join-brightgreen)](https://github.com/KaTeX/KaTeX/discussions)\n[![jsDelivr](https://data.jsdelivr.com/v1/package/npm/katex/badge?style=rounded)](https://www.jsdelivr.com/package/npm/katex)\n![katex.min.js size](https://img.badgesize.io/https://unpkg.com/katex/dist/katex.min.js?compression=gzip)\n[![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/KaTeX/KaTeX)\n[![Financial Contributors on Open Collective](https://opencollective.com/katex/all/badge.svg?label=financial+contributors)](https://opencollective.com/katex)\n\nKaTeX is a fast, easy-to-use JavaScript library for TeX math rendering on the web.\n\n * **Fast:** KaTeX renders its math synchronously and doesn't need to reflow the page. See how it compares to a competitor in [this speed test](https://www.intmath.com/cg5/katex-mathjax-comparison.php).\n * **Print quality:** KaTeX's layout is based on Donald Knuth's TeX, the gold standard for math typesetting.\n * **Self contained:** KaTeX has no dependencies and can easily be bundled with your website resources.\n * **Server side rendering:** KaTeX produces the same output regardless of browser or ",
    "url": "https://github.com/KaTeX/KaTeX",
    "last_updated": "2025-09-02T09:14:57+00:00"
  },
  {
    "full_name": "daroczig/user2015-markdown-tutorial",
    "name": "user2015-markdown-tutorial",
    "description": "Slides and example codes for my R Markdown tutorial at useR! 2015, Aalborg",
    "language": "HTML",
    "topics": [],
    "readme": "",
    "url": "https://github.com/daroczig/user2015-markdown-tutorial",
    "last_updated": "2016-10-23T19:12:41+00:00"
  },
  {
    "full_name": "rajashekar/ai-playground",
    "name": "ai-playground",
    "description": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# AI Playground\n\nThis is a playground for AI algorithms. POCs, tests, etc.\n",
    "url": "https://github.com/rajashekar/ai-playground",
    "last_updated": "2023-08-31T23:42:16+00:00"
  },
  {
    "full_name": "aws/aws-sdk-pandas",
    "name": "aws-sdk-pandas",
    "description": "pandas on AWS - Easy integration with Athena, Glue, Redshift, Timestream, Neptune, OpenSearch, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).",
    "language": "Python",
    "topics": [
      "python",
      "aws",
      "pandas",
      "apache-arrow",
      "apache-parquet",
      "data-engineering",
      "etl",
      "data-science",
      "redshift",
      "athena",
      "lambda",
      "aws-lambda",
      "aws-glue",
      "emr",
      "amazon-athena",
      "glue-catalog",
      "mysql",
      "amazon-sagemaker-notebook",
      "modin",
      "ray"
    ],
    "readme": "# AWS SDK for pandas (awswrangler)\n\n*Pandas on AWS*\n\nEasy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).\n\n![AWS SDK for pandas](https://github.com/aws/aws-sdk-pandas/blob/main/docs/source/_static/logo2.png?raw=true \"AWS SDK for pandas\")\n![tracker](https://d3tiqpr4kkkomd.cloudfront.net/img/pixel.png?asset=GVOYN2BOOQ573LTVIHEW)\n\n> An [AWS Professional Service](https://aws.amazon.com/professional-services/) open source initiative | aws-proserve-opensource@amazon.com\n\n[![PyPi](https://img.shields.io/pypi/v/awswrangler)](https://pypi.org/project/awswrangler/)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/awswrangler)](https://anaconda.org/conda-forge/awswrangler)\n[![Python Version](https://img.shields.io/pypi/pyversions/awswrangler.svg)](https://pypi.org/project/awswrangler/)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)\n![Static Checking](https://github.com/aws/aws-sdk-pandas/workflows/Static%20Checking/badge.svg?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/aws-sdk-pandas/badge/?version=latest)](https://aws-sdk-pandas.readthedocs.io/?badge=latest)\n\n| Source | Downloads | Installation Command |\n|--------|-----------|----------------------|\n| **[PyPi](https://pypi.org/project/awswrangler/)**  | [![PyPI Downloads](https://img.shields.io/pypi/dm/awswrangler)](https://pypi.org/project/awswrangler/) | `pip install awswrangler` |\n| **[Conda](https://anaconda.org/conda-forge/awswrangler)** | [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/awswrangler.svg)](https://anaconda.org/conda-forge/awswra",
    "url": "https://github.com/aws/aws-sdk-pandas",
    "last_updated": "2025-08-30T02:25:20+00:00"
  },
  {
    "full_name": "govdirectory/website",
    "name": "website",
    "description": "Website repository for Govdirectory - a crowdsourced and fact-checked directory of official governmental online accounts and services.",
    "language": "HTML",
    "topics": [
      "wikidata",
      "civictech",
      "government-data",
      "social-media",
      "opengov",
      "hacktoberfest",
      "sdg-16"
    ],
    "readme": "# Govdirectory\n\n[![CC0 1.0](https://img.shields.io/badge/License-CC0%201.0-lightgrey.svg)](LICENSE)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n[![standard-readme compliant](https://img.shields.io/badge/readme%20style-standard-brightgreen.svg)](https://github.com/RichardLitt/standard-readme)\n[<img src=\"https://github.com/govdirectory/website/blob/main/static/dpgbadge.svg\" width=\"69\"/>](https://digitalpublicgoods.net/registry/govdirectory.html)\n[![Mastodon](https://fedi-badge.deno.dev/@govdirectory@wikis.world/followers.svg?style=flat)](https://wikis.world/@govdirectory)\n\nWebsite repository for Govdirectory - a crowdsourced and fact-checked directory of official governmental online accounts and services.\n\nGovdirectory aims to become a global directory of government agencies and their online presence by utilizing Wikidata.\nWikidata's community, sourceability, versioning, and potential custom tooling that we create will ensure that not only should the information be correct but when it isn't, because of vandalism or something else, users will be able, and suggested, to validate the information.\nThe goal is for this directory to be useful to journalists, web-archivists, researchers and many others, certainly including regular citizens.\n\n## Development setup\n\nGovdirectory is a static site meaning that it already has all of its pages generated when a visitor visits it. Govdirectory uses Snowman and [SPARQL](https://www.w3.org/TR/sparql11-query/) to do this.\n\nBecause each initial build of Snowman issues thousands (yes thousands) of SPARQL queries one should never make an **initial** build against `query.wikidata.org` but rather against a local Wikidata Query Service (WDQS) instance. However, because setting up a WDQS instance is not trivial, we provide a copy of a [Snowman build cache directory](https://github.com/govdirectory/website-cache). If in use, this will ensure Snowman only queries Wi",
    "url": "https://github.com/govdirectory/website",
    "last_updated": "2025-08-26T07:39:07+00:00"
  },
  {
    "full_name": "hilaryparker/mansplainr",
    "name": "mansplainr",
    "description": "Mansplaining R",
    "language": "R",
    "topics": [],
    "readme": "# mansplainr\n\n### What is mansplainr?\n\nYou probably need me to say this to you in simple terms. **mansplainr** translates S3 objects into text using standard templates in a simple and convenient way. \n\nFor help with the **mansplainr** R-package, there is a vignette available in the /vignettes folder.\n  \n# Installation\n\nThe package can be installed with\n\n    devtools::install_github(\"hilaryparker/mansplainr\")\n\nAfter installation, the package can be loaded into R.\n\n    library(mansplainr)\n\n# Using mansplain\n\nThe main function in the **mansplainr** package is `mansplain()`.  \n\n```\np <- prop.test(x = 500, n = 1008)\nmansplain(p)\n# That's great that you were able to do a hypothesis test. You got a p-value of 0.8255. That means it's not significant at alpha = .05, but that's OK. The important thing is that you tried.\n```\n\n# Bug reports\nReport bugs as issues on the [GitHub repository](https://github.com/hilaryparker/mansplainr)\n\n# Contributors\n\n* [Hilary Parker](https://github.com/hilaryparker)\n* [David Robinson](https://github.com/dgrtwo)\n* [Stephanie Hicks](https://github.com/stephaniehicks)\n* [Roger Peng](https://github.com/rdpeng)\n",
    "url": "https://github.com/hilaryparker/mansplainr",
    "last_updated": "2024-05-17T15:29:53+00:00"
  },
  {
    "full_name": "trestletech/RpiR",
    "name": "RpiR",
    "description": "R on Raspberry Pi",
    "language": "C++",
    "topics": [],
    "readme": "\n## RpiR\n\nAn R package to interface with the RaspberryPi. Builds on the [Wiring Pi](wiringpi.com) library to support interaction with the Raspberry Pi's General Purpose I/O (GPIO) pins to read and write values to/from physical devices. You can, for instance, turn on an LED, read the state of a switch, or read the value of an analog sensor\\* or even read in values from a microphone.\n\n> \\* The Raspberry Pi doesn't have any built-in analog inputs, so analog sensors would need to be connected through an Analog-to-Digital Converter such as the [MCP3008](http://www.adafruit.com/products/856). ([Here's a guide](http://www.raspberrypi-spy.co.uk/2013/10/analogue-sensors-on-the-raspberry-pi-using-an-mcp3008/) on how to connect the Raspberry Pi to the chip).\n\n### Setup & Installation\n\nYou'll need to download and install Wiring Pi on your Raspberry Pi first. Instructions to do so are [available here](http://wiringpi.com/download-and-install/).\n\nYou can test to confirm that your hardware is setup and wired as expected using the [GPIO utility](http://wiringpi.com/the-gpio-utility/) that comes with Wiring Pi, for instance:\n\n```bash\n$ gpio -x mcp3004:100:0 aread 100\n515\n```\n\nOnce you have Wiring Pi and your hardware configured properly, you can move on to install RpiR.\n\nSince RpiR is not available on CRAN, you'll want to install from GitHub. The easiest way to do that is using the `devtools` package.\n\n```r\n> install.packages(\"devtools\")\n> library(devtools)\n> install_github(\"trestletech/RpiR\")\n> library(RpiR)\n```\n\nAlso, Wiring Pi requires sudo/root privileges for many operations. If you're not concerned with security, the easiest solution is just to run R with sudo privileges.\n\n### Digital Reading & Writing\n\n```r\n> library(RpiR)\n> RpiR::init()\n> RpiR::pin_mode(1, \"in\")\n> RpiR::read_digital(1)\n[1] TRUE\n>\n>\n> RpiR::pin_mode(2, \"out\")\n> RpiR::write_digital(2, TRUE)  # To write a 1/HIGH to pin 2\n> RpiR::write_digital(2, FALSE) # To write a 0/LOW to pin 2\n```\n\n### Analog Reading & Writin",
    "url": "https://github.com/trestletech/RpiR",
    "last_updated": "2025-03-22T11:12:30+00:00"
  },
  {
    "full_name": "facebookresearch/metaseq",
    "name": "metaseq",
    "description": "Repo for external large-scale work",
    "language": "Python",
    "topics": [],
    "readme": "\n\n# Metaseq\nA codebase for working with [Open Pre-trained Transformers](projects/OPT), originally forked from [fairseq](https://github.com/facebookresearch/fairseq).\n\n\n## Community Integrations\n\n### Using OPT with 🤗 Transformers\n\nThe OPT 125M--66B models are now available in [Hugging Face Transformers](https://github.com/huggingface/transformers/releases/tag/v4.19.0). You can access them under the `facebook` organization on the [Hugging Face Hub](https://huggingface.co/facebook)\n\n### Using OPT-175B with Alpa\n\nThe OPT 125M--175B models are now supported in the [Alpa project](https://alpa-projects.github.io/tutorials/opt_serving.html), which \nenables serving OPT-175B with more flexible parallelisms on older generations of GPUs, such as 40GB A100, V100, T4, M60, etc.\n\n### Using OPT with Colossal-AI\n\nThe OPT models are now supported in the [Colossal-AI](https://github.com/hpcaitech/ColossalAI#OPT), which helps users to efficiently and quickly deploy OPT models training and inference, reducing large AI model budgets and scaling down the labor cost of learning and deployment.\n\n### Using OPT with CTranslate2\n\nThe OPT 125M--66B models can be executed with [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models. The project integrates the [SmoothQuant](https://github.com/mit-han-lab/smoothquant) technique to allow 8-bit quantization of OPT models. See the [usage example](https://opennmt.net/CTranslate2/guides/transformers.html#opt) to get started.\n\n### Using OPT with FasterTransformer\n\nThe OPT models can be served with [FasterTransformer](https://github.com/NVIDIA/FasterTransformer), a highly optimized inference framework written and maintained by NVIDIA. We provide instructions to convert OPT checkpoints into FasterTransformer format and [a usage example](docs/faster-transformer.md) with some benchmark results.\n\n### Using OPT with DeepSpeed\n\nThe OPT models can be finetuned using [DeepSpeed](https://github.com/microsoft",
    "url": "https://github.com/facebookresearch/metaseq",
    "last_updated": "2025-08-31T23:07:54+00:00"
  },
  {
    "full_name": "cs231n/cs231n.github.io",
    "name": "cs231n.github.io",
    "description": "Public facing notes page",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "\nNotes and assignments for Stanford CS class [CS231n: Convolutional Neural Networks for Visual Recognition](http://vision.stanford.edu/teaching/cs231n/)\n\n",
    "url": "https://github.com/cs231n/cs231n.github.io",
    "last_updated": "2025-08-31T14:32:21+00:00"
  },
  {
    "full_name": "jcheng5/user2015",
    "name": "user2015",
    "description": "Sample materials for useR2015",
    "language": "R",
    "topics": [],
    "readme": "",
    "url": "https://github.com/jcheng5/user2015",
    "last_updated": "2015-07-13T18:16:53+00:00"
  },
  {
    "full_name": "TeamHG-Memex/web-page-annotator",
    "name": "web-page-annotator",
    "description": "Annotate parts of web pages in the browser",
    "language": "Python",
    "topics": [],
    "readme": "Web Page Annotator\n==================\n\nThis is a small web application for muli-class annotation of elements on web pages.\n\nWorkflow:\n\n- Create a workspace: specify labels (classes you want to use)\n  and URLs which you want to label.\n- On each page, hover over the element you want to label,\n  do a right-click and choose the label.\n- When done labeling the page, press \"Next\" to go to the next page.\n- You can edit URLs or labels in the current workspace at any time by clicking\n  the \"Workspace\" button.\n- To get you labeling results and pages saved for offline viewing/rendering,\n  click the \"Export\" button.\n\nHow this works: pages are rendered in an iframe and proxied through the local server.\nThis allows to intercept all requests to resources and save them,\nand we can communicate with an iframe because it is coming from the same domain.\nData is stored in an sqlite database.\n\nInstallation::\n\n    pip install -r requirements.txt\n\n\nUsage::\n\n    ./app.py\n\n\nLicense is MIT.\n\n----\n\n.. image:: https://hyperiongray.s3.amazonaws.com/define-hg.svg\n\t:target: https://www.hyperiongray.com/?pk_campaign=github&pk_kwd=web-page-annotator\n\t:alt: define hyperiongray\n",
    "url": "https://github.com/TeamHG-Memex/web-page-annotator",
    "last_updated": "2024-05-21T13:11:16+00:00"
  },
  {
    "full_name": "RBigData/hdfio",
    "name": "hdfio",
    "description": "A set of high-level utilities for working with hdf5.",
    "language": "R",
    "topics": [
      "r",
      "hpc",
      "hdf5",
      "pbdr"
    ],
    "readme": "# hdfio\n\n* **Version:** 0.1-0\n* **Status:** [![Build Status](https://travis-ci.org/RBigData/hdfio.png)](https://travis-ci.org/RBigData/hdfio)\n* **License:** [BSD 2-Clause](http://opensource.org/licenses/BSD-2-Clause)\n* **Author:** Drew Schmidt and Amil Williamson\n\n\nA set of high-level utilities for working with [HDF5](https://www.hdfgroup.org/).\n\nThis package is not meant to expose anywhere near the full capabilities of HDF5. For that, see the [rhdf5](https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html) and [hdf5r](https://cran.r-project.org/web/packages/hdf5r/index.html) packages (we actually use hdf5r internally). The goal of this package is to try to make HDF5 as simple to use as `read.csv()` and `write.csv()`, but with the added benefits of using binary file formats.\n\nOur main focus is on storing and reading dataframes. We call these \"h5df\" as in \"h5 dataframe\". I understand that this is annoying and difficult to convince your fingers to type if you are familiar with HDF5, but it's too good of a name to pass up. Right now we support reading from two kinds of formats written by python's pandas (really pytables), with some restrictions (no strings when `format=fixed`). We have full support for a format that is good for working with R, and should soon have a format that is useful if the goal is to regularly share data between python and R.\n\nThe current documentation is a train wreck, but we're working on it.\n\n\n\n## Installation\n\n<!-- To install the R package, run:\n\n```r\ninstall.package(\"hdfio\")\n``` -->\n\nThe development version is maintained on GitHub, and can easily be installed by any of the packages that offer installations from GitHub:\n\n```r\nremotes::install_github(\"wrathematics/lineSampler\")\nremotes::install_github(\"RBigData/hdfio\")\n```\n\n\n\n## Examples\n\nWe'll take a look at some examples using the famous [airlines dataset](http://stat-computing.org/dataexpo/2009/), and we'll be working from a directory that has all of the uncompressed csv's:\n\n```r\n",
    "url": "https://github.com/RBigData/hdfio",
    "last_updated": "2025-04-05T01:06:04+00:00"
  },
  {
    "full_name": "ben-strasser/fast-cpp-csv-parser",
    "name": "fast-cpp-csv-parser",
    "description": "fast-cpp-csv-parser",
    "language": "C++",
    "topics": [],
    "readme": "# Fast C++ CSV Parser\n\nThis is a small, easy-to-use and fast header-only library for reading comma separated value (CSV) files. \n\n## Features\n\n  * Automatically rearranges columns by parsing the header line.\n  * Disk I/O and CSV-parsing are overlapped using threads for efficiency.\n  * Parsing features such as escaped strings can be enabled and disabled at compile time using templates. You only pay in speed for the features you actually use.\n  * Can read multiple GB files in reasonable time.\n  * Support for custom columns separators (i.e. Tab separated value files are supported), quote escaped strings, automatic space trimming. \n  * Works with `*`nix and Windows newlines and automatically ignores UTF-8 BOMs.\n  * Exception classes with enough context to format useful error messages. what() returns error messages ready to be shown to a user. \n\n## Getting Started\n\nThe following small example should contain most of the syntax you need to use the library.\n\n```cpp\n# include \"csv.h\"\n\nint main(){\n  io::CSVReader<3> in(\"ram.csv\");\n  in.read_header(io::ignore_extra_column, \"vendor\", \"size\", \"speed\");\n  std::string vendor; int size; double speed;\n  while(in.read_row(vendor, size, speed)){\n    // do stuff with the data\n  }\n}\n```\n\n## Installation\n\nThe library only needs a standard conformant C++11 compiler. It has no further dependencies. The library is completely contained inside a single header file and therefore it is sufficient to copy this file to some place on your include path. The library does not have to be explicitly build. \n\nNote however, that threads are used and some compiler (for example GCC) require you to link against additional libraries to make it work. With GCC it is important to add -lpthread as the last item when linking, i.e. the order in \n\n```\ng++ -std=c++0x a.o b.o -o prog -lpthread\n```\n\nis important. If you for some reason do not want to use threads you can define CSV_IO_NO_THREAD before including the header.\n\nRemember that the library makes use of C++11 ",
    "url": "https://github.com/ben-strasser/fast-cpp-csv-parser",
    "last_updated": "2025-09-01T18:24:26+00:00"
  },
  {
    "full_name": "mediacloud/backend",
    "name": "backend",
    "description": "Media Cloud is an open source, open data platform that allows researchers to answer quantitative questions about the content of online media.",
    "language": "Python",
    "topics": [
      "media",
      "newspaper",
      "newsmedia",
      "online-news",
      "news-media",
      "content-analysis"
    ],
    "readme": "This is the source code for the [Media Cloud](http://mediacloud.org/) core system. Media Cloud, a joint project of the [Berkman Center for Internet & Society at Harvard University](http://cyber.law.harvard.edu/) and the [Center for Civic Media at MIT](http://civic.mit.edu/), is an open source, open data platform that allows researchers to answer complex quantitative and qualitative questions about the content of online media.\n\nFor more information on Media Cloud, go to [mediacloud.org](http://mediacloud.org/).\n\n**Note:** Most users prefer to use Media Cloud's [API and public tools](http://mediacloud.org/get-involved/) to query our data instead of running their own Media Cloud instance. \n\nThe code in this repository will be of interest to those users who wish to run their own Media Cloud instance and users of the public tools who want to understand how Media Cloud is implemented.\n\nThe Media Cloud code here does three things:\n\n* Runs a web app that allows you to manage a set of media sources and their feeds.\n  \n* Periodically crawls the feeds setup within the web app and downloads any new stories found within the downloaded feeds.\n  \n* Extracts the substantive text from the downloaded story content (minus the ads, navigation, comments, etc.) and associates a set of tags with each story based on that extracted text.\n\nFor very brief installation instructions, see [INSTALL.markdown](INSTALL.markdown).\n\nPlease send us a note at [info@mediacloud.org](info@mediacloud.org) if you are using any of this code or if you have any questions.  We are very interested in knowing who's using the code and for what.\n\n\nBuild Status\n------------\n\n![Pull, build, push, test](https://github.com/mediacloud/backend/workflows/Pull,%20build,%20push,%20test/badge.svg)\n\n\nHistory of the Project\n----------------------\n\nPrint newspapers are declaring bankruptcy nationwide. High-profile blogs are proliferating. Media companies are exploring new production techniques and business models in a landscape ",
    "url": "https://github.com/mediacloud/backend",
    "last_updated": "2025-08-23T15:47:34+00:00"
  },
  {
    "full_name": "zcgzcgzcg1/MediaSum",
    "name": "MediaSum",
    "description": "MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization",
    "language": "",
    "topics": [],
    "readme": "# MediaSum\nThis large-scale media interview dataset contains 463.6K transcripts with abstractive summaries, collected from interview transcripts and overview / topic descriptions from NPR and CNN.\n\n<ins>Please restrict your usage of this dataset to research purpose only</ins>. And please cite our paper:\n\n**<a href=\"https://arxiv.org/abs/2103.06410\">MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization</a>**\n\n_Chenguang Zhu*, Yang Liu*, Jie Mei and Michael Zeng (*: Equal contribution)_\n\n_North American Chapter of the Association for Computational Linguistics (**NAACL**), Mexico City, Mexico, 2021._\n\n• Sample data:\n```\n{\n  \"id\": \"NPR-11\",\n  \"program\": \"Day to Day\",\n  \"date\": \"2008-06-10\",\n  \"url\": \"https://www.npr.org/templates/story/story.php?storyId=91356794\",\n  \"title\": \"Researchers Find Discriminating Plants\",\n  \"summary\": \"The \\\"sea rocket\\\" shows preferential treatment to plants that are its kin. Evolutionary plant ecologist Susan Dudley of McMaster University in Ontario discusses her discovery.\",\n  \"utt\": [\n    \"This is Day to Day.  I'm Madeleine Brand.\",\n    \"And I'm Alex Cohen.\",\n    \"Coming up, the question of who wrote a famous religious poem turns into a very unchristian battle.\",\n    \"First, remember the 1970s?  People talked to their houseplants, played them classical music. They were convinced plants were sensuous beings and there was that 1979 movie, \\\"The Secret Life of Plants.\\\"\",\n    \"Only a few daring individuals, from the scientific establishment, have come forward with offers to replicate his experiments, or test his results. The great majority are content simply to condemn his efforts without taking the trouble to investigate their validity.\",\n    ...\n    \"OK. Thank you.\",\n    \"That's Susan Dudley. She's an associate professor of biology at McMaster University in Hamilt on Ontario. She discovered that there is a social life of plants.\"\n  ],\n  \"speaker\": [\n    \"MADELEINE BRAND, host\",\n    \"ALEX COHEN, host\",\n    \"ALEX COHEN, ",
    "url": "https://github.com/zcgzcgzcg1/MediaSum",
    "last_updated": "2025-08-07T12:26:09+00:00"
  },
  {
    "full_name": "kylemcdonald/FreeWifi",
    "name": "FreeWifi",
    "description": "How to get free wifi.",
    "language": "Python",
    "topics": [
      "wireless-network",
      "internet-access",
      "tcpdump",
      "spoof"
    ],
    "readme": "# Free Wifi\n\nThis short tutorial describes a few methods for gaining access to the Internet, [a basic human right](https://en.wikipedia.org/wiki/Right_to_Internet_access#2011:_UN_Special_Rapporteur_report), from public wireless networks.\n\nThis tutorial has been tested on Mac and a Raspberry Pi. It should generally work on Linux, and hasn't been tested on Windows.\n\n## Preparation\n\nMake sure you do this step *before* you are stuck without Internet access:\n\n1. Install [Python pip](https://pip.pypa.io/en/stable/installing/)\n2. On Linux, install Python Developer package, a dependency for the `netifaces` package.\n\n  **Ubuntu:**\n  ```\n  $ sudo apt-get install python-dev\n  ```\n  \n  **Fedora:**\n  ```\n  $ sudo dnf install python-devel\n  ```\n  Note: For Centos, substitute `dnf` with `yum`\n  \n2. Make a copy of this repository and install dependencies for the script:\n\n```\n$ git clone https://github.com/kylemcdonald/FreeWifi\n$ cd FreeWifi && sudo pip install -r requirements.txt\n```\n\n## How to get additional time\n\nIf you had free internet access but your time has run out, the first thing to try is open an incognito/private window. Here are instructions for a few browsers:\n\n* [Chrome](https://support.google.com/chrome/answer/95464?source=gsearch&hl=en) (mobile and desktop)\n* [Safari for iOS](https://support.apple.com/en-us/HT203036)\n* [Safari for Mac](https://support.apple.com/kb/ph21413?locale=en_US)\n* [Microsoft Edge](https://support.microsoft.com/en-us/instantanswers/34b9a3a6-68bc-510b-2a9e-833107495ee5/browse-inprivate-in-microsoft-edge)\n\nAn incognito/private window will temporarily clear any cookies that may have been used for tracking how much time you spent online, making you look like a \"new user\" and allowing you to log into the wireless portal again.\n\nUnfortunately, most systems track MAC addresses instead of cookies. A MAC address is a unique identifier assigned to every network interface. This means you need to get a new MAC address to get additional time. Fortunately, ",
    "url": "https://github.com/kylemcdonald/FreeWifi",
    "last_updated": "2025-08-31T23:58:29+00:00"
  },
  {
    "full_name": "cdeterman/gpuR",
    "name": "gpuR",
    "description": "R interface to use GPU's",
    "language": "R",
    "topics": [
      "r",
      "gpu",
      "gpu-computing",
      "gpgpu",
      "gpgpu-computing"
    ],
    "readme": "# gpuR\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1027597.svg)](https://doi.org/10.5281/zenodo.1027597)\n\n### Gitter rooms available for discussion\n[![chat](https://img.shields.io/badge/gitter-chat-brightgreen.svg)](https://gitter.im/cdeterman/gpuR)\n[![general](https://img.shields.io/badge/gitter-general-brightgreen.svg)](https://gitter.im/cdeterman/gpuR/general)\n[![development](https://img.shields.io/badge/gitter-development-brightgreen.svg)](https://gitter.im/cdeterman/gpuR/development)\n[![testing](https://img.shields.io/badge/gitter-testing-brightgreen.svg)](https://gitter.im/cdeterman/gpuR/Tested_GPUs)\n\n### Build Status\n|                 | Build           | Dev             |\n|-----------------|-----------------|-----------------|\n| Linux x86_64    | [![Build Status](https://travis-ci.org/cdeterman/gpuR.png?branch=master)](https://travis-ci.org/cdeterman/gpuR)      | [![Build Status](https://travis-ci.org/cdeterman/gpuR.png?branch=develop)](https://travis-ci.org/cdeterman/gpuR) |\n| OSX             | [![Build Status](https://travis-ci.org/cdeterman/gpuR.png?branch=master-macosx)](https://travis-ci.org/cdeterman/gpuR)   | [![Build Status](https://travis-ci.org/cdeterman/gpuR.png?branch=macosx)](https://travis-ci.org/cdeterman/gpuR) |\n| Windows x86     | [![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/github/cdeterman/gpuR?branch=master&svg=true)](https://ci.appveyor.com/project/cdeterman/gpuR)     | [![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/github/cdeterman/gpuR?branch=develop&svg=true)](https://ci.appveyor.com/project/cdeterman/gpuR) |\n\nTest coverage: [![Coverage Status](https://coveralls.io/repos/cdeterman/gpuR/badge.svg)](https://coveralls.io/r/cdeterman/gpuR?branch=master)\n\nCommunity Use: [![Downloads](http://cranlogs.r-pkg.org/badges/gpuR?color=brightgreen)](http://www.r-pkg.org/pkg/gpuR)\n\nWelcome to my R package for simple GPU computing.  Although there are a few\nexisting packages to leverage the powe",
    "url": "https://github.com/cdeterman/gpuR",
    "last_updated": "2025-08-12T08:50:32+00:00"
  },
  {
    "full_name": "ropensci/xslt",
    "name": "xslt",
    "description": "Extension of xml2 package for xsl transformations",
    "language": "C++",
    "topics": [
      "xslt",
      "xml",
      "r",
      "rstats",
      "r-package"
    ],
    "readme": "# xslt\n\n> XSLT 1.0 Transformations\n\n[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/xslt)](http://cran.r-project.org/package=xslt)\n[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/xslt)](http://cran.r-project.org/web/packages/xslt/index.html)\n\nAn extension for the 'xml2' package to transform XML documents by applying an XSL stylesheet.\n\n## Usage\n\nBasic example included with the package:\n\n```r\ndoc <- read_xml(system.file(\"examples/cd_catalog.xml\", package = \"xslt\"))\nstyle <- read_xml(system.file(\"examples/cd_catalog.xsl\", package = \"xslt\"))\nhtml <- xml_xslt(doc, style)\ncat(as.character(html))\n```\n",
    "url": "https://github.com/ropensci/xslt",
    "last_updated": "2025-03-31T10:02:40+00:00"
  },
  {
    "full_name": "leandropls/funcgpt",
    "name": "funcgpt",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "# funcgpt: Python library for creating functions with OpenAI's GPT\n\nfuncgpt is an easy-to-use Python library that allows you to quickly create Python functions using the power of OpenAI's GPT models. With just a few lines of code, you can create functions that generate human-like responses, answer questions, or anything else that GPT is capable of.\n\n## Features\n\n- Easy to use decorator for creating functions based on GPT models\n- Supports different GPT model versions\n- Customize GPT's behavior with adjustable temperature values\n- Generate responses in streaming or non-streaming modes\n\n## Installation\n\nTo install funcgpt, use pip:\n\n```bash\npip install funcgpt\n```\n\n## Usage\n\nTo create a function that answers questions like a pirate, you can use the following snippet:\n\n```python\nfrom funcgpt import gpt\n\n@gpt\ndef answer_like_pirate(message: str) -> str:\n    \"\"\"Answer questions like a pirate.\"\"\"\n    ...\n\n```\n\nUsage:\n\n```python\n>>> answer_like_pirate(\"How are you doing today?\")\n\"Arrr, I be doin' fine, matey.\"\n```\n\nTo do the same thing, but with a function that streams responses, you can use the following snippet:\n\n```python\nfrom typing import Iterator\nfrom funcgpt import gpt\n\n@gpt\ndef stream_like_pirate(message: str) -> Iterator[str]:\n    \"\"\"Answers questions like a pirate.\"\"\"\n    ...\n\n```\n\nUsage:\n\n```python\n>>> for token in stream_like_pirate(\"How are you doing today?\"):\n...     print(token, end=\"\", flush=True)\n...\nArrr, I be doin' fine, matey.\n```\n\nFor defining a function that returns a boolean value, you can use the following snippet:\n\n```python\nfrom funcgpt import gpt\n\n@gpt\ndef is_pirate(message: str) -> bool:\n    \"\"\"Returns true if the message is from a pirate.\"\"\"\n    ...\n\n```\n\nUsage:\n\n```python\n>>> is_pirate(\"Arrr, I be doin' fine, matey.\")\nTrue\n```\n\nFor choosing a different model or temperature, you can use the `model` and `temperature` keyword arguments:\n\n```python\nfrom funcgpt import gpt\n\n@gpt(model=\"gpt-4\", temperature=0)\ndef answer_like_pirate(message: str) -> ",
    "url": "https://github.com/leandropls/funcgpt",
    "last_updated": "2025-01-15T15:29:12+00:00"
  },
  {
    "full_name": "pablobarbera/echo_chambers",
    "name": "echo_chambers",
    "description": "Replication materials for the paper \"Tweeting from Left to Right: Is Online Political Communication More Than an Echo Chamber?\"",
    "language": "R",
    "topics": [],
    "readme": "Replication materials: _Is Online Political Communication More Than an Echo Chamber?_\n--------------\n\nThis github repository contains the replication materials for the paper \"Tweeting from Left to Right: Is Online Political Communication More Than an Echo Chamber?,\" forthcoming in ___Psychological Science___, authored by [Pablo Barbera](http://www.pablobarbera.com), [John T. Jost](http://psych.nyu.edu/jost/), [Jonathan Nagler](http://politics.as.nyu.edu/object/JonathanNagler), [Joshua Tucker](https://files.nyu.edu/jat7/public/), and [Richard Bonneau](http://bonneaulab.bio.nyu.edu/), all members of the [Social Media and Political Participation (SMaPP) Lab](http://smapp.nyu.edu/) at NYU.\n\n> __Abstract:__\n> We estimated ideological preferences of 3.8 million Twitter users and, using a dataset of 150 million tweets concerning 12 political and non-political issues, explored whether online communication resembles an \"echo chamber\" due to selective exposure and ideological segregation or a \"national conversation.\" We observed that information was exchanged primarily among individuals with similar ideological preferences for political issues (e.g., presidential election, government shutdown) but not for many other current events (e.g., Boston marathon bombing, Super Bowl). Discussion of the Newtown shootings in 2012 reflected a dynamic process, beginning as a \"national conversation\" before being transformed into a polarized exchange. With respect to political and non-political issues, liberals were more likely than conservatives to engage in cross-ideological dissemination, highlighting an important asymmetry with respect to the structure of communication that is consistent with psychological theory and research. We conclude that previous work may have overestimated the degree of ideological segregation in social media usage.\n\nThis README file provides an overview of the replications materials for the article. The [Data](https://github.com/pablobarbera/echo_chambers#data) s",
    "url": "https://github.com/pablobarbera/echo_chambers",
    "last_updated": "2025-06-02T02:37:46+00:00"
  },
  {
    "full_name": "Maratyszcza/NNPACK",
    "name": "NNPACK",
    "description": "Acceleration package for neural networks on multi-core CPUs",
    "language": "C",
    "topics": [
      "neural-network",
      "neural-networks",
      "convolutional-layers",
      "inference",
      "high-performance",
      "high-performance-computing",
      "simd",
      "cpu",
      "multithreading",
      "fast-fourier-transform",
      "winograd-transform",
      "matrix-multiplication"
    ],
    "readme": "<p align=\"center\"><img src=\"https://maratyszcza.github.io/NNPACK/NNPACK.png\" alt=\"NNPACK Logo\" title=\"NNPACK\"/></p>\n\n# NNPACK\n\n[![BSD (2 clause) License](https://img.shields.io/badge/License-BSD%202--Clause%20%22Simplified%22%20License-blue.svg)](https://github.com/Maratyszcza/NNPACK/blob/master/LICENSE)\n[![Build Status](https://img.shields.io/travis/Maratyszcza/NNPACK.svg)](https://travis-ci.org/Maratyszcza/NNPACK)\n\nNNPACK is an acceleration package for neural network computations. NNPACK aims to provide high-performance implementations of convnet layers for multi-core CPUs.\n\nNNPACK is not intended to be directly used by machine learning researchers; instead it provides low-level performance primitives leveraged in leading deep learning frameworks, such as [PyTorch](http://pytorch.org/), [Caffe2](https://caffe2.ai/), [MXNet](http://mxnet.io), \n[tiny-dnn](https://tiny-dnn.readthedocs.io/), [Caffe](http://caffe.berkeleyvision.org/), [Torch](http://torch.ch/), and [Darknet](https://pjreddie.com/darknet/).\n\n## Platforms and requirements\n\n| Environment  | Architecture  | CPU requirements                 |\n| ------------ | ------------- | -------------------------------- |\n| Linux        | x86-64        | AVX2 and 3-level cache hierarchy |\n| Linux        | ARM           | NEON                             |\n| Linux        | ARM64         |                                  |\n| macOS        | x86-64        | AVX2 and 3-level cache hierarchy |\n| Android      | ARM           | NEON                             |\n| Android      | ARM64         |                                  |\n| Android      | x86           |                                  |\n| Android      | x86-64        |                                  |\n| iOS          | ARM           |                                  |\n| iOS          | ARM64         |                                  |\n| Emscripten   | Asm.js        |                                  |\n| Emscripten   | WebAssembly   |                                 ",
    "url": "https://github.com/Maratyszcza/NNPACK",
    "last_updated": "2025-08-30T23:24:16+00:00"
  },
  {
    "full_name": "tzmartin/Google-Maps-TSP-Solver",
    "name": "Google-Maps-TSP-Solver",
    "description": "Google Maps TSP Solver. A fork of Geir Engdahl's TSP solver located at: https://code.google.com/p/google-maps-tsp-solver",
    "language": "JavaScript",
    "topics": [],
    "readme": "Google-Maps-TSP-Solver\n======================\n\nGoogle Maps TSP Solver that computes the fastest route that visits a given set of locations using JavaScript.  It's a fork of Geir Engdahl's TSP Solver project, but is/will be optimized for headless environments (node.js, Titanium)  multiple APIs.\n\n*Work in progress.*\n\n### Algorithms\n\nDifferent algorithms are selected based on the number of input locations, in order to produce results in a responsive manner. For large sets of points, the returned solution will be approximate. It is an NP-complete problem after all.\n\n- tspK3 - Uses 3-opt algorithm to find a good solution to the TSP. The 3-opt \nalgorithm is a local optimization technique similar to the 2-opt step \nwhich is performed after each wave of ants in the ant colony solver. \nSince 3-opt is much more expensive than 2-opt, it's too slow to use it \nafter each ant wave. But it is used after the ant colony / 2-opt combo \nis done, and usually improves the solution quite a bit. The solution \nquality is never degraded by this step. \n\n- tspAntColonyK2 - Computes a near-optimal solution to the TSP problem, using Ant Colony Optimization and local optimization in the form of k2-opting each candidate route.\n\n- tspBruteForce - Returns the optimal solution to the TSP problem. If mode is 1, it will return the optimal solution to the related problem of finding a path from node 0 to node numActive - 1, visiting the in-between nodes in the best order.\n\n\n###Usage\n\n```\n// Your normal Google Map object initialization\nvar myOptions = {\n  zoom: zoom,\n  center: center,\n  mapTypeId: google.maps.MapTypeId.ROADMAP\n};\nmyMap = new google.maps.Map(div, myOptions);\ndirectionsPanel = document.getElementById(\"my_textual_div\");\n\n// Create the tsp object\ntsp = new BpTspSolver(myMap, directionsPanel);\n\n// Set your preferences\ntsp.setAvoidHighways(true);\ntsp.setTravelMode(google.maps.DirectionsTravelMode.WALKING);\n\n// Add points (by coordinates, or by address).\n// The first point added is the starting",
    "url": "https://github.com/tzmartin/Google-Maps-TSP-Solver",
    "last_updated": "2025-08-21T17:30:09+00:00"
  },
  {
    "full_name": "luigi/geo_ip",
    "name": "geo_ip",
    "description": "Retreive the geolocation of an IP address based on the ipinfodb.com webservice",
    "language": "Ruby",
    "topics": [],
    "readme": "= GeoIp\n\nRetreive the geolocation of an IP address based on the {ipinfodb.com}[http://ipinfodb.com/] webservice.\n\nAs of 8th November 2010, the service is asking that all users {register}[http://ipinfodb.com/register.php] for an API key.\n\nConsider making a donation to {ipinfodb.com}[http://ipinfodb.com/] at {http://ipinfodb.com/donate.php}[http://ipinfodb.com/donate.php]\n\n== Usage\n\n=== Set API key\n  GeoIp.api_key = \"YOUR_API_KEY\"\n\nThis must be done before making the geolocation call.\n\n=== Retrieve geolocation\n  GeoIp.geolocation(ip_address)\n\n=== Example\n\n  # 209.85.227.104 = google.be (US)\n  GeoIp.geolocation('209.85.227.104')\n\nreturns:\n\n  {\n    :status           =>\"OK\",\n    :ip               =>\"209.85.227.104\"\n    :country_code     =>\"US\",\n    :country_name     =>\"United States\",\n    :region_code      =>\"06\",\n    :region_name      =>\"California\",\n    :city             =>\"Mountain View\",\n    :zip_postal_code  =>\"94043\",\n    :latitude         =>\"37.4192\",\n    :longitude        =>\"-122.057\"\n  }\n\n=== Country only\n\nThere is an option to only retreive the country information and thus excluding the city details. This results in a faster response from the service since less queries need to be done.\n\n  GeoIp.geolocation('209.85.227.104', {:precision => :country})\n\nreturns:\n\n  {\n    :status           => \"OK\",\n    :ip               => \"209.85.227.104\"\n    :country_code     => \"US\",\n    :country_name     => \"United States\"\n  }\n\n=== Timezone information\n\nThere is an option now to retrieve optional timezone information too:\n\n  GeoIp.geolocation('209.85.227.104', {:timezone => true})\n\nreturns:\n\n  {\n    :status           =>\"OK\",\n    :ip               =>\"209.85.227.104\"\n    :country_code     =>\"US\",\n    :country_name     =>\"United States\",\n    :region_code      =>\"06\",\n    :region_name      =>\"California\",\n    :city             =>\"Mountain View\",\n    :zip_postal_code  =>\"94043\",\n    :latitude         =>\"37.4192\",\n    :longitude        =>\"-122.057\"\n    :timezone_name    =>\"America/Lo",
    "url": "https://github.com/luigi/geo_ip",
    "last_updated": "2015-08-29T15:08:22+00:00"
  },
  {
    "full_name": "facebookresearch/fairseq",
    "name": "fairseq",
    "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
    "language": "Python",
    "topics": [
      "python",
      "pytorch",
      "artificial-intelligence"
    ],
    "readme": "<p align=\"center\">\n  <img src=\"docs/fairseq_logo.png\" width=\"150\">\n  <br />\n  <br />\n  <a href=\"https://opensource.fb.com/support-ukraine\"><img alt=\"Support Ukraine\" src=\"https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/releases\"><img alt=\"Latest Release\" src=\"https://img.shields.io/github/release/pytorch/fairseq.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/actions?query=workflow:build\"><img alt=\"Build Status\" src=\"https://github.com/pytorch/fairseq/workflows/build/badge.svg\" /></a>\n  <a href=\"https://fairseq.readthedocs.io/en/latest/?badge=latest\"><img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/fairseq/badge/?version=latest\" /></a>\n  <a href=\"https://app.circleci.com/pipelines/github/facebookresearch/fairseq/\"><img alt=\"CicleCI Status\" src=\"https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield\" /></a>\n</p>\n\n--------------------------------------------------------------------------------\n\nFairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.\n\nWe provide reference implementations of various sequence modeling papers:\n\n<details><summary>List of implemented papers</summary><p>\n\n* **Convolutional Neural Networks (CNN)**\n  + [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n  + [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n  + [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n  + [Hierarchical Neural Story Generation (Fan et al., 2018",
    "url": "https://github.com/facebookresearch/fairseq",
    "last_updated": "2025-09-02T09:51:16+00:00"
  },
  {
    "full_name": "strongloop/loopback",
    "name": "loopback",
    "description": "LoopBack makes it easy to build modern applications that require complex integrations.",
    "language": "JavaScript",
    "topics": [],
    "readme": "# LoopBack\n\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/strongloop/loopback?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Module LTS Adopted'](https://img.shields.io/badge/Module%20LTS-Adopted-brightgreen.svg?style=flat)](http://github.com/CloudNativeJS/ModuleLTS)\n[![IBM Support](https://img.shields.io/badge/IBM%20Support-Frameworks-brightgreen.svg?style=flat)](http://ibm.biz/node-support)\n\n**⚠️ LoopBack 3 has reached end of life. We are no longer accepting pull requests or providing \nsupport for community users. The only exception is fixes for critical bugs and security \nvulnerabilities provided as part of support for IBM API Connect customers.\nWe urge all LoopBack 3 users to migrate their applications to LoopBack 4 as soon as possible. \nLearn more about\n<a href=\"https://loopback.io/doc/en/contrib/Long-term-support.html\">LoopBack's long term support policy.</a>\nwill be provided or accepted. (See\n[Module Long Term Support Policy](#module-long-term-support-policy) below.)**\n\nWe urge all LoopBack 3 users to migrate their applications to LoopBack 4 as\nsoon as possible. Refer to our\n[Migration Guide](https://loopback.io/doc/en/lb4/migration-overview.html)\nfor more information on how to upgrade.\n\n## Overview\n\nLoopBack is a highly-extensible, open-source Node.js framework that enables you to:\n\n  * Create dynamic end-to-end REST APIs with little or no coding.\n  * Access data from Oracle, MySQL, PostgreSQL, MS SQL Server, MongoDB, SOAP and other REST APIs.\n  * Incorporate model relationships and access controls for complex APIs.\n  * Use built-in push, geolocation, and file services for mobile apps.\n  * Easily create client apps using Android, iOS, and JavaScript SDKs.\n  * Run your application on-premises or in the cloud.\n\nLoopBack consists of:\n\n  * A library of Node.js modules.\n  * [Yeoman](http://yeoman.io/) generators for scaffolding applications.\n  * Client SDKs for iOS, Android, and web clients.\n\nLoopBack tools",
    "url": "https://github.com/strongloop/loopback",
    "last_updated": "2025-09-02T08:25:18+00:00"
  },
  {
    "full_name": "swarm-lab/videoplayR",
    "name": "videoplayR",
    "description": "A computer vision library for R",
    "language": "R",
    "topics": [],
    "readme": "IMPORTANT NOTICE\n=========\n\nI have stopped developing `videoplayR`. Instead, I have created two libraries to replace it: \n\n1. [`ROpenCVLite`](https://github.com/swarm-lab/ROpenCVLite): this library downloads, compiles, and installs OpenCV within the R library in a standardized location across Mac, Windows, and Linux. It doesn't provide any computer vision functions directly, but allows developers to create packages using OpenCV without having to worry about the platform of the end-user. \n2. [`Rvision`](https://github.com/swarm-lab/Rvision): this library depends on `ROpenCVLite` and provides computer vision functions somewhat similar to the discontinued `videoplayR` package. \n\nBesides making sure that OpenCV is installed in a standardized location across platforms, this new organization also allows updating `Rvision` frequently without having to recompile OpenCV every time, like it was happening with `videoplayR` (`ROpenCVLite` needs reinstalling only when a new release of OpenCV becomes available). \n\nI recommend that you try out the two new packages instead (they should both install fine on Windows), and report bugs, feature requests, and comments on their respective repository. I don't have much time at the moment to push their development, but it'll be helpful to know what people have a need for when I get back to it. \n\n---\n\nvideoplayR\n=========\n\n`videoplayR` is an R package that provides a few functions (for now) to manipulate \nvideo and image files with R. It requires the installation of OpenCV. \n\n**>>> Package vignette: [http://rpubs.com/sjmgarnier/videoplayR](http://rpubs.com/sjmgarnier/videoplayR) <<<**\n\n### Package overview\n\nThis package uses the [OpenCV C++ library](http://opencv.org/) to provide R users \nwith some basic functions to read and manipulate video and image files. As of now, \n`videoplayR` can:\n\n* Load can access video and image files, as well as camera streams. \n* Grab frames from videos and camera streams, and convert them to images.\n* Convert ",
    "url": "https://github.com/swarm-lab/videoplayR",
    "last_updated": "2022-05-11T13:26:24+00:00"
  },
  {
    "full_name": "everypolitician/gender-balance",
    "name": "gender-balance",
    "description": "Crowdsourcing platform for gathering gender information about politicians to improve the data in EveryPolitician",
    "language": "HTML",
    "topics": [],
    "readme": "# Gender Balance\n\n[![Build Status](https://travis-ci.org/everypolitician/gender-balance.svg?branch=master)](https://travis-ci.org/everypolitician/gender-balance)\n\nCrowdsourcing platform for gathering gender information about politicians to improve the data in [EveryPolitician](http://everypolitician.org).\n\n## Installation\n\nGet the code from GitHub\n\n    git clone https://github.com/everypolitician/gender-balance\n    cd gender-balance\n\nConfigure the environment by copying `.env.example` and following the instructions inside to configure the app.\n\n    cp .env.example .env\n    vi .env\n\nThen use vagrant to build a VM with all the dependencies installed:\n\n    vagrant up\n\n## Usage\n\nLog in to the vagrant VM and start the app server and worker with foreman:\n\n    vagrant ssh\n    foreman start\n\nThen visit <http://localhost:5000> to view the app.\n\nTo run the tests use the following:\n\n    vagrant ssh\n    bundle exec rake test\n\n## Setting the featured country\n\nFirst start a pry session, if you're in a vagrant VM then run\n\n    pry -r ./app\n\nOr if you want to run pry on Heroku\n\n    heroku run 'pry -r ./app'\n\nThen you can set the featured country using the country code, which can be found in [`countries.json`](https://github.com/everypolitician/everypolitician-data/blob/master/countries.json). For example to set it to Turkey:\n\n    FeaturedCountry.current = 'TR'\n\nThis will end any previous featured country and create a new row for the given country code.\n",
    "url": "https://github.com/everypolitician/gender-balance",
    "last_updated": "2022-11-28T16:23:08+00:00"
  },
  {
    "full_name": "gadenbuie/regexplain",
    "name": "regexplain",
    "description": "🔍 An RStudio addin slash regex utility belt",
    "language": "R",
    "topics": [
      "regex-expression",
      "rstudio-addin",
      "regular-expression",
      "regex",
      "stringr",
      "shiny",
      "gadget",
      "rstats"
    ],
    "readme": "RegExplain\n================\n\n#### *Regular expressions are tricky.* RegExplain *makes it easier to see what you’re doing.*\n\n<!-- [![packageversion](https://img.shields.io/github/description/v/gadenbuie/regexplain.svg)](commits/master) -->\n\n![](https://img.shields.io/badge/lifecycle-maturing-blue.svg) [![Project\nStatus: Active – The project has reached a stable, usable state and is\nbeing actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/regexplain)](https://cran.r-project.org/package=regexplain)\n<!-- [![Last-changedate](https://img.shields.io/badge/last%20change-2020--07--02-yellowgreen.svg)](/commits/master) -->\n\n<!-- Links -->\n\n**RegExplain** is an RStudio addin slash utility belt for regular\nexpressions. Interactively build your regexp, check the output of common\nstring matching functions, consult the interactive help pages, or use\nthe included resources to learn regular expressions. And more.\n\nInspired by [RegExr.com](https://regexr.com/) and `stringr::str_view()`.\n\n## Installation\n\nInstallation is easy with `remotes`\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"gadenbuie/regexplain\")\n```\n\n## RegExplain in Action\n\n#### Overview\n\n![regexplain\nselection](https://raw.githubusercontent.com/gadenbuie/regexplain/af4fe0988a10f34dc528b4d359b80bb06af7809a/docs/regexplain-selection.gif)\n\n#### Regular Expressions Library\n\n![regexplain\nlibrary](https://raw.githubusercontent.com/gadenbuie/regexplain/af4fe0988a10f34dc528b4d359b80bb06af7809a/docs/regexplain-library.gif)\n\n#### Try the Built-In Examples\n\n![regexplain\nexamples](https://raw.githubusercontent.com/gadenbuie/regexplain/af4fe0988a10f34dc528b4d359b80bb06af7809a/docs/regexplain-try-this.gif)\n\n## RStudio Addin\n\nThe main feature of this package is the RStudio Addin **RegExplain\nSelection**. Just select the text or object containing text (such as the\nvariable name of a vector or a data.fram",
    "url": "https://github.com/gadenbuie/regexplain",
    "last_updated": "2025-08-22T18:15:31+00:00"
  },
  {
    "full_name": "jasimpson/tensorflow-on-aws",
    "name": "tensorflow-on-aws",
    "description": "Scripts to install TensorFlow on an AWS EC2 GPU Instance",
    "language": "Shell",
    "topics": [],
    "readme": "# tensorflow-on-aws\nScripts to install TensorFlow on an AWS EC2 GPU Instance\n\nThese scripts are based on the excellent guide by [@ramhiser](https://twitter.com/ramhiser) available [here](http://ramhiser.com/2016/01/05/installing-tensorflow-on-an-aws-ec2-instance-with-gpu-support/)\n\nThe first part takes about 10 mins and the second part takes about 30 mins to finish\n\n## Note about cuDNN\nThis script requires an Nvidia Accelerated Computing Developer Program account. If you do not have an account already, register for an account at: https://developer.nvidia.com Once accepted (**typically takes about a day**), download cuDNN from: https://developer.nvidia.com/rdp/assets/cudnn-65-linux-v2-asset Host this file on a private location and provide the URL when the script runs.\n\n## Note about versions\nThis script is for the following specific versions:\n\n| Name       | Version |\n|------------|---------|\n| TensorFlow | 0.9.0   |\n| CUDA       | 7.0     |\n| cuDNN      | 7.0     |\n| Java       | 8       |\n| Bazel      | 0.2.3   |\n| Python     | 2.7.6   |\n\n## Usage\n- Request an AWS EC2 (spot) instance with the following settings:\n    + AMI: Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-fce3c696\n    + Type: g2.2xlarge\n    + Storage: 16 GB (or higher)\n- SSH into the instance\n- Download part 0 of 3 of the script\n    + `wget https://raw.githubusercontent.com/jasimpson/tensorflow-on-aws/master/toa_part_0of2.sh`\n- Set executable\n    + `chmod +x toa_part_0of2.sh`\n- Execute script part 0 of 2\n    + `./toa_part_0of2.sh`\n- Execute script part 1 of 2\n    + `./toa_part_1of2.sh`\n- Enter URL to download cuDNN from\n- Reboot machine\n    + `sudo reboot`\n- SSH back into machine\n- Execute script part 2 of 2\n    + `./toa_part_2of2.sh`\n- Test TensorFlow\n    + `$ python`\n    + `>>> import tensorflow as tf`\n",
    "url": "https://github.com/jasimpson/tensorflow-on-aws",
    "last_updated": "2024-01-04T16:02:54+00:00"
  },
  {
    "full_name": "gboeing/osmnx",
    "name": "osmnx",
    "description": "Download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap.",
    "language": "Python",
    "topics": [
      "openstreetmap",
      "gis",
      "street-networks",
      "overpass-api",
      "networkx",
      "spatial-analysis",
      "geospatial",
      "urban-planning",
      "transportation",
      "geography",
      "osmnx",
      "networks",
      "spatial-data",
      "urban",
      "osm",
      "spatial",
      "python",
      "transport",
      "mapping",
      "routing"
    ],
    "readme": "# OSMnx\n\n[![PyPI Version](https://badge.fury.io/py/osmnx.svg)](https://pypi.org/project/osmnx/)\n[![PyPI Downloads](https://static.pepy.tech/personalized-badge/osmnx?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=downloads)](https://pepy.tech/project/osmnx)\n[![Documentation Status](https://readthedocs.org/projects/osmnx/badge/?version=latest)](https://osmnx.readthedocs.io/)\n[![Build Status](https://github.com/gboeing/osmnx/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/gboeing/osmnx/actions/workflows/ci.yml)\n[![Coverage Status](https://codecov.io/gh/gboeing/osmnx/branch/main/graph/badge.svg)](https://codecov.io/gh/gboeing/osmnx)\n\n**OSMnx** is a Python package to easily download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap. You can download and model walking, driving, or biking networks with a single line of code then analyze and visualize them. You can just as easily work with urban amenities/points of interest, building footprints, transit stops, elevation data, street orientations, speed/travel time, and routing.\n\nOSMnx 2.0 is released: read the [migration guide](https://github.com/gboeing/osmnx/issues/1123).\n\n## Citation\n\nIf you use OSMnx in your work, please cite the paper:\n\nBoeing, G. (2025). [Modeling and Analyzing Urban Networks and Amenities with OSMnx](https://doi.org/10.1111/gean.70009). *Geographical Analysis*, published online ahead of print. doi:10.1111/gean.70009\n\n## Getting Started\n\nFirst read the [Getting Started](https://osmnx.readthedocs.io/en/stable/getting-started.html) guide for an introduction to the package and FAQ.\n\nThen work through the [Examples Gallery](https://github.com/gboeing/osmnx-examples) for step-by-step tutorials and sample code.\n\n## Installation\n\nFollow the [Installation](https://osmnx.readthedocs.io/en/stable/installation.html) guide to install OSMnx.\n\n## Support\n\nIf you have any trouble, consult the [User Reference]",
    "url": "https://github.com/gboeing/osmnx",
    "last_updated": "2025-09-02T05:30:56+00:00"
  },
  {
    "full_name": "trinker/kmeanstext",
    "name": "kmeanstext",
    "description": "",
    "language": "R",
    "topics": [],
    "readme": "kmeanstext   [![Follow](https://img.shields.io/twitter/follow/tylerrinker.svg?style=social)](https://twitter.com/intent/follow?screen_name=tylerrinker)\n============\n\n**Devlopment Moved to clustext Package** [**-CLICK HERE-**](https://github.com/trinker/clustext)\n\n[![Project Status: Inactive - The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time allows.](http://www.repostatus.org/badges/latest/inactive.svg)](http://www.repostatus.org/#inactive)\n[![Build\nStatus](https://travis-ci.org/trinker/kmeanstext.svg?branch=master)](https://travis-ci.org/trinker/kmeanstext)\n[![Coverage\nStatus](https://coveralls.io/repos/trinker/kmeanstext/badge.svg?branch=master)](https://coveralls.io/r/trinker/kmeanstext?branch=master)\n<a href=\"https://img.shields.io/badge/Version-0.1.0-orange.svg\"><img src=\"https://img.shields.io/badge/Version-0.1.0-orange.svg\" alt=\"Version\"/></a>\n</p>\n<img src=\"inst/kmeanstext_logo/r_kmeanstext.png\" width=\"150\" alt=\"readability Logo\">\n\n**kmeanstext** is a collection of optimized tools for clustering text\ndata via kmeans clustering. There are many great R [clustering\ntools](https://cran.r-project.org/web/views/Cluster.html) to locate\ntopics within documents. Kmeans clustering is a popular method for topic\nextraction. This package builds upon my\n[hclustext](https://github.com/trinker/hclustext) package to extend the\n**hclustext** package framework to kmeans. One major difference between\nthe two techniques is that with hierchical clustering the number of\ntopics is specified after thte model has been fitted, whereas kmeans\nrequires the k topics to be specified before the model is fit.\nAdditionally, kmeans uses a random start seed, the results may vary each\ntime a model is fit. Additionally, Euclidian distance is typically used\nin a kmeans algorithm, where as any distance metric may be passed to a\nhierachical clustering fit.\n\nThe general idea is that we turn the documents into a matr",
    "url": "https://github.com/trinker/kmeanstext",
    "last_updated": "2016-03-21T00:37:18+00:00"
  },
  {
    "full_name": "tidymodels/tidypredict",
    "name": "tidypredict",
    "description": "Run predictions inside the database",
    "language": "R",
    "topics": [
      "rlang",
      "dplyr",
      "dbplyr",
      "purrr",
      "r"
    ],
    "readme": "\n# tidypredict <img src=\"man/figures/logo.png\" align=\"right\" width = \"120px\"/>\n\n[![R-CMD-check](https://github.com/tidymodels/tidypredict/workflows/R-CMD-check/badge.svg)](https://github.com/tidymodels/tidypredict/actions)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/tidypredict)](https://CRAN.r-project.org/package=tidypredict)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidymodels/tidypredict/branch/main/graph/badge.svg)](https://app.codecov.io/gh/tidymodels/tidypredict?branch=main)\n\nThe main goal of `tidypredict` is to enable running predictions inside\ndatabases. It reads the model, extracts the components needed to\ncalculate the prediction, and then creates an R formula that can be\ntranslated into SQL. In other words, it is able to parse a model such as\nthis one:\n\n``` r\nmodel <- lm(mpg ~ wt + cyl, data = mtcars)\n```\n\n`tidypredict` can return a SQL statement that is ready to run inside the\ndatabase. Because it uses `dplyr`’s database interface, it works with\nseveral databases back-ends, such as MS SQL:\n\n``` r\ntidypredict_sql(model, dbplyr::simulate_mssql())\n```\n\n    ## <SQL> (39.686261480253 + (`wt` * -3.19097213898374)) + (`cyl` * -1.5077949682598)\n\n## Installation\n\nInstall `tidypredict` from CRAN using:\n\n``` r\ninstall.packages(\"tidypredict\")\n```\n\nOr install the **development version** using `devtools` as follows:\n\n``` r\ninstall.packages(\"remotes\")\nremotes::install_github(\"tidymodels/tidypredict\")\n```\n\n## Functions\n\n`tidypredict` has only a few functions, and it is not expected that\nnumber to grow much. The main focus at this time is to add more models\nto support.\n\n| Function | Description |\n|----|----|\n| `tidypredict_fit()` | Returns an R formula that calculates the prediction |\n| `tidypredict_sql()` | Returns a SQL query based on the formula from `tidypredict_fit()` |\n| `tidypredict_to_column()` | Adds a new column using the formula from `tidypredict_fit()` |\n| `tidypredict_test()` | Tests `tidyverse` predictions against the model’s native `pre",
    "url": "https://github.com/tidymodels/tidypredict",
    "last_updated": "2025-07-16T14:24:42+00:00"
  },
  {
    "full_name": "notnews/good_nyt",
    "name": "good_nyt",
    "description": "Patterns in NYT production from 1987 to 2007",
    "language": "R",
    "topics": [
      "nyt",
      "news",
      "newspaper",
      "news-stories"
    ],
    "readme": "## The Good NYT\n\nThe New York Times (NYT) is the nation's newspaper of record. It is both well-regarded and popular. It has won more Pulitzer awards than any other newspaper. And it is the [30th most visited website in the U.S.](https://www.alexa.com/siteinfo/nytimes.com) (as of October, 2017).\n\nWe explore some patterns in production of NYT between 1987 and 2007 using the annotated [New York Times Corpus](https://catalog.ldc.upenn.edu/ldc2008t19).\n\n### Data Analysis\n\n0. [Convert NYT Corpus to CSV](https://github.com/soodoku/nytimes-corpus-extractor), and [Recode](scripts/01_nyt_recode.R)  \n\n1. **Not News**  \n   Has the proportion of news stories about topics unrelated to politics or the economy, such as, cooking, travel, fashion, music, etc.,  gone up over time? \n\n   We measure kinds of news stories using *news.desk* and *online.section*. (See the script for other ideas for how we can measure the kind of news.)  \n\n     - Proportion of Apolitical News Over Time: [Script](scripts/01_apolitical_news.R) and [Figure: Entire Newspaper (Using News Desk)](figs/all_apol_nd_by_month.pdf), [Figure: Section A1 (Using News Desk)](figs/a1_apol_nd_by_month.pdf), and [Figure: Entire Newspaper (Using News Desk and Online Section)](figs/all_apol_3_by_month.pdf)  \n     \n     <p align = \"center\"><img src=\"figs/all_apol_nd_by_month.png\" width=\"500\"></p>\n\n2. **Urban Vs. Rural**  \n    We use the *locations* (hand indexed), *online.locations* (algorithmically generated), and *dateline* fields to estimate rural vs. urban coverage within the US.  \n      - Script and Figure\n\n3. **National Vs. International**\n    We use the *news.desk* field *Foreign News* to estimate coverage of foreign news. We can also use the *locations* (hand indexed), *online.locations* (algorithmically generated), and *dateline* fields to estimate national vs. international coverage.  \n      - Proportion of Foreign Desk Stories Over Time: [Script](scripts/03_national.R) and [Figure](figs/all_int_by_month.pdf).  \n      \n",
    "url": "https://github.com/notnews/good_nyt",
    "last_updated": "2021-03-02T03:36:31+00:00"
  },
  {
    "full_name": "GaNiziolek/FoccoERPy",
    "name": "FoccoERPy",
    "description": "Biblioteca de funções que permitem se conectar à API do FoccoERP",
    "language": "Python",
    "topics": [],
    "readme": "# FoccoERPy\nBiblioteca de funções que permitem se conectar à API do FoccoERP\n",
    "url": "https://github.com/GaNiziolek/FoccoERPy",
    "last_updated": "2025-01-15T15:29:17+00:00"
  },
  {
    "full_name": "audreyfeldroy/cookiecutter-pypackage",
    "name": "cookiecutter-pypackage",
    "description": "Cookiecutter template for a Python package.",
    "language": "Python",
    "topics": [],
    "readme": "# Cookiecutter PyPackage\n\n[![PyPI version](https://img.shields.io/pypi/v/cookiecutter-pypackage.svg)](https://pypi.python.org/pypi/cookiecutter-pypackage)\n[![PyPI downloads](https://img.shields.io/pypi/dm/cookiecutter-pypackage.svg)](https://pypi.python.org/pypi/cookiecutter-pypackage)\n\n[Cookiecutter](https://github.com/cookiecutter/cookiecutter) template for a Python package.\n\n*   GitHub repo: [https://github.com/audreyfeldroy/cookiecutter-pypackage/](https://github.com/audreyfeldroy/cookiecutter-pypackage/)\n*   Free software: MIT license\n*   Discord: [https://discord.gg/PWXJr3upUE](https://discord.gg/PWXJr3upUE)\n\n## Features\n\n*   Testing setup with pytest\n*   GitHub Actions testing: Setup to easily test for Python 3.10, 3.11, 3.12, and 3.13\n*   Auto-release to [PyPI](https://pypi.python.org/pypi) when you push a new tag to master (optional)\n*   Command line interface using Typer\n\n## Quickstart\n\nInstall the latest Cookiecutter if you haven't installed it yet:\n\n```bash\npip install -U cookiecutter\n```\n\nGenerate a Python package project:\n\n```bash\ncookiecutter https://github.com/audreyfeldroy/cookiecutter-pypackage.git\n```\n\nThen:\n\n*   Create a repo and put it there.\n*   [Register](https://packaging.python.org/tutorials/packaging-projects/#uploading-the-distribution-archives) your project with PyPI.\n*   Add the repo to your [Read the Docs](https://readthedocs.io/) account + turn on the Read the Docs service hook.\n*   Release your package by pushing a new tag to master.\n\n## Not Exactly What You Want?\n\nDon't worry, you have options:\n\n### Fork This / Create Your Own\n\nIf you have differences in your preferred setup, I encourage you to fork this\nto create your own version. Or create your own; it doesn't strictly have to\nbe a fork.\n\n### Similar Cookiecutter Templates\n\nExplore other forks to get ideas. See the [network](https://github.com/audreyfeldroy/cookiecutter-pypackage/network) and [family tree](https://github.com/audreyfeldroy/cookiecutter-pypackage/network/members) for",
    "url": "https://github.com/audreyfeldroy/cookiecutter-pypackage",
    "last_updated": "2025-09-02T10:07:27+00:00"
  },
  {
    "full_name": "xuyiqing/gsynth",
    "name": "gsynth",
    "description": "Generalized Synthetic Control Method",
    "language": "R",
    "topics": [],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# gsynth\n\n<!-- badges: start -->\n\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-green.svg)](https://www.tidyverse.org/lifecycle/#stablel)\n[![License:\nMIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n<!-- badges: end -->\n\n**gsynth** implements the [generalized synthetic control\nmethod](https://doi-org.stanford.idm.oclc.org/10.1017/pan.2016.2), which\nimputes counterfactuals for each treated unit using control group\ninformation based on a linear interactive fixed effects model. This\nversion supports unbalanced panels and implements the matrix completion\nmethod.\n\n**Authors:** [Yiqing Xu](https://yiqingxu.org/) (Stanford), [Licheng\nLiu](https://polisci.mit.edu/people/licheng-liu) (MIT)\n\n**Date:** Feb 22, 2022\n\n**Repos:** [Github](https://github.com/xuyiqing/gsynth) (1.2.1)\n[CRAN](https://cran.r-project.org/web/packages/gsynth/index.html)\n(1.2.1)\n\n**Example:** R code used in the\n[tutorial](https://yiqingxu.org/packages/gsynth/articles/tutorial.html)\ncan be downloaded from\n[here](https://yiqingxu.org/packages/gsynth/gsynth_examples.R).\n\n------------------------------------------------------------------------\n\n## Installation\n\nYou can install **gsynth** directly from CRAN by typing the following\ncommand in the **R** console:\n\n``` r\ninstall.packages('gsynth', type = 'source')\n```\n\nYou can also install the development version of the package from Github\nby typing:\n\n``` r\ninstall.packages('devtools', repos = 'http://cran.us.r-project.org') # if not already installed\ndevtools::install_github('xuyiqing/gsynth')\n```\n\n**gsynth** depends on the following packages, which will be installed\nautomatically when **gsynth** is being installed; you can also install\nthem manually:\n\n``` r\n## for processing C++ code\nrequire(Rcpp) \n## for plotting\nrequire(ggplot2)  \nrequire(GGally) \n## for parallel computing \nrequire(foreach)  \nrequire(future)\nrequire(doP",
    "url": "https://github.com/xuyiqing/gsynth",
    "last_updated": "2025-07-22T03:58:30+00:00"
  },
  {
    "full_name": "ditchcarbon/ditchcarbon-python",
    "name": "ditchcarbon-python",
    "description": "Python client for the DitchCarbon API.",
    "language": "Python",
    "topics": [],
    "readme": "<div align=\"center\">\n  <img src=\"https://ditchcarbon.com/wp-content/uploads/2021/05/Group-119.svg\"><br>\n</div>\n\n-----------------\n\n# ditchcarbon-python\n\n![PyPI](https://img.shields.io/pypi/v/ditchcarbon-python?color=5CB381)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/ditchcarbon-python?color=5CB381)\n![PyPI - License](https://img.shields.io/pypi/l/ditchcarbon-python?color=5CB381)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ditchcarbon-python?color=5CB381)\n![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/ditchthis/ditchcarbon-python?color=5CB381)\n[![Code style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n## What is it?\n\n**ditchcarbon-python** is the official Python wrapper for the DitchCarbon API. DitchCarbon calculates the carbon impact of almost anything using a combination of GHG protocol approved calculations and an unparalleled database of company and product disclosures.\n\n## Where to get it?\n\nYou can install the library via PyPI (hosted [here](https://pypi.org/project/ditchcarbon-python)):\n\n```sh\npip3 install ditchcarbon-python\n```\n\nThe source code is currently hosted on GitHub, [here](https://github.com/ditchthis/ditchcarbon-python).\n\n## How to use it?\n\nFirst, import and initialise the library with your API token:\n\n```python\nfrom ditchcarbon_python import Client\nditchcarbon = Client(token=\"YOUR_TOKEN\")\n```\n\nThen, use it:\n\n```python\n# Activities\nditchcarbon.activities.retrieve(1)\nditchcarbon.activities.retrieve_many(**params)\nditchcarbon.activities.retrieve_assessment(1, **params)\nditchcarbon.activities.retrieve_categories(**params)\n\n# Categories\nditchcarbon.categories.search(**params)\n\n# Expenses\nditchcarbon.expenses.calculate_emissions(**params)\n\n# Products\nditchcarbon.products.calculate_emissions(**params)\n\n# Servers\nditchcarbon.servers.find(**params)\nditchcarbon.servers.retrieve(1)\nditchcarbon.servers.calculate_emissions(1, **params)\n\n# Suppliers\nditchcar",
    "url": "https://github.com/ditchcarbon/ditchcarbon-python",
    "last_updated": "2025-01-15T15:28:33+00:00"
  },
  {
    "full_name": "hrbrmstr/top1m-cloudflare",
    "name": "top1m-cloudflare",
    "description": "Some R & cmdline work to check prevalence of CloudFlare usage in Alexa Top 1 Million",
    "language": "",
    "topics": [],
    "readme": "# CloudFlare use in Alexa Top 1m\n\nLet's take a 1% sample of domains in the Alexa top 1m and see how many have CloudFlare IPv4s in their DNS A records.\n\n\n\n\n```r\nlibrary(sys)\nlibrary(httr)\nlibrary(iptools)\nlibrary(stringi)\nlibrary(tidyverse)\n```\n\nWe can get a list of CloudFlare IPV4s here direct from the source:\n\n\n```r\ncf_ranges_url <- \"https://www.cloudflare.com/ips-v4\"\ncf_range <- read_lines(cf_ranges_url)\nis_cloudflare <- function(x) { any(ip_in_any(x, cf_range)) }\n```\n\nAnd, get download a copy of the Alexa Top 1m:\n\n\n```r\ntop_1m_url <- \"http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\"\nfil <- basename(top_1m_url)\nif (!file.exists(fil)) download.file(URL, fil)\ntop_1m_fil <- unzip(fil)\n```\n\nRead in the top 1m and take a 1% sample:\n\n\n```r\ntop_1m <- read_csv(top_1m_fil, col_names = c(\"rank\", \"domain\"),  col_types = \"ic\")\nset.seed(1492)\nsamp_1m <- top_1m[sample(nrow(top_1m), 0.01 * nrow(top_1m)),]\ntf <- tempfile(fileext=\".txt\")\nwrite_lines(samp_1m$domain, tf)\n```\n\nThis uses [slookup](https://github.com/hrbrmstr/slookup) which allows for parallel DNS lookups. There are better utilities to use for this but I had it handy and am not doing too many lookups (bulk lookups should use something like [MassDNS](https://github.com/blechschmidt/massdns) and a fairly large list of resolvers).\n\nWe'll also cache the result to avoid doing this resource-intensive task again:\n\n\n```r\nslookup_fil <- \"slookup_output.rds\"\nif (!file.exists(slookup_fil)) {\n  res <- exec_internal(\"slookup\", c(\"-f\", \"4\", \"-p\" , \"-t\", \"A\", \"-i\", tf))\n  write_rds(res, slookup_fil)\n} else {\n  res <- read_rds(slookup_fil)\n}\n```\n\nThe output of `slookup` is uniform but ugly, so we clean it up, make it a data frame and test for CloudFlare range inclusion:\n\n\n```r\nrawToChar(res$stdout) %>%\n  stri_split_lines() %>%\n  flatten_chr() %>%\n  discard(stri_detect_fixed, \"not found\") %>%\n  stri_replace_all_fixed(\" +\", \"\") %>%\n  stri_replace_all_fixed(\" A \", \" \") %>%\n  stri_split_fixed(\" \", 2, simplify = TRUE) %>%\n  as_data_fra",
    "url": "https://github.com/hrbrmstr/top1m-cloudflare",
    "last_updated": "2025-03-22T11:18:22+00:00"
  },
  {
    "full_name": "hrbrmstr/longurl",
    "name": "longurl",
    "description": ":information_source: Small R package for no-API-required URL expansion",
    "language": "R",
    "topics": [
      "r",
      "rstats",
      "url-shortener",
      "url",
      "r-cyber"
    ],
    "readme": "\n[![Project Status: Active – The project has reached a stable, usable\nstate and is being actively\ndeveloped.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![Signed\nby](https://img.shields.io/badge/Keybase-Verified-brightgreen.svg)](https://keybase.io/hrbrmstr)\n![Signed commit\n%](https://img.shields.io/badge/Signed_Commits-100%25-lightgrey.svg)\n[![Linux build\nStatus](https://travis-ci.org/hrbrmstr/longurl.svg?branch=master)](https://travis-ci.org/hrbrmstr/longurl)\n[![Coverage\nStatus](https://codecov.io/gh/hrbrmstr/longurl/branch/master/graph/badge.svg)](https://codecov.io/gh/hrbrmstr/longurl)\n[![cran\nchecks](https://cranchecks.info/badges/worst/longurl)](https://cranchecks.info/pkgs/longurl)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/longurl)](https://www.r-pkg.org/pkg/longurl)\n![Minimal R\nVersion](https://img.shields.io/badge/R%3E%3D-3.6.0-blue.svg)\n![License](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# longurl\n\nExpand Short ‘URLs’\n\n## Description\n\nTools are provided to expand vectors of short URLs into long ‘URLs’. No\n‘API’ services are used, which may mean that this operates more slowly\nthan ‘API’ services do (since they usually cache results of expansions\nthat every user of the service requests). You can setup your own caching\nlayer with the ‘memoise’ package if you wish to have a speedup during\nsingle sessions or add larger dependencies, such as ‘Redis’, to gain a\nlonger-term performance boost at the expense of added complexity.\n\n## What’s Inside The Tin\n\nThe following functions are implemented:\n\n  - `expand_urls`: Expand a vector of (short) URLs using\n\n## Installation\n\n``` r\ninstall.packages(\"longurl\", repos = c(\"https://cinc.rud.is\", \"https://cloud.r-project.org/\"))\n# or\nremotes::install_git(\"https://git.rud.is/hrbrmstr/longurl.git\")\n# or\nremotes::install_git(\"https://git.sr.ht/~hrbrmstr/longurl\")\n# or\nremotes::install_gitlab(\"hrbrmstr/longurl\")\n# or\nremotes::install_github(\"hrbrmstr/longurl\")\n`",
    "url": "https://github.com/hrbrmstr/longurl",
    "last_updated": "2025-03-22T11:19:55+00:00"
  },
  {
    "full_name": "tomnomnom/gron",
    "name": "gron",
    "description": "Make JSON greppable!",
    "language": "Go",
    "topics": [
      "cli",
      "json"
    ],
    "readme": "# gron\n[![Build Status](https://travis-ci.org/tomnomnom/gron.svg?branch=master)](https://travis-ci.org/tomnomnom/gron)\n\nMake JSON greppable!\n\ngron transforms JSON into discrete assignments to make it easier to `grep` for what you want and see the absolute 'path' to it.\nIt eases the exploration of APIs that return large blobs of JSON but have terrible documentation.\n\n<pre>\n▶ <b>gron</b> \"https://api.github.com/repos/tomnomnom/gron/commits?per_page=1\" | fgrep \"commit.author\"\njson[0].commit.author = {};\njson[0].commit.author.date = \"2016-07-02T10:51:21Z\";\njson[0].commit.author.email = \"mail@tomnomnom.com\";\njson[0].commit.author.name = \"Tom Hudson\";\n</pre>\n\ngron can work backwards too, enabling you to turn your filtered data back into JSON:\n<pre>\n▶ gron \"https://api.github.com/repos/tomnomnom/gron/commits?per_page=1\" | fgrep \"commit.author\" | <b>gron --ungron</b>\n[\n  {\n    \"commit\": {\n      \"author\": {\n        \"date\": \"2016-07-02T10:51:21Z\",\n        \"email\": \"mail@tomnomnom.com\",\n        \"name\": \"Tom Hudson\"\n      }\n    }\n  }\n]\n</pre>\n\n> Disclaimer: the GitHub API has fantastic documentation, but it makes for a good example.\n\n## Installation\n\ngron has no runtime dependencies. You can just [download a binary for Linux, Mac, Windows or FreeBSD and run it](https://github.com/tomnomnom/gron/releases).\nPut the binary in your `$PATH` (e.g. in `/usr/local/bin`) to make it easy to use:\n```\n▶ tar xzf gron-linux-amd64-0.1.5.tgz\n▶ sudo mv gron /usr/local/bin/\n```\n\nIf you're a Mac user you can also [install gron via brew](http://braumeister.org/formula/gron):\n```\n▶ brew install gron\n```\n\nOr if you're a Go user you can use `go install`:\n\n```\n▶ go install github.com/tomnomnom/gron@latest\n```\n\nIt's recommended that you alias `ungron` or `norg` (or both!) to `gron --ungron`. Put something like this in your shell profile (e.g. in `~/.bashrc`):\n```\nalias norg=\"gron --ungron\"\nalias ungron=\"gron --ungron\"\n```\nOr you could create a shell script in your $PATH named `ungron` or `norg` to affe",
    "url": "https://github.com/tomnomnom/gron",
    "last_updated": "2025-09-01T22:59:50+00:00"
  },
  {
    "full_name": "robinsones/NYTimes-and-Trump",
    "name": "NYTimes-and-Trump",
    "description": "Topic modeling of the NYTimes news coverage of Trump since he announced his campaign",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# New York Times' Coverage of Trump\n\nMy project looks at the New York Times' coverage of Trump from when he announced his candidacy to August 2016 (the date of the project). I gathered the approximately 2,200 news articles from the Times that focused on Donald Trump and used topic modeling to map how the topics of the articles have evolved over the past year. I also wrote a [blog post](https://hookedondata.org/topic-modeling-the-new-york-times-and-trump/) documenting my analysis process, topic modeling, and findings. \n",
    "url": "https://github.com/robinsones/NYTimes-and-Trump",
    "last_updated": "2025-02-24T19:58:32+00:00"
  },
  {
    "full_name": "vnijs/quizr",
    "name": "quizr",
    "description": "Create interactive quizzes using Shiny and Rmarkdown",
    "language": "R",
    "topics": [],
    "readme": "# Quizr\n\nCreate interactive quizzes using Shiny and Knitr. See example linked below.\n\nhttps://vnijs.shinyapps.io/quizr/\n",
    "url": "https://github.com/vnijs/quizr",
    "last_updated": "2024-08-25T15:02:55+00:00"
  },
  {
    "full_name": "dgrtwo/fuzzyjoin",
    "name": "fuzzyjoin",
    "description": "Join tables together on inexact matching",
    "language": "R",
    "topics": [],
    "readme": "<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\nfuzzyjoin: Join data frames on inexact matching\n------------------\n\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/fuzzyjoin)](https://cran.r-project.org/package=fuzzyjoin)\n[![Travis-CI Build Status](https://travis-ci.org/dgrtwo/fuzzyjoin.svg?branch=master)](https://travis-ci.org/dgrtwo/fuzzyjoin)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/dgrtwo/fuzzyjoin?branch=master&svg=true)](https://ci.appveyor.com/project/dgrtwo/fuzzyjoin)\n[![Coverage Status](https://img.shields.io/codecov/c/github/dgrtwo/fuzzyjoin/master.svg)](https://codecov.io/github/dgrtwo/fuzzyjoin?branch=master)\n\n\nThe fuzzyjoin package is a variation on dplyr's join operations that allows matching not just on values that match between columns, but on inexact matching. This allows matching on:\n\n* Numeric values that are within some tolerance (`difference_inner_join`)\n* Strings that are similar in Levenshtein/cosine/Jaccard distance, or [other metrics](http://finzi.psych.upenn.edu/library/stringdist/html/stringdist-metrics.html) from the [stringdist](https://cran.r-project.org/package=stringdist) package (`stringdist_inner_join`)\n* A regular expression in one column matching to another (`regex_inner_join`)\n* Euclidean or Manhattan distance across multiple columns (`distance_inner_join`)\n* Geographic distance based on longitude and latitude (`geo_inner_join`)\n* Intervals of (start, end) that overlap (`interval_inner_join`)\n* Genomic intervals, which include both a chromosome ID and (start, end) pairs, that overlap (`genome_inner_join`)\n\nOne relevant use case is for classifying freeform text data (such as survey responses) against a finite set of options.\n\nThe package also includes:\n\n* For each of `regex_`, `stringdist_`, `difference_`, `distance_`, `geo_`, and `interval_`, variations for the six dplyr \"join\" operations- for example,\n  * `regex_inner_join` (include only rows with matches i",
    "url": "https://github.com/dgrtwo/fuzzyjoin",
    "last_updated": "2025-08-20T23:47:02+00:00"
  },
  {
    "full_name": "jrnold/masteringmetrics",
    "name": "masteringmetrics",
    "description": "R code for Angrist & Pischke Mastering Metrics",
    "language": "R",
    "topics": [
      "econometrics",
      "r"
    ],
    "readme": "# R Code for Mastering 'Metrics (Angrist and Pischke)\n\nThis repository R code and text for [R Code for Mastering 'Metrics](https://jrnold.github.io/masteringmetrics/), which contains the R code to reproduce the analyses in *Mastering 'Metrics* by Joshua D. Angrist and Jörn-Steffen Pischke.\n\n\nThe site is built using the [bookdown](https://bookdown.org/yihui/bookdown/) package.\n\n## Build\n\nTo render the book, run the following in R,\n``` r\nrender_book(\"index.Rmd\")\n```\n\nTo lint R code run\n``` console\nRscript _lint.R\n```\n\nTo check spelling run\n``` console\nRcript _spelling.R\n```\n\nTo lint markdown, install remark and various plugins for it.\n``` console\n$ npm install remark-preset-lint-recommended remark-preset-lint-consistent remark-preset-lint-markdown-style-guide remark-frontmatter\n```\n``` console\n$ remark *.Rmd\n```\n",
    "url": "https://github.com/jrnold/masteringmetrics",
    "last_updated": "2025-04-05T14:20:42+00:00"
  },
  {
    "full_name": "samsledje/ConPLex",
    "name": "ConPLex",
    "description": "Adapting protein language models and contrastive learning for highly-accurate drug-target interaction prediction.",
    "language": "Python",
    "topics": [],
    "readme": "# ConPLex\n\n![ConPLex Schematic](assets/images/Fig2_Schematic.png)\n\n[![ConPLex Releases](https://img.shields.io/github/v/release/samsledje/ConPLex?include_prereleases)](https://github.com/samsledje/ConPLex/releases)\n[![PyPI](https://img.shields.io/pypi/v/conplex-dti)](https://pypi.org/project/conplex-dti/)\n[![Build](https://github.com/samsledje/ConPLex/actions/workflows/build.yml/badge.svg)](https://github.com/samsledje/ConPLex/actions/workflows/build.yml)\n[![Documentation Status](https://readthedocs.org/projects/conplex/badge/?version=latest)](https://conplex.readthedocs.io/en/main/?badge=main)\n[![License](https://img.shields.io/github/license/samsledje/ConPLex)](https://github.com/samsledje/ConPLex/blob/main/LICENSE)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n - [Homepage](http://conplex.csail.mit.edu)\n - [Documentation](https://conplex.readthedocs.io/en/latest/)\n\n## Abstract\n\nSequence-based prediction of drug-target interactions has the potential to accelerate drug discovery by complementing experimental screens. Such computational prediction needs to be generalizable and scalable while remaining sensitive to subtle variations in the inputs. However, current computational techniques fail to simultaneously meet these goals, often sacrificing performance on one to achieve the others. We develop a deep learning model, ConPLex, successfully leveraging the advances in pre-trained protein language models (\"PLex\") and employing  a novel  protein-anchored contrastive co-embedding (\"Con\") to outperform state-of-the-art approaches. ConPLex achieves high accuracy, broad adaptivity to unseen data, and specificity against decoy compounds. It makes predictions of binding based on the distance between learned representations, enabling predictions at the scale of massive compound libraries and the human proteome. Experimental testing of 19 kinase-drug interaction predictions validated 12 interactions, includin",
    "url": "https://github.com/samsledje/ConPLex",
    "last_updated": "2025-08-21T01:07:40+00:00"
  },
  {
    "full_name": "paultopia/techuplaw",
    "name": "techuplaw",
    "description": "tech and math tutorials for lawyers",
    "language": "HTML",
    "topics": [],
    "readme": "This is a tutorial website for practicing lawyers, to explain technology and math skills that the modern practitioner needs.\n\nThe site can be found at [https://techup.lawyer/](https://techup.lawyer/).\n\nAll textual content, images, document templates, and other content that isn't primarily intended to be executable software on this site is licensed under a Creative Commons Attribution 4.0 International License. All code and other content primarily intended to be executable software is licensed under a MIT License.\n\nContent contributions welcome!  Please see [the how to contribute page on the live site](https://techup.lawyer/pages/license-and-how-to-contribute.html) for details. There you will also find license information for other elements of this site.\n\nRunning locally: clone the repo, get yourself a Python 3.7 installation; install the libraries in requirements.txt however you may see fit, and run `make devserver` to build and to get a hot-updating server on localhost:8000. \n",
    "url": "https://github.com/paultopia/techuplaw",
    "last_updated": "2022-12-28T01:32:54+00:00"
  },
  {
    "full_name": "chaturap/bmkg-latest-earthquake",
    "name": "bmkg-latest-earthquake",
    "description": "Package to get information latest earth quake on Indonesia, data source from https://www.bmkg.go.id/",
    "language": "Python",
    "topics": [],
    "readme": "# bmkg-latest-earthquake\nThis package will get the latest earthquake (Indonesia) from https://www.bmkg.go.id/  BMKG | Meteorological, Climatological, and Geophysical Agency\n\nexample using this module\n\n\n#    import gempaterkini_chatur\n#    if __name__ == '__main__':\n#    print(\"Aplikasi Utama\")\n#    result = gempaterkini_chatur.ekstrasi_data()\n#    gempaterkini_chatur.tampilkan_data(result)",
    "url": "https://github.com/chaturap/bmkg-latest-earthquake",
    "last_updated": "2025-01-15T15:29:36+00:00"
  },
  {
    "full_name": "OCA/stock-logistics-workflow",
    "name": "stock-logistics-workflow",
    "description": "Odoo Stock, Workflow and Organization",
    "language": "Python",
    "topics": [
      "erp",
      "hacktoberfest",
      "odoo",
      "python"
    ],
    "readme": "\n[![Runboat](https://img.shields.io/badge/runboat-Try%20me-875A7B.png)](https://runboat.odoo-community.org/builds?repo=OCA/stock-logistics-workflow&target_branch=18.0)\n[![Pre-commit Status](https://github.com/OCA/stock-logistics-workflow/actions/workflows/pre-commit.yml/badge.svg?branch=18.0)](https://github.com/OCA/stock-logistics-workflow/actions/workflows/pre-commit.yml?query=branch%3A18.0)\n[![Build Status](https://github.com/OCA/stock-logistics-workflow/actions/workflows/test.yml/badge.svg?branch=18.0)](https://github.com/OCA/stock-logistics-workflow/actions/workflows/test.yml?query=branch%3A18.0)\n[![codecov](https://codecov.io/gh/OCA/stock-logistics-workflow/branch/18.0/graph/badge.svg)](https://codecov.io/gh/OCA/stock-logistics-workflow)\n[![Translation Status](https://translation.odoo-community.org/widgets/stock-logistics-workflow-18-0/-/svg-badge.svg)](https://translation.odoo-community.org/engage/stock-logistics-workflow-18-0/?utm_source=widget)\n\n<!-- /!\\ do not modify above this line -->\n\n# Stock Workflow\n\nEnhance the way flows and processes are working. Find here modules that do not have their place in the other more specialized repositories.\n\nAre you looking for modules related to logistics? Or would like to contribute\nto? There are many repositories with specific purposes. Have a look at this\n[README](https://github.com/OCA/wms/blob/18.0/README.md).\n\n<!-- /!\\ do not modify below this line -->\n\n<!-- prettier-ignore-start -->\n\n[//]: # (addons)\n\nAvailable addons\n----------------\naddon | version | maintainers | summary\n--- | --- | --- | ---\n[delivery_procurement_group_carrier](delivery_procurement_group_carrier/) | 18.0.1.0.2 |  | Delivery Procurement Group Carrier\n[delivery_total_weight_from_packaging](delivery_total_weight_from_packaging/) | 18.0.1.1.0 |  | Include packaging weight on move, transfer and package.\n[procurement_auto_create_group_carrier](procurement_auto_create_group_carrier/) | 18.0.1.0.0 |  | Procurement Auto Create Group Carrier\n[product_c",
    "url": "https://github.com/OCA/stock-logistics-workflow",
    "last_updated": "2025-08-28T12:27:10+00:00"
  },
  {
    "full_name": "googledatalab/datalab",
    "name": "datalab",
    "description": "Interactive tools and developer experiences for Big Data on Google Cloud Platform.",
    "language": "TypeScript",
    "topics": [],
    "readme": "# Google Cloud DataLab\n\nDatalab is deprecated. [Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench) provides a notebook-based environment that offers capabilities beyond Datalab. We recommend that you use Vertex AI Workbench for new projects and [migrate your Datalab notebooks to Vertex AI Workbench](https://cloud.google.com/datalab/docs/resources/troubleshooting#migrate). For more information, see [Deprecation information](https://cloud.google.com/datalab/docs/resources/deprecation). To get help migrating Datalab projects to Vertex AI Workbench see [Get help](https://cloud.google.com/datalab/docs/resources/support#get-help).\n",
    "url": "https://github.com/googledatalab/datalab",
    "last_updated": "2025-08-13T07:08:48+00:00"
  },
  {
    "full_name": "algolia/docsearch",
    "name": "docsearch",
    "description": ":blue_book: The easiest way to add search to your documentation.",
    "language": "MDX",
    "topics": [
      "algolia",
      "search",
      "docsearch",
      "documentation",
      "javascript",
      "react",
      "typescript"
    ],
    "readme": "<div align=\"center\">\n\n[![DocSearch](.github/logo.svg)](https://docsearch.algolia.com)\n\nThe easiest way to add search to your documentation – for free.\n\n[![Netlify Status](https://api.netlify.com/api/v1/badges/30eacc09-d4b2-4a53-879b-04d40aaea454/deploy-status)](https://app.netlify.com/sites/docsearch/deploys) [![npm version](https://img.shields.io/npm/v/@docsearch/js.svg?style=flat-square)](https://www.npmjs.com/package/@docsearch/js/v/alpha) [![License](https://img.shields.io/badge/license-MIT-green.svg?style=flat-square)](./LICENSE)\n\n<p align=\"center\">\n  <strong>\n  <a href=\"https://docsearch.algolia.com\">Documentation</a> •\n  <a href=\"https://codesandbox.io/s/docsearchjs-v3-playground-z9oxj\">JavaScript Playground</a> •\n  <a href=\"https://codesandbox.io/s/docsearch-react-v3-playground-619yg\">React Playground</a>\n  </strong>\n</p>\n\n</div>\n\n---\n\nDocSearch crawls your documentation, pushes the content to an Algolia index and provides a dropdown search experience on your website.\n\n## Preview\n\n![Screencast](.github/screencast.gif)\n\n| Light | Dark |\n| --- | --- |\n| ![Light preview](.github/preview-light.png) | ![Dark preview](.github/preview-dark.png) |\n\n## Usage\n\n> Don't have your Algolia credentials yet? [Apply to DocSearch](https://docsearch.algolia.com/apply)!\n\n### JavaScript\n\n#### Installation\n\n```sh\nyarn add @docsearch/js@beta\n# or\nnpm install @docsearch/js@beta\n```\n\nIf you don’t want to use a package manager, you can use a standalone endpoint:\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/@docsearch/js@beta\"></script>\n```\n\n#### Get started\n\nTo get started, you need a [`container`](https://docsearch.algolia.com/docs/api#container) for your DocSearch component to go in. If you don’t have one already, you can insert one into your markup:\n\n```html\n<div id=\"docsearch\"></div>\n```\n\nThen, insert DocSearch into it by calling the [`docsearch`](https://docsearch.algolia.com/docs/api) function and providing the container. It can be a [CSS selector](https://developer.mozil",
    "url": "https://github.com/algolia/docsearch",
    "last_updated": "2025-08-31T11:45:22+00:00"
  },
  {
    "full_name": "johnmchambers/shakespeare",
    "name": "shakespeare",
    "description": "Python interface tools as examples for \"Extending R\"",
    "language": "R",
    "topics": [],
    "readme": "# shakespeare Some Data Analysis of the Plays (& an example of interfacing)\n\nThis package provides some tools for asking text-based questions about\nShakespeare's plays, mainly directed at the lists of speeches.\n\nThe analysis is based on interfaces from R to XML (for the text of the\nplays) and to Python (mainly for text search and use of the Natrual\nLanguage Toolkit).\n\nThe package started as an example of the use of interfaces from R to\nother languages, in the book *Extending R* (John M. Chambers, 2016,\nChapman & Hall).\n\nThe XML coding of the plays is used in Python to fill objects that\nrepresent structural pieces, such as acts, scenes and especially\nspeeches. The objects from these classes retain information about\ntheir origin in the plays.  The R code in the package has proxies for\nthese classes and for Python functions.\n\nUse from R is mainly via searches over lists of speeches from some or\n(by default) all of the plays.  Searches may be applied to the lines of text of the\nspeech or tokenized versions of these produced by the NLTK.\n",
    "url": "https://github.com/johnmchambers/shakespeare",
    "last_updated": "2019-02-27T07:18:51+00:00"
  },
  {
    "full_name": "CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers",
    "name": "Probabilistic-Programming-and-Bayesian-Methods-for-Hackers",
    "description": "aka \"Bayesian Methods for Hackers\": An introduction to Bayesian methods + probabilistic programming with a computation/understanding-first, mathematics-second point of view. All in pure Python ;)  ",
    "language": "Jupyter Notebook",
    "topics": [
      "bayesian-methods",
      "pymc",
      "mathematical-analysis",
      "jupyter-notebook",
      "data-science",
      "statistics"
    ],
    "readme": "# [Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n#### *Using Python and PyMC*\n\n\nThe Bayesian method is the natural approach to inference, yet it is hidden from readers behind chapters of slow, mathematical analysis. The typical text on Bayesian inference involves two to three chapters on probability theory, then enters what Bayesian inference is. Unfortunately, due to mathematical intractability of most Bayesian models, the reader is only shown simple, artificial examples. This can leave the user with a *so-what* feeling about Bayesian inference. In fact, this was the author's own prior opinion.\n\nAfter some recent success of Bayesian methods in machine-learning competitions, I decided to investigate the subject again. Even with my mathematical background, it took me three straight-days of reading examples and trying to put the pieces together to understand the methods. There was simply not enough literature bridging theory to practice. The problem with my misunderstanding was the disconnect between Bayesian mathematics and probabilistic programming. That being said, I suffered then so the reader would not have to now. This book attempts to bridge the gap.\n\nIf Bayesian inference is the destination, then mathematical analysis is a particular path towards it. On the other hand, computing power is cheap enough that we can afford to take an alternate route via probabilistic programming. The latter path is much more useful, as it denies the necessity of mathematical intervention at each step, that is, we remove often-intractable mathematical analysis as a prerequisite to Bayesian inference. Simply put, this latter computational path proceeds via small intermediate jumps from beginning to end, where as the first path proceeds by enormous leaps, often landing far away from our target. Furthermore, without a strong mathematical background, the analysis required by the first path cannot even take p",
    "url": "https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers",
    "last_updated": "2025-09-02T10:20:36+00:00"
  },
  {
    "full_name": "mlr-org/mlr",
    "name": "mlr",
    "description": "Machine Learning in R ",
    "language": "R",
    "topics": [
      "machine-learning",
      "data-science",
      "tuning",
      "cran",
      "r-package",
      "predictive-modeling",
      "classification",
      "regression",
      "statistics",
      "r",
      "survival-analysis",
      "imbalance-correction",
      "tutorial",
      "mlr",
      "learners",
      "hyperparameters-optimization",
      "feature-selection",
      "multilabel-classification",
      "clustering",
      "stacking"
    ],
    "readme": " mlr <img src=\"man/figures/logo.png\" align=\"right\" />\n\nPackage website: [release](https://mlr.mlr-org.com/) | [dev](https://mlr.mlr-org.com/dev/)\n\nMachine learning in R.\n\n<!-- badges: start -->\n\n[![tic](https://github.com/mlr-org/mlr/workflows/tic/badge.svg?branch=main)](https://github.com/mlr-org/mlr/actions)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version-ago/mlr)](https://cran.r-project.org/package=mlr)\n[![cran checks](https://badges.cranchecks.info/worst/mlr.svg)](https://cran.r-project.org/web/checks/check_results_mlr.html)\n[![CRAN Downloads](https://cranlogs.r-pkg.org/badges/mlr)](https://cran.r-project.org/package=mlr)\n[![StackOverflow](https://img.shields.io/badge/stackoverflow-mlr-blue.svg)](https://stackoverflow.com/questions/tagged/mlr)\n[![lifecycle](https://img.shields.io/badge/lifecycle-retired-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![codecov](https://codecov.io/gh/mlr-org/mlr/branch/main/graph/badge.svg)](https://app.codecov.io/gh/mlr-org/mlr)\n\n<!-- badges: end -->\n\n- [CRAN release site](https://CRAN.R-project.org/package=mlr)\n- [Online tutorial](https://mlr.mlr-org.com/index.html)\n- [Changelog](https://mlr.mlr-org.com/news/index.html)\n\n- [Stackoverflow](https://stackoverflow.com/questions/tagged/mlr): `#mlr`\n- [Mattermost](https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/)\n- [Blog](https://mlr-org.com/)\n\n## Deprecated\n\n{mlr} is considered retired from the mlr-org team.\nWe won't add new features anymore and will only fix _severe_ bugs.\nWe suggest to use the new [mlr3](https://mlr3.mlr-org.com/) framework from now on and for future projects.\n\nNot all features of {mlr} are already implemented in {mlr3}.\nIf you are missing a crucial feature, please open an issue in the respective [mlr3 extension package](https://github.com/mlr-org/mlr3/wiki/Extension-Packages) and do not hesitate to follow-up on it.\n\n## Installation\n\n**Release**\n\n```r\ninstall.packages(\"mlr\")\n```\n\n**Development**\n\n```R\nremotes::install_github(\"mlr-or",
    "url": "https://github.com/mlr-org/mlr",
    "last_updated": "2025-09-01T12:34:24+00:00"
  },
  {
    "full_name": "bmschmidt/wordVectors",
    "name": "wordVectors",
    "description": "An R package for creating and exploring word2vec and other word embedding models",
    "language": "R",
    "topics": [],
    "readme": "# Word Vectors\n\n[![Build Status](https://travis-ci.org/bmschmidt/wordVectors.svg?branch=master)](https://travis-ci.org/bmschmidt/wordVectors)\n\nAn R package for building and exploring word embedding models.\n\n# Description\n\nThis package does three major things to make it easier to work with word2vec and other vectorspace models of language.\n\n1. [Trains word2vec models](#creating-text-vectors) using an extended Jian Li's word2vec code; reads and writes the binary word2vec format so that you can import pre-trained models such as Google's; and provides tools for reading only *part* of a model (rows or columns) so you can explore a model in memory-limited situations.\n2. [Creates a new `VectorSpaceModel` class in R that gives a better syntax for exploring a word2vec or GloVe model than native matrix methods.](#vectorspacemodel-object) For example, instead of writing \n   \n   > `model[rownames(model)==\"king\",]`,\n   \n   you can write  \n   \n   > `model[[\"king\"]]`, \n   \n   and instead of writing \n   \n   > `vectors %>% closest_to(vectors[rownames(vectors)==\"king\",] - vectors[rownames(vectors)==\"man\",] + vectors[rownames(vectors)==\"woman\",])` (whew!),\n   \n   you can write\n   \n   > `vectors %>% closest_to(~\"king\" - \"man\" + \"woman\")`.\n   \n3. [Implements several basic matrix operations that are useful in exploring word embedding models including cosine similarity, nearest neighbor, and vector projection](#useful-matrix-operations) with some caching that makes them much faster than the simplest implementations.\n\n### Quick start\n\nFor a step-by-step interactive demo that includes installation and training a model on 77 historical cookbooks from Michigan State University, [see the introductory vignette.](https://github.com/bmschmidt/wordVectors/blob/master/vignettes/introduction.Rmd).\n\n### Credit\n\nThis includes an altered version of Tomas Mikolov's original C code for word2vec; those wrappers were origally written by Jian Li, and I've only tweaked them a little. Several other users have",
    "url": "https://github.com/bmschmidt/wordVectors",
    "last_updated": "2025-08-21T22:00:17+00:00"
  },
  {
    "full_name": "ai-excelsior/F2AI",
    "name": "F2AI",
    "description": "",
    "language": "HTML",
    "topics": [],
    "readme": "F2AI\n====\n\n.. image:: https://readthedocs.org/projects/f2ai/badge/?version=latest\n    :target: https://f2ai.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n\nThe architecture above is a working flow demonstration when using F2AI but not technical architecture.\n\nGetting Started\n---------------\n\n1. Install F2AI\n\n\nOverview\n-------------\n\nF2AI (Feature Store to AI) is a time centric productivity data utils that re-uses existing infrastructure to get features more consistently though different stages of AI development.\n\nF2AI is focusing on:\n\n* **Consistent API to get features for training and serving**: Powered by well encapsulated OfflineStore and OnlineStore, the features are strictly managed by F2AI, and keep the same structure not only when training models, but also inference.\n* **Prevent feature leakage** by effective point-in-time join that reduce the cumbersome works to get features ready when experimenting a feature combinations or AI model.\n* **Build-in supported period feature** that allows 2 dimensional features can be retrieved easily. This is useful when facing some deep learning tasks like, time series forecasting.\n* **Infrastructure unawareness** by different infrastructure implementations, switching between different data storage is simply changing the configuration. The AI model will works well like before.\n\nArchitecture\n------------\n\n.. image:: ./docs/static/f2ai_architecture.png\n    :alt: f2ai function architecture\n\n.. note::\n   This is a  working flow when using F2AI instead of technical architecture.\n",
    "url": "https://github.com/ai-excelsior/F2AI",
    "last_updated": "2025-01-15T15:28:42+00:00"
  },
  {
    "full_name": "zlwaterfield/scramble",
    "name": "scramble",
    "description": "Open-Source Grammarly Alternative",
    "language": "JavaScript",
    "topics": [],
    "readme": "# Scramble - Open-Source Grammarly Alternative\n\n> Note: this project has lots of users but I'm not actively developing it so if you'd like to be a maintainer on this project let me know!\n\nScramble is an open-source Chrome extension that leverages AI to enhance your writing directly in your browser. It's designed to be a more customizable alternative to Grammarly by using specific prompts and allowing you to configure the LLM provider, model, and endpoint.\n\n## Extensions\n\n- Chrome: https://chromewebstore.google.com/detail/scramble/mkaljgnigabhmjfookbokejhfghmkffo\n- Firefox: coming soon\n\n## Installation\n\n#### Chrome Installation\n\n- Clone this repository\n- Run `npm install`\n- Run `npm run build`\n- Open Chrome and go to chrome://extensions/\n- Enable \"Developer mode\" in the top right\n- Click \"Load unpacked\" and select the extension directory (dist/chrome)\n\n#### Firefox Installation\n\n- Clone this repository\n- Run `npm install`\n- Run `npm run build`\n- Open Firefox and go to about:debugging#/runtime/this-firefox\n- Click \"Load Temporary Add-on\"\n- Navigate to the extension directory (dist/firefox) and select manifest.json\n\n## Development\n\nWhen developing you'll need to run `npx tailwindcss -i src/libs/tw-input.css -o src/libs/tw-output.css --minify --watch` in order to build the css on the fly. The runs automatically when you run `npm run build`.\n\n## Usage\n\n1. Highlight text on any webpage\n2. Right-click to open the context menu\n3. Select \"Scramble\" and choose a text enhancement option\n4. Wait for the AI to process and enhance your text\n\nScreenshot:\n\n<img width=\"600\" alt=\"Screenshot 2024-09-17 at 10 14 30 PM\" src=\"https://github.com/user-attachments/assets/7a8685e5-94dd-47be-a141-f84bcbf1321f\">\n\n## Supported LLMs\n\n- OpenAI\n- Anthropic\n- Groq\n- OpenRouter\n- Ollama\n- LM Studio\n\n## Default Prompts\n\nScramble comes with several pre-configured text enhancement options:\n\n1. Fix spelling and grammar\n2. Improve writing\n3. Make more professional\n4. Simplify text\n5. Summarize text\n6. Ex",
    "url": "https://github.com/zlwaterfield/scramble",
    "last_updated": "2025-08-31T22:34:31+00:00"
  },
  {
    "full_name": "crewAIInc/crewAI",
    "name": "crewAI",
    "description": "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.",
    "language": "Python",
    "topics": [
      "agents",
      "ai",
      "ai-agents",
      "llms",
      "aiagentframework"
    ],
    "readme": "<p align=\"center\">\n  <a href=\"https://github.com/crewAIInc/crewAI\">\n    <img src=\"docs/images/crewai_logo.png\" width=\"600px\" alt=\"Open source Multi-AI Agent orchestration framework\">\n  </a>\n</p>\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/11239\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/11239\" alt=\"crewAIInc%2FcrewAI | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://crewai.com\">Homepage</a>\n  ·\n  <a href=\"https://docs.crewai.com\">Docs</a>\n  ·\n  <a href=\"https://app.crewai.com\">Start Cloud Trial</a>\n  ·\n  <a href=\"https://blog.crewai.com\">Blog</a>\n  ·\n  <a href=\"https://community.crewai.com\">Forum</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/crewAIInc/crewAI\">\n    <img src=\"https://img.shields.io/github/stars/crewAIInc/crewAI\" alt=\"GitHub Repo stars\">\n  </a>\n  <a href=\"https://github.com/crewAIInc/crewAI/network/members\">\n    <img src=\"https://img.shields.io/github/forks/crewAIInc/crewAI\" alt=\"GitHub forks\">\n  </a>\n  <a href=\"https://github.com/crewAIInc/crewAI/issues\">\n    <img src=\"https://img.shields.io/github/issues/crewAIInc/crewAI\" alt=\"GitHub issues\">\n  </a>\n  <a href=\"https://github.com/crewAIInc/crewAI/pulls\">\n    <img src=\"https://img.shields.io/github/issues-pr/crewAIInc/crewAI\" alt=\"GitHub pull requests\">\n  </a>\n  <a href=\"https://opensource.org/licenses/MIT\">\n    <img src=\"https://img.shields.io/badge/License-MIT-green.svg\" alt=\"License: MIT\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/crewai/\">\n    <img src=\"https://img.shields.io/pypi/v/crewai\" alt=\"PyPI version\">\n  </a>\n  <a href=\"https://pypi.org/project/crewai/\">\n    <img src=\"https://img.shields.io/pypi/dm/crewai\" alt=\"PyPI downloads\">\n  </a>\n  <a href=\"https://twitter.com/crewAIInc\">\n    <img src=\"https://img.shields.io/twitter/follow/crewAIInc?style=",
    "url": "https://github.com/crewAIInc/crewAI",
    "last_updated": "2025-09-02T10:00:41+00:00"
  },
  {
    "full_name": "yuanjie-ai/ChatLLM",
    "name": "ChatLLM",
    "description": "轻松玩转LLM兼容openai&langchain，支持文心一言、讯飞星火、腾讯混元、智谱ChatGLM等",
    "language": "Jupyter Notebook",
    "topics": [
      "chatgpt",
      "llm",
      "gpt4",
      "langchain",
      "lora",
      "p-tuning",
      "chatllm",
      "chatpdf",
      "chatdoc",
      "chatbase",
      "chatweb",
      "chatkb",
      "chatkg",
      "chatsearch",
      "chatdb",
      "chatsql"
    ],
    "readme": "![image](https://img.shields.io/pypi/v/chatllm.svg) ![image](https://img.shields.io/travis/yuanjie-ai/chatllm.svg) ![image](https://readthedocs.org/projects/chatllm/badge/?version=latest)\n\n<h1 align = \"center\">🔥ChatLLM 基于知识库🔥</h1>\n\n> star之后可联系wx: 313303303领取1000w tokens（国产模型）\n\n> star之后可联系wx: 313303303领取1000w tokens（国产模型）\n\n> star之后可联系wx: 313303303领取1000w tokens（国产模型）\n\n<div align=center>\n<img src=\"data/imgs/LLM.drawio.png\"/>\n</div>\n\n\n![image](https://github.com/yuanjie-ai/ChatLLM/assets/20265321/90890ea5-e92d-46c8-a600-a32e33eb4fc3)\n\n## todo: SimpleGraphRAG\n\n## [ChatLLM API 分发系统上线](https://api.chatllm.vip/)\n\n<details markdown=\"1\">\n  <summary>Click to Star⭐️免费领取tokens</summary>\n    https://api.chatllm.vip/\n\n</details>\n\n## Install\n\n```shell\npip install -U chatllm\n```\n\n## [Docs](https://yuanjie-ai.github.io/ChatLLM/)\n\n## Usages\n\n### [目前适配的LLMs](LLMS.md)\n> 计划推出国内 oneapi，支持各种主流大模型，兼容openai客户端生态。\n\n```python\nfrom chatllm.applications import ChatBase\n\nqa = ChatBase()\nqa.load_llm(model_name_or_path=\"THUDM/chatglm-6b\")\nfor i in qa(query='周杰伦是谁', knowledge_base='周杰伦是傻子'):\n    print(i, end='')\n# 根据已知信息无法回答该问题，因为周杰伦是中国内地流行歌手、演员、音乐制作人、导演，\n# 是具有一定的知名度和专业能力的人物，没有提供足够的信息无法判断他是傻子。\n```\n\n## OpenaiEcosystem\n\n<details markdown=\"1\">\n  <summary>Click to 无缝对接openai生态</summary>\n\n```shell\n# 服务端\npip install \"chatllm[openai]\" && chatllm-run openai <本地模型地址>\n```\n\n- SDK：`pip install openai`\n\n```python\nimport openai\n\nopenai.api_base = 'http://0.0.0.0:8000/v1'\nopenai.api_key = 'chatllm'\nprompt = \"你好\"\ncompletion = openai.Completion.create(prompt=prompt, stream=True, model=\"text-davinci-003\")\nfor c in completion:\n    print(c.choices[0].text, end='')\n# 你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你，欢迎问我任何问题。\n```\n\n- 客户端：[点击下载chatbox](https://chatboxapp.xyz/)，也可接入客户端\n  ![客户端](data/imgs/chatbox.png)\n\n### [openai_keys](./data/openai_keys.md): `不定期更新免费keys`\n\n</details>\n\n## ChatOCR\n\n<details markdown=\"1\">\n  <summary>Click to ChatOCR</summary>\n\n```python\nfrom meutils.pipe import *\nfrom chatllm.llmchain.applications import Chat",
    "url": "https://github.com/yuanjie-ai/ChatLLM",
    "last_updated": "2025-08-21T05:04:02+00:00"
  },
  {
    "full_name": "brightway-lca/bw_temporalis",
    "name": "bw_temporalis",
    "description": "Temporalis library for Brightway 2.5 LCA framework",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# bw_temporalis\n\n[![PyPI](https://img.shields.io/pypi/v/bw_temporalis.svg)][pypi status]\n[![Status](https://img.shields.io/pypi/status/bw_temporalis.svg)][pypi status]\n[![Python Version](https://img.shields.io/pypi/pyversions/bw_temporalis)][pypi status]\n[![License](https://img.shields.io/pypi/l/bw_temporalis)][license]\n\n[![Read the documentation at https://bw_temporalis.readthedocs.io/](https://img.shields.io/readthedocs/bw_temporalis/latest.svg?label=Read%20the%20Docs)][read the docs]\n[![Tests](https://github.com/brightway-lca/bw_temporalis/actions/workflows/python-test.yml/badge.svg)][tests]\n[![Codecov](https://codecov.io/gh/brightway-lca/bw_temporalis/branch/main/graph/badge.svg)][codecov]\n\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)][pre-commit]\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)][black]\n\n[pypi status]: https://pypi.org/project/bw_temporalis/\n[read the docs]: https://bw_temporalis.readthedocs.io/\n[tests]: https://github.com/brightway-lca/bw_temporalis/actions/workflows/python-test.yml\n[codecov]: https://app.codecov.io/gh/brightway-lca/bw_temporalis\n[pre-commit]: https://github.com/pre-commit/pre-commit\n[black]: https://github.com/psf/black\n\n## Installation\n\nYou can install _bw_temporalis_ via [pip] from [PyPI]:\n\n```console\n$ pip install bw_temporalis\n```\n\nCurrently Python 3.11 installations are broken as [scikit-network](https://scikit-network.readthedocs.io/en/latest/?badge=latest) only has pre-built wheels for version 0.31 - but this changed the [API for shortest path](https://github.com/sknetwork-team/scikit-network/blob/master/HISTORY.rst#0310-2023-05-22), and `bw_graph_tools` has not yet been adapted](https://github.com/brightway-lca/bw_graph_tools/issues/13).\n\nYou can also install using conda in the [cmutel channel](https://anaconda.org/cmutel/bw_temporalis).\n\n## Usage notes\n\nSee [the teaching repo `from-the-ground-up`](https://github.com/brightway-lca/from",
    "url": "https://github.com/brightway-lca/bw_temporalis",
    "last_updated": "2025-07-14T20:52:09+00:00"
  },
  {
    "full_name": "ManifestoProject/manifestoR",
    "name": "manifestoR",
    "description": "An R package for accessing the Manifesto Project's Data and Corpus of election programmes",
    "language": "R",
    "topics": [],
    "readme": "# ManifestoR\n\nAn R package for accessing and processing the\n[Manifesto Project](https://manifesto-project.wzb.eu)'s\nData and Corpus of election programmes.\n\n## Description\n\nProvides access to coded election programmes from the Manifesto\nCorpus and to the Manifesto Project's Main Dataset and routines to analyse this\ndata. The Manifesto Project <https://manifesto-project.wzb.eu> collects and\nanalyses election programmes across time and space to measure the political\npreferences of parties. The Manifesto Corpus contains the collected and\nannotated election programmes in the Corpus format of the package 'tm' to enable\neasy use of text processing and text mining functionality. Specific functions\nfor scaling of coded political texts are included. See https://manifesto-project.wzb.eu \nfor additional tutorials, documentation, data, and election programmes.\n\n## Quick Start Guide\n\nYou can install the package from CRAN:\n```\ninstall.packages(\"manifestoR\")\n```\n\nThen a typical script or session with `manifestoR` starts like this:\n```\nlibrary(manifestoR)\nmp_setapikey(\"manifesto_apikey.txt\") ## create and download your API key at https://manifesto-project.wzb.eu before\n\n## download election programmes texts and codings\nelection_programmes <- mp_corpus(countryname == \"Bulgaria\")\n\n## for example:\nhead(content(election_programmes[[1]])) ## view beginning of text of first manifesto\ntable(codes(election_programmes)) ## count codes of all manifestos\n\n## ...\n```\n\n## Documentation\n\nThe main user documentation is the vignette `manifestoRworkflow`. It walks you\nthrough the package's central functions giving many example code bits. For detailed\ninformation about all functions and parameters, pleaser refer to the functions'\ndocumentations with R's `?` or the packages Reference Manual.\n\n## Contributing\n\nIf you want to contribute to the development of `manifestoR` by reporting bugs,\nproposing features or writing program code, you are invited to do this on the\npackage's github page: [https://gith",
    "url": "https://github.com/ManifestoProject/manifestoR",
    "last_updated": "2025-07-16T18:34:25+00:00"
  },
  {
    "full_name": "Typing-Monkeys/complex-network-link-prediction",
    "name": "complex-network-link-prediction",
    "description": "A python library for link prediction in social networks",
    "language": "Python",
    "topics": [
      "complex-networks",
      "link-prediction",
      "social-network-analysis"
    ],
    "readme": "# **Complex Network Link Prediction**\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![PyPi](https://badge.fury.io/py/complex-network-link-prediction.svg)](https://badge.fury.io/py/complex-network-link-prediction) [![Documentation](https://img.shields.io/badge/Documentation-blue.svg)](https://typing-monkeys.github.io/social-network-link-prediction/) [![Downloads](https://pepy.tech/badge/complex-network-link-prediction/month)](https://pepy.tech/project/complex-network-link-prediction) [![Wiki](https://img.shields.io/badge/howTo-Wiki-blue.svg)](https://github.com/Typing-Monkeys/social-network-link-prediction/wiki) [![GitHubIssues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/Typing-Monkeys/social-network-link-prediction/issues) [![GitTutorial](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project)\n\n\n#### **Complex Network Link Prediction** is a python library that implements some of the main techniques and algorithms to perform link predictions.\n\n<img src=\"https://raw.githubusercontent.com/Typing-Monkeys/social-network-link-prediction/develop/imgs/logo.png\" alt=\"logo\" width=\"70%\" />\n\nThis library, implemented in python, allows you to use some of the main algorithms and methods to perform link predictions. It was designed to carry out these tasks in **Complex Networks** and, specifically, in **Social Networks**. Each method has its own specific documentation available on the [official documentation page](https://typing-monkeys.github.io/social-network-link-prediction/), where it is possible to see the required parameters and the output of the method itself. <br>\nThe methods are distinguished by belonging to categories and subcategories, below is an example image with all the categories.\n\n<img src=\"https://raw.githubusercontent.com/Typing-Monkeys/social-network-link-prediction/develop/imgs/methods_list.jpg\"",
    "url": "https://github.com/Typing-Monkeys/complex-network-link-prediction",
    "last_updated": "2025-06-19T14:40:06+00:00"
  },
  {
    "full_name": "vikjam/mostly-harmless-replication",
    "name": "mostly-harmless-replication",
    "description": "Replication of tables and figures from \"Mostly Harmless Econometrics\" in Stata, R, Python and Julia. ",
    "language": "Stata",
    "topics": [
      "economics",
      "econometrics",
      "stata",
      "julia",
      "r",
      "python",
      "replication"
    ],
    "readme": "# Mostly Harmless Replication\n<img src=\"http://img12.deviantart.net/f2cc/i/2015/017/2/5/babel_fish_poster__color__by_mrrtist21-d8eb1ea.jpg\" width=\"400\">\n\n## Synopsis\n\nA bold attempt to replicate the tables and figures from the book [_Mostly Harmless Econometrics_](http://www.mostlyharmlesseconometrics.com/) in the following languages:\n* Stata\n* R\n* Python\n* Julia\n\nWhy undertake this madness? My primary motivation was to see if I could replace Stata with either R, Python, or Julia in my workflow, so I tried to replicate _Mostly Harmless Econometrics_ in each of these languages.\n\n## Chapters\n1. Questions about _Questions_\n2. The Experimental Ideal\n3. [Making Regression Make Sense](03%20Making%20Regression%20Make%20Sense/03%20Making%20Regression%20Make%20Sense.md)\n4. [Instrumental Variables in Action](04%20Instrumental%20Variables%20in%20Action/04%20Instrumental%20Variables%20in%20Action.md)\n5. [Parallel Worlds](05%20Fixed%20Effects%2C%20DD%20and%20Panel%20Data/05%20Fixed%20Effects%2C%20DD%20and%20Panel%20Data.md)\n6. [Getting a Little Jumpy](06%20Getting%20a%20Little%20Jumpy/06%20Getting%20a%20Little%20Jumpy.md)\n7. [Quantile Regression](07%20Quantile%20Regression/07%20Quantile%20Regression.md)\n8. [Nonstandard Standard Error Issues](08%20Nonstandard%20Standard%20Error%20Issues/08%20Nonstanard%20Standard%20Error%20Issues.md)\n\n## Getting started\nCheck out [Getting Started](https://github.com/vikjam/mostly-harmless-replication/wiki/Getting-started) in the Wiki for tips on setting up your machine with each of these languages.\n\n## Contributions\nFeel free to submit [pull requests](https://github.com/blog/1943-how-to-write-the-perfect-pull-request)!\n\n",
    "url": "https://github.com/vikjam/mostly-harmless-replication",
    "last_updated": "2025-08-26T21:42:00+00:00"
  },
  {
    "full_name": "duecredit/duecredit",
    "name": "duecredit",
    "description": "Automated collection and reporting of citations for used software/methods/datasets",
    "language": "Python",
    "topics": [],
    "readme": "# duecredit\n\n\n[![Coverage Status](https://coveralls.io/repos/duecredit/duecredit/badge.svg)](https://coveralls.io/r/duecredit/duecredit)\n[![DOI](https://zenodo.org/badge/DOI/110.5281/zenodo.3376260.svg)](https://doi.org/10.5281/zenodo.3376260)\n[![PyPI version fury.io](https://badge.fury.io/py/duecredit.svg)](https://pypi.python.org/pypi/duecredit/)\n\nduecredit is being conceived to address the problem of inadequate\ncitation of scientific software and methods, and limited visibility of\ndonation requests for open-source software.\n\nIt provides a simple framework (at the moment for Python only) to\nembed publication or other references in the original code so they are\nautomatically collected and reported to the user at the necessary\nlevel of reference detail, i.e. only references for actually used\nfunctionality will be presented back if software provides multiple\nciteable implementations.\n\n## Installation\n\nDuecredit is easy to install via pip, simply type:\n\n `pip install duecredit`\n\n## Examples\n\n### To cite the modules and methods you are using\n\nYou can already start \"registering\" citations using duecredit in your\nPython modules and even registering citations (we call this approach \"injections\")\nfor modules that do not (yet) use duecredit.  duecredit will remain an optional\ndependency, i.e. your software will work correctly even without duecredit installed.\n\nFor example, list citations of the modules and methods `yourproject` uses with a few simple commands:\n```bash\ncd /path/to/yourmodule # for ~/yourproject\ncd yourproject # change directory into where the main code base is\npython -m duecredit yourproject.py\n```\nOr you can also display them in BibTex format, using:\n```bash\nduecredit summary --format=bibtex\n```\nSee this gif animation for a better illustration:\n![Example](examples/duecredit_example.gif)\n\n\n### To let others cite your software\n\n\nFor using duecredit in your software\n\n1. Copy `duecredit/stub.py` to your codebase, e.g.\n\n        wget -q -O /path/tomodule/yourmodu",
    "url": "https://github.com/duecredit/duecredit",
    "last_updated": "2025-08-15T10:16:39+00:00"
  },
  {
    "full_name": "Planeshifter/node-word2vec",
    "name": "node-word2vec",
    "description": "Node.js interface to the Google word2vec tool.",
    "language": "C",
    "topics": [
      "nlp",
      "word2vec"
    ],
    "readme": "[![NPM version][npm-image]][npm-url]\n[![Build Status][travis-image]][travis-url]\n[![Coverage Status][codecov-image]][codecov-url]\n[![Dependencies][dependencies-image]][dependencies-url]\n\nnode-word2vec\n=============\n\n> Node.js interface to the Google [word2vec](https://code.google.com/p/word2vec/) tool\n\n# What is it?\n\nThis is a Node.js interface to the *word2vec* tool developed at Google Research for \"efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words\", which can be used in a variety of NLP tasks. For further information about the *word2vec* project, consult [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/).\n\n# Installation\n\nCurrently, `node-word2vec` is ONLY supported for Unix operating systems.\n\nInstall it via npm:\n``` bash\nnpm install word2vec\n```\n\nTo use it inside Node.js, require the module as follows:\n\n``` javascript\nvar w2v = require( 'word2vec' );\n```\n\n# Usage\n\n## API\n\n### .word2phrase( input, output, params, callback )\n\nFor applications where it is important that certain pairs of words are treated as a single term (e.g. \"Barack Obama\" or \"New York\" should be treated as one word), the text corpora used for training should be pre-processed via the *word2phrases* function. Words which frequently occur next to each other will be concatenated via an underscore, e.g. the words \"New\" and \"York\" if following next to each other might be transformed to a single word \"New_York\".\n\nInternally, this function calls the C command line application of the Google *word2vec* project. This allows it to make use of multi-threading and preserves the efficiency of the original C code. It processes the texts given by the `input` text document, writing the output to a file with the name given by `output`.\n\nThe `params` parameter expects a JS object optionally containing some of the following keys and associated values. If they are not supplied, the default values are used.\n\n\n| Key ",
    "url": "https://github.com/Planeshifter/node-word2vec",
    "last_updated": "2025-04-17T08:38:43+00:00"
  },
  {
    "full_name": "napari/napari",
    "name": "napari",
    "description": "napari: a fast, interactive, multi-dimensional image viewer for python",
    "language": "Python",
    "topics": [
      "napari",
      "numpy",
      "visualization",
      "python"
    ],
    "readme": "# napari\n\n### multi-dimensional image viewer for python\n\n[![napari on Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/napari/napari/main?urlpath=%2Fdesktop)\n[![image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fnapari.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/napari)\n[![License](https://img.shields.io/pypi/l/napari.svg)](https://github.com/napari/napari/raw/main/LICENSE)\n[![Comprehensive Test](https://github.com/napari/napari/actions/workflows/test_comprehensive.yml/badge.svg)](https://github.com/napari/napari/actions/workflows/test_comprehensive.yml)\n[![Code coverage](https://codecov.io/gh/napari/napari/branch/main/graph/badge.svg)](https://codecov.io/gh/napari/napari)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/napari.svg)](https://python.org)\n[![Python package index](https://img.shields.io/pypi/v/napari.svg)](https://pypi.org/project/napari)\n[![Python package index download statistics](https://img.shields.io/pypi/dm/napari.svg)](https://pypistats.org/packages/napari)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/napari.svg)](https://anaconda.org/conda-forge/napari)\n![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/napari?label=Conda%20downloads)\n[![Development Status](https://img.shields.io/pypi/status/napari.svg)](https://en.wikipedia.o",
    "url": "https://github.com/napari/napari",
    "last_updated": "2025-09-02T06:42:56+00:00"
  },
  {
    "full_name": "statsbylopez/StatsSports",
    "name": "StatsSports",
    "description": "Material for Skidmore College, Statistics in Sports",
    "language": "R",
    "topics": [],
    "readme": "# StatsSports\n\nMaterial for Skidmore College MA 251, Statistics in Sports. Currently updated as of Fall, 2019. This will be the homepage for code, data, lectures, and homeworks. The course is designed for students with an intro stat background, looking to learn specifics of where data tools can be applied to sports. \n\nOld version of the class can be found at <https://github.com/statsbylopez/StatsSports/tree/master/2016%20class>. \n",
    "url": "https://github.com/statsbylopez/StatsSports",
    "last_updated": "2025-08-25T16:33:04+00:00"
  },
  {
    "full_name": "OpenIntroStat/oilabs-tidy",
    "name": "oilabs-tidy",
    "description": "👩🏿‍💻 OpenIntro Labs in R using the tidyverse design philosophy, grammar, and data structures",
    "language": "HTML",
    "topics": [],
    "readme": "OpenIntro Labs promote the understanding and application of statistics through  applied data analysis.\nLabs are titled based on topic area, which correspond to  particular chapters in all three versions of OpenIntro Statistics, a free and  open-source textbook.\nThe textbook as well as the html version of the labs can be found at [http://www.openintro.org/stat/labs.php](http://www.openintro.org/stat/labs.php).\n\nThis repository is a fork of the original base-R labs.\nIt incorporates the tidyverse syntax from the `dplyr` package for data manipulation, the `ggplot2` package for graphics, and the `infer` package for statistical inference.\n\n## Labs\n\n1. [Intro to R](http://openintrostat.github.io/oilabs-tidy/01_intro_to_r/intro_to_r.html)\n2. [Intro to data](http://openintrostat.github.io/oilabs-tidy/02_intro_to_data/intro_to_data.html)\n4. [Probability](http://openintrostat.github.io/oilabs-tidy/03_probability/probability.html)\n3. [Normal distribution](http://openintrostat.github.io/oilabs-tidy/04_normal_distribution/normal_distribution.html)\n5. Foundations of inference  \n  a. [Sampling distributions](https://openintro.shinyapps.io/sampling_distributions/)  \n  b. [Confidence intervals](https://openintro.shinyapps.io/confidence_intervals/)\n6. [Inference for categorical data](https://openintro.shinyapps.io/inf_for_categorical_data/)\n7. [Inference for numerical data](http://openintrostat.github.io/oilabs-tidy/07_inf_for_numerical_data/inf_for_numerical_data.html)\n8. [Simple linear regression](http://openintrostat.github.io/oilabs-tidy/08_simple_regression/simple_regression.html)\n9. [Multiple linear regression](http://openintrostat.github.io/oilabs-tidy/09_multiple_regression/multiple_regression.html)\n\n## Source code for labs\n\nWe currently support our source files in the RMarkdown (.Rmd) format, which can be output into html format (though output to pdf is also possible).\nThe source files are processed using the [knitr](http://yihui.name/knitr/) package in R, and are easiest to ",
    "url": "https://github.com/OpenIntroStat/oilabs-tidy",
    "last_updated": "2025-08-25T16:31:28+00:00"
  },
  {
    "full_name": "OCA/sale-promotion",
    "name": "sale-promotion",
    "description": "",
    "language": "HTML",
    "topics": [
      "erp",
      "odoo",
      "hacktoberfest",
      "python"
    ],
    "readme": "\n[![Runboat](https://img.shields.io/badge/runboat-Try%20me-875A7B.png)](https://runboat.odoo-community.org/builds?repo=OCA/sale-promotion&target_branch=18.0)\n[![Pre-commit Status](https://github.com/OCA/sale-promotion/actions/workflows/pre-commit.yml/badge.svg?branch=18.0)](https://github.com/OCA/sale-promotion/actions/workflows/pre-commit.yml?query=branch%3A18.0)\n[![Build Status](https://github.com/OCA/sale-promotion/actions/workflows/test.yml/badge.svg?branch=18.0)](https://github.com/OCA/sale-promotion/actions/workflows/test.yml?query=branch%3A18.0)\n[![codecov](https://codecov.io/gh/OCA/sale-promotion/branch/18.0/graph/badge.svg)](https://codecov.io/gh/OCA/sale-promotion)\n[![Translation Status](https://translation.odoo-community.org/widgets/sale-promotion-18-0/-/svg-badge.svg)](https://translation.odoo-community.org/engage/sale-promotion-18-0/?utm_source=widget)\n\n<!-- /!\\ do not modify above this line -->\n\n# sale-promotion\n\nsale-promotion\n\n<!-- /!\\ do not modify below this line -->\n\n<!-- prettier-ignore-start -->\n\n[//]: # (addons)\n\nAvailable addons\n----------------\naddon | version | maintainers | summary\n--- | --- | --- | ---\n[loyalty_criteria_multi_product](loyalty_criteria_multi_product/) | 18.0.1.0.0 | <a href='https://github.com/chienandalu'><img src='https://github.com/chienandalu.png' width='32' height='32' style='border-radius:50%;' alt='chienandalu'/></a> | Allows to set as promotion criteria multi-product conditions\n[loyalty_incompatibility](loyalty_incompatibility/) | 18.0.1.0.0 | <a href='https://github.com/chienandalu'><img src='https://github.com/chienandalu.png' width='32' height='32' style='border-radius:50%;' alt='chienandalu'/></a> | Allows to set incompatibility rules between promotions\n[loyalty_limit](loyalty_limit/) | 18.0.1.0.0 | <a href='https://github.com/chienandalu'><img src='https://github.com/chienandalu.png' width='32' height='32' style='border-radius:50%;' alt='chienandalu'/></a> | Restrict number of promotions per customer or salesma",
    "url": "https://github.com/OCA/sale-promotion",
    "last_updated": "2025-09-01T09:45:22+00:00"
  },
  {
    "full_name": "kuk/log-progress",
    "name": "log-progress",
    "description": "https://habr.com/ru/post/276725/",
    "language": "Jupyter Notebook",
    "topics": [
      "jupyter",
      "widget",
      "logging"
    ],
    "readme": "> # `log-progress` functionality was integrated into <a href=\"https://github.com/tqdm/tqdm\">`tqdm`</a>. Please, use `from tqdm.notebook import tqdm as log_progress`.\n\n## Widget based progress bar for Jupyter (IPython Notebook)\n\n### Code\nJust copy and paste it into your project:\n```python\ndef log_progress(sequence, every=None, size=None, name='Items'):\n    from ipywidgets import IntProgress, HTML, VBox\n    from IPython.display import display\n\n    is_iterator = False\n    if size is None:\n        try:\n            size = len(sequence)\n        except TypeError:\n            is_iterator = True\n    if size is not None:\n        if every is None:\n            if size <= 200:\n                every = 1\n            else:\n                every = int(size / 200)     # every 0.5%\n    else:\n        assert every is not None, 'sequence is iterator, set every'\n\n    if is_iterator:\n        progress = IntProgress(min=0, max=1, value=1)\n        progress.bar_style = 'info'\n    else:\n        progress = IntProgress(min=0, max=size, value=0)\n    label = HTML()\n    box = VBox(children=[label, progress])\n    display(box)\n\n    index = 0\n    try:\n        for index, record in enumerate(sequence, 1):\n            if index == 1 or index % every == 0:\n                if is_iterator:\n                    label.value = '{name}: {index} / ?'.format(\n                        name=name,\n                        index=index\n                    )\n                else:\n                    progress.value = index\n                    label.value = u'{name}: {index} / {size}'.format(\n                        name=name,\n                        index=index,\n                        size=size\n                    )\n            yield record\n    except:\n        progress.bar_style = 'danger'\n        raise\n    else:\n        progress.bar_style = 'success'\n        progress.value = index\n        label.value = \"{name}: {index}\".format(\n            name=name,\n            index=str(index or '?')\n        )\n```\n\n### Examples\nProgress ",
    "url": "https://github.com/kuk/log-progress",
    "last_updated": "2025-03-24T15:59:21+00:00"
  },
  {
    "full_name": "kbenoit/CSTA-APSR",
    "name": "CSTA-APSR",
    "description": "Replication Materials for \"Crowd-Sourced Text Analysis\" APSR (2016) 110(2): 278-295.",
    "language": "R",
    "topics": [],
    "readme": "## Crowd-Sourced Text Analysis - *APSR*\n\nReplication Materials for:\n\n*  Benoit, Kenneth, Drew Conway, Benjamin E. Lauderdale, Michael Laver, and\nSlava Mikhaylov (2016) \"**Crowd-Sourced Text Analysis: Crowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data.**\" *American Political Science Review* (2016) 110(2): 278-295.\n\nThis repository is designed to provide full replication materials and tools for analysis used in the paper.  Please submit [issues](https://github.com/kbenoit/CSTA-APSR/issues) if you have queries or find discrepancies, or address directly to kbenoit@lse.ac.uk.\n\n",
    "url": "https://github.com/kbenoit/CSTA-APSR",
    "last_updated": "2024-04-27T10:10:18+00:00"
  },
  {
    "full_name": "openai/spinningup",
    "name": "spinningup",
    "description": "An educational resource to help anyone learn deep reinforcement learning.",
    "language": "Python",
    "topics": [],
    "readme": "**Status:** Maintenance (expect bug fixes and minor updates)\n\nWelcome to Spinning Up in Deep RL! \n==================================\n\nThis is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).\n\nFor the unfamiliar: [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with [deep learning](http://ufldl.stanford.edu/tutorial/).\n\nThis module contains a variety of helpful resources, including:\n\n- a short [introduction](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) to RL terminology, kinds of algorithms, and basic theory,\n- an [essay](https://spinningup.openai.com/en/latest/spinningup/spinningup.html) about how to grow into an RL research role,\n- a [curated list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) of important papers organized by topic,\n- a well-documented [code repo](https://github.com/openai/spinningup) of short, standalone implementations of key algorithms,\n- and a few [exercises](https://spinningup.openai.com/en/latest/spinningup/exercises.html) to serve as warm-ups.\n\nGet started at [spinningup.openai.com](https://spinningup.openai.com)!\n\n\nCiting Spinning Up\n------------------\n\nIf you reference or use Spinning Up in your research, please cite:\n\n```\n@article{SpinningUp2018,\n    author = {Achiam, Joshua},\n    title = {{Spinning Up in Deep Reinforcement Learning}},\n    year = {2018}\n}\n```",
    "url": "https://github.com/openai/spinningup",
    "last_updated": "2025-09-01T13:38:12+00:00"
  },
  {
    "full_name": "ropensci/tokenizers",
    "name": "tokenizers",
    "description": "Fast, Consistent Tokenization of Natural Language Text",
    "language": "R",
    "topics": [
      "text-mining",
      "tokenizer",
      "rstats",
      "nlp",
      "r",
      "r-package",
      "peer-reviewed"
    ],
    "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# tokenizers\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/tokenizers)](https://cran.r-project.org/package=tokenizers)\n[![DOI](http://joss.theoj.org/papers/10.21105/joss.00655/status.svg)](https://doi.org/10.21105/joss.00655)\n[![rOpenSci peer\nreview](https://badges.ropensci.org/33_status.svg)](https://github.com/ropensci/software-review/issues/33)\n[![CRAN_Downloads](http://cranlogs.r-pkg.org/badges/grand-total/tokenizers)](https://cran.r-project.org/package=tokenizers)\n[![Travis-CI Build\nStatus](https://travis-ci.org/ropensci/tokenizers.svg?branch=master)](https://travis-ci.org/ropensci/tokenizers)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/ropensci/tokenizers/master.svg)](https://codecov.io/github/ropensci/tokenizers?branch=master)\n\n## Overview\n\nThis R package offers functions with a consistent interface to convert\nnatural language text into tokens. It includes tokenizers for shingled\nn-grams, skip n-grams, words, word stems, sentences, paragraphs,\ncharacters, shingled characters, lines, Penn Treebank, and regular\nexpressions, as well as functions for counting characters, words, and\nsentences, and a function for splitting longer texts into separate\ndocuments, each with the same number of words. The package is built on\nthe [stringi](https://www.gagolewski.com/software/stringi/) and\n[Rcpp](https://www.rcpp.org/) packages for fast yet correct tokenization\nin UTF-8.\n\nSee the “[Introduction to the tokenizers\nPackage](https://docs.ropensci.org/tokenizers/articles/introduction-to-tokenizers.html)”\nvignette for an overview of all the functions in this package.\n\nThis package complies with the standards for input and output\nrecommended by the Text Interchange Formats. The TIF initiative was\ncreated at an rOpenSci meeting in 2017, and its recommendations are\navailable as part of the [tif\npackage](https://github.com/ropenscilabs/tif). See the “[The Text\nInterchange Formats an",
    "url": "https://github.com/ropensci/tokenizers",
    "last_updated": "2025-03-07T04:38:15+00:00"
  },
  {
    "full_name": "alecstein/dolt_datascience",
    "name": "dolt_datascience",
    "description": "notebooks used to analysis projects",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# dolt_datascience\n",
    "url": "https://github.com/alecstein/dolt_datascience",
    "last_updated": "2025-07-10T11:14:15+00:00"
  },
  {
    "full_name": "Ahmad-Bamba/pyPDF-OCR",
    "name": "pyPDF-OCR",
    "description": "A python library for converting non-unicode PDFs to machine readable formats ",
    "language": "Python",
    "topics": [],
    "readme": "# pyPDF-OCR\nA python library for converting non-unicode PDFs to machine readable formats \n",
    "url": "https://github.com/Ahmad-Bamba/pyPDF-OCR",
    "last_updated": "2025-07-13T13:57:30+00:00"
  },
  {
    "full_name": "awslabs/datawig",
    "name": "datawig",
    "description": "Imputation of missing values in tables.",
    "language": "JavaScript",
    "topics": [
      "imputation",
      "missing-value-handling"
    ],
    "readme": "DataWig - Imputation for Tables\n================================\n\n[![PyPI version](https://badge.fury.io/py/datawig.svg)](https://badge.fury.io/py/datawig.svg)\n[![GitHub license](https://img.shields.io/github/license/awslabs/datawig.svg)](https://github.com/awslabs/datawig/blob/master/LICENSE)\n[![GitHub issues](https://img.shields.io/github/issues/awslabs/datawig.svg)](https://github.com/awslabs/datawig/issues)\n[![Build Status](https://travis-ci.org/awslabs/datawig.svg?branch=master)](https://travis-ci.org/awslabs/datawig)\n\nDataWig learns Machine Learning models to impute missing values in tables.\n\nSee our user-guide and extended documentation [here](https://datawig.readthedocs.io/en/latest).\n\n## Installation\n\n### CPU\n```bash\npip3 install datawig\n```\n\n### GPU\nIf you want to run DataWig on a GPU you need to make sure your version of Apache MXNet Incubating contains the GPU bindings.\nDepending on your version of CUDA, you can do this by running the following:\n\n```bash\nwget https://raw.githubusercontent.com/awslabs/datawig/master/requirements/requirements.gpu-cu${CUDA_VERSION}.txt\npip install datawig --no-deps -r requirements.gpu-cu${CUDA_VERSION}.txt\nrm requirements.gpu-cu${CUDA_VERSION}.txt\n```\nwhere `${CUDA_VERSION}` can be `75` (7.5), `80` (8.0), `90` (9.0), or `91` (9.1).\n\n## Running DataWig\nThe DataWig API expects your data as a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). Here is an example of how the dataframe might look:\n\n|Product Type | Description           | Size | Color |\n|-------------|-----------------------|------|-------|\n|   Shoe      | Ideal for Running     | 12UK | Black |\n| SDCards     | Best SDCard ever ...  | 8GB  | Blue  |\n| Dress       | This **yellow** dress | M    | **?** |\n\n### Quickstart Example\n\nFor most use cases, the `SimpleImputer` class is the best starting point. For convenience there is the function [SimpleImputer.complete](https://datawig.readthedocs.io/en/latest/source/API.html#d",
    "url": "https://github.com/awslabs/datawig",
    "last_updated": "2025-07-22T03:28:59+00:00"
  },
  {
    "full_name": "TheUpshot/nyt_weddings",
    "name": "nyt_weddings",
    "description": "List of New York Times wedding announcements used in an Upshot story on name-changing.",
    "language": "",
    "topics": [],
    "readme": "# NYT Weddings\n\nThis repository contains the list of New York Times wedding announcements used in [an Upshot story](http://www.nytimes.com/2015/06/28/upshot/maiden-names-on-the-rise-again.html) on name changes among women who marry. The [CSV file](https://github.com/TheUpshot/nyt_weddings/blob/master/nyt_wedding_announcements.csv) contains the URL of the announcement, status of the bride's name and the ages of the celebrants, if available.\n\nTo analyze The Times data, we compiled the text of 7,835 opposite-sex wedding announcements published in several different years: 1985, 1990, 1995, 2000, 2005, 2010 and 2014. The 1985 data covered only the final four months of that year, and The Times did not publish the ages of celebrants in announcements during that period.\n\nTo identify women's name decisions, we searched for common phrases such as \"the bride will be keeping her name\" or \"will be taking her husband’s name.\" The announcement texts were relatively consistent, using many of the same phrases year after year, but some variations were evident. Those announcements that did not contain immediately obvious indicators of preference were then examined in more detail to see how the bride was described on subsequent references in the text. For example, a typical second reference might say \"Mrs. Jones,\" where Jones was the groom's surname, or \"Ms. Parker,\" where Parker matched the bride’s surname. These records were marked accordingly.\n\nAbout one in five announcements in each year examined gave no clear indication of name preference. In these announcements the bride typically was referenced only as \"the bride\" after her full name was printed. We counted these records as \"unclear.\"\n\nTo analyze the data, we used an internal API to collect the text of announcements, loaded them into a PostgreSQL database and then used both SQL and the [Ruby gem String Eater](https://github.com/simplifi/string-eater) to identify and isolate patterns that would help determine the name status. Ful",
    "url": "https://github.com/TheUpshot/nyt_weddings",
    "last_updated": "2021-12-22T06:56:10+00:00"
  },
  {
    "full_name": "coderamp-labs/gitingest",
    "name": "gitingest",
    "description": "Replace 'hub' with 'ingest' in any GitHub URL to get a prompt-friendly extract of a codebase ",
    "language": "Python",
    "topics": [
      "ai",
      "code",
      "ingestion",
      "developer-tool"
    ],
    "readme": "# Gitingest\n\n[![Screenshot of Gitingest front page](https://raw.githubusercontent.com/coderamp-labs/gitingest/refs/heads/main/docs/frontpage.png)](https://gitingest.com)\n\n<!-- Badges -->\n<!-- markdownlint-disable MD033 -->\n<p align=\"center\">\n  <!-- row 1 — install & compat -->\n  <a href=\"https://pypi.org/project/gitingest\"><img src=\"https://img.shields.io/pypi/v/gitingest.svg\" alt=\"PyPI\"></a>\n  <a href=\"https://pypi.org/project/gitingest\"><img src=\"https://img.shields.io/pypi/pyversions/gitingest.svg\" alt=\"Python Versions\"></a>\n  <br>\n  <!-- row 2 — quality & community -->\n  <a href=\"https://github.com/coderamp-labs/gitingest/actions/workflows/ci.yml?query=branch%3Amain\"><img src=\"https://github.com/coderamp-labs/gitingest/actions/workflows/ci.yml/badge.svg?branch=main\" alt=\"CI\"></a>\n\n  <a href=\"https://github.com/astral-sh/ruff\"><img src=\"https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json\" alt=\"Ruff\"></a>\n  <a href=\"https://scorecard.dev/viewer/?uri=github.com/coderamp-labs/gitingest\"><img src=\"https://api.scorecard.dev/projects/github.com/coderamp-labs/gitingest/badge\" alt=\"OpenSSF Scorecard\"></a>\n  <br>\n  <a href=\"https://github.com/coderamp-labs/gitingest/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/coderamp-labs/gitingest.svg\" alt=\"License\"></a>\n  <a href=\"https://pepy.tech/project/gitingest\"><img src=\"https://pepy.tech/badge/gitingest\" alt=\"Downloads\"></a>\n  <a href=\"https://github.com/coderamp-labs/gitingest\"><img src=\"https://img.shields.io/github/stars/coderamp-labs/gitingest\" alt=\"GitHub Stars\"></a>\n  <a href=\"https://discord.com/invite/zerRaGK9EC\"><img src=\"https://img.shields.io/badge/Discord-Join_chat-5865F2?logo=discord&logoColor=white\" alt=\"Discord\"></a>\n  <br>\n  <a href=\"https://trendshift.io/repositories/13519\"><img src=\"https://trendshift.io/api/badge/repositories/13519\" alt=\"Trendshift\" height=\"50\"></a>\n</p>\n<!-- markdownlint-enable MD033 -->\n\nTurn any Git repository",
    "url": "https://github.com/coderamp-labs/gitingest",
    "last_updated": "2025-09-02T09:58:52+00:00"
  },
  {
    "full_name": "TaddyLab/maptpx",
    "name": "maptpx",
    "description": "map estimation of topic models",
    "language": "R",
    "topics": [
      "topic-modeling",
      "map-estimation"
    ],
    "readme": "maptpx\n======\n\nMAP estimation of topic models in R.  It implements estimation via posterior maximization for latent topic models in text analysis, as described in <a href=\"http://jmlr.csail.mit.edu/proceedings/papers/v22/taddy12/taddy12.pdf\">\"On Estimation and Selection for Topic Models\"</a>.  These routines were previously part of <a href=\"http://www.cran.r-project.org/web/packages/textir/index.html\">textir</a>, but have been spun off since version 2.0 of that package.  This maptpx package is no longer actively maintained; instead, we are focusing on developing faster distributed factor models based on <a href=\"http://arxiv.org/abs/1311.6139\">distributed multinomial regression</a>, as implemented in the <a href=\"http://cran.r-project.org/web/packages/distrom/index.html\">distrom</a> package.\n\nIf you want to take advantage of openmp parallelization, uncomment the relevant flags in src/MAKEVARS before compiling.\n",
    "url": "https://github.com/TaddyLab/maptpx",
    "last_updated": "2023-03-30T13:52:02+00:00"
  },
  {
    "full_name": "joschu/cgt",
    "name": "cgt",
    "description": "Computation Graph Toolkit",
    "language": "Python",
    "topics": [],
    "readme": "\nComputation Graph Toolkit (CGT) is a library for evaluation and differentiation of functions of multidimensional arrays.\n\nFull documentation can be found at [http://rll.berkeley.edu/cgt](http://rll.berkeley.edu/cgt)\n\n[Release announcement](http://joschu.github.io/index.html#Announcing CGT)",
    "url": "https://github.com/joschu/cgt",
    "last_updated": "2025-08-30T06:13:43+00:00"
  },
  {
    "full_name": "facebookresearch/URL-Sanitization",
    "name": "URL-Sanitization",
    "description": "The code processes URLs in an attempt to consolidate different web addresses that point to the same URL and to remove potentially private and/or sensitive data. It is part of the Facebook URL shares release effort, which is led by Election Research Commission (ERC). ",
    "language": "Python",
    "topics": [],
    "readme": "# URL Sanitization\n\nFor URLs with query strings, the code aims to remove query parameters unrelated to content navigation. It does so by iteratively removing each query parameter and testing the resulting content for differences with original content. The code also remove query parameters often related to user PII by using simple string matching.\n\n## Requirement\nThe URL Sanitization requires Python 3, along with the pandas, numpy, urllib, difflib, pebble, BeautifulSoup, and phonenumbers modules.\n\n## How it works\n\nThe function `process_urls()` takes a list of URLs from `url_training_data_path` and for each domain, generates a rule that determines which query parameters will be removed. Then, it applies the rule to the data in `url_full_data_path` and removes URL parameters that do not meaningfully change page content. Then it saves cleaned URLs to the `output_data_path`.\n\nThe script requires tab-separated values (TSV) files and outputs the same (URLs may contains commas). The URLs to be processed must be called `canonical_url` in the input data `url_training_data_path` and output data `url_full_data_path`.\n\n## Examples\nThe file `demo_run.py` illustrate an example. The file `demo_input.tsv` illustrates how the prepare the to-be-cleaned URLs data (both the training data and the full data). The file `demo_output.tsv` illustrates the expected processed results.\n\n## License\nApache-2.0\n",
    "url": "https://github.com/facebookresearch/URL-Sanitization",
    "last_updated": "2023-10-31T22:30:43+00:00"
  },
  {
    "full_name": "sckott/gbifms",
    "name": "gbifms",
    "description": "manuscript covering rgbif, pygbif, and gbifrb",
    "language": "TeX",
    "topics": [],
    "readme": "Instructions for compiling manuscripts\n======================================\n\n[![CircleCI](https://circleci.com/gh/sckott/gbifms.svg?style=svg)](https://circleci.com/gh/sckott/gbifms)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.997554.svg)](https://doi.org/10.5281/zenodo.997554)\n\n\n\nInstall dependencies\n--------------------\n\nInstall the `rgbif` R package, including the suggested packages, using the following inside R:\n\n```r\ninstall.packages(\"rgbif\", dependencies=TRUE)\n```\n\nNote that `rmarkdown` requires `pandoc` (>= 0.12.3) and `pandoc-citeproc` be installed. These ship with the current version of RStudio (`>=0.98`). Additionally, a LaTeX environment is required to generate the output pdf.\n\n\nInstall the `pygbif` Python library, using the following in your shell:\n\n```shell\npip install pygbif\n```\n\nInstall the `gbifrb` Ruby library, using the following in your shell:\n\n```shell\ngem install gbifrb\n```\n\n\nBuild the manuscript\n--------------------\n\nIn the directory\n\n```\nmake\n```\n\nOpen the pdf file\n\n```\nopen manuscript.pdf\n```\n",
    "url": "https://github.com/sckott/gbifms",
    "last_updated": "2020-02-27T16:56:03+00:00"
  },
  {
    "full_name": "ufoym/deepo",
    "name": "deepo",
    "description": "Setup and customize deep learning environment in seconds.",
    "language": "Python",
    "topics": [
      "deep-learning",
      "jupyter",
      "lasagne",
      "caffe",
      "tensorflow",
      "sonnet",
      "keras",
      "theano",
      "chainer",
      "torch",
      "pytorch",
      "mxnet",
      "cntk",
      "dockerfile-generator",
      "docker-image",
      "caffe2",
      "onnx"
    ],
    "readme": "![deepo](https://user-images.githubusercontent.com/2270240/32102393-aecf573c-bb4e-11e7-811c-dc673cae7b9c.png)\n\n![workflows](https://github.com/ufoym/deepo/workflows/deepo%20CI/badge.svg)\n[![docker](https://img.shields.io/docker/pulls/ufoym/deepo.svg)](https://hub.docker.com/r/ufoym/deepo)\n![build](https://img.shields.io/docker/automated/ufoym/deepo.svg)\n![license](https://img.shields.io/github/license/ufoym/deepo.svg)\n\n\n***PLEASE NOTE, THE DEEP LEARNING FRAMEWORK WAR IS OVER, THIS PROJECT IS NO LONGER BEING MAINTAINED.***\n\n---\n\n***Deepo*** is an open framework to assemble specialized [*docker*](http://www.docker.com/) images for deep learning research without pain. It provides a “lego set” of dozens of standard components for preparing deep learning tools and a framework for assembling them into custom docker images. \n\nAt the core of Deepo is a Dockerfile generator that\n- allows you to [customize your deep learning environment](#Build) with Lego-like modules\n  - define your environment in a single command line,\n  - then deepo will generate Dockerfiles with best practices\n  - and do all the configuration for you\n- automatically resolves the dependencies for you\n  - deepo knows which combos (CUDA/cuDNN/Python/PyTorch/Tensorflow, ..., tons of dependancies) are compatible\n  - and will pick the right versions for you\n  - and arrange sequence of installation procedures using [topological sorting](https://en.wikipedia.org/wiki/Topological_sorting)\n\nWe also prepare a series of pre-built docker images that\n- allows you to instantly set up common deep learning research environment\n- supports almost all [commonly used deep learning frameworks](#Available-tags)\n- supports [GPU acceleration](#GPU) (CUDA and cuDNN included), also works in [CPU-only mode](#CPU)\n- works on Linux ([CPU version](#CPU)/[GPU version](#GPU)), Windows ([CPU version](#CPU)) and OS X ([CPU version](#CPU))\n\n---\n\n# Table of contents\n- [Quick Start](#Quick-Start)\n  - [GPU Version](#GPU)\n    - [Installation](#",
    "url": "https://github.com/ufoym/deepo",
    "last_updated": "2025-08-27T18:52:02+00:00"
  },
  {
    "full_name": "tdhopper/notes-on-dirichlet-processes",
    "name": "notes-on-dirichlet-processes",
    "description": ":game_die: Notes explaining Dirichlet Processes, HDPs, and Latent Dirichlet Allocation",
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "<!-- Please don't remove this: Grab your social icons from https://github.com/carlsednaoui/gitsocial -->\n\n<!-- display the social media buttons in your README -->\n\nBy Tim Hopper:\n[tdhopper.com](http://www.tdhopper.com)\n\n[![alt text][1.1]][1]\n[![alt text][6.1]][6]\n\n\n<!-- links to social media icons -->\n<!-- no need to change these -->\n\n<!-- icons with padding -->\n\n[1.1]: http://i.imgur.com/tXSoThF.png (twitter icon with padding)\n[2.1]: http://i.imgur.com/P3YfQoD.png (facebook icon with padding)\n[3.1]: http://i.imgur.com/yCsTjba.png (google plus icon with padding)\n[4.1]: http://i.imgur.com/YckIOms.png (tumblr icon with padding)\n[5.1]: http://i.imgur.com/1AGmwO3.png (dribbble icon with padding)\n[6.1]: http://i.imgur.com/0o48UoR.png (github icon with padding)\n\n<!-- icons without padding -->\n\n[1.2]: http://i.imgur.com/wWzX9uB.png (twitter icon without padding)\n[2.2]: http://i.imgur.com/fep1WsG.png (facebook icon without padding)\n[3.2]: http://i.imgur.com/VlgBKQ9.png (google plus icon without padding)\n[4.2]: http://i.imgur.com/jDRp47c.png (tumblr icon without padding)\n[5.2]: http://i.imgur.com/Vvy3Kru.png (dribbble icon without padding)\n[6.2]: http://i.imgur.com/9I6NRUm.png (github icon without padding)\n\n\n<!-- links to your social media accounts -->\n<!-- update these accordingly -->\n\n[1]: http://www.twitter.com/tdhopper\n[6]: http://www.github.com/tdhopper\n\n<!-- Please don't remove this: Grab your social icons from https://github.com/carlsednaoui/gitsocial -->\n\n\n# Notes Nonparametric Bayesian Methods and Dirichlet Processes\n\nI taught myself Dirichlet processes and Hierarchical DPs in the spring of 2015 in order to understand nonparametric Bayesian models and related inference algorithms. In the process, I wrote a bunch of code and took a bunch of notes. I preserved those notes [here](https://dp.tdhopper.com) for the benefit of others trying to learn this material.\n",
    "url": "https://github.com/tdhopper/notes-on-dirichlet-processes",
    "last_updated": "2025-08-03T23:29:42+00:00"
  },
  {
    "full_name": "SMAPPNYU/smappdragon",
    "name": "smappdragon",
    "description": " :dragon: smappdragon is a set of tools for working with twitter data.",
    "language": "Python",
    "topics": [],
    "readme": "```\n                                     _                             \n ___ _ __ ___   __ _ _ __  _ __   __| |_ __ __ _  __ _  ___  _ __  \n/ __| '_ ` _ \\ / _` | '_ \\| '_ \\ / _` | '__/ _` |/ _` |/ _ \\| '_ \\ \n\\__ \\ | | | | | (_| | |_) | |_) | (_| | | | (_| | (_| | (_) | | | |\n|___/_| |_| |_|\\__,_| .__/| .__/ \\__,_|_|  \\__,_|\\__, |\\___/|_| |_|\n                    |_|   |_|                    |___/             \n```\n\n[![PyPI](https://img.shields.io/pypi/v/smappdragon.svg)](https://pypi.python.org/pypi/smappdragon) [![PyPI](https://img.shields.io/pypi/l/smappdragon.svg)](https://github.com/SMAPPNYU/smappdragon/blob/master/LICENSE)\n\n:dragon: smappdragon is a set of tools for working with twitter data. a more abstract / contextual wrapper for smappdragon can be found in [pysmap](https://github.com/SMAPPNYU/pysmap) (work in progress). the old smapp-toolkit can be found on our github repositories page.\n\n- [collection](https://github.com/SMAPPNYU/smappdragon#collection)\n\t- [mongo_collection](https://github.com/SMAPPNYU/smappdragon#mongo_collection)\n\t- [bson_collection](https://github.com/SMAPPNYU/smappdragon#bson_collection)\n\t- [json_collection](https://github.com/SMAPPNYU/smappdragon#json_collection)\n\t- [csv_collection](https://github.com/SMAPPNYU/smappdragon#csv_collection)\n\t- [base_collection](https://github.com/SMAPPNYU/smappdragon#base_collection)\n\t\t- [get_iterator](https://github.com/SMAPPNYU/smappdragon#get_iterator)\n\t\t- [set_limit](https://github.com/SMAPPNYU/smappdragon#set_limit)\n\t\t- [strip_tweets](https://github.com/SMAPPNYU/smappdragon#strip_tweets)\n\t\t- [set_filter](https://github.com/SMAPPNYU/smappdragon#set_filter)\n\t\t- [set_custom_filter](https://github.com/SMAPPNYU/smappdragon#set_custom_filter)\n\t\t- [set_custom_filter_list](https://github.com/SMAPPNYU/smappdragon#set_custom_filter_list)\n\t\t- [dump_to_bson](https://github.com/SMAPPNYU/smappdragon#dump_to_bson)\n\t\t- [dump_to_json](https://github.com/SMAPPNYU/smappdragon#dump_to_json)\n\t\t- [dump_to_csv](https://githu",
    "url": "https://github.com/SMAPPNYU/smappdragon",
    "last_updated": "2024-03-22T11:30:24+00:00"
  },
  {
    "full_name": "arnicas/interactive-vis-course",
    "name": "interactive-vis-course",
    "description": "Repo for U of Miami course on interactive vis.",
    "language": "HTML",
    "topics": [],
    "readme": "# Interactive Data Vis Course Repo\n\nRepo for U of Miami School of Communication course on interactive data visualization for the web by Lynn Cherny (fall 2015 and spring 2016).  The repo is best viewed on github.io: http://arnicas.github.io/interactive-vis-course/. Lynn is @arnicas on twitter.\n\n    Office Hours: Wolfson 1020A, M & Th 1-3 or by appt.\n    Emails for homework: arnicas@gmail.com\n\n## What the Course Covers\n\n### 1. Interactive Data Vis: Design Principles, Techniques, Best-Practices...\n\nOriginally intended as having a journalistic focus, the course contents will expand a little more in spring 2016 to address broader topics in visualization. (Those additions are in progress.)\n\n* [Week1](Week1): Intro to Tools and the Course, Setup, CSVs\n* [Week2](Week2): Loading CSV Data, Highcharts\n* [Week3](Week3): Data Loading, Tables in D3\n* [Week4](Week4): More tables, Scales, SVG\n* [Week5](Week5): Bar Charts, Axes, Text Labels, Scatterplots\n* [Week6](Week6): Linecharts, Events, Simple Tooltips\n* [Week7](Week7): Improved Line Charts, Transitions\n* [Week8](Week8): Updates to Data, More Transitions\n* [Week9](Week9): Stacking Chart Types, Intro to Small Multiples\n* [Week10](Week10): Small Multiples, Intro to Maps\n* [Week11](Week11): Maps: D3, Leaflet, CartoDB...\n* [Week12](Week12): Storytelling Techniques: Scrollytelling, Steppers\n* [Week13](Week13): Animation: Lines, Play/Pause...\n* [Week14](Week14):  Reusable charts, Other Layouts, Project Tips/Grading\n* [Week15](Week15): Helpful Tips: How to File a Bug Report, Debugging\n\nAll the made-for-class example files are [here](examples.html). Many other examples are linked in each week's folder.\n\n### 2. Programming Techniques and Tools We'll Cover\n\n* Good practices with D3.js for data vis\n* Javascript and useful libraries like jQuery, lodash\n* Web Charting libs like Highcharts, D3, libs on top of D3 like Dimple.js\n* GitHub use\n* Debugging how-to's\n\n### 3. Evaluation\n\nGrading based on weekly homeworks (60%) and a final project (",
    "url": "https://github.com/arnicas/interactive-vis-course",
    "last_updated": "2025-02-23T12:04:20+00:00"
  },
  {
    "full_name": "janishar/mit-deep-learning-book-pdf",
    "name": "mit-deep-learning-book-pdf",
    "description": "MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville",
    "language": "Java",
    "topics": [
      "deep-learning",
      "machine-learning",
      "linear-algebra",
      "mit",
      "deeplearning",
      "pdf",
      "neural-network",
      "neural-networks",
      "machine",
      "thinking",
      "book",
      "chapter",
      "learning",
      "lecture-notes",
      "excercises",
      "good",
      "clear",
      "printable",
      "print"
    ],
    "readme": "[![Download](https://img.shields.io/badge/download-bookmarked%20book-orange.svg)](https://github.com/janishar/mit-deep-learning-book-pdf/raw/master/complete-book-pdf/deeplearningbook.pdf)\n[![Download](https://img.shields.io/badge/download-book-brightgreen.svg)](https://github.com/janishar/mit-deep-learning-book-pdf/blob/master/complete-book-pdf/Ian%20Goodfellow%2C%20Yoshua%20Bengio%2C%20Aaron%20Courville%20-%20Deep%20Learning%20(2017%2C%20MIT).pdf)\n\n# MIT Deep Learning Book (beautiful and flawless PDF version)\nMIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville.\n\n## Project Starter Template\nA good project structure is very important for data-science and data-analytics work. I have open-sourced a very effective repo with project starter template: [Repo Link](https://github.com/janishar/data-analytics-project-template)\n\n[https://github.com/janishar/data-analytics-project-template](https://github.com/janishar/data-analytics-project-template)\n\n# About The Author\nYou can connect with me here:\n* [Janishar Ali](https://janisharali.com)\n* [Twitter](https://twitter.com/janisharali)\n* [YouTube Channel](https://www.youtube.com/@unusualcode)\n\n## If this repository helps you in anyway, show your love :heart: by putting a :star: on this project :v:\n\n### Deep Learning\nAn MIT Press book\nIan Goodfellow and Yoshua Bengio and Aaron Courville\n\nThis is the most comprehensive book available on the deep learning and available as free html book for reading at http://www.deeplearningbook.org/\n\n**Comment on this book by Elon Musk**\n>Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX\n\n**This is not available as PDF download. So, I have taken the prints of the HTML content and binded into a flawless PDF version of the book, as suggested by the website itself**\n\n**http://www.deeplearningbook.org/ says:**\n>What is the ",
    "url": "https://github.com/janishar/mit-deep-learning-book-pdf",
    "last_updated": "2025-09-02T09:32:32+00:00"
  },
  {
    "full_name": "fsolt/icpsrdata",
    "name": "icpsrdata",
    "description": "Reproducible data downloads from the ICPSR data archive",
    "language": "R",
    "topics": [
      "webservice-client",
      "r-package",
      "rstats",
      "research-data"
    ],
    "readme": "<!-- badges: start -->\n[![CRAN version](https://www.r-pkg.org/badges/version/icpsrdata)](https://cran.r-project.org/package=icpsrdata) ![](https://cranlogs.r-pkg.org/badges/grand-total/icpsrdata)\n[![R-CMD-check](https://github.com/fsolt/icpsrdata/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/fsolt/icpsrdata/actions/workflows/R-CMD-check.yaml)\n<!-- badges: end -->\n\n------------------------------------------------------------------------\nicpsrdata\n=========\n\n`icpsrdata` is an R package that provides reproducible, programmatic access to datasets stored in the Inter-university Consortium for Political and Social Research archive.\n\nTo install:\n\n* the latest released version: `install.packages(\"icpsrdata\")`\n* the latest development version:\n\n```R\nif (!require(remotes)) install.packages(\"remotes\")\nremotes::install_github(\"fsolt/icpsrdata\")\n```\n\nThe Inter-university Consortium for Political and Social Research, in its own words, \"provides leadership and training in data access, curation, and methods of analysis for the social science research community.\"\nThe ICPSR data archive stores thousands of datasets on a wide range of topics. Researchers taking advantage of these datasets, however, are caught in a bind.\nThe terms and conditions for downloading any ICPSR dataset state that one agrees \"not to redistribute data or other materials without the written agreement of ICPSR.\"\nBut to ensure that one's work can be reproduced, assessed, and built upon by others, one must provide access to the raw data one employed.\nThe `icpsrdata` package cuts this knot by providing programmatic, reproducible access to ICPSR's datasets from within R.\n\nFor more details, check out [the vignette](https://fsolt.org/icpsrdata/articles/icpsrdata-vignette.html).\n",
    "url": "https://github.com/fsolt/icpsrdata",
    "last_updated": "2025-06-23T03:53:56+00:00"
  },
  {
    "full_name": "byteclubfr/prez",
    "name": "prez",
    "description": "Generate Reveal.js slideshows from a set of markdowns",
    "language": "JavaScript",
    "topics": [
      "reveal-js"
    ],
    "readme": "[![npm version](https://badge.fury.io/js/prez.svg)](http://badge.fury.io/js/prez)\n[![Dependency Status](https://david-dm.org/byteclubfr/prez.png)](https://david-dm.org/byteclubfr/prez)\n\n## prez\n\nOpinionated [Reveal slideshow](http://lab.hakim.se/reveal-js) generator with nice PDF output and ability to treat [notes](https://github.com/hakimel/reveal.js#speaker-notes) as first-class content.\n\n### Install\n\n```sh\nnpm install -g prez\n```\n\n### Feel the magic\n\nCheck that you have **node 6** installed:\n\n```sh\nnode --version\n6.x.x\n```\n\nIn your terminal, go to an empty directory and run:\n\n```sh\nprez --init\n```\n\nA sample workspace has been generated for you. Then run:\n\n```sh\nprez --serve --print --watch\n```\n\n* edit your slides from `slides` folder (html or [markdown](https://www.npmjs.com/package/marked))\n* if you need some assets, you can work in `images`, `css`, `js`, `media` folders and use `includes.txt`\n* your slideshow is automatically built into `build` folder\n* your slideshow is hosted and your browser is opened to localhost:9000\n* any change you make will rebuild and refresh your browser\n* oh, and a printer-friendly `slides.pdf` is automatically generated on any change too\n\n### Slides structure and format\n\n* Slides and chapters (vertical stacks) are sorted by name.\n* If a slide or a chapter name starts with a dot `.` (hidden), it will be skipped. (or use `--keep-hidden`)\n* You can number your slides to ensure order by adding a \"*number*-\" prefix, it will be automagically stripped out.\n* Each slide will have an id generated from filename (removing accents and spaces).\n* If you use `--sub-covers` the title is the stripped name, accents and case remain untouched.\n\n#### Sample structure\n\n```\nimages/\njs/\ncss/\nslides/\n  01-intro.md\n  02-Chapter 1/\n    01-hello-world.md\n    02-bonjour-monde.md\n  03-conclusion.md\n```\n\n#### Sample slide\n\n```md\n# Slide's title\n\nContent of your slide\n\nnote:\n\nYour notes go here.\n\nComplex (multiline, code samples, etc.) notes are really supported ",
    "url": "https://github.com/byteclubfr/prez",
    "last_updated": "2025-08-26T03:55:51+00:00"
  },
  {
    "full_name": "google-deepmind/rc-data",
    "name": "rc-data",
    "description": "Question answering dataset featured in \"Teaching Machines to Read and Comprehend",
    "language": "Python",
    "topics": [],
    "readme": "# Question Answering Corpus\n\nThis repository contains a script to generate question/answer pairs using\nCNN and Daily Mail articles downloaded from the Wayback Machine.\n\nFor a detailed description of this corpus please read:\n[Teaching Machines to Read and Comprehend][arxiv], Hermann et al., NIPS 2015.\nPlease cite the paper if you use this corpus in your work.\n\n### Bibtex\n\n```\n@inproceedings{nips15_hermann,\nauthor = {Karl Moritz Hermann and Tom\\'a\\v{s} Ko\\v{c}isk\\'y and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},\ntitle = {Teaching Machines to Read and Comprehend},\nurl = {http://arxiv.org/abs/1506.03340},\nbooktitle = \"Advances in Neural Information Processing Systems (NIPS)\",\nyear = \"2015\",\n}\n```\n\n## Download Processed Version\n\nIn case the script does not work you can also download the processed data sets from [http://cs.nyu.edu/~kcho/DMQA/]. This should help in situations where the underlying data is not accessible (Wayback Machine partially down).\n\n## Running the Script\n\n### Prerequisites\n\nPython 2.7, `wget`, `libxml2`, `libxslt`, `python-dev` and `virtualenv`. `libxml2` must be version 2.9.1. \nYou can install `libxslt` from here: [http://xmlsoft.org/libxslt/downloads.html](http://xmlsoft.org/libxslt/downloads.html)\n\n```\nsudo pip install virtualenv\nsudo apt-get install python-dev\n```\n\n### Download Script\n\n```\nmkdir rc-data\ncd rc-data\nwget https://github.com/deepmind/rc-data/raw/master/generate_questions.py\n```\n\n### Download and Extract Metadata\n\n```\nwget https://storage.googleapis.com/deepmind-data/20150824/data.tar.gz -O - | tar -xz --strip-components=1\n```\n\nThe news article metadata is ~1 GB.\n\n### Enter Virtual Environment and Install Packages\n\n```\nvirtualenv venv\nsource venv/bin/activate\nwget https://github.com/deepmind/rc-data/raw/master/requirements.txt\npip install -r requirements.txt\n```\n\nYou may need to install `libxml2` development packages to install `lxml`:\n\n```\nsudo apt-get install libxml2-dev libxslt-dev\n```",
    "url": "https://github.com/google-deepmind/rc-data",
    "last_updated": "2025-08-09T16:03:17+00:00"
  },
  {
    "full_name": "TalAter/UpUp",
    "name": "UpUp",
    "description": "✈️ Easily create sites that work offline as well as online",
    "language": "JavaScript",
    "topics": [
      "hacktoberfest",
      "pwa",
      "progressive-web-app",
      "offline",
      "offline-first",
      "service-worker"
    ],
    "readme": "### UpUp - Kickstarting the Offline First Revolution\n<a href=\"https://www.talater.com/upup/\"><img align=\"right\" src=\"https://github.com/TalAter/UpUp/raw/master/demo/img/upup-readme.gif\" alt=\"Offline First with UpUp\"></a>\n\nUpUp is a tiny JavaScript library that makes sure your users can always access your site's content, even when they're on a plane, in an elevator, or 20,000 leagues under the sea.\n\nMobile-First has become the de-facto standard for building modern sites. But in a world where everyone is mobile, an always-on connection isn't something we can rely on. It's time to start thinking **Offline First**.\n\nWith UpUp you control the content your users see, even when they are offline. And you can do it with just a single JavaScript command.\n\n### Demo & Tutorial\nThe easiest path to understanding is to see [UpUp in action and try a quick tutorial](https://www.talater.com/upup/).\n\n### Hello World\nGetting started with UpUp is as easy as adding two JavaScript files to your site, [upup.min.js](https://raw.githubusercontent.com/TalAter/UpUp/master/dist/upup.min.js) & [upup.sw.min.js](https://raw.githubusercontent.com/TalAter/UpUp/master/dist/upup.sw.min.js), and defining the content you want your users to see when they are offline.\n\nFor example:\n````html\n<script src=\"/upup.min.js\"></script>\n<script>\nUpUp.start({\n  'content-url': 'offline.html',\n  'assets': ['/img/logo.png', '/css/style.css', 'headlines.json']\n});\n</script>\n````\nNow every time your users return to your site and their connection is down, they will see the contents of `offline.html` instead of an error.\n\n**Check out some [live demos and a full tutorial](https://www.talater.com/upup/). Once you're up and rolling, you can read the full [API Docs](https://github.com/TalAter/UpUp/blob/master/docs/README.md).**\n\n### HTTPS Requirement\nUpUp requires a secure connection to your site (this is a requirement of service workers, the technology at the heart of UpUp). So make sure your users visit your site over HTTPS ",
    "url": "https://github.com/TalAter/UpUp",
    "last_updated": "2025-08-19T02:12:24+00:00"
  },
  {
    "full_name": "slavingia/askmybook",
    "name": "askmybook",
    "description": "",
    "language": "Python",
    "topics": [],
    "readme": "## Setup\n\n1. Create and fill in `.env` using `.env.example` as an example.\n\n2. Install required Python packages\n\n```\npip install -r requirements.txt\n```\n\nMac M1 / OS X Note: if you get an error installing psycopg2, you may need:\n\n```\nbrew install postgresql\n```\n\nSee https://github.com/psycopg/psycopg2/issues/1200\n\n\n3. Turn your PDF into embeddings for GPT-3:\n\n```\npython scripts/pdf_to_pages_embeddings.py --pdf book.pdf\n```\n\n4. Set up database tables and collect static files:\n\n```\npython manage.py makemigrations\npython manage.py migrate\npython manage.py collectstatic\n```\n\n5. Other things to update:\n\n- Book title\n- Book cover image\n- URL to purchase book\n- Author name and bio\n\n## Deploy to Heroku\n\n1. Create a Heroku app:\n\n```\nheroku create askmybook\n```\n\nSet config variables on Heroku to match your `.env`.\n\n2. Push to Heroku:\n\n```\ngit push heroku main\nheroku ps:scale web=1\nheroku run python manage.py migrate\nheroku open\nheroku domains:add askmybook.com\n```\n\n### Run locally\n\n```\nsource venv/source/activate\npip install -r requirements.txt\nheroku local\n```\n\nNote: macOS Monterey uses port 5000 (the default port) for AirPlay sharing, so you will need to run heroku local on a different port. For example:\n\n```\nheroku local -p 5001\n```\n\n### Deploying\n\nDeploy to Heroku with:\n\n```\ngit push heroku main --no-verify\n```\n\n`no-verify` is added due to using Git LFS for the large embedding file.\n",
    "url": "https://github.com/slavingia/askmybook",
    "last_updated": "2025-08-27T12:14:31+00:00"
  }
]